[
  {
    "objectID": "lab/scratchpad.html",
    "href": "lab/scratchpad.html",
    "title": "Predict",
    "section": "",
    "text": "import requests\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\n# Create a dummy dataset\ndata = requests.get(\"https://raw.githubusercontent.com/jigsawpieces/dog-api-images/main/pitbull/dog-3981033_1280.jpg\")\nimage = Image.open(BytesIO(data.content)).reduce(5)\nlabel = np.random.rand(1, 5) / 10 + 0.5\nlabel[:, 0] = 0\n!mkdir -p /tmp/dummy_dataset/images\n!mkdir -p /tmp/dummy_dataset/labels\nimage.save(\"/tmp/dummy_dataset/images/0.jpg\")\nnp.savetxt(\"/tmp/dummy_dataset/labels/0.txt\", label, fmt=\"%d %f %f %f %f\")\nwith open(\"/tmp/dummy_dataset/dataset.yml\", \"w\") as f:\n    f.write(\"\"\"train: _\nval: _\ntest: _\nnc: 1\nnames: [\"dummy\"]\"\"\")\n\n# Load as supervision dataset\ndataset = sv.DetectionDataset.from_yolo(\"/tmp/dummy_dataset/images\", \"/tmp/dummy_dataset/labels\", \"/tmp/dummy_dataset/dataset.yml\")\n\n# Visualize the first instance\nimage_path, image, detection = dataset[0]\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\ndisplay(Image.fromarray(annotated_image))\n\n# Visualize the prediction\nmodel = get_roboflow_model(\"yolov8s-640\")\nprediction = model.infer(image)[0]\ndetection = sv.Detections.from_inference(prediction)\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\ndisplay(Image.fromarray(annotated_image))\n\n# Modified the detection to display class name\n# image_path, image, detection = dataset[0]\n# detection.data = {\"class_name\": np.array(['dummy'])}\n# box_annotator = sv.BoxAnnotator()\n# label_annotator = sv.LabelAnnotator()\n# annotated_image = box_annotator.annotate(image.copy(), detection)\n# annotated_image = label_annotator.annotate(annotated_image, detection)\n# display(Image.fromarray(annotated_image))\n\n[01/20/25 22:15:20] WARNING  Your inference package version 0.33.0 is out of date! Please upgrade to __init__.py:41\n                             version 0.34.0 of inference for the latest features and bug fixes by                  \n                             running `pip install --upgrade inference`.                                            \n\n\n\n\n\n\n\n\n\n\nUserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\nUserWarning: Specified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n2025-01-20 22:15:22.319412181 [E:onnxruntime:Default, provider_bridge_ort.cc:1862 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn_adv.so.9: cannot open shared object file: No such file or directory\n\n2025-01-20 22:15:22.319453861 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:993 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\nimport os\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\n# Download dataset\nif not os.path.exists(\"/tmp/rf_animals\"):\n    !wget https://universe.roboflow.com/ds/1LLwpXz2td?key=8JnJML5YF6 -O /tmp/rf_animals.zip\n    !unzip /tmp/dataset.zip -d /tmp/rf_animals\n\n# Load dataset\ndataset = sv.DetectionDataset.from_yolo(\"/tmp/rf_animals/train/images\", \"/tmp/rf_animals/train/labels\", \"/tmp/rf_animals/data.yaml\")\n\n# Inference\nmodel = YOLO(\"yolov8s\")\ntargets, detections = [], []\nfor image_path, image, target in dataset:\n    targets.append(target)\n    \n    prediction = model(image, verbose=False)[0]\n    detection = sv.Detections.from_ultralytics(prediction)\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n    detection.class_id = np.array([dataset.classes.index(class_name) for class_name in detection['class_name']])\n    detections.append(detection)\n    \n# Method #1\nmAP = sv.metrics.MeanAveragePrecision().update(detections, targets).compute()\nprint(f\"mAP50: {mAP.map50:.4f}\")\n\n# Method #2\nmAP = sv.MeanAveragePrecision.from_detections(detections, targets)\nprint(f\"mAP50: {mAP.map50:.4f}\")\n\nmAP50: 0.1553\nmAP50: 0.2100\ndef callback(image):\n    prediction = model(image, verbose=False)[0]\n    detection = sv.Detections.from_ultralytics(prediction)\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n    detection.class_id = np.array([dataset.classes.index(class_name) for class_name in detection['class_name']])\n    return detection\n\nsv.MeanAveragePrecision.benchmark(dataset, callback).map50\n\nnp.float64(0.2100207753031282)\ndataset.classes\n\n['butterfly',\n 'cat',\n 'crocodile',\n 'dear',\n 'deer',\n 'dog',\n 'elephant',\n 'fog',\n 'frog',\n 'giraffe',\n 'goat',\n 'hippo',\n 'kangaroo',\n 'lion',\n 'parrot',\n 'shark',\n 'sheep',\n 'spider',\n 'tiger',\n 'zebra']\nimport leafmap\nm = leafmap.Map(center=(28.25, 77.40), zoom=18)\n# m.add_basemap(\"SATELLITE\")\nm.add_wms_layer(\"https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/GoogleMapsCompatible/MapServer/tile/32553/{z}/{y}/{x}\", layers=\"0\")\nm\nm.zoom\n\n19.0\nm = leafmap.Map(center=(28.25, 77.40), zoom=17)\nm.add_basemap(\"SATELLITE\")\n# m.add_wms_layer(\"https://wayback.maptiles.arcgis.com/arcgis/rest/services/World_Imagery/WMTS/1.0.0/GoogleMapsCompatible/MapServer/tile/32553/{z}/{y}/{x}\", layers=\"0\")\nm\nimport geopandas as gpd\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom joblib import Parallel, delayed\nfrom glob import glob\ndelhi_kilns = gpd.read_file(\"/home/patel_zeel/kiln_compass_24/regions/labels/delhi_airshed.geojson\")\nlen(delhi_kilns)\n\n783\nzoom = 19\njobs = []\nfor kiln in tqdm(delhi_kilns.geometry):\n    lon_min, lat_min, lon_max, lat_max = kiln.bounds\n    lon_margin = (lon_max - lon_min)/4\n    lat_margin = (lat_max - lat_min)/4\n    outer_bounds = [lon_min - lon_margin, lat_min - lat_margin, lon_max + lon_margin, lat_max + lat_margin]\n    download_path = f\"/home/patel_zeel/kiln_compass_24/regions/high_res/{zoom}/{','.join(map(str, kiln.bounds))}.tif\"\n    jobs.append(delayed(leafmap.map_tiles_to_geotiff)(download_path, outer_bounds, zoom=zoom, to_cog=True, source=\"SATELLITE\", quiet=True))\n\n_ = Parallel(n_jobs=-1)(tqdm(jobs))\n\n\n\n\n\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439517,28.203442,77.440547,28.203933.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.385046,28.221207,77.385974,28.221709.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439517,28.203442,77.440547,28.203933.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.385046,28.221207,77.385974,28.221709.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.41962,28.208412,77.420769,28.208927.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.41962,28.208412,77.420769,28.208927.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.400691,28.212222,77.402094,28.212772.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399209,28.215745,77.400439,28.21634.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.400691,28.212222,77.402094,28.212772.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399209,28.215745,77.400439,28.21634.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380984,28.223921,77.382308,28.224487.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423613,28.214831,77.424976,28.215277.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380984,28.223921,77.382308,28.224487.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423613,28.214831,77.424976,28.215277.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420321,28.226731,77.421568,28.227297.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420321,28.226731,77.421568,28.227297.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422024,28.218841,77.423172,28.219389.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397178,28.245326,77.398362,28.245804.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422024,28.218841,77.423172,28.219389.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397178,28.245326,77.398362,28.245804.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378493,28.231351,77.379637,28.231909.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403983,28.253302,77.405073,28.253971.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378493,28.231351,77.379637,28.231909.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403983,28.253302,77.405073,28.253971.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399211,28.249648,77.400477,28.250317.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421627,28.226688,77.422657,28.227306.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381049,28.2314,77.381853,28.232478.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.377595,28.230781,77.378218,28.231793.tif\n\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380083,28.233122,77.381338,28.233603.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414066,28.22562,77.415028,28.226247.tif\n\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.377595,28.230781,77.378218,28.231793.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399211,28.249648,77.400477,28.250317.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381049,28.2314,77.381853,28.232478.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421627,28.226688,77.422657,28.227306.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426747,28.223595,77.428081,28.224178.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393264,28.218964,77.394545,28.219577.tif\n\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380083,28.233122,77.381338,28.233603.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414066,28.22562,77.415028,28.226247.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411022,28.244281,77.412055,28.244761.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.406952,28.252856,77.408023,28.253388.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426747,28.223595,77.428081,28.224178.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393264,28.218964,77.394545,28.219577.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411022,28.244281,77.412055,28.244761.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.398997,28.242479,77.399698,28.243509.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.406952,28.252856,77.408023,28.253388.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381981,28.23996,77.383141,28.240486.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403869,28.244107,77.404506,28.244952.tif\n\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.398997,28.242479,77.399698,28.243509.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403869,28.244107,77.404506,28.244952.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404645,28.250969,77.40556,28.251879.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381981,28.23996,77.383141,28.240486.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.379979,28.231168,77.380917,28.231783.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.295803,28.24481,77.296446,28.245788.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40962,28.233303,77.411022,28.234024.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404645,28.250969,77.40556,28.251879.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.379979,28.231168,77.380917,28.231783.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.295803,28.24481,77.296446,28.245788.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395784,28.24831,77.397011,28.248911.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380263,28.228121,77.380828,28.229099.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.334249,28.254088,77.334997,28.255125.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40962,28.233303,77.411022,28.234024.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430124,28.253558,77.431044,28.254059.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.380263,28.228121,77.380828,28.229099.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395784,28.24831,77.397011,28.248911.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.334249,28.254088,77.334997,28.255125.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430124,28.253558,77.431044,28.254059.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.409951,28.246183,77.411256,28.246749.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.557539,28.235139,77.558522,28.235908.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.333566,28.250313,77.334763,28.250958.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.384357,28.238603,77.38497,28.239681.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.409951,28.246183,77.411256,28.246749.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.557539,28.235139,77.558522,28.235908.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.333566,28.250313,77.334763,28.250958.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423652,28.226954,77.424177,28.227829.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39999,28.250901,77.400555,28.251758.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.384357,28.238603,77.38497,28.239681.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392542,28.244058,77.393126,28.245018.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392542,28.244058,77.393126,28.245018.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39999,28.250901,77.400555,28.251758.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423652,28.226954,77.424177,28.227829.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382656,28.242525,77.383947,28.243172.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.384356,28.240206,77.385652,28.240789.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404071,28.24931,77.405363,28.250143.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.384356,28.240206,77.385652,28.240789.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382656,28.242525,77.383947,28.243172.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420263,28.225702,77.421373,28.226251.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.383301,28.237917,77.384119,28.23916.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382349,28.222811,77.383578,28.224024.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382747,28.236606,77.383991,28.237372.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404071,28.24931,77.405363,28.250143.tif\n\n\n\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420263,28.225702,77.421373,28.226251.tif\nAdding overviews...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382747,28.236606,77.383991,28.237372.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382349,28.222811,77.383578,28.224024.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.409604,28.241284,77.410961,28.241942.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.383301,28.237917,77.384119,28.23916.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404832,28.244738,77.406239,28.245398.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.409604,28.241284,77.410961,28.241942.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402862,28.244088,77.403679,28.24515.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399208,28.248389,77.400481,28.249037.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402862,28.244088,77.403679,28.24515.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.404832,28.244738,77.406239,28.245398.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.54838,28.22488,77.549438,28.225545.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.399208,28.248389,77.400481,28.249037.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.335186,28.251218,77.336479,28.251801.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.54838,28.22488,77.549438,28.225545.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374381,28.23553,77.37553,28.235994.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40628,28.242719,77.407334,28.243676.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.335186,28.251218,77.336479,28.251801.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374381,28.23553,77.37553,28.235994.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.410336,28.239227,77.411631,28.239777.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436496,28.253485,77.437447,28.254035.tif\n\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40628,28.242719,77.407334,28.243676.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436496,28.253485,77.437447,28.254035.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396776,28.243946,77.398141,28.244546.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.410336,28.239227,77.411631,28.239777.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346543,28.22565,77.347147,28.226628.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416385,28.219885,77.417119,28.220877.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396776,28.243946,77.398141,28.244546.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346543,28.22565,77.347147,28.226628.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416385,28.219885,77.417119,28.220877.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434905,28.23123,77.436051,28.232266.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438724,28.253028,77.439387,28.25404.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438724,28.253028,77.439387,28.25404.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434905,28.23123,77.436051,28.232266.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376227,28.23481,77.377405,28.23553.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439581,28.252959,77.440711,28.253474.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441003,28.253354,77.441607,28.254383.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376227,28.23481,77.377405,28.23553.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439581,28.252959,77.440711,28.253474.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441003,28.253354,77.441607,28.254383.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.330694,28.255359,77.331843,28.255828.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.450088,28.25145,77.451256,28.251964.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415978,28.262976,77.41703,28.263491.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.330694,28.255359,77.331843,28.255828.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.450088,28.25145,77.451256,28.251964.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415978,28.262976,77.41703,28.263491.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325479,28.261083,77.326648,28.261644.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325479,28.261083,77.326648,28.261644.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328932,28.260927,77.329976,28.261426.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.611995,28.256777,77.613127,28.25748.tif\n\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328932,28.260927,77.329976,28.261426.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.611995,28.256777,77.613127,28.25748.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402288,28.240987,77.40297,28.242342.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.332996,28.264283,77.334249,28.265031.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393165,28.284485,77.394197,28.285017.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402288,28.240987,77.40297,28.242342.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.332996,28.264283,77.334249,28.265031.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393165,28.284485,77.394197,28.285017.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.417644,28.244725,77.41928,28.24644.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.388491,28.274279,77.389289,28.27524.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393729,28.273362,77.39484,28.274019.tif\n\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.388491,28.274279,77.389289,28.27524.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393729,28.273362,77.39484,28.274019.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.417644,28.244725,77.41928,28.24644.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.272707,28.288054,77.27337,28.288894.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.272707,28.288054,77.27337,28.288894.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.329479,28.2648,77.330813,28.265646.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.329479,28.2648,77.330813,28.265646.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378491,28.291501,77.379445,28.291946.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395667,28.281381,77.396465,28.282324.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378491,28.291501,77.379445,28.291946.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.05445,28.299134,77.054995,28.30006.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392814,28.285344,77.394061,28.28591.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395667,28.281381,77.396465,28.282324.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.05445,28.299134,77.054995,28.30006.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392814,28.285344,77.394061,28.28591.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382094,28.29126,77.3826,28.292101.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368841,28.287985,77.370107,28.288551.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382094,28.29126,77.3826,28.292101.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368841,28.287985,77.370107,28.288551.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397799,28.279495,77.39885,28.280527.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.280575,28.29301,77.281685,28.293558.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.388539,28.275375,77.390012,28.27643.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.280575,28.29301,77.281685,28.293558.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.057992,28.30206,77.059087,28.302553.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.057992,28.30206,77.059087,28.302553.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39686,28.310894,77.397453,28.31185.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387887,28.286527,77.389153,28.287145.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397799,28.279495,77.39885,28.280527.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396582,28.283319,77.397205,28.284502.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.388539,28.275375,77.390012,28.27643.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39686,28.310894,77.397453,28.31185.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387887,28.286527,77.389153,28.287145.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396582,28.283319,77.397205,28.284502.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.055102,28.29886,77.055667,28.2997.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.055102,28.29886,77.055667,28.2997.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37223,28.295396,77.372795,28.296442.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37223,28.295396,77.372795,28.296442.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394255,28.334431,77.395307,28.334928.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389503,28.332717,77.390691,28.333249.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394255,28.334431,77.395307,28.334928.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389503,28.332717,77.390691,28.333249.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397898,28.310147,77.399084,28.310693.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397898,28.310147,77.399084,28.310693.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389484,28.286081,77.390847,28.286716.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37092,28.295907,77.371777,28.296921.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389776,28.33162,77.390867,28.332134.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389484,28.286081,77.390847,28.286716.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37092,28.295907,77.371777,28.296921.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389776,28.33162,77.390867,28.332134.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391794,28.336363,77.392355,28.337244.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582304,28.353717,77.583298,28.354337.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582778,28.352286,77.583749,28.352902.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391794,28.336363,77.392355,28.337244.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582304,28.353717,77.583298,28.354337.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582778,28.352286,77.583749,28.352902.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391529,28.367164,77.392035,28.367901.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.591217,28.361328,77.592152,28.361705.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391529,28.367164,77.392035,28.367901.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.567521,28.361494,77.568469,28.362254.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.591217,28.361328,77.592152,28.361705.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.567521,28.361494,77.568469,28.362254.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.35727,28.302428,77.35877,28.303196.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.576011,28.364573,77.576889,28.365287.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582164,28.345883,77.583188,28.346603.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.576011,28.364573,77.576889,28.365287.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.35727,28.302428,77.35877,28.303196.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.58579,28.350779,77.586734,28.351472.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582164,28.345883,77.583188,28.346603.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.58579,28.350779,77.586734,28.351472.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573756,28.369634,77.574639,28.370282.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573756,28.369634,77.574639,28.370282.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.5687,28.356236,77.569633,28.357074.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.579859,28.344916,77.580884,28.345734.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.578047,28.36547,77.578979,28.366176.tif\n\nAdding overviews...\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.577563,28.355356,77.578563,28.356157.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.5687,28.356236,77.569633,28.357074.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.584383,28.352828,77.585199,28.353929.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.579859,28.344916,77.580884,28.345734.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.578047,28.36547,77.578979,28.366176.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.577563,28.355356,77.578563,28.356157.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.584383,28.352828,77.585199,28.353929.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.580639,28.332723,77.581861,28.333598.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.373846,28.414445,77.374937,28.415045.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.580639,28.332723,77.581861,28.333598.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394576,28.377987,77.395569,28.378552.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.373846,28.414445,77.374937,28.415045.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.583154,28.367847,77.584115,28.368713.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394576,28.377987,77.395569,28.378552.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.379309,28.411173,77.379951,28.411996.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573654,28.363088,77.574602,28.364024.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.379309,28.411173,77.379951,28.411996.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.585125,28.367876,77.586074,28.368697.tif\n\nAdding overviews...\nUpdating dataset tags...\nAdding overviews...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.583154,28.367847,77.584115,28.368713.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573654,28.363088,77.574602,28.364024.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586444,28.399282,77.587516,28.400037.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.585125,28.367876,77.586074,28.368697.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586444,28.399282,77.587516,28.400037.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.562144,28.349463,77.563504,28.35039.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574721,28.363434,77.575752,28.364597.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.562144,28.349463,77.563504,28.35039.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574721,28.363434,77.575752,28.364597.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.5834,28.345409,77.584414,28.34647.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.630723,28.398897,77.631673,28.399733.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.5834,28.345409,77.584414,28.34647.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382717,28.420471,77.383807,28.420933.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.630723,28.398897,77.631673,28.399733.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382717,28.420471,77.383807,28.420933.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.400725,28.406126,77.401871,28.406861.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378525,28.407155,77.379336,28.408164.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.400725,28.406126,77.401871,28.406861.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378525,28.407155,77.379336,28.408164.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574408,28.335977,77.575719,28.336887.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.590371,28.337407,77.591509,28.338601.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574408,28.335977,77.575719,28.336887.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.590371,28.337407,77.591509,28.338601.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.38782,28.388839,77.389035,28.389902.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555503,28.395057,77.556764,28.395932.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.38782,28.388839,77.389035,28.389902.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555503,28.395057,77.556764,28.395932.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597054,28.421614,77.598106,28.422378.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597004,28.428189,77.598064,28.428905.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597054,28.421614,77.598106,28.422378.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582468,28.383531,77.583666,28.384522.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597004,28.428189,77.598064,28.428905.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.645184,28.45388,77.646158,28.454548.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.676666,28.434924,77.677534,28.435544.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.645184,28.45388,77.646158,28.454548.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.676666,28.434924,77.677534,28.435544.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.582468,28.383531,77.583666,28.384522.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.671598,28.430455,77.672823,28.431464.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.671598,28.430455,77.672823,28.431464.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597862,28.43561,77.59873,28.436718.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.649144,28.446776,77.650364,28.447675.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597862,28.43561,77.59873,28.436718.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.644936,28.450308,77.646087,28.45115.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.606742,28.406935,77.607844,28.407877.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.592911,28.426001,77.594206,28.426818.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.619007,28.410973,77.620136,28.411956.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.649144,28.446776,77.650364,28.447675.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.644936,28.450308,77.646087,28.45115.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.592911,28.426001,77.594206,28.426818.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.606742,28.406935,77.607844,28.407877.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.619007,28.410973,77.620136,28.411956.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.641895,28.450275,77.643003,28.451172.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.641895,28.450275,77.643003,28.451172.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.602472,28.407715,77.603892,28.408836.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.646111,28.453562,77.64719,28.454407.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593847,28.481626,77.594897,28.482573.tif\n\nAdding overviews...\nUpdating dataset tags...\nAdding overviews...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.602472,28.407715,77.603892,28.408836.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.646111,28.453562,77.64719,28.454407.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593847,28.481626,77.594897,28.482573.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.923943,28.724198,76.924571,28.725237.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.923943,28.724198,76.924571,28.725237.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.654202,28.449042,77.655425,28.449933.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.892405,28.641308,76.893691,28.641803.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.654202,28.449042,77.655425,28.449933.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.892405,28.641308,76.893691,28.641803.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597879,28.437297,77.598954,28.438487.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60059,28.520599,77.601846,28.521552.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.896381,28.633003,76.897692,28.633587.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.597879,28.437297,77.598954,28.438487.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.606773,28.519039,77.607973,28.519999.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.896381,28.633003,76.897692,28.633587.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60059,28.520599,77.601846,28.521552.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.627612,28.487476,77.628753,28.488303.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.605367,28.521803,77.606598,28.522723.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395823,28.244212,77.396465,28.245036.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.606773,28.519039,77.607973,28.519999.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.599257,28.530944,77.600313,28.531893.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.395823,28.244212,77.396465,28.245036.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604296,28.52906,77.605367,28.530121.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.627612,28.487476,77.628753,28.488303.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.605367,28.521803,77.606598,28.522723.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604296,28.52906,77.605367,28.530121.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.599257,28.530944,77.600313,28.531893.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.863749,28.644316,76.864898,28.64488.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.863749,28.644316,76.864898,28.64488.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60946,28.528705,77.610897,28.52971.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60946,28.528705,77.610897,28.52971.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.35692,28.720956,77.357871,28.721958.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.531815,28.594874,77.53279,28.595945.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.35692,28.720956,77.357871,28.721958.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.531815,28.594874,77.53279,28.595945.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.928659,28.734502,76.930064,28.734963.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.521977,28.541158,77.523188,28.54226.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411367,28.733316,77.412423,28.733866.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.928659,28.734502,76.930064,28.734963.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411367,28.733316,77.412423,28.733866.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.521977,28.541158,77.523188,28.54226.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60608,28.412178,77.60763,28.413427.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.605627,28.530084,77.606712,28.531215.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.605627,28.530084,77.606712,28.531215.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.60608,28.412178,77.60763,28.413427.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.607555,28.519595,77.608658,28.520652.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.643833,28.675536,77.644681,28.676664.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426727,28.717934,77.427864,28.719015.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.643833,28.675536,77.644681,28.676664.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.611452,28.528599,77.612755,28.529724.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.607555,28.519595,77.608658,28.520652.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.930947,28.733199,76.932218,28.73381.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.53028,28.590155,77.531362,28.591403.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.601955,28.5376,77.603468,28.538821.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426727,28.717934,77.427864,28.719015.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.930947,28.733199,76.932218,28.73381.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362787,28.722948,77.364138,28.723998.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.53028,28.590155,77.531362,28.591403.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.611452,28.528599,77.612755,28.529724.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.601955,28.5376,77.603468,28.538821.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362787,28.722948,77.364138,28.723998.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.413781,28.729296,77.414463,28.730468.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.603531,28.538614,77.604863,28.539878.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.413781,28.729296,77.414463,28.730468.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442072,28.722959,77.443251,28.724072.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.924006,28.737248,76.925496,28.737791.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.412711,28.724839,77.413583,28.726029.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.603531,28.538614,77.604863,28.539878.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421989,28.731187,77.42261,28.732262.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.412711,28.724839,77.413583,28.726029.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421989,28.731187,77.42261,28.732262.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.924006,28.737248,76.925496,28.737791.tif\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42751,28.729953,77.42821,28.730879.tif\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442072,28.722959,77.443251,28.724072.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421358,28.726853,77.422091,28.728028.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425279,28.733268,77.425996,28.734352.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42751,28.729953,77.42821,28.730879.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421358,28.726853,77.422091,28.728028.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425279,28.733268,77.425996,28.734352.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.437877,28.719557,77.439105,28.720884.tif\n\nAdding overviews...\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.932392,28.737181,76.932978,28.738409.tif\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.437877,28.719557,77.439105,28.720884.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.92683,28.738921,76.927458,28.739961.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.929012,28.737692,76.930306,28.738261.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.932392,28.737181,76.932978,28.738409.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.929012,28.737692,76.930306,28.738261.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.92683,28.738921,76.927458,28.739961.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436464,28.726587,77.437863,28.727577.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.565184,28.715374,77.566544,28.716433.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415773,28.728794,77.416588,28.730119.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436464,28.726587,77.437863,28.727577.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.418617,28.733934,77.419949,28.734676.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.565184,28.715374,77.566544,28.716433.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415773,28.728794,77.416588,28.730119.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.418617,28.733934,77.419949,28.734676.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920781,28.739362,76.921518,28.740496.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920781,28.739362,76.921518,28.740496.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.518341,28.726193,77.519521,28.727224.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.932329,28.741682,76.933006,28.74309.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.349287,28.737819,77.350269,28.738923.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.932329,28.741682,76.933006,28.74309.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.518341,28.726193,77.519521,28.727224.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.349287,28.737819,77.350269,28.738923.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370372,28.736791,77.371589,28.737673.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370372,28.736791,77.371589,28.737673.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445863,28.744009,77.446748,28.744985.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445863,28.744009,77.446748,28.744985.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423476,28.743285,77.424735,28.744367.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411872,28.737316,77.413159,28.738264.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423476,28.743285,77.424735,28.744367.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.435478,28.742127,77.43679,28.743004.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411872,28.737316,77.413159,28.738264.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423105,28.742161,77.424483,28.743185.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914064,28.748339,76.914746,28.749346.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.435478,28.742127,77.43679,28.743004.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914064,28.748339,76.914746,28.749346.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415366,28.739043,77.416753,28.740137.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362702,28.73804,77.363795,28.739191.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423105,28.742161,77.424483,28.743185.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416457,28.741858,77.417809,28.742712.tif\n\nAdding overviews...\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915321,28.746591,76.916147,28.74774.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415366,28.739043,77.416753,28.740137.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362702,28.73804,77.363795,28.739191.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440436,28.735691,77.441665,28.736545.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416457,28.741858,77.417809,28.742712.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.919578,28.752905,76.920835,28.753456.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915321,28.746591,76.916147,28.74774.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356845,28.741321,77.358155,28.742256.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440436,28.735691,77.441665,28.736545.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.919578,28.752905,76.920835,28.753456.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.419738,28.737114,77.420787,28.738253.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916183,28.745961,76.917351,28.746512.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433533,28.741994,77.434686,28.743217.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356845,28.741321,77.358155,28.742256.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350176,28.737668,77.35157,28.738706.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916183,28.745961,76.917351,28.746512.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.419738,28.737114,77.420787,28.738253.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.429763,28.739069,77.431202,28.740038.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433533,28.741994,77.434686,28.743217.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.413784,28.740562,77.415241,28.741668.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350176,28.737668,77.35157,28.738706.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.922963,28.744732,76.924222,28.745362.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.429763,28.739069,77.431202,28.740038.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.922963,28.744732,76.924222,28.745362.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.413784,28.740562,77.415241,28.741668.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.923111,28.749089,76.924325,28.749746.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.923111,28.749089,76.924325,28.749746.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42127,28.740169,77.422402,28.74138.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420238,28.736745,77.421446,28.737915.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446262,28.742522,77.447695,28.743381.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425026,28.736699,77.426186,28.737879.tif\n\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420238,28.736745,77.421446,28.737915.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42127,28.740169,77.422402,28.74138.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446262,28.742522,77.447695,28.743381.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42459,28.740948,77.426,28.742123.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425026,28.736699,77.426186,28.737879.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42459,28.740948,77.426,28.742123.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.360944,28.747144,77.362171,28.747956.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.360944,28.747144,77.362171,28.747956.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.428742,28.746448,77.429976,28.74749.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.455849,28.735985,77.457301,28.737144.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387069,28.343111,77.388004,28.343522.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425252,28.746076,77.426414,28.747205.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387069,28.343111,77.388004,28.343522.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.428742,28.746448,77.429976,28.74749.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425252,28.746076,77.426414,28.747205.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.455849,28.735985,77.457301,28.737144.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.3559,28.746014,77.357159,28.74732.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.345519,28.753813,77.346962,28.7548.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.3559,28.746014,77.357159,28.74732.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.345519,28.753813,77.346962,28.7548.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434931,28.746573,77.436104,28.747911.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.352773,28.747897,77.354283,28.748955.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434931,28.746573,77.436104,28.747911.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.352773,28.747897,77.354283,28.748955.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43642,28.746962,77.437816,28.747859.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351163,28.745347,77.352644,28.746451.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43642,28.746962,77.437816,28.747859.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351163,28.745347,77.352644,28.746451.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449107,28.749177,77.450327,28.750249.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449107,28.749177,77.450327,28.750249.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.522613,28.741342,77.524065,28.742711.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.566611,28.34057,77.567861,28.341851.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.522613,28.741342,77.524065,28.742711.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.359709,28.745599,77.361243,28.746612.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.471012,28.747688,77.472291,28.748687.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.566611,28.34057,77.567861,28.341851.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43747,28.753013,77.438884,28.754136.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.453736,28.745112,77.455031,28.746296.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586977,28.334712,77.58781,28.335848.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449761,28.748434,77.450963,28.749562.tif\n\n\nAdding overviews...\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420269,28.746228,77.421749,28.747472.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.359709,28.745599,77.361243,28.746612.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.471012,28.747688,77.472291,28.748687.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586977,28.334712,77.58781,28.335848.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43747,28.753013,77.438884,28.754136.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.453736,28.745112,77.455031,28.746296.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449761,28.748434,77.450963,28.749562.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420269,28.746228,77.421749,28.747472.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446959,28.750691,77.448351,28.751787.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43514,28.753454,77.436648,28.754795.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446959,28.750691,77.448351,28.751787.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43514,28.753454,77.436648,28.754795.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.340341,28.758871,77.34125,28.759911.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.340341,28.758871,77.34125,28.759911.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.369066,28.759512,77.370177,28.760617.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.369066,28.759512,77.370177,28.760617.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37018,28.760358,77.371142,28.761429.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34456,28.755009,77.346002,28.756079.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37018,28.760358,77.371142,28.761429.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34456,28.755009,77.346002,28.756079.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449522,28.752244,77.450608,28.753502.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438875,28.759944,77.440035,28.761055.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438647,28.755698,77.440065,28.75659.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438875,28.759944,77.440035,28.761055.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449522,28.752244,77.450608,28.753502.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438647,28.755698,77.440065,28.75659.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.344301,28.75838,77.345818,28.759456.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374773,28.762884,77.376052,28.763905.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440239,28.757653,77.441573,28.758745.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442567,28.760813,77.44367,28.76203.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.345923,28.763033,77.347359,28.764226.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.344301,28.75838,77.345818,28.759456.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442567,28.760813,77.44367,28.76203.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440239,28.757653,77.441573,28.758745.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374773,28.762884,77.376052,28.763905.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348221,28.763572,77.349596,28.76484.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440762,28.758223,77.44214,28.759303.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.345923,28.763033,77.347359,28.764226.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.647265,28.455689,77.648489,28.456623.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348221,28.763572,77.349596,28.76484.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34503,28.760859,77.346402,28.761991.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440762,28.758223,77.44214,28.759303.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.647265,28.455689,77.648489,28.456623.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34503,28.760859,77.346402,28.761991.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.632056,28.452044,77.633377,28.45302.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.632056,28.452044,77.633377,28.45302.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376729,28.763234,77.378062,28.764403.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447601,28.754954,77.448828,28.7561.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376729,28.763234,77.378062,28.764403.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447601,28.754954,77.448828,28.7561.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364789,28.762969,77.366501,28.764261.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.450428,28.760757,77.451724,28.761867.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368395,28.766226,77.369461,28.767303.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.450428,28.760757,77.451724,28.761867.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442161,28.757968,77.443461,28.759041.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368395,28.766226,77.369461,28.767303.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364789,28.762969,77.366501,28.764261.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448716,28.756714,77.4501,28.757835.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.442161,28.757968,77.443461,28.759041.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.642208,28.682066,77.643102,28.68313.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.453191,28.760263,77.454507,28.761367.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.642208,28.682066,77.643102,28.68313.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448716,28.756714,77.4501,28.757835.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.453191,28.760263,77.454507,28.761367.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447194,28.762389,77.448498,28.763671.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447194,28.762389,77.448498,28.763671.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.594389,28.400605,77.596279,28.401993.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325741,28.767554,77.327062,28.76855.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319744,28.766391,77.320904,28.767495.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368159,28.773908,77.369488,28.774925.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325741,28.767554,77.327062,28.76855.tif\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368159,28.773908,77.369488,28.774925.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319744,28.766391,77.320904,28.767495.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.594389,28.400605,77.596279,28.401993.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.366958,28.765243,77.368178,28.766552.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444459,28.755202,77.445835,28.756572.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.366958,28.765243,77.368178,28.766552.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444459,28.755202,77.445835,28.756572.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439023,28.771396,77.440116,28.772573.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376327,28.766997,77.377455,28.768274.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439023,28.771396,77.440116,28.772573.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376327,28.766997,77.377455,28.768274.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.338077,28.768703,77.339528,28.769902.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434954,28.773155,77.4363,28.774119.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.365989,28.773855,77.367365,28.774898.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364154,28.767956,77.36542,28.769439.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.434954,28.773155,77.4363,28.774119.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.338077,28.768703,77.339528,28.769902.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.365989,28.773855,77.367365,28.774898.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364154,28.767956,77.36542,28.769439.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.363384,28.773193,77.364794,28.774749.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411491,28.727581,77.41291,28.728371.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.437314,28.765986,77.438686,28.767078.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.411491,28.727581,77.41291,28.728371.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593488,28.770902,77.594638,28.772008.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.363384,28.773193,77.364794,28.774749.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.437314,28.765986,77.438686,28.767078.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440694,28.766191,77.441961,28.767599.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593488,28.770902,77.594638,28.772008.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436299,28.771941,77.437783,28.772984.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440694,28.766191,77.441961,28.767599.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.44426,28.765179,77.445636,28.76633.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436299,28.771941,77.437783,28.772984.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.44426,28.765179,77.445636,28.76633.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439184,28.76917,77.440359,28.770457.tif\n\nAdding overviews...\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441356,28.764995,77.44263,28.766318.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439184,28.76917,77.440359,28.770457.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.589809,28.773299,77.591117,28.774377.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36273,28.728479,77.364122,28.729604.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.307744,28.774735,77.309216,28.775631.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.589809,28.773299,77.591117,28.774377.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441356,28.764995,77.44263,28.766318.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604262,28.774147,77.605617,28.774969.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.307744,28.774735,77.309216,28.775631.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36273,28.728479,77.364122,28.729604.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604262,28.774147,77.605617,28.774969.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.569684,28.773985,77.570871,28.775227.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.569684,28.773985,77.570871,28.775227.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.314924,28.776945,77.316023,28.778218.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34208,28.77478,77.343493,28.775764.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.314924,28.776945,77.316023,28.778218.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.34208,28.77478,77.343493,28.775764.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.360863,28.776403,77.362418,28.777528.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313962,28.782084,77.315502,28.783184.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.360863,28.776403,77.362418,28.777528.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.323234,28.780901,77.324651,28.78198.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348386,28.783799,77.349665,28.784894.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313962,28.782084,77.315502,28.783184.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.596297,28.773129,77.597768,28.774436.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.323234,28.780901,77.324651,28.78198.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348386,28.783799,77.349665,28.784894.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.602905,28.766215,77.604246,28.767467.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.455781,28.743484,77.45698,28.74458.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.596297,28.773129,77.597768,28.774436.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.602905,28.766215,77.604246,28.767467.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370639,28.764829,77.372224,28.766385.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.455781,28.743484,77.45698,28.74458.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313449,28.778717,77.314692,28.780092.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370639,28.764829,77.372224,28.766385.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313449,28.778717,77.314692,28.780092.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36319,28.775465,77.36486,28.776688.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.31586,28.775972,77.317439,28.777175.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.564231,28.777744,77.56577,28.778673.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446598,28.783626,77.447934,28.784563.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36319,28.775465,77.36486,28.776688.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.31586,28.775972,77.317439,28.777175.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.596915,28.766768,77.598568,28.768204.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.564231,28.777744,77.56577,28.778673.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.446598,28.783626,77.447934,28.784563.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.947081,28.790763,76.94753,28.791928.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.947081,28.790763,76.94753,28.791928.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927175,28.78835,76.928317,28.78899.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.565324,28.777047,77.566557,28.778136.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927175,28.78835,76.928317,28.78899.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.596915,28.766768,77.598568,28.768204.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.565324,28.777047,77.566557,28.778136.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370628,28.774993,77.372182,28.776167.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.363992,28.7831,77.365356,28.784597.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.952502,28.789701,76.953659,28.790189.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.952502,28.789701,76.953659,28.790189.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370628,28.774993,77.372182,28.776167.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.363992,28.7831,77.365356,28.784597.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420911,28.749509,77.422206,28.750651.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420911,28.749509,77.422206,28.750651.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393642,28.779181,77.395321,28.780406.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441948,28.750509,77.443014,28.75169.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.552464,28.783114,77.553904,28.784337.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.944367,28.79409,76.944893,28.795218.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441948,28.750509,77.443014,28.75169.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364755,28.777285,77.366221,28.778942.tif\nUpdating dataset tags...\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.393642,28.779181,77.395321,28.780406.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.944367,28.79409,76.944893,28.795218.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420037,28.750257,77.421452,28.75146.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439518,28.746085,77.440854,28.746982.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.592007,28.78286,77.593378,28.783684.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.552464,28.783114,77.553904,28.784337.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.439518,28.746085,77.440854,28.746982.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.562674,28.775677,77.564047,28.776915.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.420037,28.750257,77.421452,28.75146.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.364755,28.777285,77.366221,28.778942.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.592007,28.78286,77.593378,28.783684.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.562674,28.775677,77.564047,28.776915.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.355279,28.751545,77.356688,28.752725.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.951123,28.788449,76.952847,28.789189.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.577146,28.77488,77.578443,28.775998.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.951123,28.788449,76.952847,28.789189.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555181,28.782961,77.55661,28.784202.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356537,28.751511,77.358093,28.752631.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574241,28.781285,77.575638,28.782461.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.355279,28.751545,77.356688,28.752725.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.566401,28.7804,77.567839,28.781605.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.577146,28.77488,77.578443,28.775998.tif\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.574241,28.781285,77.575638,28.782461.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555181,28.782961,77.55661,28.784202.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.316595,28.79245,77.317639,28.793517.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356537,28.751511,77.358093,28.752631.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.566401,28.7804,77.567839,28.781605.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.316595,28.79245,77.317639,28.793517.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.366517,28.776091,77.368382,28.77747.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593462,28.775935,77.594835,28.777124.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.584325,28.779093,77.585688,28.780292.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.318081,28.794084,77.319394,28.795044.tif\n\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.366517,28.776091,77.368382,28.77747.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.593462,28.775935,77.594835,28.777124.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.318081,28.794084,77.319394,28.795044.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.584325,28.779093,77.585688,28.780292.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325143,28.791396,77.326476,28.792341.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320603,28.788732,77.321664,28.789842.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.325143,28.791396,77.326476,28.792341.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346971,28.793623,77.348156,28.79469.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.443476,28.752078,77.444757,28.753004.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320603,28.788732,77.321664,28.789842.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346971,28.793623,77.348156,28.79469.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.587339,28.776679,77.588878,28.777978.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.443476,28.752078,77.444757,28.753004.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.336304,28.764197,77.337484,28.765267.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.326488,28.789608,77.327922,28.790651.tif\n\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.336304,28.764197,77.337484,28.765267.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.587339,28.776679,77.588878,28.777978.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.326488,28.789608,77.327922,28.790651.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328991,28.790909,77.330241,28.792217.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324084,28.79007,77.325352,28.791319.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.317802,28.789826,77.319171,28.790893.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.575583,28.778195,77.577203,28.779656.tif\n\nAdding overviews...\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.359641,28.788344,77.360764,28.789438.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328991,28.790909,77.330241,28.792217.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324084,28.79007,77.325352,28.791319.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.317802,28.789826,77.319171,28.790893.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.355221,28.755255,77.356324,28.756254.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.359641,28.788344,77.360764,28.789438.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350163,28.760839,77.351566,28.761884.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.575583,28.778195,77.577203,28.779656.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.355221,28.755255,77.356324,28.756254.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.850943,28.799945,76.852398,28.800496.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313467,28.792684,77.314777,28.793966.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.850943,28.799945,76.852398,28.800496.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844777,28.798646,76.845398,28.79978.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.353793,28.786232,77.355052,28.787531.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350163,28.760839,77.351566,28.761884.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844777,28.798646,76.845398,28.79978.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.924499,28.801519,76.925164,28.802558.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313467,28.792684,77.314777,28.793966.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914908,28.798953,76.915447,28.800071.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916497,28.802148,76.917688,28.8026.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.924499,28.801519,76.925164,28.802558.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914908,28.798953,76.915447,28.800071.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916497,28.802148,76.917688,28.8026.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.353793,28.786232,77.355052,28.787531.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.926524,28.797143,76.927189,28.798182.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.926524,28.797143,76.927189,28.798182.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356603,28.785378,77.357764,28.786621.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350146,28.7616,77.351815,28.762753.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.356603,28.785378,77.357764,28.786621.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348332,28.788119,77.349726,28.78921.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.350146,28.7616,77.351815,28.762753.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348332,28.788119,77.349726,28.78921.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.547596,28.793098,77.548664,28.794104.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.547596,28.793098,77.548664,28.794104.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.933296,28.795963,76.933961,28.796986.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.507843,28.790402,77.509214,28.791599.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.933296,28.795963,76.933961,28.796986.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.358582,28.792517,77.36008,28.793642.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.507843,28.790402,77.509214,28.791599.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.358582,28.792517,77.36008,28.793642.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362218,28.791709,77.363569,28.793095.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351374,28.799503,77.352318,28.80047.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.361926,28.788358,77.363249,28.789636.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351374,28.799503,77.352318,28.80047.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362218,28.791709,77.363569,28.793095.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368745,28.762421,77.37011,28.763687.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.361926,28.788358,77.363249,28.789636.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368745,28.762421,77.37011,28.763687.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351737,28.796406,77.352934,28.797365.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555977,28.791282,77.557307,28.792281.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351737,28.796406,77.352934,28.797365.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.555977,28.791282,77.557307,28.792281.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.572004,28.786398,77.573226,28.78751.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320092,28.79789,77.32117,28.799067.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445419,28.790159,77.4467,28.791443.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.550714,28.793385,77.552062,28.794533.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.554294,28.793159,77.555615,28.794369.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320092,28.79789,77.32117,28.799067.tif\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445419,28.790159,77.4467,28.791443.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.550714,28.793385,77.552062,28.794533.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.572004,28.786398,77.573226,28.78751.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.554294,28.793159,77.555615,28.794369.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351676,28.798465,77.352996,28.799467.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319983,28.80237,77.32139,28.803396.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.329798,28.800491,77.330977,28.801702.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324903,28.800462,77.326324,28.80146.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351676,28.798465,77.352996,28.799467.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319983,28.80237,77.32139,28.803396.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.329798,28.800491,77.330977,28.801702.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324903,28.800462,77.326324,28.80146.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.375588,28.804412,77.37674,28.805199.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.367579,28.801358,77.368752,28.802196.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37061,28.800435,77.371534,28.80156.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.375588,28.804412,77.37674,28.805199.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.367579,28.801358,77.368752,28.802196.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328528,28.804005,77.329665,28.805265.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37061,28.800435,77.371534,28.80156.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.375858,28.798023,77.377101,28.798847.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.344299,28.804002,77.345525,28.80518.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.91909,28.811323,76.920412,28.811865.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.937065,28.811135,76.938253,28.811601.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328528,28.804005,77.329665,28.805265.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.375858,28.798023,77.377101,28.798847.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.91909,28.811323,76.920412,28.811865.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.937065,28.811135,76.938253,28.811601.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.344299,28.804002,77.345525,28.80518.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920528,28.811296,76.921139,28.812405.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920528,28.811296,76.921139,28.812405.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.354843,28.796089,77.356008,28.797179.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.332342,28.799474,77.33357,28.800783.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.45339,28.798339,77.454709,28.799275.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374343,28.804902,77.375575,28.805689.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.354843,28.796089,77.356008,28.797179.tif\nAdding overviews...\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.563516,28.795457,77.564549,28.796255.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.332342,28.799474,77.33357,28.800783.tif\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.45339,28.798339,77.454709,28.799275.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.374343,28.804902,77.375575,28.805689.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347724,28.794735,77.348996,28.795784.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.563516,28.795457,77.564549,28.796255.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.848985,28.808151,76.849666,28.809344.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347724,28.794735,77.348996,28.795784.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.848985,28.808151,76.849666,28.809344.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.551507,28.803384,77.552788,28.804405.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844198,28.812821,76.845654,28.813567.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.847125,28.812669,76.849038,28.813407.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844198,28.812821,76.845654,28.813567.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.551507,28.803384,77.552788,28.804405.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.847125,28.812669,76.849038,28.813407.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849026,28.813797,76.850536,28.814523.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.532089,28.802776,77.533219,28.803914.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849026,28.813797,76.850536,28.814523.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449381,28.803685,77.450699,28.804721.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.532089,28.802776,77.533219,28.803914.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449381,28.803685,77.450699,28.804721.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.294141,28.805063,77.295358,28.805989.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.31241,28.796202,77.31392,28.797543.tif\nUpdating dataset tags...\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.294141,28.805063,77.295358,28.805989.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328377,28.778093,77.329511,28.779191.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.544378,28.799685,77.545591,28.800839.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.302414,28.811316,77.303604,28.81261.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.328377,28.778093,77.329511,28.779191.tif\nAdding overviews...\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.544378,28.799685,77.545591,28.800839.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.31241,28.796202,77.31392,28.797543.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.302414,28.811316,77.303604,28.81261.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347889,28.80743,77.349771,28.807962.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347889,28.80743,77.349771,28.807962.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.470509,28.809922,77.471664,28.811036.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.57246,28.798558,77.573539,28.799538.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.470509,28.809922,77.471664,28.811036.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.340993,28.806853,77.342147,28.807982.tif\n\nAdding overviews...\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.480786,28.805537,77.482111,28.806542.tif\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.57246,28.798558,77.573539,28.799538.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844012,28.816557,76.845175,28.817133.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.845233,28.821603,76.846635,28.82219.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.340993,28.806853,77.342147,28.807982.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.844012,28.816557,76.845175,28.817133.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.480786,28.805537,77.482111,28.806542.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.309701,28.810701,77.311179,28.811736.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920343,28.815163,76.92154,28.81561.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.845233,28.821603,76.846635,28.82219.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.530834,28.813706,77.531994,28.814876.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920343,28.815163,76.92154,28.81561.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.309701,28.810701,77.311179,28.811736.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.530834,28.813706,77.531994,28.814876.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.533024,28.810578,77.534142,28.81172.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849191,28.816402,76.850566,28.816934.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.851395,28.830629,76.851948,28.831621.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.56557,28.8051,77.566293,28.806338.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849191,28.816402,76.850566,28.816934.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.533024,28.810578,77.534142,28.81172.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.851395,28.830629,76.851948,28.831621.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.30894,28.812889,77.310367,28.813934.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.321123,28.804873,77.32256,28.806199.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.56557,28.8051,77.566293,28.806338.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.877693,28.83413,76.8783,28.835169.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.877693,28.83413,76.8783,28.835169.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422211,28.805184,77.423706,28.806162.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.316865,28.806026,77.318241,28.807386.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313437,28.809409,77.314684,28.810688.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.960004,28.82227,76.961138,28.822781.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.30894,28.812889,77.310367,28.813934.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.851905,28.819129,76.852626,28.820375.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.960004,28.82227,76.961138,28.822781.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.321123,28.804873,77.32256,28.806199.tif\nAdding overviews...\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.851905,28.819129,76.852626,28.820375.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422211,28.805184,77.423706,28.806162.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.95858,28.822863,76.959236,28.823842.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.313437,28.809409,77.314684,28.810688.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.95858,28.822863,76.959236,28.823842.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.316865,28.806026,77.318241,28.807386.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.290246,28.817769,77.291254,28.818445.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.856739,28.821235,76.857475,28.822434.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.290246,28.817769,77.291254,28.818445.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.306324,28.817327,77.30688,28.818479.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.856739,28.821235,76.857475,28.822434.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.306324,28.817327,77.30688,28.818479.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.948548,28.819426,76.949443,28.82067.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864128,28.8354,76.865137,28.835862.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.948548,28.819426,76.949443,28.82067.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864128,28.8354,76.865137,28.835862.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.301883,28.827145,77.302801,28.827909.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.301883,28.827145,77.302801,28.827909.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368122,28.827311,77.369161,28.828181.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.862663,28.835888,76.863666,28.836418.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.534406,28.812102,77.535806,28.813222.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.862663,28.835888,76.863666,28.836418.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368122,28.827311,77.369161,28.828181.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.534406,28.812102,77.535806,28.813222.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536143,28.809452,77.537547,28.81054.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.861135,28.83385,76.862194,28.834368.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.990364,28.842177,76.991035,28.843146.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.861135,28.83385,76.862194,28.834368.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.990364,28.842177,76.991035,28.843146.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536143,28.809452,77.537547,28.81054.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.371176,28.825624,77.37232,28.82642.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.878808,28.84229,76.879977,28.842772.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.878808,28.84229,76.879977,28.842772.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.371176,28.825624,77.37232,28.82642.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.958927,28.824924,76.960553,28.825451.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.297729,28.832594,77.298992,28.833493.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.958927,28.824924,76.960553,28.825451.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.297729,28.832594,77.298992,28.833493.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.275666,28.836555,77.276798,28.837342.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.541615,28.811052,77.54309,28.812347.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.925608,28.789461,76.926857,28.790274.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.275666,28.836555,77.276798,28.837342.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.925608,28.789461,76.926857,28.790274.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.541615,28.811052,77.54309,28.812347.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426625,28.843175,77.427459,28.844197.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.426625,28.843175,77.427459,28.844197.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433702,28.844202,77.4346,28.845223.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433702,28.844202,77.4346,28.845223.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.53055,28.83775,77.531789,28.838272.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.53055,28.83775,77.531789,28.838272.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.955666,28.853937,76.956239,28.854921.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.955666,28.853937,76.956239,28.854921.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.292388,28.834105,77.293747,28.835142.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.888486,28.849237,76.889794,28.849759.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.888486,28.849237,76.889794,28.849759.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.533118,28.830917,77.534394,28.831924.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.525559,28.843177,77.526696,28.844002.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.292388,28.834105,77.293747,28.835142.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.533118,28.830917,77.534394,28.831924.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.525559,28.843177,77.526696,28.844002.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.043509,28.850643,77.044151,28.851567.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.550966,28.840755,77.552084,28.841475.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.043509,28.850643,77.044151,28.851567.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.454177,28.835745,77.45534,28.836681.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381884,28.854646,77.382817,28.855262.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.95383,28.850743,76.955045,28.851366.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.550966,28.840755,77.552084,28.841475.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536208,28.836549,77.537073,28.837565.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381884,28.854646,77.382817,28.855262.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.95383,28.850743,76.955045,28.851366.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536208,28.836549,77.537073,28.837565.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.454177,28.835745,77.45534,28.836681.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.50953,28.838229,77.510425,28.83928.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378511,28.8545,77.379516,28.855262.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.546794,28.842254,77.547909,28.843169.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.50953,28.838229,77.510425,28.83928.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.378511,28.8545,77.379516,28.855262.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.546794,28.842254,77.547909,28.843169.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.527433,28.832762,77.528829,28.833787.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448291,28.842929,77.449389,28.843847.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.527433,28.832762,77.528829,28.833787.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448291,28.842929,77.449389,28.843847.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.962591,28.85522,76.963807,28.855722.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.962591,28.85522,76.963807,28.855722.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382044,28.850399,77.383024,28.851449.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382044,28.850399,77.383024,28.851449.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.964082,28.85757,76.965228,28.858132.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.964082,28.85757,76.965228,28.858132.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.535455,28.837993,77.536679,28.839134.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.169404,28.856465,77.170482,28.857068.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.298331,28.846606,77.299605,28.847408.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.169404,28.856465,77.170482,28.857068.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.285525,28.847659,77.286912,28.848554.tif\n\nAdding overviews...\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.535455,28.837993,77.536679,28.839134.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.298331,28.846606,77.299605,28.847408.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.2943,28.847591,77.295541,28.848592.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.952902,28.856166,76.953582,28.857208.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.285525,28.847659,77.286912,28.848554.tif\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.471116,28.848687,77.472301,28.849646.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.952902,28.856166,76.953582,28.857208.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368875,28.785527,77.370107,28.786592.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.2943,28.847591,77.295541,28.848592.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287533,28.857049,77.288491,28.85803.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.471116,28.848687,77.472301,28.849646.tif\nAdding overviews...\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.46839,28.85956,77.468904,28.860501.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.368875,28.785527,77.370107,28.786592.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447553,28.787207,77.448867,28.788072.tif\n\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.559975,28.846864,77.561057,28.847934.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287533,28.857049,77.288491,28.85803.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.46839,28.85956,77.468904,28.860501.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.028012,28.861109,77.029305,28.861763.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447553,28.787207,77.448867,28.788072.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.028012,28.861109,77.029305,28.861763.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.559975,28.846864,77.561057,28.847934.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.390245,28.852095,77.391246,28.853328.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.290793,28.84583,77.292141,28.846967.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.034059,28.857248,77.035297,28.857811.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430102,28.854218,77.431071,28.855262.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.034059,28.857248,77.035297,28.857811.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.390245,28.852095,77.391246,28.853328.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.290793,28.84583,77.292141,28.846967.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382919,28.860446,77.383617,28.861401.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430102,28.854218,77.431071,28.855262.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.382919,28.860446,77.383617,28.861401.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.284304,28.850469,77.285799,28.851544.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436509,28.863599,77.437368,28.864615.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436509,28.863599,77.437368,28.864615.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348934,28.86195,77.350287,28.86291.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.284304,28.850469,77.285799,28.851544.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.167907,28.870087,77.169093,28.870606.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.348934,28.86195,77.350287,28.86291.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.167907,28.870087,77.169093,28.870606.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448422,28.848973,77.449674,28.849902.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.525954,28.839259,77.527259,28.84048.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.385272,28.857863,77.386243,28.858883.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448422,28.848973,77.449674,28.849902.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.428479,28.84846,77.42971,28.849591.tif\n\nUpdating dataset tags...\nAdding overviews...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.385272,28.857863,77.386243,28.858883.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.525954,28.839259,77.527259,28.84048.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.039955,28.874601,77.041266,28.87512.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.295883,28.867939,77.297112,28.868506.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.039955,28.874601,77.041266,28.87512.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.428479,28.84846,77.42971,28.849591.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.295883,28.867939,77.297112,28.868506.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441193,28.847954,77.44197,28.849205.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441193,28.847954,77.44197,28.849205.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.296584,28.864739,77.297833,28.865512.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.296584,28.864739,77.297833,28.865512.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.03126,28.871338,77.032134,28.872477.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.476012,28.86095,77.477292,28.861882.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.03126,28.871338,77.032134,28.872477.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.476012,28.86095,77.477292,28.861882.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.432489,28.858258,77.433525,28.85947.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.432489,28.858258,77.433525,28.85947.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038572,28.876217,77.039728,28.876808.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.861877,28.881644,76.863134,28.882304.tif\nUpdating dataset tags...\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038572,28.876217,77.039728,28.876808.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.302563,28.86724,77.303415,28.868374.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.861877,28.881644,76.863134,28.882304.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.302563,28.86724,77.303415,28.868374.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.311904,28.867939,77.313369,28.868676.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.311904,28.867939,77.313369,28.868676.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.040611,28.879031,77.041149,28.880077.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.040611,28.879031,77.041149,28.880077.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376311,28.883798,77.377354,28.884411.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.873137,28.89189,76.873688,28.89297.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.86054,28.888196,76.861238,28.889445.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.376311,28.883798,77.377354,28.884411.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.873137,28.89189,76.873688,28.89297.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864046,28.886548,76.864674,28.887633.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.86054,28.888196,76.861238,28.889445.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.172673,28.884157,77.173357,28.885261.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864046,28.886548,76.864674,28.887633.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.172673,28.884157,77.173357,28.885261.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.470542,28.862928,77.471957,28.863941.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436074,28.864892,77.437224,28.866058.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.436074,28.864892,77.437224,28.866058.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.470542,28.862928,77.471957,28.863941.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.29232,28.86873,77.293737,28.869714.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.301137,28.869215,77.302288,28.870436.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.29232,28.86873,77.293737,28.869714.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.032577,28.883232,77.033314,28.884427.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.301137,28.869215,77.302288,28.870436.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.032577,28.883232,77.033314,28.884427.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.028783,28.890532,77.02979,28.891009.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.882577,28.890701,76.883419,28.891808.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.882919,28.891848,76.884122,28.892351.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.028783,28.890532,77.02979,28.891009.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.882577,28.890701,76.883419,28.891808.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.882919,28.891848,76.884122,28.892351.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876902,28.890181,76.878213,28.890826.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876902,28.890181,76.878213,28.890826.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.252089,28.881329,77.253163,28.882544.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.047153,28.89309,77.048446,28.89353.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038464,28.888292,77.039776,28.888924.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.047153,28.89309,77.048446,28.89353.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.870968,28.893161,76.872247,28.894057.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038464,28.888292,77.039776,28.888924.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.252089,28.881329,77.253163,28.882544.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.870968,28.893161,76.872247,28.894057.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.254325,28.894242,77.25517,28.89526.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.262355,28.883902,77.263618,28.885108.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.254325,28.894242,77.25517,28.89526.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.45465,28.880723,77.455924,28.881913.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438637,28.869431,77.439948,28.870789.tif\nAdding overviews...\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.45465,28.880723,77.455924,28.881913.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.262355,28.883902,77.263618,28.885108.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864692,28.88263,76.866866,28.883347.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438637,28.869431,77.439948,28.870789.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.043476,28.898498,77.044912,28.899049.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251953,28.899649,77.252679,28.900808.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.976183,28.901973,76.977782,28.902555.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.864692,28.88263,76.866866,28.883347.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.043476,28.898498,77.044912,28.899049.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43221,28.796215,77.433453,28.797039.tif\nUpdating dataset tags...\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251953,28.899649,77.252679,28.900808.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.265469,28.892011,77.266774,28.89291.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.976183,28.901973,76.977782,28.902555.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.43221,28.796215,77.433453,28.797039.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.265469,28.892011,77.266774,28.89291.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.024315,28.891926,77.025052,28.893452.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.024315,28.891926,77.025052,28.893452.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.250911,28.888992,77.25207,28.890165.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.040979,28.899851,77.041554,28.901187.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.250911,28.888992,77.25207,28.890165.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.040979,28.899851,77.041554,28.901187.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.05123,28.897806,77.051877,28.898923.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266054,28.890528,77.267098,28.891708.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.05123,28.897806,77.051877,28.898923.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.045303,28.895227,77.046632,28.895762.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.045303,28.895227,77.046632,28.895762.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266054,28.890528,77.267098,28.891708.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.044965,28.899996,77.046431,28.900664.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.044965,28.899996,77.046431,28.900664.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.260048,28.889159,77.26125,28.890421.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.531282,28.811477,77.532354,28.81262.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.260048,28.889159,77.26125,28.890421.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038008,28.893457,77.03955,28.894399.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.531282,28.811477,77.532354,28.81262.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.877077,28.919511,76.877768,28.920599.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.263118,28.886879,77.264444,28.888208.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.877077,28.919511,76.877768,28.920599.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.038008,28.893457,77.03955,28.894399.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.263118,28.886879,77.264444,28.888208.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.247521,28.910042,77.248417,28.911224.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256492,28.887152,77.257811,28.888528.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.321949,28.913876,77.323009,28.914615.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.880422,28.918483,76.881661,28.919065.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.247521,28.910042,77.248417,28.911224.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.880422,28.918483,76.881661,28.919065.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.321949,28.913876,77.323009,28.914615.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37849,28.907998,77.379812,28.908794.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604211,28.893824,77.60542,28.895016.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347713,28.805071,77.349265,28.806072.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.867253,28.918092,76.867916,28.919211.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256492,28.887152,77.257811,28.888528.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.37849,28.907998,77.379812,28.908794.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.867253,28.918092,76.867916,28.919211.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.604211,28.893824,77.60542,28.895016.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.347713,28.805071,77.349265,28.806072.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.865303,28.916926,76.86656,28.917477.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.870063,28.918106,76.87062,28.919253.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.865303,28.916926,76.86656,28.917477.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.24377,28.904835,77.245205,28.905678.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.870063,28.918106,76.87062,28.919253.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.386738,28.899994,77.387903,28.901375.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.24377,28.904835,77.245205,28.905678.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251168,28.89594,77.252175,28.89729.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.323558,28.912049,77.324703,28.913286.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.300485,28.910495,77.301892,28.911529.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.268471,28.893036,77.269967,28.894135.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.386738,28.899994,77.387903,28.901375.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251168,28.89594,77.252175,28.89729.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.323558,28.912049,77.324703,28.913286.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.268471,28.893036,77.269967,28.894135.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.300485,28.910495,77.301892,28.911529.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394603,28.904198,77.3961,28.90526.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.874198,28.922114,76.874961,28.923424.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.394603,28.904198,77.3961,28.90526.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.874198,28.922114,76.874961,28.923424.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.94914,28.919705,76.95013,28.920672.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.94914,28.919705,76.95013,28.920672.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.883925,28.921816,76.885326,28.922508.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.249271,28.916525,77.250725,28.91715.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370497,28.818212,77.371909,28.819376.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.249271,28.916525,77.250725,28.91715.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.883925,28.921816,76.885326,28.922508.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370497,28.818212,77.371909,28.819376.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.874307,28.925557,76.875669,28.926033.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.874307,28.925557,76.875669,28.926033.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.249015,28.905595,77.250543,28.906765.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.884571,28.923891,76.885882,28.924536.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.884571,28.923891,76.885882,28.924536.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876316,28.922714,76.877748,28.923576.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.875734,28.928389,76.876327,28.929458.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.249015,28.905595,77.250543,28.906765.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.875734,28.928389,76.876327,28.929458.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.898177,28.930259,76.899578,28.930841.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876316,28.922714,76.877748,28.923576.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39508,28.918222,77.396294,28.919073.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.897171,28.932963,76.898374,28.933561.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.898177,28.930259,76.899578,28.930841.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.897171,28.932963,76.898374,28.933561.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.39508,28.918222,77.396294,28.919073.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287776,28.82722,77.289045,28.828096.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397744,28.922422,77.398976,28.923672.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.866704,28.925682,76.868069,28.926316.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.373435,28.820064,77.374641,28.821211.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287776,28.82722,77.289045,28.828096.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.885291,28.926016,76.88598,28.927352.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.866704,28.925682,76.868069,28.926316.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.397744,28.922422,77.398976,28.923672.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.885291,28.926016,76.88598,28.927352.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319729,28.917191,77.321108,28.917964.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.906632,28.926062,76.907351,28.92732.tif\nUpdating dataset tags...\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.373435,28.820064,77.374641,28.821211.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.319729,28.917191,77.321108,28.917964.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.906632,28.926062,76.907351,28.92732.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914836,28.928373,76.915483,28.929568.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.904607,28.93213,76.905362,28.933388.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914836,28.928373,76.915483,28.929568.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256037,28.911106,77.257078,28.91247.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.875666,28.926238,76.877105,28.927012.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.904607,28.93213,76.905362,28.933388.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.903188,28.930197,76.904571,28.930747.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.875666,28.926238,76.877105,28.927012.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.903188,28.930197,76.904571,28.930747.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256037,28.911106,77.257078,28.91247.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.398581,28.921898,77.39973,28.923136.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.951926,28.853897,76.953188,28.854439.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.951926,28.853897,76.953188,28.854439.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.398581,28.921898,77.39973,28.923136.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.255411,28.920668,77.256678,28.921915.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.918285,28.926785,76.918985,28.92787.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.255411,28.920668,77.256678,28.921915.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.918285,28.926785,76.918985,28.92787.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.912075,28.929552,76.913529,28.930134.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915626,28.928294,76.916327,28.929583.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914374,28.930731,76.914984,28.932272.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.912075,28.929552,76.913529,28.930134.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915626,28.928294,76.916327,28.929583.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920422,28.931674,76.921141,28.932947.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914374,28.930731,76.914984,28.932272.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920422,28.931674,76.921141,28.932947.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.530007,28.832895,77.531028,28.834103.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.909962,28.926185,76.91151,28.926898.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916755,28.926784,76.917409,28.92792.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.909962,28.926185,76.91151,28.926898.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916755,28.926784,76.917409,28.92792.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381082,28.93381,77.381968,28.934706.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.530007,28.832895,77.531028,28.834103.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.901284,28.935685,76.902631,28.936298.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.901284,28.935685,76.902631,28.936298.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381082,28.93381,77.381968,28.934706.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.918321,28.932224,76.919775,28.932822.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.918321,28.932224,76.919775,28.932822.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414274,28.919033,77.415722,28.920224.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.901826,28.931758,76.902876,28.933089.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414274,28.919033,77.415722,28.920224.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.901826,28.931758,76.902876,28.933089.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.908256,28.936154,76.90943,28.936734.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.908256,28.936154,76.90943,28.936734.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.906931,28.938292,76.908238,28.938849.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.906931,28.938292,76.908238,28.938849.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.272674,28.933719,77.273739,28.934798.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.272674,28.933719,77.273739,28.934798.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916201,28.931155,76.917782,28.931784.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.916201,28.931155,76.917782,28.931784.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.259371,28.933412,77.260493,28.934482.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.926873,28.927528,76.927908,28.928875.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.896325,28.941761,76.897155,28.943049.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.259371,28.933412,77.260493,28.934482.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.258113,28.943559,77.259317,28.944645.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.884405,28.948867,76.885815,28.949482.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.896325,28.941761,76.897155,28.943049.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.898105,28.94721,76.898841,28.94831.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.884405,28.948867,76.885815,28.949482.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.926873,28.927528,76.927908,28.928875.tif\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.905626,28.951297,76.906812,28.951878.tif\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.258113,28.943559,77.259317,28.944645.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.898105,28.94721,76.898841,28.94831.tif\nUpdating dataset tags...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915698,28.935449,76.916596,28.936723.tif\n\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.905626,28.951297,76.906812,28.951878.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256898,28.934974,77.258232,28.935959.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915698,28.935449,76.916596,28.936723.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256898,28.934974,77.258232,28.935959.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.268644,28.934886,77.269672,28.936023.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.289628,28.942986,77.290971,28.94393.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.903601,28.947053,76.90502,28.947635.tif\n\nAdding overviews...\nUpdating dataset tags...\nAdding overviews...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.903601,28.947053,76.90502,28.947635.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.289628,28.942986,77.290971,28.94393.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.268644,28.934886,77.269672,28.936023.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.907662,28.950844,76.90835,28.951969.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.907662,28.950844,76.90835,28.951969.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251985,28.94787,77.25335,28.948452.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.88633,28.948476,76.887096,28.94956.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.251985,28.94787,77.25335,28.948452.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.88633,28.948476,76.887096,28.94956.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.886334,28.950175,76.887182,28.951397.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.917351,28.950071,76.918716,28.950699.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.913062,28.947949,76.914535,28.948656.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.912954,28.951218,76.913692,28.952507.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.886334,28.950175,76.887182,28.951397.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.917351,28.950071,76.918716,28.950699.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.913062,28.947949,76.914535,28.948656.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.912954,28.951218,76.913692,28.952507.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.431279,28.936963,77.432459,28.938163.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.217525,28.955605,77.218273,28.956667.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.217525,28.955605,77.218273,28.956667.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.259178,28.942939,77.260521,28.944094.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.401468,28.939561,77.402701,28.940702.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.431279,28.936963,77.432459,28.938163.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415133,28.938179,77.416279,28.939415.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.259178,28.942939,77.260521,28.944094.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.401468,28.939561,77.402701,28.940702.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415133,28.938179,77.416279,28.939415.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915491,28.953007,76.916947,28.953641.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324468,28.942848,77.325864,28.94388.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414298,28.93673,77.41581,28.937925.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.915491,28.953007,76.916947,28.953641.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.224086,28.952098,77.225003,28.953237.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.324468,28.942848,77.325864,28.94388.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.414298,28.93673,77.41581,28.937925.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.407728,28.956252,77.408428,28.957289.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.224086,28.952098,77.225003,28.953237.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.407728,28.956252,77.408428,28.957289.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266991,28.943217,77.268631,28.944473.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440341,28.873538,77.441825,28.874925.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266991,28.943217,77.268631,28.944473.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440341,28.873538,77.441825,28.874925.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.221169,28.952151,77.22219,28.953507.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.291493,28.933235,77.29294,28.934675.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.221169,28.952151,77.22219,28.953507.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447961,28.958248,77.448554,28.959222.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.427677,28.946816,77.428841,28.948161.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.447961,28.958248,77.448554,28.959222.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.291493,28.933235,77.29294,28.934675.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430814,28.958514,77.43227,28.959079.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.427677,28.946816,77.428841,28.948161.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.430814,28.958514,77.43227,28.959079.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423406,28.959032,77.424661,28.959707.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.219602,28.959455,77.220595,28.960561.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.423406,28.959032,77.424661,28.959707.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.221756,28.956899,77.22304,28.957726.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.219602,28.959455,77.220595,28.960561.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.221756,28.956899,77.22304,28.957726.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425208,28.958155,77.426535,28.958868.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.418174,28.945075,77.419569,28.946435.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.425208,28.958155,77.426535,28.958868.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.232493,28.956615,77.233848,28.957765.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421306,28.951913,77.422879,28.952881.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.225407,28.960811,77.226616,28.96238.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.418174,28.945075,77.419569,28.946435.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.232493,28.956615,77.233848,28.957765.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.421306,28.951913,77.422879,28.952881.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.225407,28.960811,77.226616,28.96238.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.880135,28.916879,76.881266,28.917382.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.880135,28.916879,76.881266,28.917382.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42902,28.946138,77.4303,28.947424.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.88796,28.973951,76.888717,28.9751.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351114,28.95661,77.352266,28.957959.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.250986,28.95936,77.252325,28.960745.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.88796,28.973951,76.888717,28.9751.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42902,28.946138,77.4303,28.947424.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.351114,28.95661,77.352266,28.957959.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.291097,28.945003,77.292804,28.94633.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.250986,28.95936,77.252325,28.960745.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.291097,28.945003,77.292804,28.94633.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.293315,28.948056,77.29497,28.949171.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433782,28.959234,77.434904,28.960545.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.293315,28.948056,77.29497,28.949171.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441876,28.96009,77.443214,28.961068.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256212,28.895766,77.257349,28.89702.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.270443,28.971888,77.271871,28.972685.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433782,28.959234,77.434904,28.960545.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.270443,28.971888,77.271871,28.972685.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.256212,28.895766,77.257349,28.89702.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441876,28.96009,77.443214,28.961068.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346764,28.955006,77.348023,28.956306.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.346764,28.955006,77.348023,28.956306.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.26793,28.895126,77.269475,28.896238.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.228188,28.975158,77.229128,28.97626.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444652,28.962986,77.446015,28.963965.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.228188,28.975158,77.229128,28.97626.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444652,28.962986,77.446015,28.963965.tif\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.229458,28.955108,77.231257,28.95632.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.92039,28.815175,76.921463,28.815598.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.26793,28.895126,77.269475,28.896238.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.23228,28.97215,77.233554,28.973512.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.92039,28.815175,76.921463,28.815598.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.229458,28.955108,77.231257,28.95632.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.23228,28.97215,77.233554,28.973512.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.223663,28.992069,77.224585,28.99318.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.223663,28.992069,77.224585,28.99318.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.890943,28.987499,76.892324,28.988273.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42095,28.950123,77.422685,28.951445.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.260693,28.968166,77.261917,28.969265.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.228707,28.97247,77.229954,28.973988.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.890943,28.987499,76.892324,28.988273.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.235394,28.97924,77.236685,28.980241.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.260693,28.968166,77.261917,28.969265.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422288,28.96797,77.423587,28.968999.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.29058,28.922459,77.291843,28.923733.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.42095,28.950123,77.422685,28.951445.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.228707,28.97247,77.229954,28.973988.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234277,28.978332,77.235809,28.979443.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.235394,28.97924,77.236685,28.980241.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.422288,28.96797,77.423587,28.968999.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.29058,28.922459,77.291843,28.923733.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440034,28.965291,77.441488,28.966403.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.252804,28.919736,77.253839,28.921065.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234277,28.978332,77.235809,28.979443.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.233107,28.989203,77.234562,28.990181.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440034,28.965291,77.441488,28.966403.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.252804,28.919736,77.253839,28.921065.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.233107,28.989203,77.234562,28.990181.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242409,28.981966,77.243709,28.982998.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927728,28.997463,76.928392,28.998484.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242409,28.981966,77.243709,28.982998.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444144,28.991224,77.445482,28.991824.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242662,28.986902,77.243978,28.988101.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927728,28.997463,76.928392,28.998484.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266362,28.975527,77.267611,28.976745.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444144,28.991224,77.445482,28.991824.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242662,28.986902,77.243978,28.988101.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.266362,28.975527,77.267611,28.976745.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.238437,28.986875,77.239703,28.988132.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441475,28.991872,77.44271,28.992574.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.441475,28.991872,77.44271,28.992574.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.238437,28.986875,77.239703,28.988132.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387019,28.343102,77.387985,28.343462.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.225361,28.995365,77.226303,28.996462.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387019,28.343102,77.387985,28.343462.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448081,28.848099,77.449061,28.849067.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440135,28.998405,77.441464,28.998987.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.225361,28.995365,77.226303,28.996462.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.440135,28.998405,77.441464,28.998987.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.448081,28.848099,77.449061,28.849067.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849086,28.813816,76.850417,28.81438.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.955645,28.85393,76.956203,28.854832.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876252,28.839627,76.877453,28.840116.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.955645,28.85393,76.956203,28.854832.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.849086,28.813816,76.850417,28.81438.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403778,28.412947,77.404786,28.413436.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876252,28.839627,76.877453,28.840116.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.241341,28.992554,77.242952,28.993526.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.403778,28.412947,77.404786,28.413436.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.049801,28.854869,77.05096,28.855359.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573315,28.36872,77.574238,28.36964.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.049801,28.854869,77.05096,28.855359.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.241341,28.992554,77.242952,28.993526.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.244783,28.987145,77.246338,28.988209.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573315,28.36872,77.574238,28.36964.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.595223,28.421471,77.596189,28.421991.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.595223,28.421471,77.596189,28.421991.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.244783,28.987145,77.246338,28.988209.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.333294,28.993503,77.334969,28.994655.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242459,28.988296,77.244017,28.989758.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.035317,28.851863,77.035789,28.852821.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.333294,28.993503,77.334969,28.994655.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.035317,28.851863,77.035789,28.852821.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.242459,28.988296,77.244017,28.989758.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.244539,28.986063,77.24616,28.987295.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.354189,28.784581,77.355326,28.785607.tif\n\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362289,28.791722,77.363577,28.793003.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.354189,28.784581,77.355326,28.785607.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.925257,28.809107,76.926596,28.809595.tif\n\nAdding overviews...\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.244539,28.986063,77.24616,28.987295.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.925257,28.809107,76.926596,28.809595.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.362289,28.791722,77.363577,28.793003.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.892829,28.851524,76.89405,28.852051.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.051239,28.897861,77.051797,28.898836.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.591579,28.363717,77.592623,28.364617.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.892829,28.851524,76.89405,28.852051.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.051239,28.897861,77.051797,28.898836.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391783,28.336326,77.392405,28.337207.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.44643,28.995227,77.447866,28.996104.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.591579,28.363717,77.592623,28.364617.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920369,28.815156,76.921484,28.815579.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391783,28.336326,77.392405,28.337207.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.920369,28.815156,76.921484,28.815579.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.44643,28.995227,77.447866,28.996104.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438629,28.755661,77.439973,28.756577.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370182,28.760435,77.371005,28.761378.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.41164,28.98945,77.413204,28.990748.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.370182,28.760435,77.371005,28.761378.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.921227,28.810047,76.9223,28.810498.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.656268,28.442301,77.657153,28.443089.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.656268,28.442301,77.657153,28.443089.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438629,28.755661,77.439973,28.756577.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.921227,28.810047,76.9223,28.810498.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927972,28.724121,76.928558,28.725145.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.41164,28.98945,77.413204,28.990748.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927972,28.724121,76.928558,28.725145.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449793,28.748491,77.450824,28.74945.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.037849,28.874662,77.038321,28.875611.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389357,28.379969,77.390411,28.380556.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.449793,28.748491,77.450824,28.74945.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.037849,28.874662,77.038321,28.875611.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.389357,28.379969,77.390411,28.380556.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.921484,28.808138,76.922042,28.809266.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.921484,28.808138,76.922042,28.809266.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586957,28.334628,77.587804,28.335808.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.241675,28.999019,77.243251,29.000096.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.586957,28.334628,77.587804,28.335808.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402736,28.406643,77.403811,28.407207.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.402736,28.406643,77.403811,28.407207.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.241675,28.999019,77.243251,29.000096.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415738,28.925848,77.417098,28.927017.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36497,28.774063,77.366304,28.775493.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927385,28.810009,76.928029,28.811062.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.415738,28.925848,77.417098,28.927017.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.927385,28.810009,76.928029,28.811062.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.36497,28.774063,77.366304,28.775493.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391558,28.284599,77.392073,28.285392.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.391558,28.284599,77.392073,28.285392.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438014,28.864431,77.439215,28.865792.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.681387,28.4739,77.682416,28.474743.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536172,28.83655,77.537137,28.83761.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.681387,28.4739,77.682416,28.474743.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.438014,28.864431,77.439215,28.865792.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.536172,28.83655,77.537137,28.83761.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876985,28.837903,76.877578,28.838889.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416685,28.334453,77.4172,28.335341.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387266,28.284569,77.387824,28.285441.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.416685,28.334453,77.4172,28.335341.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.876985,28.837903,76.877578,28.838889.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.387266,28.284569,77.387824,28.285441.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381382,28.344613,77.382465,28.345067.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.941848,28.814333,76.942449,28.815349.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.381382,28.344613,77.382465,28.345067.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.941848,28.814333,76.942449,28.815349.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287778,28.8272,77.289087,28.828087.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.659764,28.44711,77.660694,28.447977.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.352365,28.773937,77.353674,28.775451.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.613234,28.256518,77.614019,28.257205.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.659754,28.447119,77.660704,28.448006.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.613234,28.256518,77.614019,28.257205.tif\nUpdating dataset tags...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.659764,28.44711,77.660694,28.447977.tif\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.287778,28.8272,77.289087,28.828087.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320484,28.806018,77.321655,28.807118.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.64256,28.449063,77.643148,28.449603.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.659754,28.447119,77.660704,28.448006.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.64256,28.449063,77.643148,28.449603.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.352365,28.773937,77.353674,28.775451.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.320484,28.806018,77.321655,28.807118.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.496861,28.99441,77.497355,28.995353.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914495,28.946471,76.915285,28.947556.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.496861,28.99441,77.497355,28.995353.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914495,28.946471,76.915285,28.947556.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444558,28.744613,77.445846,28.74537.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.497985,28.994591,77.499063,28.99581.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.318972,28.794426,77.320377,28.795357.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.444558,28.744613,77.445846,28.74537.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.497985,28.994591,77.499063,28.99581.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.318972,28.794426,77.320377,28.795357.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914409,28.944832,76.915348,28.946099.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392928,28.277293,77.394007,28.278242.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.558232,28.355867,77.559151,28.356814.tif\n\nAdding overviews...\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.246554,28.894409,77.247526,28.895545.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433127,28.754224,77.434403,28.75538.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.914409,28.944832,76.915348,28.946099.tif\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.392928,28.277293,77.394007,28.278242.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.558232,28.355867,77.559151,28.356814.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40708,28.9954,77.408685,28.996862.tif\n\nAdding overviews...\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.246554,28.894409,77.247526,28.895545.tif\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.433127,28.754224,77.434403,28.75538.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.40708,28.9954,77.408685,28.996862.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.929078,28.993159,76.929773,28.994319.tif\n\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573165,28.764444,77.574517,28.765589.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/76.929078,28.993159,76.929773,28.994319.tif\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.573165,28.764444,77.574517,28.765589.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.237019,28.959754,77.238697,28.961028.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.237019,28.959754,77.238697,28.961028.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.298072,28.967272,77.299566,28.968437.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.298072,28.967272,77.299566,28.968437.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234164,28.958778,77.235502,28.9604.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234164,28.958778,77.235502,28.9604.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445232,28.966706,77.446808,28.967925.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.445232,28.966706,77.446808,28.967925.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.443547,28.9985,77.444212,28.999741.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.443547,28.9985,77.444212,28.999741.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396826,28.326171,77.397619,28.326662.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.396826,28.326171,77.397619,28.326662.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234529,29.002138,77.236003,29.003329.tif\n\nAdding overviews...\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.234529,29.002138,77.236003,29.003329.tif\nReading input: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.275279,28.225628,77.275879,28.226649.tif\n\nUpdating dataset tags...\nWriting output to: /home/patel_zeel/kiln_compass_24/regions/high_res/19/77.275279,28.225628,77.275879,28.226649.tif\nfiles = glob(\"/home/patel_zeel/kiln_compass_24/regions/high_res/19/*.tif\")\nprint(len(files))\nsizes = []\nfor file in files:\n    with Image.open(file) as img:\n        sizes.append(img.size[0])\n        sizes.append(img.size[1])\n\n783\nmax(sizes), min(sizes)\n\n(1216, 229)\nImage.open(files[1])\nfor file in tqdm(files):\n    new_file = file.replace(\".tif\", \".png\")\n    with Image.open(file) as img:\n        img.save(new_file)\nfrom ultralytics import YOLO\nmodel = YOLO(\"yolo11m-obb\")\nmodel.train(data=\"../lab/trench_width/data.yaml\", epochs=51, batch=-1, imgsz=1280, save_period=5)\n\nNew https://pypi.org/project/ultralytics/8.3.64 available  Update with 'pip install -U ultralytics'\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nengine/trainer: task=obb, mode=train, model=yolo11m-obb.pt, data=../lab/trench_width/data.yaml, epochs=51, time=None, patience=100, batch=-1, imgsz=1280, save=True, save_period=5, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/obb/train5\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n 23        [16, 19, 22]  1   2261401  ultralytics.nn.modules.head.OBB              [2, 1, [256, 512, 512]]       \nYOLO11m-obb summary: 434 layers, 20,903,385 parameters, 20,903,369 gradients, 71.9 GFLOPs\n\nTransferred 685/691 items from pretrained weights\n\n\nwandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: patel_zeel (sustainability-lab). Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.18.5\n\n\nRun data is saved locally in /home/patel_zeel/blog/lab/wandb/run-20250120_153755-5wtpe9o7\n\n\nSyncing run train5 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sustainability-lab/Ultralytics\n\n\n View run at https://wandb.ai/sustainability-lab/Ultralytics/runs/5wtpe9o7\n\n\nFreezing layer 'model.23.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed \n\n\ntrain: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\n\n\nAutoBatch: Computing optimal batch size for imgsz=1280 at 60.0% CUDA memory utilization.\nAutoBatch: CUDA:0 (NVIDIA A100-SXM4-80GB) 79.25G total, 0.21G reserved, 0.20G allocated, 78.84G free\n\n\n\n\n\n      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n    20903385       287.6         4.488         46.53           nan      (1, 3, 1280, 1280)                    list\n    20903385       575.1        10.364         44.88           nan      (2, 3, 1280, 1280)                    list\n    20903385        1150        19.864         47.99           nan      (4, 3, 1280, 1280)                    list\n    20903385        2300        39.047         75.16           nan      (8, 3, 1280, 1280)                    list\n    20903385        4601        76.993         149.4           nan     (16, 3, 1280, 1280)                    list\nCUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 39.50 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.52 GiB is allocated by PyTorch, and 164.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nCUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 171.50 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.98 GiB is allocated by PyTorch, and 587.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nAutoBatch: Using batch-size 10 for CUDA:0 49.53G/79.25G (62%) \n\n\ntrain: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\nval: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\n\n\nPlotting labels to runs/obb/train5/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.001667, momentum=0.9) with parameter groups 112 weight(decay=0.0), 122 weight(decay=0.00046875), 121 bias(decay=0.0)\nImage sizes 1280 train, 1280 val\nUsing 8 dataloader workers\nLogging results to runs/obb/train5\nStarting training for 51 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/51      21.9G      3.042      5.112      3.885         57       1280: 100%|| 1/1 [00:00&lt;00:00,  1.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.08it/s]\n\n\n                   all         10         20    0.00146        0.2    0.00126   0.000148\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/51      21.9G      2.846      4.908      4.012         58       1280: 100%|| 1/1 [00:00&lt;00:00,  3.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.09it/s]\n\n\n                   all         10         20    0.00106       0.15   0.000779   9.96e-05\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/51      21.9G      3.022      5.258      3.667         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.07it/s]\n\n\n                   all         10         20    0.00103       0.15   0.000764   9.76e-05\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/51      21.9G      2.789      5.113      3.852         58       1280: 100%|| 1/1 [00:00&lt;00:00,  3.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.13it/s]\n\n\n                   all         10         20   0.000675        0.1   0.000516   7.25e-05\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/51        22G      2.526       5.45      3.457         42       1280: 100%|| 1/1 [00:00&lt;00:00,  2.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.13it/s]\n\n\n                   all         10         20     0.0017       0.25    0.00121    0.00022\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/51      22.1G      2.832      4.983      3.874         48       1280: 100%|| 1/1 [00:00&lt;00:00,  3.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.13it/s]\n\n\n                   all         10         20    0.00257       0.35    0.00268   0.000685\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/51      22.1G      2.534      4.688      3.428         55       1280: 100%|| 1/1 [00:00&lt;00:00,  3.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.13it/s]\n\n\n                   all         10         20    0.00417       0.55     0.0161     0.0029\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/51      22.1G      2.091      4.292       3.43         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.06it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.14it/s]\n\n\n                   all         10         20    0.00667        0.6     0.0156    0.00451\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/51      22.1G      2.307      4.644      3.652         35       1280: 100%|| 1/1 [00:00&lt;00:00,  3.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.14it/s]\n\n\n                   all         10         20     0.0105       0.55     0.0608     0.0119\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/51      22.1G        1.8      4.124      2.982         47       1280: 100%|| 1/1 [00:00&lt;00:00,  3.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.19it/s]\n\n\n                   all         10         20     0.0111        0.8     0.0696      0.017\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      11/51      22.1G      1.751      3.753      2.608         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.18it/s]\n\n\n                   all         10         20     0.0111        0.8     0.0696      0.017\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      12/51      22.2G      1.821      3.793      2.822         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.29it/s]\n\n\n                   all         10         20    0.00953          1      0.145     0.0346\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      13/51      22.1G      1.748       3.64      2.721         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.30it/s]\n\n\n                   all         10         20    0.00953          1      0.145     0.0346\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      14/51      22.2G      1.711      3.417      2.728         69       1280: 100%|| 1/1 [00:00&lt;00:00,  3.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.46it/s]\n\n\n                   all         10         20    0.00856          1      0.301      0.107\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      15/51      22.1G      1.443       3.42      3.097         45       1280: 100%|| 1/1 [00:00&lt;00:00,  3.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.45it/s]\n\n\n                   all         10         20    0.00856          1      0.301      0.107\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      16/51      22.2G      1.468      3.528      2.344         38       1280: 100%|| 1/1 [00:00&lt;00:00,  3.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.63it/s]\n\n\n                   all         10         20       0.22       0.65      0.358      0.154\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      17/51      22.1G      1.452      2.938      2.311         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.45it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.60it/s]\n\n\n                   all         10         20       0.22       0.65      0.358      0.154\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      18/51      22.2G      1.321       2.94      3.078         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.07it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.78it/s]\n\n\n                   all         10         20      0.322        0.6      0.393       0.17\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      19/51      22.1G      1.385      2.793      2.518         53       1280: 100%|| 1/1 [00:00&lt;00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.79it/s]\n\n\n                   all         10         20      0.322        0.6      0.393       0.17\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      20/51      22.2G      1.287      2.709      2.337         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.85it/s]\n\n\n                   all         10         20      0.385        0.8       0.54      0.237\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      21/51      22.1G      1.191       2.52      2.295         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.84it/s]\n\n\n                   all         10         20      0.385        0.8       0.54      0.237\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      22/51      22.2G      1.354      2.634      2.612         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.78it/s]\n\n\n                   all         10         20      0.463       0.85      0.708       0.39\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      23/51      22.1G      1.266      2.719       2.22         35       1280: 100%|| 1/1 [00:00&lt;00:00,  3.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.78it/s]\n\n\n                   all         10         20      0.463       0.85      0.708       0.39\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      24/51      22.2G      1.189       2.15      2.245         59       1280: 100%|| 1/1 [00:00&lt;00:00,  3.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.72it/s]\n\n\n                   all         10         20      0.479       0.85      0.729      0.418\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      25/51      22.1G      1.069      2.278      1.992         52       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.73it/s]\n\n\n                   all         10         20      0.479       0.85      0.729      0.418\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      26/51      22.2G       1.42      2.523      2.593         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.59it/s]\n\n\n                   all         10         20      0.711        0.9      0.841       0.52\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      27/51      22.1G      1.175      2.119      1.997         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.60it/s]\n\n\n                   all         10         20      0.711        0.9      0.841       0.52\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      28/51      22.2G      1.173      2.096      2.219         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.53it/s]\n\n\n                   all         10         20      0.856      0.889      0.933      0.623\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      29/51      22.1G      1.173      2.163      2.431         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.52it/s]\n\n\n                   all         10         20      0.856      0.889      0.933      0.623\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      30/51      22.2G      1.196      2.354      1.996         41       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.51it/s]\n\n\n                   all         10         20      0.856      0.889      0.933      0.623\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      31/51      22.2G      1.289      2.233      2.543         49       1280: 100%|| 1/1 [00:00&lt;00:00,  3.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.971       0.89      0.966      0.654\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      32/51      22.1G       1.22      2.208      2.129         51       1280: 100%|| 1/1 [00:00&lt;00:00,  3.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.971       0.89      0.966      0.654\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      33/51      22.2G      1.175      2.047      1.935         45       1280: 100%|| 1/1 [00:00&lt;00:00,  3.50it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.971       0.89      0.966      0.654\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      34/51      22.2G      1.268      1.903      1.914         62       1280: 100%|| 1/1 [00:00&lt;00:00,  3.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.806       0.93      0.952      0.676\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      35/51      22.1G      1.199      2.076       2.07         52       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.806       0.93      0.952      0.676\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      36/51      22.2G      1.139      2.094      2.366         51       1280: 100%|| 1/1 [00:00&lt;00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.806       0.93      0.952      0.676\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      37/51      22.2G      1.141      2.146      1.818         49       1280: 100%|| 1/1 [00:00&lt;00:00,  3.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.768       0.94      0.946      0.671\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      38/51      22.1G      1.095      1.823      1.974         60       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.768       0.94      0.946      0.671\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      39/51      22.2G      1.037      1.773      2.015         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.47it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.768       0.94      0.946      0.671\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      40/51      22.2G      1.049      1.755      1.971         60       1280: 100%|| 1/1 [00:00&lt;00:00,  3.08it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.51it/s]\n\n\n                   all         10         20       0.87       0.95      0.957      0.669\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      41/51      22.1G       1.16      2.066      2.284         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20       0.87       0.95      0.957      0.669\n\n\n\n\n\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      42/51      22.2G      1.142      3.141      1.802         20       1280: 100%|| 1/1 [00:00&lt;00:00,  1.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.46it/s]\n\n\n                   all         10         20       0.87       0.95      0.957      0.669\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      43/51      22.2G      1.076      2.939      1.903         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.45it/s]\n\n\n                   all         10         20      0.905       0.93      0.963      0.677\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      44/51      22.1G      1.062      2.816      1.615         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.51it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.43it/s]\n\n\n                   all         10         20      0.905       0.93      0.963      0.677\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      45/51      22.2G      1.043      2.889      1.868         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.50it/s]\n\n\n                   all         10         20      0.905       0.93      0.963      0.677\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      46/51      22.2G      1.308      3.073      1.914         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.895       0.95      0.957      0.701\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      47/51      22.1G     0.9313      2.941      1.799         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.50it/s]\n\n\n                   all         10         20      0.895       0.95      0.957      0.701\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      48/51      22.2G     0.9851      2.767      1.656         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.48it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.50it/s]\n\n\n                   all         10         20      0.895       0.95      0.957      0.701\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      49/51      22.2G      1.306      2.876      2.089         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.47it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.895       0.95      0.957      0.701\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      50/51      22.2G      1.156      2.817      1.904         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.49it/s]\n\n\n                   all         10         20      0.913      0.999      0.968      0.726\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      51/51      22.1G      1.119      2.846      1.677         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.48it/s]\n\n\n                   all         10         20      0.913      0.999      0.968      0.726\n\n\n\n\n\n\n51 epochs completed in 0.034 hours.\nOptimizer stripped from runs/obb/train5/weights/last.pt, 43.1MB\nOptimizer stripped from runs/obb/train5/weights/best.pt, 43.1MB\n\nValidating runs/obb/train5/weights/best.pt...\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nYOLO11m-obb summary (fused): 322 layers, 20,880,025 parameters, 0 gradients, 71.3 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.51it/s]\n\n\n                   all         10         20      0.913      0.999      0.968      0.734\n             inner_box         10         10      0.909      0.998       0.94      0.592\n             outer_box         10         10      0.918          1      0.995      0.875\nSpeed: 0.4ms preprocess, 6.9ms inference, 0.0ms loss, 54.4ms postprocess per image\nResults saved to runs/obb/train5\n\n\n\n\n\n\nRun history:\n\n\n\nlr/pg0\n\n\n\nlr/pg1\n\n\n\nlr/pg2\n\n\n\nmetrics/mAP50(B)\n\n\n\nmetrics/mAP50-95(B)\n\n\n\nmetrics/precision(B)\n\n\n\nmetrics/recall(B)\n\n\n\nmodel/GFLOPs\n\n\n\nmodel/parameters\n\n\n\nmodel/speed_PyTorch(ms)\n\n\n\ntrain/box_loss\n\n\n\ntrain/cls_loss\n\n\n\ntrain/dfl_loss\n\n\n\nval/box_loss\n\n\n\nval/cls_loss\n\n\n\nval/dfl_loss\n\n\n\n\nRun summary:\n\n\n\nlr/pg0\n2e-05\n\n\nlr/pg1\n2e-05\n\n\nlr/pg2\n2e-05\n\n\nmetrics/mAP50(B)\n0.96773\n\n\nmetrics/mAP50-95(B)\n0.73366\n\n\nmetrics/precision(B)\n0.9135\n\n\nmetrics/recall(B)\n0.99876\n\n\nmodel/GFLOPs\n71.889\n\n\nmodel/parameters\n20903385\n\n\nmodel/speed_PyTorch(ms)\n13.092\n\n\ntrain/box_loss\n1.11869\n\n\ntrain/cls_loss\n2.84641\n\n\ntrain/dfl_loss\n1.6767\n\n\nval/box_loss\n0.93457\n\n\nval/cls_loss\n1.79106\n\n\nval/dfl_loss\n1.45649\n\n\n\n\n\n\n View run train5 at: https://wandb.ai/sustainability-lab/Ultralytics/runs/5wtpe9o7 View project at: https://wandb.ai/sustainability-lab/UltralyticsSynced 4 W&B file(s), 0 media file(s), 5 artifact file(s) and 16 other file(s)\n\n\nFind logs at: ./wandb/run-20250120_153755-5wtpe9o7/logs\n\n\nultralytics.utils.metrics.OBBMetrics object with attributes:\n\nap_class_index: array([0, 1])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x7fe64f199420&gt;\ncurves: []\ncurves_results: []\nfitness: np.float64(0.7570705318296267)\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.59184,     0.87549])\nnames: {0: 'inner_box', 1: 'outer_box'}\nplot: True\nresults_dict: {'metrics/precision(B)': np.float64(0.9134993192895953), 'metrics/recall(B)': np.float64(0.9987569029703861), 'metrics/mAP50(B)': np.float64(0.9677272727272728), 'metrics/mAP50-95(B)': np.float64(0.7336642272854437), 'fitness': np.float64(0.7570705318296267)}\nsave_dir: PosixPath('runs/obb/train5')\nspeed: {'preprocess': 0.35784244537353516, 'inference': 6.9373369216918945, 'loss': 0.0010251998901367188, 'postprocess': 54.367613792419434}\nimport numpy as np\nimport supervision as sv\npred_model = YOLO(\"/home/patel_zeel/blog/lab/runs/obb/train5/weights/best.pt\")\nimport os\nfiles = glob(\"/home/patel_zeel/kiln_compass_24/regions/high_res/19/*.png\")\n# np.random.seed(1)\nrandom_file = np.random.choice(files)\nbase_name = os.path.basename(random_file)\nif base_name in [os.path.basename(file) for file in glob(\"../lab/trench_width/images/*.png\")]:\n    print(\"Part of the training dataset\")\n\nresult = pred_model(random_file, imgsz=1280, verbose=False)[0]\ndetection = sv.Detections.from_ultralytics(result)\n\nimg = Image.open(random_file)\nbox_annotator = sv.OrientedBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_image = box_annotator.annotate(img.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\ndisplay(annotated_image)"
  },
  {
    "objectID": "lab/scratchpad.html#run-seg",
    "href": "lab/scratchpad.html#run-seg",
    "title": "Predict",
    "section": "Run seg",
    "text": "Run seg\n\nmodel = YOLO(\"yolov8m-seg\")\n\n\nmodel.train(data=\"../lab/trench_width/data.yaml\", epochs=51, batch=-1, imgsz=1280, save_period=5)\n\nNew https://pypi.org/project/ultralytics/8.3.64 available  Update with 'pip install -U ultralytics'\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nengine/trainer: task=segment, mode=train, model=yolov8m-seg.pt, data=../lab/trench_width/data.yaml, epochs=51, time=None, patience=100, batch=-1, imgsz=1280, save=True, save_period=5, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n 22        [15, 18, 21]  1   5160182  ultralytics.nn.modules.head.Segment          [2, 32, 192, [192, 384, 576]] \nYOLOv8m-seg summary: 331 layers, 27,240,806 parameters, 27,240,790 gradients, 110.4 GFLOPs\n\nTransferred 531/537 items from pretrained weights\n\n\nTracking run with wandb version 0.18.5\n\n\nRun data is saved locally in /home/patel_zeel/blog/lab/wandb/run-20250120_154343-tvkavy42\n\n\nSyncing run train to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/sustainability-lab/Ultralytics\n\n\n View run at https://wandb.ai/sustainability-lab/Ultralytics/runs/tvkavy42\n\n\nFreezing layer 'model.22.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed \n\n\ntrain: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\n\n\nAutoBatch: Computing optimal batch size for imgsz=1280 at 60.0% CUDA memory utilization.\nAutoBatch: CUDA:0 (NVIDIA A100-SXM4-80GB) 79.25G total, 3.34G reserved, 0.75G allocated, 75.16G free\n\n\n\n\n\n      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n    27240806       441.6         4.140         29.42           nan      (1, 3, 1280, 1280)                    list\n    27240806       883.2         9.200         36.04           nan      (2, 3, 1280, 1280)                    list\n    27240806        1766        17.132         41.55           nan      (4, 3, 1280, 1280)                    list\n    27240806        3533        33.320          73.9           nan      (8, 3, 1280, 1280)                    list\n    27240806        7065        64.274         142.8           nan     (16, 3, 1280, 1280)                    list\n    27240806   1.413e+04       127.767         281.6           nan     (32, 3, 1280, 1280)                    list\nCUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 167.50 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.43 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nAutoBatch: Using batch-size 11 for CUDA:0 49.63G/79.25G (63%) \n\n\ntrain: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\nval: Scanning /home/patel_zeel/blog/lab/trench_width/labels.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|| 10/10 [00:00&lt;?, ?it/s]\n\n\nPlotting labels to runs/segment/train/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.001667, momentum=0.9) with parameter groups 86 weight(decay=0.0), 97 weight(decay=0.000515625), 96 bias(decay=0.0)\nImage sizes 1280 train, 1280 val\nUsing 8 dataloader workers\nLogging results to runs/segment/train\nStarting training for 51 epochs...\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/51      19.4G      2.736      7.179      6.442      2.486         57       1280: 100%|| 1/1 [00:01&lt;00:00,  1.02s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.53it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/51      19.4G      2.783      6.387      6.172      2.612         59       1280: 100%|| 1/1 [00:00&lt;00:00,  2.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.37it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/51      19.4G      2.929      7.076      6.595      2.577         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.19it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/51      19.4G      2.726      7.228      6.208      2.532         58       1280: 100%|| 1/1 [00:00&lt;00:00,  3.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  4.10it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/51      19.3G      2.499      6.182      7.267      2.396         42       1280: 100%|| 1/1 [00:00&lt;00:00,  3.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.71it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/51      19.3G      3.034      8.223      7.171      2.728         48       1280: 100%|| 1/1 [00:00&lt;00:00,  2.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.97it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/51      19.4G       3.12      7.957      7.105      2.859         55       1280: 100%|| 1/1 [00:00&lt;00:00,  2.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  4.12it/s]\n\n\n                   all         10         20          0          0          0          0          0          0          0          0\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/51      19.4G      2.931      7.516      6.602      2.776         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.57it/s]\n\n\n                   all         10         20      0.696       0.15      0.145     0.0523      0.617        0.2     0.0585     0.0102\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/51      19.4G      2.629      5.092      6.662      2.429         35       1280: 100%|| 1/1 [00:00&lt;00:00,  2.88it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.32it/s]\n\n\n                   all         10         20       0.13       0.25       0.16     0.0768      0.132        0.1      0.154     0.0326\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/51      19.5G      1.897      4.349      4.317      1.925         48       1280: 100%|| 1/1 [00:00&lt;00:00,  3.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.21it/s]\n\n\n                   all         10         20      0.452      0.351       0.39      0.175      0.172       0.35       0.25     0.0854\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      11/51      19.6G      1.887      4.137      3.486      1.877         54       1280: 100%|| 1/1 [00:00&lt;00:00,  2.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.67it/s]\n\n\n                   all         10         20        0.8       0.15      0.357      0.219      0.105      0.614      0.284     0.0629\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      12/51      19.6G       1.43      3.731      3.628      1.507         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.44it/s]\n\n\n                   all         10         20        0.8       0.15      0.357      0.219      0.105      0.614      0.284     0.0629\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      13/51      19.7G      1.559      3.545       3.55      1.526         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.87it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.00it/s]\n\n\n                   all         10         20      0.904        0.3        0.5      0.193      0.904        0.3       0.43      0.108\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      14/51      19.7G       1.39      2.989      2.648      1.478         69       1280: 100%|| 1/1 [00:00&lt;00:00,  3.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.83it/s]\n\n\n                   all         10         20      0.904        0.3        0.5      0.193      0.904        0.3       0.43      0.108\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      15/51      19.6G      1.729      2.974      3.255      1.676         45       1280: 100%|| 1/1 [00:00&lt;00:00,  2.87it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.31it/s]\n\n\n                   all         10         20      0.455      0.727      0.398      0.158      0.451      0.677      0.349      0.137\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      16/51      19.4G      1.711      2.639      3.226      1.627         38       1280: 100%|| 1/1 [00:00&lt;00:00,  3.28it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.32it/s]\n\n\n                   all         10         20      0.455      0.727      0.398      0.158      0.451      0.677      0.349      0.137\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      17/51      19.7G      1.329      2.381      2.383      1.373         54       1280: 100%|| 1/1 [00:00&lt;00:00,  2.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.26it/s]\n\n\n                   all         10         20      0.284       0.75      0.629      0.298      0.299        0.8      0.649      0.261\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      18/51      19.4G      1.228      1.977      2.472      1.299         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.18it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.22it/s]\n\n\n                   all         10         20      0.284       0.75      0.629      0.298      0.299        0.8      0.649      0.261\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      19/51      19.7G      1.415      2.453      2.654      1.415         54       1280: 100%|| 1/1 [00:00&lt;00:00,  2.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.27it/s]\n\n\n                   all         10         20      0.792       0.35      0.358      0.212      0.792       0.35      0.346      0.179\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      20/51      19.5G      1.282      1.927      2.378      1.391         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.41it/s]\n\n\n                   all         10         20      0.792       0.35      0.358      0.212      0.792       0.35      0.346      0.179\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      21/51      19.7G      1.192      1.933       2.26       1.32         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.64it/s]\n\n\n                   all         10         20      0.474        0.5      0.538      0.356      0.474        0.5      0.539      0.285\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      22/51      19.6G      1.181      1.726      2.088      1.278         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  1.61it/s]\n\n\n                   all         10         20      0.474        0.5      0.538      0.356      0.474        0.5      0.539      0.285\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      23/51      19.4G        1.3      2.048      2.568      1.359         35       1280: 100%|| 1/1 [00:00&lt;00:00,  2.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.50it/s]\n\n\n                   all         10         20      0.778        0.6      0.659      0.352      0.722       0.55      0.637      0.374\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      24/51      19.6G       1.12      1.541      1.564      1.297         60       1280: 100%|| 1/1 [00:00&lt;00:00,  3.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  2.32it/s]\n\n\n                   all         10         20      0.778        0.6      0.659      0.352      0.722       0.55      0.637      0.374\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      25/51      19.6G      1.043      1.471      1.694      1.228         52       1280: 100%|| 1/1 [00:00&lt;00:00,  2.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.49it/s]\n\n\n                   all         10         20      0.415        0.7      0.443      0.248      0.436       0.75      0.445      0.307\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      26/51      19.5G      1.355       1.56      1.975       1.43         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  3.66it/s]\n\n\n                   all         10         20      0.415        0.7      0.443      0.248      0.436       0.75      0.445      0.307\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      27/51      19.7G       1.02      1.483      1.475      1.218         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.42it/s]\n\n\n                   all         10         20      0.771       0.55      0.749      0.416      0.771       0.55       0.76       0.51\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      28/51      19.6G      1.079      1.262      1.616      1.309         56       1280: 100%|| 1/1 [00:00&lt;00:00,  3.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.32it/s]\n\n\n                   all         10         20      0.771       0.55      0.749      0.416      0.771       0.55       0.76       0.51\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      29/51      19.7G      1.093      1.497      1.691      1.302         56       1280: 100%|| 1/1 [00:00&lt;00:00,  2.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.35it/s]\n\n\n                   all         10         20      0.774       0.75      0.799      0.436      0.774       0.75      0.788      0.514\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      30/51      19.5G      1.135      1.419      1.848      1.339         42       1280: 100%|| 1/1 [00:00&lt;00:00,  3.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.87it/s]\n\n\n                   all         10         20      0.774       0.75      0.799      0.436      0.774       0.75      0.788      0.514\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      31/51      19.6G      1.042      1.496      1.698      1.272         50       1280: 100%|| 1/1 [00:00&lt;00:00,  2.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.63it/s]\n\n\n                   all         10         20      0.632        0.7      0.652      0.429      0.663       0.75      0.698      0.501\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      32/51      19.6G       1.05      1.488      1.513      1.214         52       1280: 100%|| 1/1 [00:00&lt;00:00,  3.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.62it/s]\n\n\n                   all         10         20      0.632        0.7      0.652      0.429      0.663       0.75      0.698      0.501\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      33/51      19.6G      1.043      1.309       1.44      1.269         45       1280: 100%|| 1/1 [00:00&lt;00:00,  3.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.10it/s]\n\n\n                   all         10         20      0.632        0.7      0.652      0.429      0.663       0.75      0.698      0.501\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      34/51      19.7G     0.9872      1.389      1.329      1.191         62       1280: 100%|| 1/1 [00:00&lt;00:00,  2.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.38it/s]\n\n\n                   all         10         20      0.646        0.7      0.724      0.512      0.713        0.8      0.763       0.58\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      35/51      19.6G      1.019      1.202      1.568      1.191         52       1280: 100%|| 1/1 [00:00&lt;00:00,  3.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.40it/s]\n\n\n                   all         10         20      0.646        0.7      0.724      0.512      0.713        0.8      0.763       0.58\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      36/51      19.6G      1.029       1.35      1.522      1.232         51       1280: 100%|| 1/1 [00:00&lt;00:00,  3.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.22it/s]\n\n\n                   all         10         20      0.646        0.7      0.724      0.512      0.713        0.8      0.763       0.58\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      37/51      19.6G      1.056      1.289      1.494      1.221         50       1280: 100%|| 1/1 [00:00&lt;00:00,  2.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.64it/s]\n\n\n                   all         10         20      0.635       0.85      0.803        0.6       0.69        0.9      0.846      0.657\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      38/51      19.6G     0.8953      1.298       1.12      1.068         60       1280: 100%|| 1/1 [00:00&lt;00:00,  3.19it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.65it/s]\n\n\n                   all         10         20      0.635       0.85      0.803        0.6       0.69        0.9      0.846      0.657\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      39/51      19.6G     0.8898      1.057      1.206      1.153         54       1280: 100%|| 1/1 [00:00&lt;00:00,  3.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.70it/s]\n\n\n                   all         10         20      0.635       0.85      0.803        0.6       0.69        0.9      0.846      0.657\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      40/51      19.7G     0.9538      1.113      1.201      1.206         60       1280: 100%|| 1/1 [00:00&lt;00:00,  2.89it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.69it/s]\n\n\n                   all         10         20      0.736        0.8       0.85      0.621      0.812        0.9      0.919      0.683\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      41/51      19.5G     0.9001      1.213      1.314      1.171         46       1280: 100%|| 1/1 [00:00&lt;00:00,  3.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.36it/s]\n\n\n                   all         10         20      0.736        0.8       0.85      0.621      0.812        0.9      0.919      0.683\n\n\n\n\n\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      42/51      19.3G      1.192      1.432      2.694      1.399         20       1280: 100%|| 1/1 [00:00&lt;00:00,  1.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  4.71it/s]\n\n\n                   all         10         20      0.736        0.8       0.85      0.621      0.812        0.9      0.919      0.683\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      43/51      19.3G      1.034      1.252      2.616      1.418         20       1280: 100%|| 1/1 [00:00&lt;00:00,  2.93it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  4.62it/s]\n\n\n                   all         10         20      0.814       0.85      0.876      0.651      0.866        0.9      0.931      0.702\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      44/51      19.2G     0.9355      1.308       2.01       1.27         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.24it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  4.74it/s]\n\n\n                   all         10         20      0.814       0.85      0.876      0.651      0.866        0.9      0.931      0.702\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      45/51      19.3G     0.9141      1.258      2.125      1.412         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.62it/s]\n\n\n                   all         10         20      0.814       0.85      0.876      0.651      0.866        0.9      0.931      0.702\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      46/51      19.3G      1.256      1.421      2.302      1.435         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.75it/s]\n\n\n                   all         10         20      0.782      0.829       0.84        0.6      0.804      0.819      0.881      0.687\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      47/51      19.2G     0.8384      1.046      1.981      1.267         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.27it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.70it/s]\n\n\n                   all         10         20      0.782      0.829       0.84        0.6      0.804      0.819      0.881      0.687\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      48/51      19.3G      1.146      1.197      2.063      1.319         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.25it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.71it/s]\n\n\n                   all         10         20      0.782      0.829       0.84        0.6      0.804      0.819      0.881      0.687\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      49/51      19.3G      1.144      1.099      2.118      1.411         20       1280: 100%|| 1/1 [00:00&lt;00:00,  2.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.79it/s]\n\n\n                   all         10         20      0.811      0.776      0.909      0.675      0.797      0.879      0.926      0.728\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      50/51      19.2G      1.022      1.027      1.993      1.325         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.60it/s]\n\n\n                   all         10         20      0.811      0.776      0.909      0.675      0.797      0.879      0.926      0.728\n\n      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      51/51      19.3G      1.167      1.066        2.1      1.344         20       1280: 100%|| 1/1 [00:00&lt;00:00,  3.12it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.51it/s]\n\n\n                   all         10         20      0.811      0.776      0.909      0.675      0.797      0.879      0.926      0.728\n\n\n\n\n\n\n51 epochs completed in 0.031 hours.\nOptimizer stripped from runs/segment/train/weights/last.pt, 55.0MB\nOptimizer stripped from runs/segment/train/weights/best.pt, 55.0MB\n\nValidating runs/segment/train/weights/best.pt...\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nYOLOv8m-seg summary (fused): 245 layers, 27,223,542 parameters, 0 gradients, 110.0 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00&lt;00:00,  5.44it/s]\n\n\n                   all         10         20      0.811      0.776      0.909      0.675      0.797      0.879      0.926      0.727\n             inner_box         10         10      0.622        0.7      0.823      0.508      0.594        0.8      0.857      0.678\n             outer_box         10         10          1      0.852      0.995      0.842          1      0.959      0.995      0.776\nSpeed: 0.4ms preprocess, 6.9ms inference, 0.0ms loss, 3.5ms postprocess per image\nResults saved to runs/segment/train\n\n\n\n\n\n\nRun history:\n\n\n\nlr/pg0\n\n\n\nlr/pg1\n\n\n\nlr/pg2\n\n\n\nmetrics/mAP50(B)\n\n\n\nmetrics/mAP50(M)\n\n\n\nmetrics/mAP50-95(B)\n\n\n\nmetrics/mAP50-95(M)\n\n\n\nmetrics/precision(B)\n\n\n\nmetrics/precision(M)\n\n\n\nmetrics/recall(B)\n\n\n\nmetrics/recall(M)\n\n\n\nmodel/GFLOPs\n\n\n\nmodel/parameters\n\n\n\nmodel/speed_PyTorch(ms)\n\n\n\ntrain/box_loss\n\n\n\ntrain/cls_loss\n\n\n\ntrain/dfl_loss\n\n\n\ntrain/seg_loss\n\n\n\nval/box_loss\n\n\n\nval/cls_loss\n\n\n\nval/dfl_loss\n\n\n\nval/seg_loss\n\n\n\n\nRun summary:\n\n\n\nlr/pg0\n2e-05\n\n\nlr/pg1\n2e-05\n\n\nlr/pg2\n2e-05\n\n\nmetrics/mAP50(B)\n0.90923\n\n\nmetrics/mAP50(M)\n0.92577\n\n\nmetrics/mAP50-95(B)\n0.67538\n\n\nmetrics/mAP50-95(M)\n0.72695\n\n\nmetrics/precision(B)\n0.81082\n\n\nmetrics/precision(M)\n0.79694\n\n\nmetrics/recall(B)\n0.77614\n\n\nmetrics/recall(M)\n0.87927\n\n\nmodel/GFLOPs\n110.395\n\n\nmodel/parameters\n27240806\n\n\nmodel/speed_PyTorch(ms)\n13.361\n\n\ntrain/box_loss\n1.16748\n\n\ntrain/cls_loss\n2.09975\n\n\ntrain/dfl_loss\n1.34438\n\n\ntrain/seg_loss\n1.0662\n\n\nval/box_loss\n1.02605\n\n\nval/cls_loss\n2.05508\n\n\nval/dfl_loss\n1.23359\n\n\nval/seg_loss\n1.12731\n\n\n\n\n\n\n View run train at: https://wandb.ai/sustainability-lab/Ultralytics/runs/tvkavy42 View project at: https://wandb.ai/sustainability-lab/UltralyticsSynced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 28 other file(s)\n\n\nFind logs at: ./wandb/run-20250120_154343-tvkavy42/logs\n\n\nultralytics.utils.metrics.SegmentMetrics object with attributes:\n\nap_class_index: array([0, 1])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x7fe84e12c040&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)', 'Precision-Recall(M)', 'F1-Confidence(M)', 'Precision-Confidence(M)', 'Recall-Confidence(M)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.38462,     0.38462,           0],\n       [          1,           1,           1, ...,           1,           1,           0]], shape=(2, 1000)), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.07326,     0.07326,     0.07326, ...,     0.33565,           0,           0],\n       [  0.0072807,   0.0072807,   0.0072807, ...,     0.30149,      0.2242,           0]], shape=(2, 1000)), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.038023,    0.038023,    0.038023, ...,           1,           1,           1],\n       [  0.0036536,   0.0036536,   0.0036536, ...,           1,           1,           1]], shape=(2, 1000)), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.20167,           0,           0],\n       [          1,           1,           1, ...,      0.1775,     0.12625,           0]], shape=(2, 1000)), 'Confidence', 'Recall'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,         0.5,         0.5,           0],\n       [          1,           1,           1, ...,           1,           1,           0]], shape=(2, 1000)), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.07326,     0.07326,     0.07326, ...,     0.33565,           0,           0],\n       [  0.0072807,   0.0072807,   0.0072807, ...,     0.30149,      0.2242,           0]], shape=(2, 1000)), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.038023,    0.038023,    0.038023, ...,           1,           1,           1],\n       [  0.0036536,   0.0036536,   0.0036536, ...,           1,           1,           1]], shape=(2, 1000)), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,     0.20167,           0,           0],\n       [          1,           1,           1, ...,      0.1775,     0.12625,           0]], shape=(2, 1000)), 'Confidence', 'Recall']]\nfitness: np.float64(1.4455970167060048)\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'metrics/precision(M)', 'metrics/recall(M)', 'metrics/mAP50(M)', 'metrics/mAP50-95(M)']\nmaps: array([     1.1864,      1.6182])\nnames: {0: 'inner_box', 1: 'outer_box'}\nplot: True\nresults_dict: {'metrics/precision(B)': np.float64(0.8108231337398004), 'metrics/recall(B)': np.float64(0.7761444955593891), 'metrics/mAP50(B)': np.float64(0.9092307692307693), 'metrics/mAP50-95(B)': np.float64(0.6753775662745483), 'metrics/precision(M)': np.float64(0.7969436046359123), 'metrics/recall(M)': np.float64(0.8792737357610776), 'metrics/mAP50(M)': np.float64(0.9257692307692309), 'metrics/mAP50-95(M)': np.float64(0.726952452287679), 'fitness': np.float64(1.4455970167060048)}\nsave_dir: PosixPath('runs/segment/train')\nseg: ultralytics.utils.metrics.Metric object\nspeed: {'preprocess': 0.447845458984375, 'inference': 6.885099411010742, 'loss': 0.0012874603271484375, 'postprocess': 3.5344362258911133}\ntask: 'segment'"
  },
  {
    "objectID": "lab/scratchpad.html#predict",
    "href": "lab/scratchpad.html#predict",
    "title": "Predict",
    "section": "Predict",
    "text": "Predict\n\nimport numpy as np\nimport supervision as sv\npred_model = YOLO(\"/home/patel_zeel/blog/lab/runs/segment/train/weights/best.pt\")\n\n\nimport os\nfiles = glob(\"/home/patel_zeel/kiln_compass_24/regions/high_res/19/*.png\")\n# np.random.seed(1)\nrandom_file = np.random.choice(files)\nbase_name = os.path.basename(random_file)\nif base_name in [os.path.basename(file) for file in glob(\"../lab/trench_width/images/*.png\")]:\n    print(\"Part of the training dataset\")\n\nresult = pred_model(random_file, imgsz=1280, verbose=False)[0]\ndetection = sv.Detections.from_ultralytics(result)\n\nimg = Image.open(random_file)\nbox_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_image = box_annotator.annotate(img.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\ndisplay(annotated_image)"
  },
  {
    "objectID": "posts/PurpleAir.html",
    "href": "posts/PurpleAir.html",
    "title": "Download low-cost data from OpenAQ",
    "section": "",
    "text": "import requests\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom glob import glob\n\nfrom geopy.geocoders import Nominatim\n\nOpenAQ has an aws s3 bucket that contains all the data to download for free. This is a guide to download the data from the bucket. If you have enough space and bandwidth, aws s3 commands are the fastest way to download the data. If you dont have enough space/bandwidth or you want to download specific data only then follow along.\nAcknowledgement: Much help is taken from ChatGPT for some complex Linux commands.\nWe will mostly use the following commands:\naws s3 ls\naws s3 cp\nScenario 1: We want to download PurpleAir sensors data for Delhi for entire 2022. I am taking Delhis example since there are far lesser sensors in Delhi than in the US. So, it will be easier for this demo.\nSome statistics that I have calculated for PurpleAir sensors in US are as follows:\n\n\n\nCountry\nNumber of sensors\nTotal size\nyears\n\n\n\n\nUSA\n22497\n90.945 GB\n2018, 2019, 2020, 2021, 2022, 2023\n\n\n\nLets see how to calculate these statistics for India.\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ &gt; /tmp/in.txt\n\nThe output of above command contains all sensor IDs within India. These sensor IDs are assigned by OpenAQ and they are different from PurpleAir sensor_index. The output looks like the following.\n\nwith open('/tmp/in.txt', 'r') as f:\n    lines = [line.strip() for line in f.readlines()]\n  \nprint(f\"Number of locations = {len(lines)}\\n\")\nprint(*lines[:3], sep='\\n')\n\nNumber of locations = 623\n\nPRE locationid=160485/\nPRE locationid=218334/\nPRE locationid=218336/\n\n\nCounting total size of files:\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ --recursive &gt; /tmp/in_files.txt\n\n\n!awk '{ sum += $3 } END { print (sum/1024/1024/1024) \" GB\"}' /tmp/in_files.txt\n\n1.20947 GB\n\n\nNumber of years:\n\n!cat /tmp/in_files.txt | grep -oP 'year=\\K\\w+' | sort | uniq\n\n2019\n2020\n2021\n2022\n2023\n\n\nLets find out which sensors among these belong to Delhi.\n\n!cat /tmp/in.txt | grep -oP 'locationid=\\K\\w+' | sort | uniq &gt; /tmp/in_locations.txt\n\nNow we use OpenAQ REST API. It has limit of 300 requests per 5 minutes. After the limit exceeds, it will return an error.\n\nurl = 'https://api.openaq.org/v2/locations'\nparams = {\n    'limit': 1000,\n    'country_id': 'IN',\n    \"modelName\": \"PurpleAir Sensor\"\n}\n\n# Request headers\nheaders = {\n    'accept': 'application/json'\n}\n\nresponse = requests.get(url, params=params, headers=headers)\n\nif response.status_code == 200:\n    data = response.json()\nelse:\n    print(response.status_code, response.reason)\n\n500 Internal Server Error\n\n\n\ndf = pd.DataFrame(data['results'])\ndf.shape\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df = pd.DataFrame(data['results'])\n      2 df.shape\n\nNameError: name 'data' is not defined\n\n\n\nThe following method works but takes too much time since it is online:\n\ndef get_city_name(coords):\n    latitude = coords[\"latitude\"]\n    longitude = coords[\"longitude\"]\n    geolocator = Nominatim(user_agent=\"myGeocoder\")  # Replace \"myGeocoder\" with your desired user agent\n    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n\n    if location:\n        address = location.raw.get('address', {})\n        city = address.get('city', '')\n        return city\n\n    return None\n\ndf[\"city\"] = df[\"coordinates\"].apply(get_city_name)\ndf\n\n\n\n\n\n\n\n\nid\ncity\nname\nentity\ncountry\nsources\nisMobile\nisAnalysis\nparameters\nsensorType\ncoordinates\nlastUpdated\nfirstUpdated\nmeasurements\nbounds\nmanufacturers\n\n\n\n\n0\n318146\nGangtok\nNASA_AQCS_201_cpa\nNone\nIN\nNone\nFalse\nNone\n[{'id': 135, 'unit': 'particles/cm', 'count':...\nNone\n{'latitude': 27.31013, 'longitude': 88.59687}\n2023-07-26T02:34:05+00:00\n2022-04-23T07:42:24+00:00\n1168071\n[88.59687, 27.31013, 88.59687, 27.31013]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n1\n220706\nGangtok\nNASA_AQCS_139\nNone\nIN\nNone\nFalse\nNone\n[{'id': 100, 'unit': 'c', 'count': 28600, 'ave...\nNone\n{'latitude': 27.310116, 'longitude': 88.59682}\n2023-07-26T02:34:04+00:00\n2021-02-17T09:56:06+00:00\n2205110\n[88.59682, 27.310116, 88.59682, 27.310116]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n2\n66673\nHisar\nNASA_AQCS_160\nNone\nIN\nNone\nFalse\nNone\n[{'id': 132, 'unit': 'mb', 'count': 28651, 'av...\nNone\n{'latitude': 29.146254, 'longitude': 75.72236}\n2023-07-26T02:33:56+00:00\n2021-01-19T23:59:16+00:00\n2396934\n[75.72236, 29.146254, 75.72236, 29.146254]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n3\n72977\nBengaluru\nUT Sensor 101\nNone\nIN\nNone\nFalse\nNone\n[{'id': 126, 'unit': 'particles/cm', 'count':...\nNone\n{'latitude': 13.045313, 'longitude': 77.573395}\n2023-07-26T02:33:55+00:00\n2021-01-14T01:18:23+00:00\n2498544\n[77.573395, 13.045313, 77.573395, 13.045313]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n4\n235916\nBengaluru\nUW Sensor 311\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm', 'count':...\nNone\n{'latitude': 13.048528, 'longitude': 77.582275}\n2023-07-26T02:33:49+00:00\n2021-09-16T12:29:52+00:00\n1773924\n[77.582275, 13.048528, 77.582275, 13.048528]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n604\n73922\nNew Delhi District\nUS Embassy A\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'g/m', 'count': 347, 'ave...\nNone\n{'latitude': 28.5979, 'longitude': 77.1847}\n2021-02-05T18:25:34+00:00\n2021-01-08T12:12:29+00:00\n2082\n[77.1847, 28.5979, 77.1847, 28.5979]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n605\n73924\nNew Delhi District\nUS Embassy B\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'g/m', 'count': 347, 'ave...\nNone\n{'latitude': 28.5982, 'longitude': 77.1837}\n2021-02-05T18:23:59+00:00\n2021-01-08T12:11:29+00:00\n2082\n[77.1837, 28.5982, 77.1837, 28.5982]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n606\n219370\nBhubaneswar Municipal Corporation\nBhubaneswar India\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'g/m', 'count': 180, 'ave...\nNone\n{'latitude': 20.2853, 'longitude': 85.7685}\n2021-02-03T10:18:41+00:00\n2021-02-03T03:02:39+00:00\n1080\n[85.7685, 20.2853, 85.7685, 20.2853]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n607\n71465\nGurugram District\nNASA_AQCS_152\nNone\nIN\nNone\nFalse\nNone\n[{'id': 1, 'unit': 'g/m', 'count': 1, 'avera...\nNone\n{'latitude': 28.4522, 'longitude': 77.0949}\n2021-01-14T01:18:54+00:00\n2021-01-14T01:18:54+00:00\n6\n[77.0949, 28.4522, 77.0949, 28.4522]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n608\n221974\nBengaluru\nUT sensor\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm', 'count':...\nNone\n{'latitude': 13.0449, 'longitude': 77.5788}\n2019-12-17T10:32:35+00:00\n2019-12-17T10:32:35+00:00\n6\n[77.5788, 13.0449, 77.5788, 13.0449]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n\n\n609 rows  16 columns\n\n\n\nNow, we use another method of shapefile to do this:\n\n!wget --no-check-certificate \"https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\" -O /tmp/delhi.zip\n!unzip -o /tmp/delhi.zip -d /tmp/delhi\n\n--2023-07-26 08:37:34--  https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\nResolving groups.google.com (groups.google.com)... 216.239.32.177, 216.239.36.177, 216.239.38.177, ...\nConnecting to groups.google.com (groups.google.com)|216.239.32.177|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0 [following]\n--2023-07-26 08:37:35--  https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0\nResolving 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)... 142.251.10.137, 2404:6800:4003:c0f::89\nConnecting to 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)|142.251.10.137|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/zip]\nSaving to: /tmp/delhi.zip\n\n/tmp/delhi.zip          [ &lt;=&gt;                ]  16.42K  --.-KB/s    in 0.04s   \n\n2023-07-26 08:37:37 (429 KB/s) - /tmp/delhi.zip saved [16812]\n\nArchive:  /tmp/delhi.zip\n  inflating: /tmp/delhi/Delhi.kml    \n  inflating: /tmp/delhi/Districts.dbf  \n  inflating: /tmp/delhi/Districts.prj  \n  inflating: /tmp/delhi/Districts.qpj  \n  inflating: /tmp/delhi/Districts.shp  \n  inflating: /tmp/delhi/Districts.shx  \n\n\n\ngdf = gpd.read_file('/tmp/delhi/Districts.shp')\ngdf.plot(color=\"none\", edgecolor=\"black\");\n\n\n\n\n\n\n\n\n\n# check if a point is within Delhi\ndef is_within_delhi(coords):\n    point = Point(coords[\"longitude\"], coords[\"latitude\"])\n    for i, row in gdf.iterrows():\n        if row.geometry.contains(point):\n            return True\n    return False\n\ndf[\"is_within_delhi\"] = df[\"coordinates\"].apply(is_within_delhi)\n\n\ndelhi_df = df[df[\"is_within_delhi\"]]\ndelhi_df.shape\n\n(311, 17)\n\n\n\ndelhi_df.city.value_counts()\n\n                      194\nNew Delhi District    112\nDwarka                  3\nGhaziabad               2\nName: city, dtype: int64\n\n\nSeems like many points were not detected by the online geopy encoder.\nNow, we know that out of 623, 311 sensors belong to Delhi. Lets download the data for these sensors. For illustration, I will download data for 3 sensors for year 2022 and month of Jan.\n\n# dump delhi_df.id to a file\ndelhi_df.id.to_csv('/tmp/delhi_locations.txt', index=False, header=False)\n!head -n3 /tmp/delhi_locations.txt\n\n274208\n221227\n273205\n\n\n\n!head -n3 /tmp/delhi_locations.txt &gt; /tmp/delhi_locations_3.txt\n!while read -r sensor_id; do aws s3 --no-sign-request cp s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=$sensor_id/year=2022/month=01 /tmp/delhi_data --recursive; done &lt; /tmp/delhi_locations_3.txt\n\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220107.csv.gz to ../../../../tmp/delhi_data/location-274208-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220120.csv.gz to ../../../../tmp/delhi_data/location-274208-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220131.csv.gz to ../../../../tmp/delhi_data/location-274208-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220122.csv.gz to ../../../../tmp/delhi_data/location-274208-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220130.csv.gz to ../../../../tmp/delhi_data/location-274208-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220129.csv.gz to ../../../../tmp/delhi_data/location-274208-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220114.csv.gz to ../../../../tmp/delhi_data/location-274208-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220123.csv.gz to ../../../../tmp/delhi_data/location-274208-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220113.csv.gz to ../../../../tmp/delhi_data/location-274208-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220121.csv.gz to ../../../../tmp/delhi_data/location-274208-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220107.csv.gz to ../../../../tmp/delhi_data/location-221227-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220104.csv.gz to ../../../../tmp/delhi_data/location-221227-20220104.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220115.csv.gz to ../../../../tmp/delhi_data/location-221227-20220115.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220105.csv.gz to ../../../../tmp/delhi_data/location-221227-20220105.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220108.csv.gz to ../../../../tmp/delhi_data/location-221227-20220108.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220106.csv.gz to ../../../../tmp/delhi_data/location-221227-20220106.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220102.csv.gz to ../../../../tmp/delhi_data/location-221227-20220102.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220117.csv.gz to ../../../../tmp/delhi_data/location-221227-20220117.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220118.csv.gz to ../../../../tmp/delhi_data/location-221227-20220118.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220110.csv.gz to ../../../../tmp/delhi_data/location-221227-20220110.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220131.csv.gz to ../../../../tmp/delhi_data/location-221227-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220113.csv.gz to ../../../../tmp/delhi_data/location-221227-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220119.csv.gz to ../../../../tmp/delhi_data/location-221227-20220119.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220121.csv.gz to ../../../../tmp/delhi_data/location-221227-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220123.csv.gz to ../../../../tmp/delhi_data/location-221227-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220116.csv.gz to ../../../../tmp/delhi_data/location-221227-20220116.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220128.csv.gz to ../../../../tmp/delhi_data/location-221227-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220122.csv.gz to ../../../../tmp/delhi_data/location-221227-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220101.csv.gz to ../../../../tmp/delhi_data/location-221227-20220101.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220111.csv.gz to ../../../../tmp/delhi_data/location-221227-20220111.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220120.csv.gz to ../../../../tmp/delhi_data/location-221227-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220109.csv.gz to ../../../../tmp/delhi_data/location-221227-20220109.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220129.csv.gz to ../../../../tmp/delhi_data/location-221227-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220103.csv.gz to ../../../../tmp/delhi_data/location-221227-20220103.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220112.csv.gz to ../../../../tmp/delhi_data/location-221227-20220112.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220114.csv.gz to ../../../../tmp/delhi_data/location-221227-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220130.csv.gz to ../../../../tmp/delhi_data/location-221227-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220120.csv.gz to ../../../../tmp/delhi_data/location-273205-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220124.csv.gz to ../../../../tmp/delhi_data/location-273205-20220124.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220107.csv.gz to ../../../../tmp/delhi_data/location-273205-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220121.csv.gz to ../../../../tmp/delhi_data/location-273205-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220126.csv.gz to ../../../../tmp/delhi_data/location-273205-20220126.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220114.csv.gz to ../../../../tmp/delhi_data/location-273205-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220129.csv.gz to ../../../../tmp/delhi_data/location-273205-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220130.csv.gz to ../../../../tmp/delhi_data/location-273205-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220113.csv.gz to ../../../../tmp/delhi_data/location-273205-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220123.csv.gz to ../../../../tmp/delhi_data/location-273205-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220122.csv.gz to ../../../../tmp/delhi_data/location-273205-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220131.csv.gz to ../../../../tmp/delhi_data/location-273205-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220128.csv.gz to ../../../../tmp/delhi_data/location-273205-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220125.csv.gz to ../../../../tmp/delhi_data/location-273205-20220125.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220127.csv.gz to ../../../../tmp/delhi_data/location-273205-20220127.csv.gz\n\n\nVerify if we got all the sensors data as we needed.\n\n!ls /tmp/delhi_data/location-221227* | wc -l\n!ls /tmp/delhi_data/location-274208* | wc -l\n!ls /tmp/delhi_data/location-273205* | wc -l\n\n27\n10\n15\n\n\n\nsensor_df = pd.read_csv('/tmp/delhi_data/location-274208-20220107.csv.gz')\nsensor_df.parameter.value_counts()\n\npm10     64\npm25     64\npm1      64\num010    64\num025    64\num100    64\nName: parameter, dtype: int64"
  },
  {
    "objectID": "posts/climate-modeling-with-siren.html",
    "href": "posts/climate-modeling-with-siren.html",
    "title": "Climate Modeling with SIRENs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\nimport numpy as np\nimport xarray as xr\nfrom tqdm.keras import TqdmCallback\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, initializers, activations\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\n\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-07-18 05:13:52.439735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-18 05:13:53.232689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    first_init = lambda input_dim: initializers.RandomUniform(-1 / input_dim, 1 / input_dim)\n    other_init = lambda input_dim: initializers.RandomUniform(-np.sqrt(6 / input_dim) / activation_scale, np.sqrt(6 / input_dim) / activation_scale)\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=first_init(input_dim), activation=lambda x: tf.sin(activation_scale*x)))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=other_init(features[i-1]), activation=lambda x: tf.sin(activation_scale*x)))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=other_init(features[-1]), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../../super_res/data/era5_low_res/2m_temperature/2m_temperature_2018_5.625deg.nc\")\ndata1 = xr.open_dataset(\"../../super_res/data/era5_high_res/2m_temperature/2m_temperature_2018_1.40625deg.nc\")\n\n\ndata5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lon: 64, lat: 32, time: 8760)\nCoordinates:\n  * lon      (lon) float64 0.0 5.625 11.25 16.88 ... 337.5 343.1 348.8 354.4\n  * lat      (lat) float64 -87.19 -81.56 -75.94 -70.31 ... 75.94 81.56 87.19\n  * time     (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00\nData variables:\n    t2m      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  CF-1.6\n    history:      2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...xarray.DatasetDimensions:lon: 64lat: 32time: 8760Coordinates: (3)lon(lon)float640.0 5.625 11.25 ... 348.8 354.4array([  0.   ,   5.625,  11.25 ,  16.875,  22.5  ,  28.125,  33.75 ,  39.375,\n        45.   ,  50.625,  56.25 ,  61.875,  67.5  ,  73.125,  78.75 ,  84.375,\n        90.   ,  95.625, 101.25 , 106.875, 112.5  , 118.125, 123.75 , 129.375,\n       135.   , 140.625, 146.25 , 151.875, 157.5  , 163.125, 168.75 , 174.375,\n       180.   , 185.625, 191.25 , 196.875, 202.5  , 208.125, 213.75 , 219.375,\n       225.   , 230.625, 236.25 , 241.875, 247.5  , 253.125, 258.75 , 264.375,\n       270.   , 275.625, 281.25 , 286.875, 292.5  , 298.125, 303.75 , 309.375,\n       315.   , 320.625, 326.25 , 331.875, 337.5  , 343.125, 348.75 , 354.375])lat(lat)float64-87.19 -81.56 ... 81.56 87.19array([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875])time(time)datetime64[ns]2018-01-01 ... 2018-12-31T23:00:00long_name :timearray(['2018-01-01T00:00:00.000000000', '2018-01-01T01:00:00.000000000',\n       '2018-01-01T02:00:00.000000000', ..., '2018-12-31T21:00:00.000000000',\n       '2018-12-31T22:00:00.000000000', '2018-12-31T23:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)t2m(time, lat, lon)float32...units :Klong_name :2 metre temperature[17940480 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([    0.0,   5.625,   11.25,  16.875,    22.5,  28.125,   33.75,  39.375,\n          45.0,  50.625,   56.25,  61.875,    67.5,  73.125,   78.75,  84.375,\n          90.0,  95.625,  101.25, 106.875,   112.5, 118.125,  123.75, 129.375,\n         135.0, 140.625,  146.25, 151.875,   157.5, 163.125,  168.75, 174.375,\n         180.0, 185.625,  191.25, 196.875,   202.5, 208.125,  213.75, 219.375,\n         225.0, 230.625,  236.25, 241.875,   247.5, 253.125,  258.75, 264.375,\n         270.0, 275.625,  281.25, 286.875,   292.5, 298.125,  303.75, 309.375,\n         315.0, 320.625,  326.25, 331.875,   337.5, 343.125,  348.75, 354.375],\n      dtype='float64', name='lon'))latPandasIndexPandasIndex(Index([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875],\n      dtype='float64', name='lat'))timePandasIndexPandasIndex(DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',\n               '2018-01-01 02:00:00', '2018-01-01 03:00:00',\n               '2018-01-01 04:00:00', '2018-01-01 05:00:00',\n               '2018-01-01 06:00:00', '2018-01-01 07:00:00',\n               '2018-01-01 08:00:00', '2018-01-01 09:00:00',\n               ...\n               '2018-12-31 14:00:00', '2018-12-31 15:00:00',\n               '2018-12-31 16:00:00', '2018-12-31 17:00:00',\n               '2018-12-31 18:00:00', '2018-12-31 19:00:00',\n               '2018-12-31 20:00:00', '2018-12-31 21:00:00',\n               '2018-12-31 22:00:00', '2018-12-31 23:00:00'],\n              dtype='datetime64[ns]', name='time', length=8760, freq=None))Attributes: (2)Conventions :CF-1.6history :2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmwf/eccodes/bin/grib_to_netcdf -o /cache/data2/adaptor.mars.internal-1573035683.1772008-2550-1-601d5659-dae2-45e1-902b-45825d30e8d0.nc /cache/tmp/601d5659-dae2-45e1-902b-45825d30e8d0-adaptor.mars.internal-1573035683.1790879-2550-1-tmp.grib\n\n\n\ntime_stamp = slice(\"2018-01\", \"2018-03\")\ntrain_df = data5.sel(time=time_stamp).to_dataframe().reset_index()\ntest_df = data1.sel(time=time_stamp).to_dataframe().reset_index()\n\nX = np.stack([train_df.lat.values, train_df.lon.values, train_df.time.astype(np.int64) / 10**9], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values, test_df.time.astype(np.int64) / 10**9], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\n# rff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(4423680, 3), y.shape=(4423680, 1)\nX_test.shape=(70778880, 3), y_test.shape=(70778880, 1)\nX.shape=(4423680, 3), X_test.shape=(70778880, 3)\n\n\n\n32*64*24*(31+28+31)\n\n4423680\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\n# Scaling time\nif X.shape[1] == 3:\n    X_scaled[:, 2] = X_scaled[:, 2] * 10 - 5\n    X_test_scaled[:, 2] = X_test_scaled[:, 2] * 10 - 5\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = SIREN(3, 1, [256]*4, 30.0, 0.0)\n# model = MLP(3, 1, [256]*4, 0.0)\n# model = ResNet()S\n# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-3,\n#     maximal_learning_rate=1e-2,\n#     scale_fn=lambda x: 1/(2.**(x-1)),\n#     step_size=2\n# )\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n2023-07-18 05:14:34.531498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n2023-07-18 05:14:34.531583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78884 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n\n\n\n0.00148\n\n0.00148\n\n\n\ncallbacks = [TqdmCallback(verbose=1)]\nhistory = model.fit(X_scaled, y_scaled, epochs=1000, batch_size=X_scaled.shape[0], verbose=0, callbacks=callbacks)\n\n  0%|          | 0/1000 [00:00&lt;?, ?epoch/s]2023-07-18 05:14:38.677299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n2023-07-18 05:14:39.357828: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fb81b946130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2023-07-18 05:14:39.357901: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2023-07-18 05:14:39.363158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-07-18 05:14:40.399794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n2023-07-18 05:14:40.557542: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100%|| 1000/1000 [04:46&lt;00:00,  3.49epoch/s, loss=0.000295]\n\n\n\nplt.plot(history.history['loss'][200:], label='loss');\n# plt.plot(history.history['val_loss'][200:], label='val_loss');\nplt.legend();\n\n\n\n\n\n\n\n\n\n128*256*24\n\n786432\n\n\n\nimg_index = 0\ny_pred = model.predict(X_test_scaled, batch_size=20480) * (y_max - y_min) + y_min\nprint(y_pred.shape)\nplt.imshow(y_pred[img_index*(256*128):(img_index+1)*(256*128)].reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n3456/3456 [==============================] - 5s 1ms/step\n(70778880, 1)\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\ndef get_lat_weights(lat):\n    lat_weights = np.cos(np.deg2rad(lat))\n    lat_weights = lat_weights / lat_weights.mean()\n    return lat_weights\n\nlat_weights = get_lat_weights(X_test[:, 0])\nprint(f\"{lat_weights.shape=}\")\n\nlat_squared_error = lat_weights * (y_pred.ravel() - y_test.ravel())**2\nlat_rmse = np.sqrt(lat_squared_error.mean())\nprint(f\"{lat_rmse=}\")\n# y_pred.shape, lat_weights.shape\n\nlat_weights.shape=(70778880,)\nlat_rmse=2.6118446730600438\n\n\n\n# lat_rmse=3.4826956884024356\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html",
    "href": "posts/2022-10-21-gaussian-processes.html",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\n\nfrom tinygp.kernels import ExpSquared\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#regression",
    "href": "posts/2022-10-21-gaussian-processes.html#regression",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Regression",
    "text": "Regression\nIn this post, we will consider the regression problem of finding a reasonable map \\(X \\to \\boldsymbol{y}\\) along with uncertainty. We can do this in a simplest setting with Bayesian linear regression assuming a MultiVariate Normal (MVN) prior \\(\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\theta, \\Sigma_\\theta)\\) (why MVN? because \\(\\theta \\in (-\\infty, \\infty)\\)) and Normal likelihood \\(y \\sim \\mathcal{N}(\\boldsymbol{x}^T\\theta, \\sigma_n^2)\\) with i.i.d. assumption.\nTo start with Gaussian process regression, let us first focus on \\(\\boldsymbol{y}\\) (and ignore \\(X\\)). We assume \\(\\boldsymbol{f}\\) as a random variable and \\(\\boldsymbol{y}\\) as a realization of \\(\\boldsymbol{f}\\) with some noise. It would be a natural probabilistic assumption to assume \\(\\boldsymbol{f}\\) to be MVN distributed since its range is \\((-\\infty, \\infty)\\).\n\\[\np(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\n\\tag{prior}\n\\]\nNow, we need to bring in \\(X\\) in a reasonable way to this formulation. A core assumption connecting \\(X\\) with \\(\\boldsymbol{y}\\) is the following: &gt; if two inputs \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{x}'\\) are close to each other (how to define the closeness? kernels!), corresponding \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{y}'\\) are likely to be similar.\nWe use something known as covariance function or kernel (later is more prevalent) to define this closeness. For example, RBF or squared exponential is a well-known kernel:\n\\[\nk_{RBF}(\\boldsymbol{x}, \\boldsymbol{x}') = \\sigma^2 \\exp \\left(-{\\frac {\\|\\boldsymbol{x} -\\boldsymbol{x}' \\|^{2}}{2\\ell ^{2}}}\\right)\n\\tag{kernel}\n\\]\n\nx = jnp.array(0.0).reshape(1, 1)\nx_prime = jnp.linspace(-5,5,100).reshape(-1, 1)\n\nplt.plot(x_prime, ExpSquared()(x_prime, x));\nplt.xlabel(\"$x'$\")\nplt.title(f\"$k(x,x')$ where $x={x[0][0]}$ and $x' \\in ${plt.xlim()}\");\n\n\n\n\n\n\n\n\nThe plot above shows that value of \\(k(\\boldsymbol{x}, \\boldsymbol{x}')\\) increases as \\(\\boldsymbol{x}'\\) approaches \\(\\boldsymbol{x}\\) and reduces as it moves far from \\(\\boldsymbol{x}\\). Now, we will connect \\(X\\) with \\(\\boldsymbol{f}\\) (and thus with \\(\\boldsymbol{y}\\)) through kernel \\(k\\) with two following assumptions:\n\nDiagonal entries of \\(K_{ff}\\) represent variance of \\(f_i\\), which can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_i)\\).\nNon-diagonal entries of \\(K_{ff}\\) represent covariance between \\(f_i\\) and \\(f_j\\) and can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\).\n\nAt this point, we have made everything clear about prior \\(p(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\\). Now, we will look at the likelihood. As mentioned earlier, \\(\\boldsymbol{y}\\) is noisy realization of \\(f\\) so the following likelihood would be a simple and natural choice.\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{f}, \\sigma_n^2I)\n\\tag{likelihood}\n\\]\nTill now, we followed bottom-up approach and defined prior and likelihood for this problem. Now we will explore the top-down approach.\nOur ultimate goal is derive \\(p(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X)\\) at new inputs \\(X^*\\). This can be written as:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) = \\int p(\\boldsymbol{y}^*|\\boldsymbol{f}^*)p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)d\\boldsymbol{f}^*\n\\tag{pred post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) is the posterior distribution at inputs \\(X^*\\). Once we derive posterior \\(p(\\boldsymbol{f}|\\boldsymbol{y},X)\\), We can find \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) like following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) = \\int p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)p(\\boldsymbol{f}|\\boldsymbol{y}, X)d\\boldsymbol{f}\n\\tag{post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)\\) is a conditional Gaussian distribution with the following closed form:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{f}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*})\n\\tag{cond}\n\\]\nPosterior \\(p(\\boldsymbol{f}|\\boldsymbol{y}, X)\\) can be derived following Bayes rule for Gaussians (section 2.2.6.2 in pml book2):\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff})\n\\tag{post}\n\\]\nWe can now substitute Eq. (post) and Eq. (cond) in Eq. (post new). The integral can be solved with using Eq. 2.90 in section 2.2.6.2 in pml book2 and also mentioned in Eq. (int gaussians) in Appendix.\n\\[\n\\begin{aligned}\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{\\mu}^*, \\Sigma^*)\\\\\n\\boldsymbol{\\mu}^* &= \\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\left[\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\right]-\\boldsymbol{m}_{f})\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f))\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\\\\n\\\\\n\\Sigma^* &= K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}\\left[K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[I - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[K_{ff}^{-1} - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}\\right]K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}K_{ff^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*})\n\\end{aligned}\n\\]\nNow, we are almost there. Plugging in the above formula in Eq. (pred post) and using known result in Eq. (int gaussians), we get the predictive posterior as following:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*} + \\sigma_n^2I)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe did not exploit the special structure of likelihood variance \\(\\sigma_n^2I\\) anywhere, so, these derivations hold true for full rank likelihood covariance matrices also.\n\n\n\nOptimization\nWe perform type-II likelihood estimation (in other words, minimize log marginal likelihood or evidence term). Our goal is to find optimal model \\(\\mathcal{M}\\) represented by prior (or kernel) hyperparameters and likelihood hyperparameters. We can get the log marginal likelihood using Eq. (int gaussians):\n\\[\n\\begin{aligned}\np(\\boldsymbol{y}|X, \\mathcal{M}) &= \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\\\\n&\\sim \\int \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{f}, \\sigma_n^2I) \\mathcal{N}(\\boldsymbol{f}|\\boldsymbol{m}_f, K_{ff})\\\\\n&\\sim \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{m}_f, K_{ff}+\\sigma_n^2I)\n\\end{aligned}\n\\]\nFor case of RBF kernel, \\(\\mathcal{M}\\) parameters will be \\(\\{\\sigma, \\ell, \\sigma_n\\}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "href": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Classification (with Laplace approximation)",
    "text": "Classification (with Laplace approximation)\nWe will derive a GP predictive posterior for binary case only because for multi-class, it gets a bit complex. Our assumption for prior over the \\(\\boldsymbol{f}\\) can still be the same but likelihood needs to be changed because \\(\\boldsymbol{y}\\) is no more a real number but rather a binary value e.g.0 or 1. From Bayesian point-of-view, Bernoulli likelihood would be the most appropriate as a likelihood here:\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) = \\prod_{i=1}^{N} \\sigma(f_i)^{y_i=1}(1-\\sigma(f_i))^{y_i=0}\n\\tag{class likelihood}\n\\]\nSince, MVN prior and Bernoulli likelihood are not conjugate, we need to use an approximate method of inference here. We use Laplace approximation to get the MAP estimate \\(\\boldsymbol{\\hat{f}}\\) and by computing the Hessian \\(H\\) of negative log joint (log prior + log likelihood) with respect to \\(\\boldsymbol{\\hat{f}}\\), we can get the posterior distribution as the following:\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{\\hat{f}}, H^{-1})\n\\tag{class post}\n\\]\nEq. (cond) will be the same in this case, and thus, we can solve Eq. (post new) as we did for regression case, like the following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{\\hat{f}}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}H^{-1}K_{ff}^{-1}K_{ff^*})\n\\]\n\nOptimization\nTo perform Type-II likelihood estimation for binary classification, we first need to derive the log marginal likelihood which can be approximated with Laplace approximation. First, we define the following quantity:\n\\[\n\\boldsymbol{\\psi}(\\boldsymbol{f}) \\triangleq \\log p(\\boldsymbol{y}|\\boldsymbol{f}) + \\log p(\\boldsymbol{f})\n\\]\nNow, computing the log marginal likelihood as suggested in section 3.4.4 of GPML book:\n\\[\n\\begin{aligned}\n\\log p(\\boldsymbol{y}|X, \\mathcal{M}) &\\sim \\log \\left[ \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\right]\\\\\n&= \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{f})\\right)d\\boldsymbol{f} \\right]\\\\\n&\\thickapprox \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) -\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f} \\right]\\\\\n&= \\log \\left[ \\exp \\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) \\int exp\\left(-\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f}\\right]\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) + \\log p(\\boldsymbol{\\hat{f}}) - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}|\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) -\\frac{1}{2}\\boldsymbol{\\hat{f}}^TK_{ff}^{-1}\\boldsymbol{\\hat{f}} - \\frac{1}{2}\\log|K_{ff}| - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}| - \\frac{N}{2}\\log(2\\pi)\n\\end{aligned}\n\\]\nOur final optimization algorithm would be as following: 1. For N iterations do 2. to 4. 2. Optimize for \\(\\boldsymbol{\\hat{f}}\\) with M iterations using standard MAP estimation (maybe use non-centered parametrization). 3. Compute gradient of parameters of \\(\\mathcal{M}\\) w.r.t. log marginal likelihood 4. Update parameters of \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#appendix",
    "href": "posts/2022-10-21-gaussian-processes.html#appendix",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Appendix",
    "text": "Appendix\n\\[\n\\int \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{x}+\\boldsymbol{b}, \\Sigma) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}, K) = \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{\\mu}+b, WKW^T+\\Sigma)\n\\tag{int gaussians}\n\\]"
  },
  {
    "objectID": "posts/Basis_functions.html",
    "href": "posts/Basis_functions.html",
    "title": "Basis functions",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"../../beat_stgnp/dataset/bjair/NP/processed_raw.csv\")\ndata[\"time\"] = pd.to_datetime(data[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\ndata[\"time\"] = data[\"time\"].apply(lambda x: x.timestamp())\n\nx = [\"latitude\", \"longitude\", \"time\"]\ny = [\"PM25_Concentration\"]\n\nx_train, x_test, y_train, y_test = train_test_split(data[x], data[y], test_size=0.2, random_state=42)\nx_train, x_test, y_train, y_test = map(lambda x: x.values, [x_train, x_test, y_train, y_test])\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nx_train = x_scaler.fit_transform(x_train)\ny_train = y_scaler.fit_transform(y_train)\nx_test = x_scaler.transform(x_test)\n\nmodel = RandomForestRegressor(n_estimators=1000, random_state=42)\nmodel.fit(x_train, y_train.ravel())\ny_pred = model.predict(x_test)\nprint(\"RMSE\", np.sqrt(np.mean((y_scaler.inverse_transform(y_pred).ravel() - y_test.ravel())**2)))\n\n /tmp/ipykernel_922642/3470971270.py:18: DataConversionWarning:A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel()."
  },
  {
    "objectID": "posts/2021-10-23-warped-gp.html",
    "href": "posts/2021-10-23-warped-gp.html",
    "title": "Input Warped GPs - A failed idea",
    "section": "",
    "text": "Comments\n\nWe are warping inputs \\(\\mathbf{x}\\) into \\(\\mathbf{w}\\cdot\\mathbf{x}\\)\nLearning second level GP over \\(\\mathbf{w}\\).\nAppling penalty over \\(\\mathbf{w}\\) if varies too much unnecessary.\nSee problems at the end of the notebook.\nWe need to check mathematical concerns related to this transformation.\n\n\nimport math\nimport numpy as np\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport regdata as rd\nfrom sklearn.cluster import KMeans\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nclass ExactNSGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_latent):\n        super(ExactNSGPModel, self).__init__(train_x, train_y, likelihood)\n#         inds = np.random.choice(train_x.shape[0], size=num_latent, replace=False)\n#         self.x_bar = train_x[inds]\n        self.x_bar = torch.tensor(KMeans(n_clusters=num_latent).fit(train_x).cluster_centers_).to(train_x)\n        self.w_bar = torch.nn.Parameter(torch.ones(num_latent,).to(self.x_bar))\n        self.bias = torch.nn.Parameter(torch.zeros(1,).to(self.x_bar))\n        self.latent_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n#       We can fix noise to be minimum but it is not ideal. Ideally, noise should automatically reduce to reasonable value.\n#         self.latent_likelihood.raw_noise.requires_grad = False\n#         self.latent_likelihood.raw_noise = torch.tensor(-10.)\n        self.latent_model = ExactGPModel(self.x_bar, self.w_bar, self.latent_likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        self.latent_model.eval()\n        with gpytorch.settings.detach_test_caches(False):  # needed to back propagate thru predictive posterior\n            self.latent_model.set_train_data(self.x_bar, self.w_bar, strict=False)\n            self.w = self.latent_likelihood(self.latent_model(x))  # predictive posterior\n        x_warped = x*self.w.mean[:, None] + self.bias\n        mean_x = self.mean_module(x_warped)\n        covar_x = self.covar_module(x_warped)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\ndef training(model, likelihood):\n    training_iter = 100\n\n    # Find optimal model hyperparameters\n    model.train()\n    likelihood.train()\n\n    # Use the adam optimizer\n    optimizer = torch.optim.Adam([\n        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n    ], lr=0.1)\n\n    # \"Loss\" for GPs - the marginal log likelihood\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    for i in range(training_iter):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        output = model(train_x)\n        # Calc loss and backprop gradients\n        try:\n            loss = -mll(output, train_y) + torch.square(model.w.mean-1).mean()\n#             print(model.latent_likelihood.noise)\n        except AttributeError:\n            loss = -mll(output, train_y)\n        loss.backward()\n#         print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n#             i + 1, training_iter, loss.item(),\n#             model.covar_module.base_kernel.lengthscale.item(),\n#             model.likelihood.noise.item()\n#         ))\n        optimizer.step()\n    \ndef predict_plot(model, likelihood, title):\n    # Get into evaluation (predictive posterior) mode\n    model.eval()\n    likelihood.eval()\n\n    # Test points are regularly spaced along [0,1]\n    # Make predictions by feeding model through likelihood\n    with torch.no_grad():\n        observed_pred = likelihood(model(test_x))\n\n    with torch.no_grad():\n        # Initialize plot\n        f, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n        # Get upper and lower confidence bounds\n        lower, upper = observed_pred.confidence_region()\n        # Plot training data as black stars\n        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n        # Plot predictive means as blue line\n        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n        # Shade between the lower and upper confidence bounds\n        ax.fill_between(test_x.numpy().ravel(), lower.numpy(), upper.numpy(), alpha=0.5)\n        ax.legend(['Observed Data', 'Mean', 'Confidence'])\n        ax.set_title(title)\n    return observed_pred\n\n\ndef GP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactGPModel(train_x, train_y, likelihood)\n    \n    training(model, likelihood)\n    predict_plot(model, likelihood, 'GP')\n\ndef NSGP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactNSGPModel(train_x, train_y, likelihood, num_latent)\n    \n    training(model, likelihood)\n    observed_pred = predict_plot(model, likelihood, 'NSGP')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x*model.w.mean[:, None], observed_pred.mean.numpy())\n        plt.title('Warped test inputs v/s test outputs')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x, model.w.mean, label='interpolated')\n        plt.scatter(model.x_bar, model.w_bar, label='learned')\n        plt.ylim(0,2)\n        plt.title('Test input v/s weights')\n        plt.legend()\n\n\n\nTesting over various datasets\n\ntrain_x, train_y, test_x = rd.DellaGattaGene(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Heinonen4(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Jump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.MotorcycleHelmet(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Olympic(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineJump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineNoisy(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblems\n\nTransformation from x to x_warped is not monotonic."
  },
  {
    "objectID": "posts/2022-10-27-mogp.html",
    "href": "posts/2022-10-27-mogp.html",
    "title": "Multi-Output Gaussian Processes",
    "section": "",
    "text": "Inspired from this GPSS video.\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\n\nimport optax\n\nimport matplotlib.pyplot as plt\nfrom tinygp import kernels"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#helper-functions",
    "href": "posts/2022-10-27-mogp.html#helper-functions",
    "title": "Multi-Output Gaussian Processes",
    "section": "Helper functions",
    "text": "Helper functions\n\ndef random_fill(key, params):\n    values, unravel_fn = ravel_pytree(params)\n    random_values = jax.random.normal(key, shape=values.shape)\n    return unravel_fn(random_values)\n\ndef get_real_params(params):\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = params[f'a{i}'].reshape(n_outputs, rank)\n    if method == 'icm':\n        params['var'] = jnp.exp(params['log_var'])\n        params['scale'] = jnp.exp(params['log_scale'])\n        params['noise'] = jnp.exp(params['log_noise'])\n    elif method == 'lmc':\n        for i in range(1, q_len+1):\n            params[f'var{i}'] = jnp.exp(params[f'log_var{i}'])\n            params[f'scale{i}'] = jnp.exp(params[f'log_scale{i}'])\n            params[f'noise{i}'] = jnp.exp(params[f'log_noise{i}'])\n    return params\n\ndef kron_cov_fn(params, x1, x2, add_noise=False):\n    params = get_real_params(params)\n    a_list = [params[f'a{i}'] for i in range(1, q_len+1)]\n\n    if method == 'icm':\n        kernel_fn = params['var'] * kernels.ExpSquared(scale=params['scale'])\n        cov = kernel_fn(x1, x2)\n        if add_noise:\n            cov = cov + jnp.eye(cov.shape[0])*params['noise']\n\n        B = jax.tree_util.tree_reduce(lambda x1, x2: x1@x1.T+x2@x2.T, a_list)\n#         print(B.shape, cov.shape)\n        return jnp.kron(B, cov)\n\n    elif method == 'lmc':\n        cov_list = []\n        for idx in range(1, q_len+1):\n            kernel_fn = params[f'var{idx}'] * kernels.ExpSquared(scale=params[f'scale{idx}'])\n            cov = kernel_fn(x1, x2)\n            if add_noise:\n                cov = cov + jnp.eye(cov.shape[0])*params[f'noise{idx}']\n\n            B = a_list[idx-1]@a_list[idx-1].T\n            cov_list.append(jnp.kron(B, cov))\n            \n        return jax.tree_util.tree_reduce(lambda x1, x2: x1+x2, cov_list)"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#configuration",
    "href": "posts/2022-10-27-mogp.html#configuration",
    "title": "Multi-Output Gaussian Processes",
    "section": "Configuration",
    "text": "Configuration\n\nq_len = 2\nrank = 2 # if 1, slfm\nn_outputs = 2\n\nmethod = 'lmc' # lmc, icm\n\nif rank = 1, lmc becomes slfm.\n\nGenerative process\n\nx_key = jax.random.PRNGKey(4)\n\nx = jax.random.uniform(x_key, shape=(40, 1)).sort(axis=0)\nx_test = jnp.linspace(0,1,100).reshape(-1, 1)\n\ne1_key, e2_key = jax.random.split(x_key)\n\ne1 = jax.random.normal(e1_key, shape=(x.shape[0],))\ne2 = jax.random.normal(e2_key, shape=(x.shape[0],))\n\nif method == 'icm':\n    noise = 0.01\n    gen_kernel = 1.2*kernels.ExpSquared(scale=0.2)\n    gen_covariance = gen_kernel(x, x) + jnp.eye(x.shape[0])*noise\n    gen_chol = jnp.linalg.cholesky(gen_covariance)\n    \n    y1 = gen_chol@e1\n    y2 = gen_chol@e2\n\n    y = jnp.concatenate([y1, y2])\n    \nelif method == 'lmc':\n    noise1 = 0.01\n    noise2 = 0.1\n    gen_kernel1 = 1.2*kernels.ExpSquared(scale=0.1)\n    gen_covariance1 = gen_kernel1(x, x) + jnp.eye(x.shape[0])*noise1\n    gen_chol1 = jnp.linalg.cholesky(gen_covariance1)\n\n    gen_kernel2 = 0.8*kernels.ExpSquared(scale=0.2)\n    gen_covariance2 = gen_kernel2(x, x) + jnp.eye(x.shape[0])*noise2\n    gen_chol2 = jnp.linalg.cholesky(gen_covariance2)\n    \n    y1 = gen_chol1@e1\n    y2 = gen_chol2@e2\n\n    y = jnp.concatenate([y1, y2])\n    \n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.legend();\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\n\n\n\n\ndef loss_fn(params):\n    mo_cov = kron_cov_fn(params, x, x, add_noise=True)\n#     print(y.shape, mo_cov.shape)\n    return -jax.scipy.stats.multivariate_normal.logpdf(y, jnp.zeros_like(y), mo_cov)\n\n\nkey = jax.random.PRNGKey(1)\nif method == 'icm':\n    params = {'log_var':0.0, 'log_scale':0.0, 'log_noise':0.0}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\nelif method == 'lmc':\n    params = {}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\n        params[f'log_var{i}'] = 0.0\n        params[f'log_scale{i}'] = 0.0\n        params[f'log_noise{i}'] = 0.0\n\nparams = random_fill(key, params)\nparams\n\n{'a1': DeviceArray([[-0.764527 ,  1.0286916],\n              [-1.0690447, -0.7921495]], dtype=float32),\n 'a2': DeviceArray([[ 0.8845895, -1.1941622],\n              [-1.7434924,  1.5159688]], dtype=float32),\n 'log_noise1': DeviceArray(-1.1254696, dtype=float32),\n 'log_noise2': DeviceArray(-0.22446911, dtype=float32),\n 'log_scale1': DeviceArray(0.39719132, dtype=float32),\n 'log_scale2': DeviceArray(-0.22453257, dtype=float32),\n 'log_var1': DeviceArray(-0.7590596, dtype=float32),\n 'log_var2': DeviceArray(-0.08601531, dtype=float32)}\n\n\n\nloss_fn(params)\n\nDeviceArray(116.04026, dtype=float32)\n\n\n\nkey = jax.random.PRNGKey(3)\nparams = random_fill(key, params)\n\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), (params, loss)\n\n(tuned_params, state), (params_history, loss_history) = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(loss_history);\n\n\n\n\n\n\n\n\n\ndef predict_fn(params, x_test):\n    cov = kron_cov_fn(params, x, x, add_noise=True)\n    test_cov = kron_cov_fn(params, x_test, x_test, add_noise=True)\n    cross_cov = kron_cov_fn(params, x_test, x, add_noise=False)\n    \n    chol = jnp.linalg.cholesky(cov)\n    k_inv_y = jax.scipy.linalg.cho_solve((chol, True), y)\n    k_inv_cross_cov = jax.scipy.linalg.cho_solve((chol, True), cross_cov.T)\n\n    pred_mean = cross_cov@k_inv_y\n    pred_cov = test_cov - cross_cov@k_inv_cross_cov\n    return pred_mean, pred_cov\n\n\npred_mean, pred_cov = predict_fn(tuned_params, x_test)\npred_conf = 2 * jnp.diag(pred_cov)**0.5\n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.plot(x_test, pred_mean[:x_test.shape[0]], label='pred_y1')\nplt.plot(x_test, pred_mean[x_test.shape[0]:], label='pred_y2')\nplt.fill_between(x_test.ravel(), pred_mean[:x_test.shape[0]] - pred_conf[:x_test.shape[0]], pred_mean[:x_test.shape[0]] + pred_conf[:x_test.shape[0]], label='pred_conf_y1', alpha=0.3)\nplt.fill_between(x_test.ravel(), pred_mean[x_test.shape[0]:] - pred_conf[x_test.shape[0]:], pred_mean[x_test.shape[0]:] + pred_conf[x_test.shape[0]:], label='pred_conf_y2', alpha=0.3)\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\nfor name, value in get_real_params(tuned_params).items():\n    if not name.startswith('log_'):\n        print(name, value)\n\na1 [[0.03664799 0.00039898]\n [0.3191718  0.00344488]]\na2 [[ 0.1351072   0.00248941]\n [-0.05392759 -0.04239884]]\nnoise1 0.6797133\nnoise2 0.4154678\nscale1 5.048228\nscale2 0.10743636\nvar1 0.016275918\nvar2 41.034225"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html",
    "href": "posts/2022-08-01-conditional_neural_processes.html",
    "title": "Conditional Neural Processes in JAX",
    "section": "",
    "text": "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n# https://github.com/tensorflow/probability/issues/1523\nimport logging\n\nlogger = logging.getLogger()\n\n\nclass CheckTypesFilter(logging.Filter):\n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n  import flax.linen as nn\nexcept ModuleNotFoundError:\n  %pip install flax\n  import flax.linen as nn\n\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install optax\n  import optax\n\ntry:\n  import tensorflow_probability.substrates.jax as tfp\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#model",
    "href": "posts/2022-08-01-conditional_neural_processes.html#model",
    "title": "Conditional Neural Processes in JAX",
    "section": "Model",
    "text": "Model\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(2)(x)\n    loc, raw_scale = x[:, 0], x[:, 1]\n    scale = jax.nn.softplus(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#data",
    "href": "posts/2022-08-01-conditional_neural_processes.html#data",
    "title": "Conditional Neural Processes in JAX",
    "section": "Data",
    "text": "Data\n\nN = 100\nseed = jax.random.PRNGKey(0)\nx = jnp.linspace(-1, 1, N).reshape(-1, 1)\nf = lambda x: (jnp.sin(10*x) + x).flatten()\nnoise = jax.random.normal(seed, shape=(N,)) * 0.2\ny = f(x) + noise\n\nx_test = jnp.linspace(-2, 2, N*2+10).reshape(-1, 1)\ny_test = f(x_test) \n\nplt.scatter(x, y, label='train', zorder=5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.legend();"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#training",
    "href": "posts/2022-08-01-conditional_neural_processes.html#training",
    "title": "Conditional Neural Processes in JAX",
    "section": "Training",
    "text": "Training\n\ndef train_fn(model, optimizer, seed, n_iterations, n_context):\n  params = model.init(seed, x, y, x)\n  value_and_grad_fn = jax.value_and_grad(model.loss_fn)\n  state = optimizer.init(params)\n  indices = jnp.arange(N)\n  \n  def one_step(params_and_state, seed):\n    params, state = params_and_state\n    shuffled_indices = jax.random.permutation(seed, indices)\n    context_indices = shuffled_indices[:n_context]\n    target_indices = shuffled_indices[n_context:]\n    x_context, y_context = x[context_indices], y[context_indices]\n    x_target, y_target = x[target_indices], y[target_indices]\n    loss, grads = value_and_grad_fn(params, x_context, y_context, x_target, y_target)\n    updates, state = optimizer.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n\n  seeds = jax.random.split(seed, num=n_iterations)\n  (params, state), losses = jax.lax.scan(one_step, (params, state), seeds)\n  return params, losses\n\n\nencoder_features = [64, 16, 8]\nencoding_dims = 1\ndecoder_features = [16, 8]\nmodel = CNP(encoder_features, encoding_dims, decoder_features)\noptimizer = optax.adam(learning_rate=0.001)\n\nseed = jax.random.PRNGKey(2)\nn_context = int(0.7 * N)\nn_iterations = 20000\n\nparams, losses = train_fn(model, optimizer, seed, n_iterations=n_iterations, n_context=n_context)\n\n\nplt.plot(losses);"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "href": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "title": "Conditional Neural Processes in JAX",
    "section": "Predict",
    "text": "Predict\n\nloc, scale = model.apply(params, x, y, x_test)\nlower, upper = loc - 2*scale, loc + 2*scale\n\nplt.scatter(x, y, label='train', alpha=0.5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.plot(x_test, loc);\nplt.fill_between(x_test.flatten(), lower, upper, alpha=0.4);\nplt.ylim(-5, 5);"
  },
  {
    "objectID": "posts/2022-10-18-kfac-laplace.html",
    "href": "posts/2022-10-18-kfac-laplace.html",
    "title": "Train NN with KFAC-Laplace in JAX",
    "section": "",
    "text": "from math import prod\nfrom functools import partial\nfrom time import time\n\nimport blackjax\nimport flax.linen as nn\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.tree_util as jtu\nimport jax.numpy as jnp\n# jnp.set_printoptions(linewidth=2000)\n\nimport optax\nfrom tqdm import trange\n\nimport arviz as az\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\njax.config.update(\"jax_enable_x64\", False)\n\n%reload_ext watermark\n\nSome helper functions:\n\njitter = 1e-6\n\ndef get_shapes(params):\n    return jtu.tree_map(lambda x:x.shape, params)\n\ndef svd_inverse(matrix):\n    U, S, V = jnp.linalg.svd(matrix+jnp.eye(matrix.shape[0])*jitter)\n    \n    return V.T/S@U.T\n\n\nDataset\nWe take XOR dataset to begin with:\n\nX = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = jnp.array([0, 1, 1, 0])\n\nX.shape, y.shape\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n((4, 2), (4,))\n\n\n\n\nNN model\n\nclass MLP(nn.Module):\n    features: []\n\n    @nn.compact\n    def __call__(self, x):\n        for n_features in self.features[:-1]:\n            x = nn.Dense(n_features, kernel_init=jax.nn.initializers.glorot_normal(), bias_init=jax.nn.initializers.normal())(x)\n            x = nn.relu(x)\n        \n        x = nn.Dense(features[-1])(x)\n        return x.ravel()\n\nLet us initialize the weights of NN and inspect shapes of the parameters:\n\nfeatures = [2, 1]\nkey = jax.random.PRNGKey(0)\n\nmodel = MLP(features)\nparams = model.init(key, X).unfreeze()\n\nget_shapes(params)\n\n{'params': {'Dense_0': {'bias': (2,), 'kernel': (2, 2)},\n  'Dense_1': {'bias': (1,), 'kernel': (2, 1)}}}\n\n\n\nmodel.apply(params, X)\n\nDeviceArray([ 0.00687164, -0.01380461,  0.        ,  0.        ], dtype=float32)\n\n\n\n\nNegative Log Joint\n\nnoise_var = 0.1\n\ndef neg_log_joint(params):\n    y_pred = model.apply(params, X)\n    flat_params = ravel_pytree(params)[0]\n    log_prior = jax.scipy.stats.norm.logpdf(flat_params).sum()\n    log_likelihood = jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=noise_var).sum()\n    \n    return -(log_prior + log_likelihood)\n\nTesting if it works:\n\nneg_log_joint(params)\n\nDeviceArray(105.03511, dtype=float32)\n\n\n\n\nFind MAP\n\nkey = jax.random.PRNGKey(0)\nparams = model.init(key, X).unfreeze()\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(neg_log_joint))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n    \n(params, state), losses = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\ny_map = model.apply(params, X)\ny_map\n\nDeviceArray([0.01383345, 0.98666817, 0.98563665, 0.01507111], dtype=float32)\n\n\n\nx = jnp.linspace(-0.1,1.1,100)\nX1, X2 = jnp.meshgrid(x, x)\n\ndef predict_fn(x1, x2):\n    return model.apply(params, jnp.array([x1,x2]).reshape(1,2))\n\npredict_fn_vec = jax.jit(jax.vmap(jax.vmap(predict_fn)))\n\nZ = predict_fn_vec(X1, X2).squeeze()\n\nplt.contourf(X1, X2, Z)\nplt.colorbar();\n\n\n\n\n\n\n\n\n\n\nFull Hessian Laplace\n\nflat_params, unravel_fn = ravel_pytree(params)\n\ndef neg_log_joint_flat(flat_params):\n    return neg_log_joint(unravel_fn(flat_params))\n\nH = jax.hessian(neg_log_joint_flat)(flat_params)\n\nsns.heatmap(H);\n\n\n\n\n\n\n\n\n\nposterior_cov = svd_inverse(H)\n\nsns.heatmap(posterior_cov);\n\n\n\n\n\n\n\n\nNote that we can sample parameters from the posterior and revert them to correct structure with the unravel_fn. Here is a class to do it all:\n\nclass FullHessianLaplace:\n    def __init__(self, map_params, model):\n        flat_params, self.unravel_fn = ravel_pytree(map_params)\n\n        def neg_log_joint_flat(flat_params):\n            params = unravel_fn(flat_params)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(flat_params)\n        \n        self.mean = flat_params\n        self.cov = svd_inverse(self.H)\n        self.model = model\n\n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample(self, seed):\n        sample = jax.random.multivariate_normal(seed, mean=self.mean, cov=self.cov)\n        return self.unravel_fn(sample)\n    \n    def sample(self, seed, shape):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nposterior = FullHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 100000\ny_pred_full = posterior.predict(X, seed=seed, shape=(n_samples,))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i]);\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_pred_mean={y_pred_full[:, i].mean():.3f}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nKFAC-Laplace\nWe need to invert partial Hessians to do KFAC-Laplace. We can use tree_flatten with ravel_pytree to ease the workflow. We need to: 1. pick up partial Hessians in pure matrix form to be able to invert them. 2. Create layer-wise distributions and sample them. These samples will be 1d arrays. 3. We need to convert those 1d arrays to params dictionary form so that we can plug it into the flax model and get posterior predictions.\nFirst we need to segregate the parameters layer-wise. We will use is_leaf condition to stop traversing the parameter PyTree at a perticular depth. See how it is different from vanilla tree_flatten:\n\nflat_params, tree_def = jtu.tree_flatten(params)\ndisplay(flat_params, tree_def)\n\n[DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n DeviceArray([[ 0.8275324 , -0.8314813 ],\n              [-0.8276633 ,  0.83254045]], dtype=float32),\n DeviceArray([0.01351773], dtype=float32),\n DeviceArray([[1.1750739],\n              [1.1685134]], dtype=float32)]\n\n\nPyTreeDef({'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}})\n\n\n\nis_leaf = lambda param: 'bias' in param\nlayers, tree_def = jtu.tree_flatten(params, is_leaf=is_leaf)\ndisplay(layers, tree_def)\n\n[{'bias': DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n  'kernel': DeviceArray([[ 0.8275324 , -0.8314813 ],\n               [-0.8276633 ,  0.83254045]], dtype=float32)},\n {'bias': DeviceArray([0.01351773], dtype=float32),\n  'kernel': DeviceArray([[1.1750739],\n               [1.1685134]], dtype=float32)}]\n\n\nPyTreeDef({'params': {'Dense_0': *, 'Dense_1': *}})\n\n\nThe difference is clearly evident. Now, we need to flatten the inner dictionaries to get 1d arrays.\n\nflat_params = list(map(lambda x: ravel_pytree(x)[0], layers))\nunravel_fn_list = list(map(lambda x: ravel_pytree(x)[1], layers))\ndisplay(flat_params, unravel_fn_list)\n\n[DeviceArray([-2.4912864e-04,  2.7019347e-04,  8.2753241e-01,\n              -8.3148128e-01, -8.2766330e-01,  8.3254045e-01],            dtype=float32),\n DeviceArray([0.01351773, 1.1750739 , 1.1685134 ], dtype=float32)]\n\n\n[&lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;,\n &lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;]\n\n\n\ndef modified_neg_log_joint_fn(flat_params):\n    layers = jtu.tree_map(lambda unravel_fn, flat_param: unravel_fn(flat_param), unravel_fn_list, flat_params)\n    params = tree_def.unflatten(layers)\n    return neg_log_joint(params)\n\nfull_hessian = jax.hessian(modified_neg_log_joint_fn)(flat_params)\n\n# Pick diagonal entries from the Hessian\nuseful_hessians = [full_hessian[i][i] for i in range(len(full_hessian))]\nuseful_hessians\n\n[DeviceArray([[139.07985,   0.     , 138.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 410.62708,   0.     , 136.54236,   0.     ,\n               273.08472],\n              [138.07985,   0.     , 139.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 136.54236,   0.     , 137.54236,   0.     ,\n               136.54236],\n              [  0.     ,   0.     ,   0.     ,   0.     ,   1.     ,\n                 0.     ],\n              [  0.     , 273.08472,   0.     , 136.54236,   0.     ,\n               274.08472]], dtype=float32),\n DeviceArray([[400.99997,  82.72832,  83.44101],\n              [ 82.72832,  69.43975,   0.     ],\n              [ 83.44101,   0.     ,  70.35754]], dtype=float32)]\n\n\nEach entry in above list corresponds to layer-wise hessian matrices. Now, we need to create layer-wise distributions, sample from them and reconstruct params using the similar tricks we used above:\n\nclass KFACHessianLaplace:\n    def __init__(self, map_params, model):\n        self.model = model\n        layers, self.tree_def = jtu.tree_flatten(map_params, is_leaf=lambda x: 'bias' in x)\n        flat_layers = [ravel_pytree(layer) for layer in layers]\n        self.means = list(map(lambda x: x[0], flat_layers))\n        self.unravel_fn_list = list(map(lambda x: x[1], flat_layers))\n\n        def neg_log_joint_flat(flat_params):\n            flat_layers = [self.unravel_fn_list[i](flat_params[i]) for i in range(len(flat_params))]\n            params = self.tree_def.unflatten(flat_layers)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(self.means)\n        self.useful_H = [self.H[i][i] for i in range(len(self.H))]\n        \n        self.covs = [svd_inverse(matrix) for matrix in self.useful_H]\n        \n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample_partial(self, seed, unravel_fn, mean, cov):\n        sample = jax.random.multivariate_normal(seed, mean=mean, cov=cov)\n        return unravel_fn(sample)\n    \n    def _sample(self, seed):\n        seeds = [seed for seed in jax.random.split(seed, num=len(self.means))]\n        flat_sample = jtu.tree_map(self._sample_partial, seeds, self.unravel_fn_list, self.means, self.covs)\n        sample = self.tree_def.unflatten(flat_sample)\n        return sample\n    \n    def sample(self, seed, n_samples=1):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nkfac_posterior = KFACHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 1000000\ny_pred_kfac = kfac_posterior.predict(X, seed=seed, shape=(n_samples, ))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that KFAC is approximating the trend of Full Hessian Laplace. We can visualize the Covariance matrices as below.\n\nfig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\n\n\n\n\n\n\n\n\n\n\nComparison with MCMC\nInspired from a blackjax docs example.\n\nkey = jax.random.PRNGKey(0)\nwarmup_key, inference_key = jax.random.split(key, 2)\nnum_warmup = 5000\nnum_samples = n_samples\n\ninitial_position = model.init(key, X)\ndef logprob(params): \n    return -neg_log_joint(params)\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, _ = kernel(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n\ninit = time()\nadapt = blackjax.window_adaptation(blackjax.nuts, logprob, num_warmup)\nfinal_state, kernel, _ = adapt.run(warmup_key, initial_position)\nstates = inference_loop(inference_key, kernel, final_state, num_samples)\nsamples = states.position.unfreeze()\nprint(f\"Sampled {n_samples} samples in {time()-init:.2f} seconds\")\n\nSampled 1000000 samples in 27.85 seconds\n\n\n\ny_pred_mcmc = jax.vmap(model.apply, in_axes=(0, None))(samples, X)\n\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    az.plot_dist(y_pred_mcmc[:, i], ax=ax[i], label='mcmc', color='k')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,3,figsize=(18,5))\nfig.subplots_adjust(wspace=0.1)\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\nmcmc_cov = jnp.cov(jax.vmap(lambda x: ravel_pytree(x)[0])(samples).T)\n\nsns.heatmap(mcmc_cov, ax=ax[2], annot=True, fmt = '.2f')\nax[2].set_title('MCMC');\n\n\n\n\n\n\n\n\n\n\nLibrary versions\n\n%watermark --iversions\n\nflax      : 0.6.1\nblackjax  : 0.8.2\noptax     : 0.1.3\nmatplotlib: 3.5.1\njax       : 0.3.23\narviz     : 0.12.1\nseaborn   : 0.11.2\njson      : 2.0.9"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html",
    "href": "posts/2024-12-29-object-detection-how-to.html",
    "title": "Object Detection - A how-to guide",
    "section": "",
    "text": "# Config\nimport os\n\n# Basic\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport matplotlib.pyplot as plt\n\n# Monitoring\nfrom tqdm.notebook import tqdm\n\n# IO\nfrom os.path import join, exists, basename, dirname, splitext, expanduser\nfrom glob import glob\n\n# Parallel processing\nfrom joblib import Parallel, delayed\n\nimport yaml\nfrom PIL import Image\nimport supervision as sv\nimport cv2\nfrom supervision.utils.file import list_files_with_extensions, read_txt_file\nfrom supervision.detection.utils import polygon_to_xyxy\nfrom ultralytics import YOLO\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\nfrom inference.models.utils import get_roboflow_model\nfrom roboflow import Roboflow\nfrom typing import List, Tuple\nfrom dotenv import load_dotenv\nload_dotenv()\n\n%reload_ext memory_profiler\n\nsv.__version__\n\n'0.25.1'"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#dataset",
    "href": "posts/2024-12-29-object-detection-how-to.html#dataset",
    "title": "Object Detection - A how-to guide",
    "section": "Dataset",
    "text": "Dataset\n\nDownload\n\nrf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\nws = rf.workspace(\"plan-zkend\")\nproject = ws.project(\"animals-ksxhf-plgrl\")\nversion = project.version(2)\nrf_dataset = version.download(\"yolov8\", location=\"/tmp/tmp\", overwrite=True)\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\nDownloading Dataset Version Zip in /tmp/tmp to yolov8:: 100%|| 3047/3047 [00:02&lt;00:00, 1202.83it/s]\n\n\n\n\n\n\nExtracting Dataset Version Zip to /tmp/tmp in yolov8:: 100%|| 212/212 [00:00&lt;00:00, 5963.05it/s]\n\n\n\nrf_dataset.location\n\n'/tmp/tmp'\n\n\n\n!ls -lh {rf_dataset.location}\n\ntotal 24K\n-rw-rw-r-- 1 patel_zeel patel_zeel  423 Feb  3 11:40 data.yaml\n-rw-rw-r-- 1 patel_zeel patel_zeel  141 Feb  3 11:40 README.dataset.txt\n-rw-rw-r-- 1 patel_zeel patel_zeel  896 Feb  3 11:40 README.roboflow.txt\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 18:47 test\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 18:47 train\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 22:52 valid\n\n\n\n!ls -lh {rf_dataset.location}/test\n\ntotal 8.0K\ndrwxrwxr-x 2 patel_zeel patel_zeel 4.0K Jan 17 18:47 images\ndrwxrwxr-x 2 patel_zeel patel_zeel 4.0K Jan 17 18:47 labels\n\n\n\n!ls -l {rf_dataset.location}/test/images/*.jpg | wc -l\n\n7\n\n\n\n!ls -l {rf_dataset.location}/test/labels/*.txt | wc -l\n\n7\n\n\n\nCheck a sample manually\n\nimage_paths = glob(f\"{rf_dataset.location}/test/images/*.jpg\")\nsample_image_path = image_paths[0]\nsample_image = Image.open(sample_image_path)\nsample_image\n\n\n\n\n\n\n\n\n\nlabel_paths = glob(f\"{rf_dataset.location}/test/labels/*.txt\")\nsample_label_path = label_paths[0]\nsample_label = np.loadtxt(sample_label_path, ndmin=2)\nsample_label.shape\n\n(1, 5)\n\n\n\nsample_label\n\narray([[         18,     0.67217,     0.47797,     0.53625,     0.51148]])\n\n\n\n\n\nLoad with supervision\n\n%%time\n\ndataset = sv.DetectionDataset.from_yolo(images_directory_path=f\"{rf_dataset.location}/test/images\", annotations_directory_path=f\"{rf_dataset.location}/test/labels\", data_yaml_path=f\"{rf_dataset.location}/data.yaml\")\nlen(dataset)\n\nCPU times: user 11.6 ms, sys: 0 ns, total: 11.6 ms\nWall time: 10.6 ms\n\n\n7\n\n\n\n\nVisualize\nIdeally, LabelAnnotator should show the class names on top of the bounding boxes but currently it shows class IDs. This issue is tracked here.\n\nimage_path, image, detection = dataset[0]\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_frame = box_annotator.annotate(image.copy(), detection)\nannotated_frame = label_annotator.annotate(annotated_frame.copy(), detection)\n\nImage.fromarray(annotated_frame)\n\n\n\n\n\n\n\n\nA quick fix for now.\n\nimage_path, image, detection = dataset[0]\nnp_classes = np.array(dataset.classes)\ndetection.data['class_name'] = np_classes[detection.class_id]\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_frame = box_annotator.annotate(image.copy(), detection)\nannotated_frame = label_annotator.annotate(annotated_frame.copy(), detection)\n\nImage.fromarray(annotated_frame)"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#inference",
    "href": "posts/2024-12-29-object-detection-how-to.html#inference",
    "title": "Object Detection - A how-to guide",
    "section": "Inference",
    "text": "Inference\n\nWith roboflow models\n\nrf_model = get_roboflow_model(\"yolov8s-640\")\nprediction = rf_model.infer(image)[0]\ndetection = sv.Detections.from_inference(prediction)\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\nImage.fromarray(annotated_image)\n\nSpecified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\nSpecified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n2025-02-03 11:42:55.119414030 [E:onnxruntime:Default, provider_bridge_ort.cc:1862 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn_adv.so.9: cannot open shared object file: No such file or directory\n\n2025-02-03 11:42:55.119453180 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:993 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\n\n\n\n\n\n\n\n\n\n\n\nWith ultralytics models\n\nmodel = YOLO(\"yolov8s\")\nprediction = model(image)[0]\ndetection = sv.Detections.from_ultralytics(prediction)\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\nImage.fromarray(annotated_image)\n\n\n0: 640x640 1 dog, 2 horses, 1 sheep, 1 cow, 6.3ms\nSpeed: 10.2ms preprocess, 6.3ms inference, 274.3ms postprocess per image at shape (1, 3, 640, 640)"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#metrics",
    "href": "posts/2024-12-29-object-detection-how-to.html#metrics",
    "title": "Object Detection - A how-to guide",
    "section": "Metrics",
    "text": "Metrics\n\nmodel = YOLO(\"yolov8x\")\ntargets = []\npredictions = []\nnp_classes = np.array(dataset.classes)\nfor image_path, image, target in tqdm(dataset):\n    # add class names to detection\n    # target.data['class_name'] = np_classes[target.class_id]\n\n    # remove classes not in model\n    # target = target[np.isin(target['class_name'], list(model.names.values()))]\n    # if len(target) == 0:\n    #     print(f\"Skipping {image_path} as it has no classes in model\")\n    #     continue\n    \n    prediction = model(image, verbose=False)[0]\n    detection = sv.Detections.from_ultralytics(prediction)\n    \n    # remove classes not in dataset\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n    \n    # remap class ids\n    detection.class_id = np.array([dataset.classes.index(class_name) for class_name in detection['class_name']])\n    \n    targets.append(target)\n    predictions.append(detection)\n\n\n\n\n\nmAP = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\nmAP50 = mAP.mAP_scores[0]\nmAP5095 = mAP.mAP_scores.mean()\nprint(f\"mAP50: {mAP50:.2f}, mAP50-95: {mAP5095:.2f}\")\n\nmAP50: 0.29, mAP50-95: 0.19"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#dataset-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#dataset-1",
    "title": "Object Detection - A how-to guide",
    "section": "Dataset",
    "text": "Dataset\n\nDownload\n\nif not exists('/tmp/DOTAv1.zip'):\n    # Downloaded in 4m 5s with 6.09 MB/s\n    !wget https://github.com/ultralytics/assets/releases/download/v0.0.0/DOTAv1.zip -O /tmp/DOTAv1.zip\nelse:\n    print('DOTAv1.zip already downloaded')\n    \nif not exists('/tmp/DOTAv1'):\n    !unzip /tmp/DOTAv1.zip -d /tmp\nelse:\n    print('DOTAv1 already unzipped')\n\nDOTAv1.zip already downloaded\nDOTAv1 already unzipped\n\n\n\n!ls /tmp/DOTAv1\n\n21.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nimages  labels\n\n\n\n!ls /tmp/DOTAv1/images\n\n27.24s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\ntest  train  val\n\n\n\nprint(f\"Number of train samples: {len(glob('/tmp/DOTAv1/images/train/*.jpg'))}\")\nprint(f\"Number of val samples: {len(glob('/tmp/DOTAv1/images/val/*.jpg'))}\")\n\nNumber of train samples: 1411\nNumber of val samples: 458\n\n\nKeep 20 samples in each and delete the rest\n\npaths = {'images': {}, 'labels': {}}\n\nfor split in ['train', 'val']:\n    paths['images'][split] = glob(f\"/tmp/DOTAv1/images/{split}/*.jpg\")[:100]\n    paths['labels'][split] = [p.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\") for p in paths['images'][split]]\n    !mkdir -p /tmp/DOTAv1_small/images/{split}\n    !mkdir -p /tmp/DOTAv1_small/labels/{split}\n\n    for img, label in tqdm(zip(paths['images'][split], paths['labels'][split])):\n        os.system(f\"cp {img} /tmp/DOTAv1_small/images/{split}/\")\n        os.system(f\"cp {label} /tmp/DOTAv1_small/labels/{split}/\")\n\n32.81s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n38.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\n\n\n\n44.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n49.55s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\n\n\n\n\nprint(f\"Number of train samples: {len(glob('/tmp/DOTAv1_small/images/train/*.jpg'))}\")\nprint(f\"Number of val samples: {len(glob('/tmp/DOTAv1_small/images/val/*.jpg'))}\")\n\nNumber of train samples: 100\nNumber of val samples: 100\n\n\n\n\nCheck a sample\n\ntrain_images = glob(\"/tmp/DOTAv1_small/images/train/*.jpg\")\nsample_image_path = train_images[1]\nsample_image_path\n\n'/tmp/DOTAv1_small/images/train/P2732.jpg'\n\n\n\nsample_image = Image.open(sample_image_path)\nsample_image.size\n\n(3087, 2632)\n\n\n\nsample_image.reduce(10)\n\n\n\n\n\n\n\n\n\nsample_label_path = sample_image_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\nassert exists(sample_label_path), f\"Error: {sample_label_path} does not exist\"\n\n\nsample_label = np.loadtxt(sample_label_path, ndmin=2)\nsample_label.shape\n\n(51, 9)\n\n\n\nsample_label[0]\n\narray([          7,     0.72076,     0.45023,     0.72238,     0.44871,     0.73178,     0.46087,     0.73016,     0.46201])\n\n\nThe above is YOLO-Oriented Bounding Box (OBB) format: class_id, x1, y1, x2, y2, x3, y3, x4, y4\n\n\nLoad with supervision\nsupervision does not support DOTA dataset yet, but ultralytics has already converted it to YOLO format. Lets create data.yml file for DOTA dataset.\n\n%%writefile /tmp/DOTAv1_small/data.yml\ntrain: /tmp/DOTAv1_small/images/train\nval: /tmp/DOTAv1_small/images/val\ntest: /tmp/DOTAv1_small/images/test\nnc: 15\nnames: ['plane', 'ship', 'storage tank', 'baseball diamond', 'tennis court', 'basketball court', 'ground track field', 'harbor', 'bridge', 'large vehicle', 'small vehicle', 'helicopter', 'roundabout', 'soccer ball field', 'swimming pool']\n\nOverwriting /tmp/DOTAv1_small/data.yml\n\n\n\n# %%memit\n\ndataset = sv.DetectionDataset.from_yolo(\n    \"/tmp/DOTAv1_small/images/train\",\n    \"/tmp/DOTAv1_small/labels/train\",\n    data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n    is_obb=True,\n)\n\n\n\nVisualize\n\nsample = dataset[0]\nimg_array = sample[1]\nimg_detections = sample[2]\n\nannotator = sv.OrientedBoxAnnotator()\nannotated_img = annotator.annotate(img_array, img_detections)\n\nplt.imshow(annotated_img)\nplt.ylim(0, annotated_img.shape[1] // 2)\nplt.axis(\"off\")"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#inference-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#inference-1",
    "title": "Object Detection - A how-to guide",
    "section": "Inference",
    "text": "Inference\n\niou = Non-max suppression IOU threshold\nconf = Object confidence threshold\n\n\nInline method\n\nmodel = YOLO(\"yolo11x-obb\")\n\ndetections = []\npredictions = []\nfor img_path, img, detection in tqdm(dataset):\n    prediction = model(img, imgsz=1024, iou=0.33, max_det=300, conf=0.001, verbose=False)[0]\n    predictions.append(sv.Detections.from_ultralytics(prediction))\n    detections.append(detection)\n\n\n\n\n\n\nCLI method\n\n!cd /tmp && yolo obb predict model=yolo11x-obb source=/tmp/DOTAv1_small/images/val exist_ok=True save=False save_txt=True imgsz=1024 iou=0.33 max_det=300 conf=0.001 verbose=False\n\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nYOLO11x-obb summary (fused): 483 layers, 58,752,928 parameters, 0 gradients, 202.8 GFLOPs\nResults saved to runs/obb/predict\n20 labels saved to runs/obb/predict/labels\n Learn more at https://docs.ultralytics.com/modes/predict"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#metrics-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#metrics-1",
    "title": "Object Detection - A how-to guide",
    "section": "Metrics",
    "text": "Metrics\n\nInline method\n\nConfusion matrix\n\nAt the time of writing this post, supervisions ConfusionMatrix does not support OBB. Follow this issue for updates.\n\n\nconf_threshold  minimum confidence threshold for a detection to be considered. Instances with confidence below this threshold are ignored as if they were not predicted.\niou_threshold  minimum intersection over union (IoU) threshold for a detection to be considered a true positive. Predictions with IoU below this threshold are considered false positives.\n\n\ncm = sv.ConfusionMatrix.from_detections(\n    predictions, detections, classes=dataset.classes, conf_threshold=0.25, iou_threshold=0.33\n)\n_ = cm.plot()\n\n\n\n\n\n\n\n\n\n\nPrecision, Recall & F1 Score\nYou know the formulas of Precision and Recall.\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nWe can also write them as the following:\nPrecision = TP / PP\nRecall = TP / AP\nwhere PP is the number of predicted positives and AP is the number of actual positives.\nTo calculate TP, we can sum the values along the diagonal of the confusion matrix.\n\nTP = cm.matrix.diagonal().sum()\nTP\n\nnp.float64(847.0)\n\n\nTo calculate PP, we should remove all cells which represent not predicted instances. That is nothing but FN. Thus, we will remove the last column representing FN.\n\nPP = cm.matrix[:, :-1].sum()\nPP\n\nnp.float64(931.0)\n\n\nTo calculate AP, we should remove all cells which represent predicted but wrong instances. That is nothing but FP. Thus, we will remove the last row representing FP.\n\nAP = cm.matrix[:-1, :].sum()\nAP\n\nnp.float64(1059.0)\n\n\n\nP = TP / PP\nR = TP / AP\nF1 = 2 * P * R / (P + R)\nprint(f\"P: {P:.2f}, R: {R:.2f}, F1: {F1:.2f}\")\n\nP: 0.91, R: 0.80, F1: 0.85\n\n\nNotice that to compute P, R and F1, we need to fix a confidence threshold and an IOU threshold. Now, we will see some metrics which integrate over confidence thresholds and use only IoU threshold.\nThere are specific methods in supervision to compute Precision, Recall and F1 Score, but they are significantly slow. If they become faster in future, one can use them with the following code.\n\n# f1_score = sv.metrics.MeanAveragePrecision(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# f1_score.update(predictions, detections).compute()\n\n\n# precision = sv.metrics.Precision(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# precision.update(predictions, detections).compute()\n\n\n# recall = sv.metrics.Recall(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# recall.update(predictions, detections).compute()\n\n\n\n\nCLI method\nWhen we use CLI method of inference in Ultralytics, results are saved on the disk. Now, we need to load them back to calculate metrics.\n\nfrom ultralytics.engine.results import Results\n\nThe following method is extremely space consuming unless the following issue is resolved: https://github.com/roboflow/supervision/issues/1762. Not adding further steps until the issue is resolved but the steps should be similar to the axis-aligned bounding box case.\n\npredicted_dataset = sv.DetectionDataset.from_yolo(\n    \"/tmp/DOTAv1_small/images/val\",\n    \"/tmp/runs/obb/predict/labels\",\n    data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n    is_obb=True,\n)\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[78], line 1\n----&gt; 1 predicted_dataset = sv.DetectionDataset.from_yolo(\n      2     \"/tmp/DOTAv1_small/images/val\",\n      3     \"/tmp/runs/obb/predict/labels\",\n      4     data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n      5     is_obb=True,\n      6 )\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/core.py:497, in DetectionDataset.from_yolo(cls, images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\n    445 @classmethod\n    446 def from_yolo(\n    447     cls,\n   (...)\n    452     is_obb: bool = False,\n    453 ) -&gt; DetectionDataset:\n    454     \"\"\"\n    455     Creates a Dataset instance from YOLO formatted data.\n    456 \n   (...)\n    495         ```\n    496     \"\"\"\n--&gt; 497     classes, image_paths, annotations = load_yolo_annotations(\n    498         images_directory_path=images_directory_path,\n    499         annotations_directory_path=annotations_directory_path,\n    500         data_yaml_path=data_yaml_path,\n    501         force_masks=force_masks,\n    502         is_obb=is_obb,\n    503     )\n    504     return DetectionDataset(\n    505         classes=classes, images=image_paths, annotations=annotations\n    506     )\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:182, in load_yolo_annotations(images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\n    180     with_masks = _with_mask(lines=lines)\n    181     with_masks = force_masks if force_masks else with_masks\n--&gt; 182     annotation = yolo_annotations_to_detections(\n    183         lines=lines,\n    184         resolution_wh=resolution_wh,\n    185         with_masks=with_masks,\n    186         is_obb=is_obb,\n    187     )\n    188     annotations[image_path] = annotation\n    189 return classes, image_paths, annotations\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:120, in yolo_annotations_to_detections(lines, resolution_wh, with_masks, is_obb)\n    115     return Detections(class_id=class_id, xyxy=xyxy, data=data)\n    117 polygons = [\n    118     (polygon * np.array(resolution_wh)).astype(int) for polygon in relative_polygon\n    119 ]\n--&gt; 120 mask = _polygons_to_masks(polygons=polygons, resolution_wh=resolution_wh)\n    121 return Detections(class_id=class_id, xyxy=xyxy, data=data, mask=mask)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:50, in _polygons_to_masks(polygons, resolution_wh)\n     47 def _polygons_to_masks(\n     48     polygons: List[np.ndarray], resolution_wh: Tuple[int, int]\n     49 ) -&gt; np.ndarray:\n---&gt; 50     return np.array(\n     51         [\n     52             polygon_to_mask(polygon=polygon, resolution_wh=resolution_wh)\n     53             for polygon in polygons\n     54         ],\n     55         dtype=bool,\n     56     )\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/GNNs_and_GPs.html",
    "href": "posts/GNNs_and_GPs.html",
    "title": "GNNs and GPs",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport regdata as rd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\nx_train, y_train, x_test = rd.Step().get_data()\ny_train = y_train.reshape(-1, 1)\nx_test = x_test * 1.5\nprint(x_train.shape, y_train.shape, x_test.shape)\n\nplt.scatter(x_train, y_train, label='train');\n\n(50, 1) (50, 1) (100, 1)\n\n\n\n\n\n\n\n\n\n\nkernel = GPy.kern.RBF(1, variance=1, lengthscale=1)\nmodel = GPy.models.GPRegression(x_train, y_train.reshape(-1, 1), kernel)\nmodel.Gaussian_noise.variance = 0.1\n\ny_pred_gp, y_var = model.predict(x_test)\n\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_test, y_pred_gp, label='pred');\n\n\n\n\n\n\n\n\n\nclass GCN_Forward(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = self.fc(x)\n        x = torch.matmul(A, x)\n        return x\n    \nclass GCN_Reverse(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = torch.matmul(A, x)\n        x = self.fc(x)\n        return x\n\nclass NN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.features = features\n        \n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', nn.Linear(in_features, out_features))\n            \n        self.last_layer = nn.Linear(features[-1], 1)\n        \n    def forward(self, x, A):\n        for i in range(len(self.features) - 1):\n            if isinstance(getattr(self, f'layer_{i}'), GCN_Forward):\n                x = getattr(self, f'layer_{i}')(x, A)\n            else:\n                x = getattr(self, f'layer_{i}')(x)\n            x = nn.functional.gelu(x)\n            \n        x = self.last_layer(x)\n        return x\n\nclass GCN(NN):\n    def __init__(self, features):\n        super().__init__(features)\n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', GCN_Forward(in_features, out_features))\n\n\nA = torch.tensor(kernel.K(x_train, x_train)).float()\n# A.fill_diagonal_(0)\nA = A / A.sum(dim=0, keepdim=True)\n# A.fill_diagonal_(1)\n\nnum_epochs = 500\nfeatures = [1, 1024]\n\ngcn_model = GCN(features=features)\nnn_model = NN(features=features)\n\ngcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\nnn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.01)\n\ncriterion = nn.MSELoss()\n\nx_train_torch = torch.from_numpy(x_train).float()\ny_train_torch = torch.from_numpy(y_train).float()\n\ngcn_losses = []\nnn_losses = []\nfor epoch in range(num_epochs):\n    gcn_optimizer.zero_grad()\n    nn_optimizer.zero_grad()\n    \n    y_out_gcn = gcn_model(x_train_torch, A)\n    y_out_nn = nn_model(x_train_torch, A)\n    gcn_loss = criterion(y_out_gcn, y_train_torch)\n    nn_loss = criterion(y_out_nn, y_train_torch)\n    \n    gcn_loss.backward()\n    nn_loss.backward()\n    \n    gcn_losses.append(gcn_loss.item())\n    nn_losses.append(nn_loss.item())\n    \n    gcn_optimizer.step()\n    nn_optimizer.step()\n        \nplt.plot(gcn_losses, label='gcn');\nplt.plot(nn_losses, label='nn');\nplt.legend();\n\n\n\n\n\n\n\n\n\nA_test = torch.tensor(kernel.K(x_test, x_test)).float()\n# A_test.fill_diagonal_(0)\nA_test = A_test / A_test.sum(dim=0, keepdim=True)\n# A_test.fill_diagonal_(1)\n\ny_pred_nn = nn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\ny_pred_gcn = gcn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_train, y_out_gcn.detach().numpy(), label='pred GCN train');\nplt.plot(x_train, y_out_nn.detach().numpy(), label='pred NN train');\nplt.plot(x_test, y_pred_gp, label='pred GP', linestyle='--');\nplt.plot(x_test, y_pred_nn, label='pred NN');\nplt.plot(x_test, y_pred_gcn, label='pred GCN');\nplt.ylim(-3, 3);\nplt.legend();"
  },
  {
    "objectID": "posts/presentation_tips.html",
    "href": "posts/presentation_tips.html",
    "title": "Conference Presentation Tips",
    "section": "",
    "text": "General\n\nFirst page goes like this:\n\nTitle\nAuthors (Underline presenting author, no need to put * in case of equal contribution)\nAffiliations\nConference name\n\nIf importing figures from paper, avoid including the captions.\nInclude lot of images and less maths\nTalk should end with summary and not the future work or thank you slide or something.\nCite the references on the same slide in bottom.\n\nRefer to Giving talks section of this blog.\n\n\nDos and Donts\n\nNever put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram."
  },
  {
    "objectID": "posts/GNN_for_regression.html",
    "href": "posts/GNN_for_regression.html",
    "title": "Graph Neural Networks for Regression",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport GPy\n\nimport torch\nimport torch.nn as nn\n\nfrom tqdm import trange\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ndevice = \"cuda\""
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "href": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a synthetic dataset",
    "text": "Create a synthetic dataset\n\nnp.random.seed(0)\ntorch.random.manual_seed(4)\n\nN = 50\nx = np.linspace(-1, 1, N).reshape(-1, 1)\nkernel = GPy.kern.RBF(input_dim=1, variance=1, lengthscale=0.1)\ny = np.random.multivariate_normal(np.zeros(N), kernel.K(x)).reshape(-1, 1)\ny_noisy = y + np.random.normal(0, 0.1, N).reshape(-1, 1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y_noisy, test_size=0.4, random_state=0)\n\nplt.plot(x, y, label=\"True\");\nplt.plot(train_x, train_y, 'o', label='train')\nplt.plot(test_x, test_y, 'o', label='test')\nplt.legend();\n\nx, y, y_noisy = map(lambda x: torch.tensor(x).float().to(device), (x, y, y_noisy))\ntrain_x, test_x, train_y, test_y = map(lambda x: torch.tensor(x).float().to(device), (train_x, test_x, train_y, test_y))\nprint(x.shape, y.shape, y_noisy.shape)\n\ntorch.Size([50, 1]) torch.Size([50, 1]) torch.Size([50, 1])"
  },
  {
    "objectID": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "href": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "title": "Graph Neural Networks for Regression",
    "section": "Fit with a simple MLP",
    "text": "Fit with a simple MLP\n\ndef fit(model, x, y, A=None, lr=0.01, epochs=100):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n    \n    if A is None:\n        inputs = (x,)\n    else:\n        inputs = (x, A)\n    \n    losses = []\n    pbar = trange(epochs)\n    for epoch in pbar:\n        optimizer.zero_grad()\n        y_hat = model(*inputs)\n        loss = loss_fn(y_hat, y)\n        losses.append(loss.item())\n        pbar.set_description(f\"Epoch {epoch} Loss: {loss.item()}\")\n        loss.backward()\n        optimizer.step()\n            \n    return losses\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [nn.Linear(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\ntorch.manual_seed(0)\nmodel = SimpleMLP([10, 10, 10]).to(device)\nfit(model, train_x, train_y, lr=0.01, epochs=1000);\n\npred_y = model(x)\n\n(x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\nplt.plot(x_, y_, label=\"True\");\nplt.plot(train_x_, train_y_, 'o', label='train')\nplt.plot(test_x_, test_y_, 'o', label='test')\nplt.plot(x_, pred_y_, label='pred')\nplt.legend();\n\nEpoch 999 Loss: 0.07143261283636093: 100%|| 1000/1000 [00:02&lt;00:00, 410.79it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "href": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a GCN layer",
    "text": "Create a GCN layer\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x, A):    \n        return self.linear(A @ x)\n    \n    \nclass GCN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [GCNLayer(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(GCNLayer(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x, A):\n        for layer in self.layers:\n            if isinstance(layer, GCNLayer):\n                x = layer(x, A)\n            else:\n                x = layer(x)\n        return x\n    \ndef get_eucledean_A(x, exponent):\n    d = ((x - x.T)**2)**0.5\n    d = torch.where(d==0, torch.min(d[d!=0])/2, d)  # self distance is 0, so replace it with half of the min distance\n    A = 1/(d**exponent)\n    return A/A.sum(dim=1, keepdim=True)\n\ndef get_KNN_A(x, k):\n    d = torch.abs(x - x.T)\n    A = torch.zeros_like(d)\n    _, indices = torch.topk(d, k, dim=1, largest=False)\n    for i, index in enumerate(indices):\n        A[i, index] = 1\n    return A/A.sum(dim=1, keepdim=True)\n\ndef fit_and_plot(title):\n    model = GCN([10, 10, 10]).to(device)\n    losses = fit(model, train_x, train_y, A=A_train, lr=0.001, epochs=3000);\n\n    pred_y = model(x, A_all)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    axes = ax[0]\n    axes.plot(losses)\n    axes.set_title(\"Losses\")\n\n    (x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\n    axes = ax[1]\n    axes.plot(x_, y_, label=\"True\");\n    axes.plot(train_x_, train_y_, 'o', label='train')\n    axes.plot(test_x_, test_y_, 'o', label='test')\n    axes.plot(x_, pred_y_, label='pred')\n    axes.set_title(title)\n    axes.legend();"
  },
  {
    "objectID": "posts/GNN_for_regression.html#idw-setting",
    "href": "posts/GNN_for_regression.html#idw-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "IDW setting",
    "text": "IDW setting\n\nexponent = 1\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.05447980388998985: 100%|| 3000/3000 [00:07&lt;00:00, 390.93it/s] \n\n\n\n\n\n\n\n\n\n\nexponent = 2\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.06475391983985901: 100%|| 3000/3000 [00:07&lt;00:00, 413.49it/s]\n\n\n\n\n\n\n\n\n\n\nexponent = 3\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.043554823845624924: 100%|| 3000/3000 [00:08&lt;00:00, 367.28it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#knn-setting",
    "href": "posts/GNN_for_regression.html#knn-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "KNN Setting",
    "text": "KNN Setting\n\nK = 1\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.04107221961021423: 100%|| 3000/3000 [00:07&lt;00:00, 383.88it/s] \n\n\n\n\n\n\n\n\n\n\nK = 3\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.14372628927230835: 100%|| 3000/3000 [00:07&lt;00:00, 404.74it/s]\n\n\n\n\n\n\n\n\n\n\nK = 7\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.13950258493423462: 100%|| 3000/3000 [00:07&lt;00:00, 381.66it/s]\n\n\n\n\n\n\n\n\n\n\nK = 15\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.33879855275154114: 100%|| 3000/3000 [00:07&lt;00:00, 376.56it/s]"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html",
    "href": "posts/non-gaussian-likelihood-mlps.html",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "",
    "text": "# %pip install mapie\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mapie.metrics import regression_coverage_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\n\nN = 100\nx = dist.Uniform(-1, 1).sample((N, 1)).sort(dim=0).values\nx_test = torch.linspace(-1, 1, 2 * N).view(-1, 1).sort(dim=0).values\ny = 3 * x**3 - 2 * x + 1\ny_noisy = y + dist.Gamma(0.1, 0.3).sample((N, 1))\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\n\nplt.legend()\nprint(\"x.shape:\", x.shape, \"y.shape:\", y.shape)\n\nx.shape: torch.Size([100, 1]) y.shape: torch.Size([100, 1])"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Define a Gaussian/Gamma MLP",
    "text": "Define a Gaussian/Gamma MLP\n\nclass ProbabilisticMLP(nn.Module):\n    def __init__(self, input_dim, feature_dims, type):\n        super().__init__()\n        self.input_dim = input_dim\n        self.feature_dims = feature_dims\n        self.type = type  # \"gaussian\" or \"gamma\"\n\n        self.layers = nn.ModuleList()\n        self.layers.append(nn.Linear(input_dim, feature_dims[0]))\n        for i in range(len(feature_dims) - 1):\n            self.layers.append(nn.Linear(feature_dims[i], feature_dims[i + 1]))\n        self.layers.append(nn.Linear(feature_dims[-1], 2))\n\n        # likelihood parameters\n        # if self.type == \"gaussian\":\n        #     self.register_buffer(\"likelihood_mean\", torch.zeros(1))\n        #     self.likelihood_log_std = nn.Parameter(torch.zeros(1))\n        # elif self.type == \"gamma\":\n        #     self.likelihood_log_concentration = nn.Parameter(torch.zeros(1))\n        #     self.likelihood_log_rate = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = torch.relu(layer(x))\n\n        if self.type == \"gaussian\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_mean = self.likelihood_mean.expand(y_pred.shape[0])\n            # likelihood_log_std = self.likelihood_log_std.expand(y_pred.shape[0])\n            # likelihood_std = torch.exp(likelihood_log_std)\n            # return y_pred, likelihood_mean, likelihood_std\n\n            y_out = self.layers[-1](x)\n            mean = y_out[:, 0]\n            log_std = y_out[:, 1]\n            std = torch.exp(log_std)\n            return mean.ravel(), std.ravel()\n\n        elif self.type == \"gamma\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_log_concentration = self.likelihood_log_concentration.expand(\n            #     y_pred.shape[0]\n            # )\n            # likelihood_log_rate = self.likelihood_log_rate.expand(y_pred.shape[0])\n            # likelihood_concentration = torch.exp(likelihood_log_concentration)\n            # likelihood_rate = torch.exp(likelihood_log_rate)\n            # return y_pred, likelihood_concentration, likelihood_rate\n\n            y_out = self.layers[-1](x)\n            log_concentration = y_out[:, 0]\n            log_rate = y_out[:, 1]\n            concentration = torch.exp(log_concentration)\n            rate = torch.exp(log_rate)\n            return concentration, rate\n\n    def loss_fn(self, y, param1, param2):\n        if self.type == \"gaussian\":\n            # epsilon = y - y_pred\n            # mean = param1\n            # std = param2\n            # dist = torch.distributions.Normal(mean, std + 1e-6)\n            # return -dist.log_prob(epsilon).mean()\n            mean = param1\n            std = param2\n            dist = torch.distributions.Normal(mean, std + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()\n\n        elif self.type == \"gamma\":\n            # epsilon = torch.clip(y - y_pred, min=1e-6, max=1e6)\n            # concentration = param1\n            # rate = param2\n            # dist = torch.distributions.Gamma(concentration, rate)\n            # return -dist.log_prob(epsilon).mean()\n            concentration = param1\n            rate = param2\n            dist = torch.distributions.Gamma(concentration + 1e-3, rate + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gaussian MLP",
    "text": "Fit Gaussian MLP\n\ntorch.manual_seed(0)\n\nmodel = ProbabilisticMLP(1, [32, 32], \"gaussian\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 500\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.4503: 100%|| 500/500 [00:01&lt;00:00, 291.18it/s]\n\n\n\n\n\n\n\n\n\n\n# sns.kdeplot(param2.cpu().detach().numpy(), label=\"std\")\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x_test.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n    # y_mean = y_pred.cpu().numpy().ravel() + mean.cpu().numpy().ravel()\n    # y_std = std.cpu().numpy().ravel()\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mean, label=\"y_mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    y_mean - 2 * y_std,\n    y_mean + 2 * y_std,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n\nupper = y_mean + 2 * y_std\nlower = y_mean - 2 * y_std\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.91"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gamma MLP",
    "text": "Fit Gamma MLP\n\nmodel = ProbabilisticMLP(1, [32, 32, 32], \"gamma\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 1000\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0775: 100%|| 1000/1000 [00:03&lt;00:00, 266.98it/s]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.special import gammaincinv, gamma\n\nwith torch.no_grad():\n    concetration, rate = model(x_test.to(device))\n    concetration = concetration.cpu().ravel().numpy()\n    rate = rate.cpu().ravel().numpy()\n\n    y_mode = (concetration - 1) / rate\n\n    quantile_fn = lambda p: gammaincinv(concetration, gamma(concetration) * p) / rate\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mode, label=\"mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    lower,\n    upper,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    param1, param2 = model(x.to(device))\n    concetration = param1.cpu().numpy().ravel()\n    rate = param2.cpu().numpy().ravel()\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.07"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html",
    "href": "posts/2022-03-08-torch-essentials.html",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "href": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "href": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "title": "Torch essentials",
    "section": "Gradient is all you need",
    "text": "Gradient is all you need\n\nimport matplotlib.pyplot as plt\n\n\nx = torch.rand(5,1)\ny = 3 * x + 2 + torch.randn_like(x)*0.1\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nx_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\nx_plus_ones.shape\n\ntorch.Size([5, 2])\n\n\n\ntheta = torch.zeros(2,1, requires_grad=True)\ntheta\n\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\ntheta.grad\n\n\ntheta.grad_fn\n\n\nlr = 0.1\n\ny_pred = x_plus_ones@theta\nloss = ((y_pred - y)**2).mean()\nloss.backward()\n# y_pred = torch.matmul(x_plus_ones, theta)\n# y_pred = torch.mm(x_plus_ones, theta)\n\n\ntheta.grad # dloss/dtheta\n\ntensor([[-6.3681],\n        [-2.8128]])\n\n\n\ntheta.grad_fn\n\n\ntheta.data -= lr * theta.grad.data\n\n\ntheta\n\ntensor([[0.6368],\n        [0.2813]], requires_grad=True)\n\n\n\ntheta.grad_fn\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)\n\n\n\n\n\n\n\n\n\nfor i in range(10):\n    theta.grad.data.zero_()\n    y_pred = x_plus_ones@theta\n    loss = ((y_pred - y)**2).mean()\n    loss.backward()\n    theta.data -= lr * theta.grad\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#advanced",
    "href": "posts/2022-03-08-torch-essentials.html#advanced",
    "title": "Torch essentials",
    "section": "Advanced",
    "text": "Advanced\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = torch.nn.Parameter(torch.zeros(2,1))\n#         self.register_parameter(theta, torch.zeros(2,1))\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = x_plus_ones@self.theta\n        return y_pred\n\n\nmodel = LinearRegression()\nmodel\n\nLinearRegression()\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value)\n\ntheta Parameter containing:\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    \n    optimizer.step()\n\n\nmodel.state_dict()\n\nOrderedDict([('theta',\n              tensor([[0.9799],\n                      [0.9808]]))])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "href": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "title": "Torch essentials",
    "section": "Wanna run on GPU?",
    "text": "Wanna run on GPU?\n\nx_gpu = x.to(device)\ny_gpu = y.to(device)\n\n\nprint(model.theta)\nmodel.to(device)\nprint(model.theta)\n\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], requires_grad=True)\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], device='cuda:0', requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x_gpu)\n    loss = loss_fn(y_pred, y_gpu)\n    loss.backward()\n    \n    optimizer.step()"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "href": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "title": "Torch essentials",
    "section": "State dictionary",
    "text": "State dictionary\n\n# torch.save(model.state_dict(), path)\n# model.load_state_dict(torch.load(path))"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#nn-way",
    "href": "posts/2022-03-08-torch-essentials.html#nn-way",
    "title": "Torch essentials",
    "section": "NN way",
    "text": "NN way\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(2, 1) # torch.nn.Linear(128, 64)\n        # What else? \n#         self.activation = torch.nn.ReLU()\n#         torch.nn.LSTM()\n#         torch.nn.Conv2d()\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = self.layer(x_plus_ones)\n        return y_pred"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html",
    "href": "posts/bayesian-gaussian-basis-regression.html",
    "title": "Bayesian Basis Regression",
    "section": "",
    "text": "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport pandas as pd\nimport regdata as rd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\"\nrd.set_backend(\"torch\")"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "href": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "title": "Bayesian Basis Regression",
    "section": "Generate data",
    "text": "Generate data\n\n# x = torch.linspace(-1, 1, 100)\n# y = (torch.sin(x * 2 * torch.pi) + torch.randn(x.size()) * 0.1).unsqueeze(1)\nx, y, _ = rd.MotorcycleHelmet().get_data()\nx = x.ravel().to(torch.float32)\nidx = np.argsort(x)\nx = x[idx]\ny = y.to(torch.float32)\ny = y[idx]\n\nx = torch.vstack([torch.ones_like(x), x]).T\nprint(x.shape, y.shape)\nx = x.to(device)\ny = y.to(device)\nprint(x.dtype, y.dtype)\n\nplt.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n\ntorch.Size([94, 2]) torch.Size([94])\ntorch.float32 torch.float32\n\n\n\n\n\n\n\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, neurons, transform=None):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.transform = transform\n        if transform is None:\n            self.transform = lambda x: x\n            self.layers.append(nn.Linear(in_dim, neurons[0]))\n        else:\n            self.layers.append(nn.Linear(self.transform.n_grid + 1, neurons[0]))\n        for i in range(1, len(neurons)):\n            self.layers.append(nn.Linear(neurons[i - 1], neurons[i]))\n        self.layers.append(nn.Linear(neurons[-1], out_dim))\n\n    def forward(self, x):\n        x = self.transform(x)\n        # print(x.shape)\n        for layer in self.layers[:-1]:\n            x = F.gelu(layer(x))\n        return self.layers[-1](x)\n\n\nclass RBF(nn.Module):\n    def __init__(self, log_gauss_var, n_grid):\n        super().__init__()\n        self.log_gauss_var = nn.Parameter(torch.tensor(log_gauss_var))\n        self.n_grid = n_grid\n        self.grid = nn.Parameter(torch.linspace(-1, 1, n_grid))\n        self.register_buffer(\"bias\", torch.zeros(1))\n\n    def forward(self, x):\n        self.dist = dist.Normal(self.grid, torch.exp(self.log_gauss_var))\n        features = torch.exp(self.dist.log_prob(x[:, 1:2]))\n        # print(features.shape)\n        features = torch.cat(\n            [\n                torch.ones_like(self.bias.repeat(features.shape[0])).reshape(-1, 1),\n                features,\n            ],\n            dim=1,\n        )\n        return features\n\n\nRBF(0.0, 10).to(device)(x).shape\n\ntorch.Size([94, 11])\n\n\n\n# def transform_fn(x):\n#     all_x = []\n#     for i in range(2, 11):\n#         all_x.append(x[:, 1:2] ** i)\n#     return torch.hstack([x] + all_x)\n\n\ndef get_mn_sn(x, s0):\n    x = transform_fn(x)\n    sn_inv = (x.T @ x) / torch.exp(log_var_noise)\n    diag = sn_inv.diagonal()\n    diag += 1 / s0\n    sn = torch.inverse(sn_inv)\n    mn = sn @ ((x.T @ y) / torch.exp(log_var_noise))\n    return mn, sn\n\n\ndef neg_log_likelihood(x, y, m0, s0):\n    x = transform_fn(x)\n    cov = (x @ x.T) / s0\n    diag = cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    return (\n        -dist.MultivariateNormal(m0.repeat(y.shape[0]), cov).log_prob(y.ravel()).sum()\n    )\n\n\ndef get_pred_post(sn, mn, x):\n    x = transform_fn(x)\n    pred_cov = x @ sn @ x.T\n    diag = pred_cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    pred_mean = x @ mn\n    return pred_mean, pred_cov\n\n\ndef plot_preds_and_95(ax, x, pred_mean, pred_cov):\n    with torch.no_grad():\n        x = x[:, 1].cpu().numpy()\n        pred_mean = pred_mean.ravel().cpu().numpy()\n        pred_var = pred_cov.diagonal().cpu().numpy()\n        ax.plot(x, pred_mean, color=\"red\", label=\"mean\")\n        ax.fill_between(\n            x,\n            (pred_mean - 2 * np.sqrt(pred_var)),\n            (pred_mean + 2 * np.sqrt(pred_var)),\n            color=\"red\",\n            alpha=0.2,\n            label=\"95% CI\",\n        )\n        return ax\n\n\nmlp = MLP(2, 1, [256, 256, 256]).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = True\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 30.6285: 100%|| 500/500 [00:02&lt;00:00, 209.49it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    # ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\ntorch.exp(log_var_noise), s0, m0\n\n(tensor(0.1191, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;),\n tensor(1.3897, device='cuda:0', requires_grad=True),\n tensor([-0.0693], device='cuda:0', requires_grad=True))"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "href": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "title": "Bayesian Basis Regression",
    "section": "Add Gaussian transform",
    "text": "Add Gaussian transform\n\nmlp = MLP(2, 1, [256, 256, 256], transform=RBF(0.1, 10)).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: -29.9227: 100%|| 500/500 [00:03&lt;00:00, 156.90it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "href": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "title": "Bayesian Basis Regression",
    "section": "Just Gaussian basis",
    "text": "Just Gaussian basis\n\n# mlp = MLP(2, 1, [32, 32, 32], transform=RBF(0.1, 10)).to(device)\nmlp = RBF(1.0, 5).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.001)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 207.0843: 100%|| 500/500 [00:02&lt;00:00, 195.61it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "href": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "title": "Bayesian Basis Regression",
    "section": "Appendix",
    "text": "Appendix\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\ndata = pd.read_csv(\"~/datasets/uci/bike/hour.csv\", header=None).iloc[:, 1:]\ndata.shape\n\n(17379, 18)\n\n\n\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nX_train = x_scaler.fit_transform(X_train)\ny_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\nX_test = x_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test.reshape(-1, 1))\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((10427, 17), (6952, 17), (10427, 1), (6952, 1))\n\n\n\n[X_train, X_test, y_train, y_test] = map(\n    lambda x: torch.tensor(x, dtype=torch.float32).to(device),\n    [X_train, X_test, y_train, y_test],\n)\n\n\nmlp = MLP(17, 1, [10, 10]).to(device)\n\noptimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = F.mse_loss(mlp(X_train), y_train)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0040: 100%|| 500/500 [00:01&lt;00:00, 482.25it/s]\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_pred = mlp(X_test).cpu().numpy()\n    if isinstance(y_test, torch.Tensor):\n        y_test = y_test.cpu().numpy()\n    print(y_pred.shape, y_test.shape)\n    print(\"RMSE\", mean_squared_error(y_test, y_pred, squared=False))\n\n(6952, 1) (6952, 1)\nRMSE 0.08354535"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html",
    "href": "posts/2022-01-24-query_by_committee.html",
    "title": "Query by Committee",
    "section": "",
    "text": "# Common imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\n\nplt.style.use('fivethirtyeight')\nrc('animation', html='jshtml')\n\n# Copy the models\nfrom copy import deepcopy\n\n# Sklearn imports\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Entropy function\nfrom scipy.stats import entropy\n\n# Progress helper\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "title": "Query by Committee",
    "section": "QBC by posterior sampling",
    "text": "QBC by posterior sampling\n\nInteresting fact: For probabilistic models, QBC is similar to uncertainty sampling. How?\n\nDraw \\(k\\) parameter sets from the posterior distribution representing \\(k\\) different models.\nQuery a point which shows maximum disagreement among the points."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "href": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "title": "Query by Committee",
    "section": "An example: Bayesian linear regression",
    "text": "An example: Bayesian linear regression\n\nnp.random.seed(0)\nN = 10\nX = np.linspace(-1,1,N).reshape(-1,1)\n\nt0 = 3\nt1 = 2\n\ny = X * t1 + t0 + np.random.rand(N,1)\n\nplt.scatter(X, y);\n\n\n\n\n\n\n\n\n\nAssume a posterior\n\nn_samples = 50\n\nt0_dist_samples = np.random.normal(t0, 0.1, size=n_samples)\nt1_dist_samples = np.random.normal(t1, 1, size=n_samples)\n\n\n\nPlot the models\n\nplt.scatter(X, y)\n\nfor i in range(len(t0_dist_samples)):\n    sample_t0 = t0_dist_samples[i]\n    sample_t1 = t1_dist_samples[i]\n    \n    plt.plot(X, X * sample_t1 + sample_t0,alpha=0.1)"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "title": "Query by Committee",
    "section": "QBC by bootstrapping",
    "text": "QBC by bootstrapping\n\n2 class dataset\n\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=3, shuffle=True)\n\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=y);\n\n\n\n\n\n\n\n\n\n\nFull data fit with RF\n\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X, y);\n\nRandomForestClassifier(random_state=0)\n\n\n\n\nVisualize decision boundary\n\ngrid_X1, grid_X2 = np.meshgrid(np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100), \n                    np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100))\n\ngrid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())]\n\ngrid_pred = model.predict(grid_X)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], c=y);\nplt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2);\n\n\n\n\n\n\n\n\n\n\nTrain, pool, test split\n\nX_train_pool, X_test, y_train_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nX_train, X_pool, y_train, y_pool = train_test_split(X_train_pool, y_train_pool, train_size=20, random_state=0)\n\nX_list = [X_train, X_pool, X_test]\ny_list = [y_train, y_pool, y_test]\nt_list = ['Train', 'Pool', 'Test']\n\nfig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True)\nfor i in range(3):\n    ax[i].scatter(X_list[i][:,0], X_list[i][:,1], c=y_list[i])\n    ax[i].set_title(t_list[i])\n    \n\n\n\n\n\n\n\n\n\n\nFitting a model on initial train data\n\nAL_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\nAL_model.fit(X_train, y_train);\n\nRandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\n\nGet the votes from trees on pool dataset\n\nvotes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n\nfor learner_idx, learner in enumerate(AL_model.estimators_):\n    votes[:, learner_idx] = learner.predict(X_pool)\n\n\nvotes.shape\n\n(780, 100)\n\n\n\nvotes\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 1., 1., ..., 0., 1., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nConvert to probabilities\n\np_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n\nfor vote_idx, vote in enumerate(votes):\n    vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()}\n\n    for class_idx, class_label in enumerate(range(X.shape[1])):\n        p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n\n\np_vote\n\narray([[1.  , 0.  ],\n       [0.89, 0.11],\n       [0.06, 0.94],\n       ...,\n       [0.93, 0.07],\n       [1.  , 0.  ],\n       [1.  , 0.  ]])\n\n\n\n\nCalculate dissimilarity (entropy)\n\nexample_id = 2\n\n\nans = 0\nfor category in range(X_pool.shape[1]):\n    ans += (-p_vote[example_id][category] * np.log(p_vote[example_id][category]))\n\nans\n\n0.22696752250060448\n\n\n\nentr = entropy(p_vote, axis=1)\n\n\nentr[example_id]\n\n0.22696752250060448\n\n\n\n\nActive Learning Flow\n\ndef get_query_idx():\n    # Gather the votes\n    votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n    for learner_idx, learner in enumerate(AL_model.estimators_):\n        votes[:, learner_idx] = learner.predict(X_pool)\n    \n    # Calcuate probability of votes\n    p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n    for vote_idx, vote in enumerate(votes):\n        vote_counter = {0 : (1-vote).sum(), \n                    1 : vote.sum()}\n\n        for class_idx, class_label in enumerate(range(X.shape[1])):\n            p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n    \n    # Calculate entropy for each example\n    entr = entropy(p_vote, axis=1)\n    \n    # Choose example with highest entropy (disagreement)\n    return entr.argmax()\n\n\n\nPrepare data for random sampling\n\nX_train_rand = X_train.copy()\ny_train_rand = y_train.copy()\nX_pool_rand = X_pool.copy()\ny_pool_rand = y_pool.copy()\n\nrandom_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\nRun active learning\n\nAL_iters = 100\nnp.random.seed(0)\n\nAL_inds = []\nAL_models = []\nrandom_inds = []\nrandom_models = []\n\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    ######## Active Learning ############\n    # Fit the model\n    AL_model.fit(X_train, y_train)\n    AL_models.append(deepcopy(AL_model))\n    \n    # Query a point\n    query_idx = get_query_idx()\n    AL_inds.append(query_idx)\n    \n    # Add it to the train data\n    X_train = np.concatenate([X_train, X_pool[query_idx:query_idx+1, :]], axis=0)\n    y_train = np.concatenate([y_train, y_pool[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool = np.delete(X_pool, query_idx, axis=0)\n    y_pool = np.delete(y_pool, query_idx, axis=0)\n    \n    ######## Random Sampling ############\n     # Fit the model\n    random_model.fit(X_train_rand, y_train_rand)\n    random_models.append(deepcopy(random_model))\n    \n    # Query a point\n    query_idx = np.random.choice(len(X_pool))\n    random_inds.append(query_idx)\n    # Add it to the train data\n    X_train_rand = np.concatenate([X_train_rand, X_pool_rand[query_idx:query_idx+1, :]], axis=0)\n    y_train_rand = np.concatenate([y_train_rand, y_pool_rand[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool_rand = np.delete(X_pool_rand, query_idx, axis=0)\n    y_pool_rand = np.delete(y_pool_rand, query_idx, axis=0)\n\niteration 99\n\n\n\n\nPlot accuracy\n\nrandom_scores = []\nAL_scores = []\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test)))\n    random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test)))\n    \nplt.plot(AL_scores, label='Active Learning');\nplt.plot(random_scores, label='Random Sampling');\nplt.legend();\nplt.xlabel('Iterations');\nplt.ylabel('Accuracy\\n(Higher is better)');\n\niteration 99\n\n\n\n\n\n\n\n\n\n\n\nPlot decision boundary\n\ndef update(i):\n    for each in ax:\n        each.cla()\n        \n    AL_grid_preds = AL_models[i].predict(grid_X)\n    random_grid_preds = random_models[i].predict(grid_X)\n    \n    # Active learning\n    ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label='initial_train', alpha=0.2)\n    ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], \n                  c=y_train[n_train:n_train+i], label='new_points')\n    ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[0].set_title('New points')\n    \n    ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[1].set_title('Test points');\n    ax[0].text(locs[0],locs[1],'Active Learning')\n    \n    # Random sampling\n    ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label='initial_train', alpha=0.2)\n    ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], \n                  c=y_train_rand[n_train:n_train+i], label='new_points')\n    ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[2].set_title('New points')\n    \n    ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[3].set_title('Test points');\n    ax[2].text(locs[0],locs[1],'Random Sampling');\n\n\nlocs = (2.7, 4)\nfig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True)\nax = ax.ravel()\nn_train = X_train.shape[0]-AL_iters\n\nanim = FuncAnimation(fig, func=update, frames=range(100))\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/ssh-macos.html",
    "href": "posts/ssh-macos.html",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that youd like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThats it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/ssh-macos.html#terminology",
    "href": "posts/ssh-macos.html#terminology",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that youd like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThats it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html",
    "href": "posts/2022-10-31-stochastic-variational-gp.html",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "",
    "text": "I recently read a compact and clean explanation of SVGP in the following blog post by Dr.Martin Ingram:\nNow, I am attempting to implement a practical code from scratch for the same (What is practical about it? Sometimes math does not simply translate to code without careful modifications). I am assuming that you have read the blog post cited above before moving further. Lets go for coding!"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Imports",
    "text": "Imports\n\n# JAX\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\n# Partially initialize functions\nfrom functools import partial\n\n# TFP\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\n# GP Kernels\nfrom tinygp import kernels\n\n# sklearn\nfrom sklearn.datasets import make_moons, make_blobs, make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Optimization\nimport optax\n\n# Plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['scatter.edgecolors'] = \"k\"\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Jitter\nJITTER = 1e-6\n\n# Enable JAX 64bit\njax.config.update(\"jax_enable_x64\", True)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Dataset",
    "text": "Dataset\nFor this blog post, we will stick to the classification problem and pick a reasonable classification dataset.\n\nn_samples = 100\nnoise = 0.1\nrandom_state = 0\nshuffle = True\n\nX, y = make_moons(\n    n_samples=n_samples, random_state=random_state, noise=noise, shuffle=shuffle\n)\nX = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n\nX, y = map(jnp.array, (X, y))\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Methodology",
    "text": "Methodology\nTo define a GP, we need a kernel function. Let us use the RBF or Exponentiated Quadratic or Squared Exponential kernel.\n\nlengthscale = 1.0\nvariance = 1.0\n\nkernel_fn = variance * kernels.ExpSquared(scale=lengthscale)\n\nkernel_fn(X, X).shape\n\n(100, 100)\n\n\nAs explained in the blog post, we want to minimize the following loss function:\n\\[\nKL[q(u|\\eta) || p(u|y, \\theta)] = KL[q(u|\\eta) || p(u | \\theta)] - \\mathbb{E}_{u \\sim q(u|\\eta)} \\log p(y | u, \\theta) + const\n\\]\nLet us break down the loss and discuss each componant.\n\nKL divergence\nIn the first term, we want to compute the KL divergence between prior and variational distribution of GP at inducing points. First, we need to define the inducing points.\n\nkey = jax.random.PRNGKey(0)\nn_inducing = 10\nn_dim = X.shape[1]\n\nX_inducing = jax.random.normal(key, shape=(n_inducing, n_dim))\nX_inducing.shape\n\n(10, 2)\n\n\nNow, defining the prior and variational distributions.\n\ngp_mean = 0.43  # a scalar parameter to train\n\nprior_mean = gp_mean * jnp.zeros(n_inducing)\nprior_cov = kernel_fn(X_inducing, X_inducing)\n\nprior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n\nvariational_mean = jax.random.uniform(key, shape=(n_inducing,)) # a vector parameter to train\n\nA covariance matrix can not be learned directly due to positive definite constraint. We can decompose a covariance matrix in a following way:\n\\[\n\\begin{aligned}\nK &= diag(\\boldsymbol{\\sigma})\\Sigma diag(\\boldsymbol{\\sigma})\\\\\n  &= diag(\\boldsymbol{\\sigma})LL^T diag(\\boldsymbol{\\sigma})\n\\end{aligned}\n\\]\nWhere, \\(\\Sigma\\) is a correlation matrix, \\(L\\) is a lower triangular cholesky decomposition of \\(\\Sigma\\) and \\(\\boldsymbol{\\sigma}\\) is the variance vector. We can use tfb.CorrelationCholesky to generate \\(L\\) from an unconstrained vector:\n\nrandom_vector = jax.random.normal(key, shape=(3,))\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\ncorrelation = corr_chol@corr_chol.T\ncorrelation\n\nDeviceArray([[ 1.        ,  0.54464529, -0.7835968 ],\n             [ 0.54464529,  1.        , -0.33059078],\n             [-0.7835968 , -0.33059078,  1.        ]], dtype=float64)\n\n\nTo constrain \\(\\boldsymbol{\\sigma}\\), any positivity constraint would suffice. So, combining these tricks, we can model the covariance as following:\n\nrandom_vector = jax.random.normal(\n    key, shape=(n_inducing * (n_inducing - 1) // 2,)\n)  # a trainable parameter\nlog_sigma = jax.random.normal(key, shape=(n_inducing, 1))  # a trainable parameter\n\n\nsigma = jnp.exp(log_sigma)\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\nvariational_cov = sigma * sigma.T * (corr_chol @ corr_chol.T)\nprint(variational_cov.shape)\n\nvariational_distribution = tfd.MultivariateNormalFullCovariance(variational_mean, variational_cov\n)\n\n(10, 10)\n\n\nNow, we can compute the KL divergence:\n\nvariational_distribution.kl_divergence(prior_distribution)\n\nDeviceArray(416.89357355, dtype=float64)\n\n\n\n\nExpectation over the likelihood\nWe want to compute the following expectation:\n\\[\n-\\sum_{i=1}^N \\mathbb{E}_{f_i \\sim q(f_i | \\eta, \\theta)} \\log p(y_i| f_i, \\theta)\n\\]\nNote that, \\(p(y_i| f_i, \\theta)\\) can be any likelihood depending upon the problem, but for classification, we may use a Bernoulli likelihood.\n\nf = jax.random.normal(key, shape=y.shape)\nlikelihood_distribution = tfd.Bernoulli(logits=f)\n\nlog_likelihood = likelihood_distribution.log_prob(y).sum()\nlog_likelihood\n\nDeviceArray(-72.04665624, dtype=float64)\n\n\nWe need to sample \\(f_i\\) from \\(q(f_i | \\eta, \\theta)\\) which has the following form:\n\\[\n\\begin{aligned}\nq(u) &\\sim \\mathcal{N}(\\boldsymbol{m}, S)\\\\\nq(f_i | \\eta, \\theta) &\\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\\\\n\\mu_i &= A\\boldsymbol{m}\\\\\n\\sigma_i^2 &= K_{ii} + A(S - K_{mm})A^T\\\\\nA &= K_{im}K_{mm}^{-1}\n\\end{aligned}\n\\]\nNote that matrix inversion is often unstable with jnp.linalg.inv and thus we will use cholesky tricks to compute \\(A\\).\n\ndef q_f(x_i):\n    x_i = x_i.reshape(1, -1) # ensure correct shape\n    K_im = kernel_fn(x_i, X_inducing)\n    K_mm = kernel_fn(X_inducing, X_inducing)\n    chol_mm = jnp.linalg.cholesky(K_mm + jnp.eye(K_mm.shape[0])*JITTER)\n    A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n    \n    mu_i = A@variational_mean\n    sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_cov - prior_cov)@A.T\n    \n    return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n\nHere is a function to compute log likelihood for a single data-point:\n\ndef log_likelihood(x_i, y_i, seed):\n    sample = q_f(x_i).sample(seed=seed)\n    log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n    return log_likelihood.squeeze()\n\n\nlog_likelihood(X[0], y[0], seed=key)\n\nDeviceArray(-0.17831203, dtype=float64)\n\n\nWe can use jax.vmap to compute log_likelihood over a batch. With that, we can leverage the stochastic variational inference following section 10.3.1 (Eq. 10.108) from pml book2. Basically, in each iteration, we need to multiply the batch log likelihood with \\(\\frac{N}{B}\\) to get an unbiased minibatch approximation where \\(N\\) is size of the full dataset and \\(B\\) is the batch size.\n\nbatch_size = 10\n\nseeds = jax.random.split(key, num=batch_size)\n\nll = len(y)/batch_size * jax.vmap(log_likelihood)(X[:batch_size], y[:batch_size], seeds).sum()\nll\n\nDeviceArray(-215.46520331, dtype=float64)\n\n\nNote that, once the parameters are optimized, we can use the derivations of \\(q(f_i | \\eta, \\theta)\\) to compute the posterior distribution. We have figured out all the pieces by now so it is the time to put it togather in a single class. Some pointers to note are the following:\n\nWe define a single function get_constrained_params to transform all unconstrained parameters.\njax.lax.scan gives a huge boost to a training loop.\nThere is some repeatation of code due to lack of super code optimization. You can do it at your end if needed."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "All in one",
    "text": "All in one\n\nclass SVGP:\n    def __init__(self, X_inducing, data_size):\n        self.X_inducing = X_inducing\n        self.n_inducing = len(X_inducing)\n        self.data_size = data_size\n        \n    def init_params(self, seed):\n        variational_corr_chol_param = tfb.CorrelationCholesky().inverse(jnp.eye(self.n_inducing))\n        \n        dummy_params = {\"log_variance\": jnp.zeros(()),\n               \"log_scale\": jnp.zeros(()), \n               \"mean\": jnp.zeros(()),\n               \"X_inducing\": self.X_inducing,\n               \"variational_mean\": jnp.zeros(self.n_inducing),\n               \"variational_corr_chol_param\": variational_corr_chol_param,\n               \"log_variational_sigma\": jnp.zeros((self.n_inducing, 1)),\n               }\n        \n        flat_params, unravel_fn = ravel_pytree(dummy_params)\n        random_params = jax.random.normal(key, shape=(len(flat_params), ))\n        params = unravel_fn(random_params)\n        return params\n    \n    @staticmethod\n    def get_constrained_params(params):\n        return {\"mean\": params[\"mean\"],\n                \"variance\": jnp.exp(params['log_variance']), \n                \"scale\": jnp.exp(params['log_scale']), \n                \"X_inducing\": params[\"X_inducing\"],\n                \"variational_mean\": params[\"variational_mean\"],\n                \"variational_corr_chol_param\": params[\"variational_corr_chol_param\"],\n                \"variational_sigma\": jnp.exp(params[\"log_variational_sigma\"])}\n    \n    @staticmethod\n    def get_q_f(params, x_i, prior_distribution, variational_distribution):\n        x_i = x_i.reshape(1, -1) # ensure correct shape\n        \n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        K_im = kernel_fn(x_i, params[\"X_inducing\"])\n        K_mm = prior_distribution.covariance()\n        chol_mm = jnp.linalg.cholesky(K_mm)\n        A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n\n        mu_i = A@params[\"variational_mean\"]\n        sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_distribution.covariance() - K_mm)@A.T\n\n        return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n    \n    def get_distributions(self, params):\n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        prior_mean = params[\"mean\"]\n        prior_cov = kernel_fn(params[\"X_inducing\"], params[\"X_inducing\"]) + jnp.eye(self.n_inducing)*JITTER\n        prior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n        corr_chol = tfb.CorrelationCholesky()(params[\"variational_corr_chol_param\"])\n        sigma = jnp.diag(params[\"variational_sigma\"])\n        variational_cov = sigma*sigma.T*(corr_chol@corr_chol.T) + jnp.eye(self.n_inducing)*JITTER\n        variational_distribution = tfd.MultivariateNormalFullCovariance(params[\"variational_mean\"], variational_cov)\n        \n        return prior_distribution, variational_distribution\n    \n    def loss_fn(self, params, X_batch, y_batch, seed):\n        params = self.get_constrained_params(params)\n        \n        # Get distributions\n        prior_distribution, variational_distribution = self.get_distributions(params)\n        \n        # Compute kl\n        kl = variational_distribution.kl_divergence(prior_distribution)\n\n        # Compute log likelihood\n        def log_likelihood_fn(x_i, y_i, seed):\n            q_f = self.get_q_f(params, x_i, prior_distribution, variational_distribution)\n            sample = q_f.sample(seed=seed)\n            log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n            return log_likelihood.squeeze()\n        \n        seeds = jax.random.split(seed, num=len(y_batch))\n        log_likelihood = jax.vmap(log_likelihood_fn)(X_batch, y_batch, seeds).sum() * self.data_size/len(y_batch)\n\n        return kl - log_likelihood\n    \n    def fit_fn(self, X, y, init_params, optimizer, n_iters, batch_size, seed):\n        state = optimizer.init(init_params)\n        value_and_grad_fn = jax.value_and_grad(self.loss_fn)\n        \n        def one_step(params_and_state, seed):\n            params, state = params_and_state\n            idx = jax.random.choice(seed, self.data_size, (batch_size,), replace=False)\n            X_batch, y_batch = X[idx], y[idx]\n            \n            seed2 = jax.random.split(seed, 1)[0]\n            loss, grads = value_and_grad_fn(params, X_batch, y_batch, seed2)\n            updates, state = optimizer.update(grads, state)\n            params = optax.apply_updates(params, updates)\n            return (params, state), (loss, params)\n        \n        seeds = jax.random.split(seed, num=n_iters)\n        (best_params, _), (loss_history, params_history) = jax.lax.scan(one_step, (init_params, state), xs=seeds)\n        return best_params, loss_history, params_history\n\n    def predict_fn(self, params, X_new):\n        constrained_params = self.get_constrained_params(params)\n        prior_distribution, variational_distribution = self.get_distributions(constrained_params)\n        \n        def _predict_fn(x_i):    \n            # Get posterior\n            q_f = self.get_q_f(constrained_params, x_i, prior_distribution, variational_distribution)\n            return q_f.mean().squeeze(), q_f.variance().squeeze()\n        \n        mean, var = jax.vmap(_predict_fn)(X_new)\n        return mean.squeeze(), var.squeeze()"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Train and predict",
    "text": "Train and predict\n\nn_inducing = 20\nn_epochs = 100\nbatch_size = 10\ndata_size = len(y)\nn_iters = n_epochs*(data_size/batch_size)\nn_iters\n\n1000.0\n\n\n\nkey = jax.random.PRNGKey(0)\nkey2, subkey = jax.random.split(key)\noptimizer = optax.adam(learning_rate=0.01)\n\nX_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\nmodel = SVGP(X_inducing, data_size)\n\ninit_params = model.init_params(key2)\n\nmodel.loss_fn(init_params, X, y, key)\nbest_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\nplt.figure()\nplt.plot(loss_history);\nplt.title(\"Loss\");\n\n\n\n\n\n\n\n\n\nx = jnp.linspace(-3.5, 3.5, 100)\nseed = jax.random.PRNGKey(123)\n\nX1, X2 = jnp.meshgrid(x, x)\nf = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\npred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\nlogits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\nproba = jax.nn.sigmoid(logits)\n\nproba_mean = proba.mean(axis=0)\nproba_std2 = proba.std(axis=0)*2\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12,4))\ncplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot1, ax=ax[0])\n\ncplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot2, ax=ax[1])\n\nax[0].scatter(X[:, 0], X[:, 1], c=y);\nax[1].scatter(X[:, 0], X[:, 1], c=y);\n\nax[0].set_title(\"Posterior $\\mu$\");\nax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Some more datasets",
    "text": "Some more datasets\n\ndef fit_and_plot(X, y):\n    X = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n    X, y = map(jnp.array, (X, y))\n\n    X_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\n    model = SVGP(X_inducing, data_size)\n\n    init_params = model.init_params(key2)\n\n    model.loss_fn(init_params, X, y, key)\n    best_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\n    plt.figure()\n    plt.plot(loss_history);\n    plt.title(\"Loss\");\n    \n    f = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\n    pred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\n    logits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\n    proba = jax.nn.sigmoid(logits)\n\n    proba_mean = proba.mean(axis=0)\n    proba_std2 = proba.std(axis=0)*2\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    cplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot1, ax=ax[0])\n\n    cplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot2, ax=ax[1])\n\n    ax[0].scatter(X[:, 0], X[:, 1], c=y);\n    ax[1].scatter(X[:, 0], X[:, 1], c=y);\n\n    ax[0].set_title(\"Posterior $\\mu$\");\n    ax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");\n\n\nmake_blobs\n\nX, y = make_blobs(n_samples=n_samples, random_state=random_state, centers=2)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmake_circles\n\nX, y = make_circles(n_samples=n_samples, random_state=random_state, noise=noise, factor=0.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html",
    "href": "posts/2024-12-10-cpcb-download.html",
    "title": "Download CPCB live data",
    "section": "",
    "text": "import os\nimport re\nfrom glob import glob\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom time import sleep\n\nHOME_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\"\nDOWNLOAD_OLD_DATA_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/caaqm-data-repository\"\nDOWNLOAD_PAGE_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/data\"\ndef click_it(driver, element):\n    driver.execute_script(\"arguments[0].click();\", element)\n    \ndef find_it(element, option):\n    return element.find_element(By.XPATH, f\"//li[contains(text(), '{option}')]\")\n\ndef select_dropdown_option(driver, element, option):\n    element.click()\n    option = find_it(element, option)\n    click_it(driver, option)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "href": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "title": "Download CPCB live data",
    "section": "Dry run to get metadata",
    "text": "Dry run to get metadata\n\n# headless chrome\noptions = Options()\noptions.add_argument(\"--headless\")\n\n# open the browser\ndriver = webdriver.Chrome(options=options)\n\n# open the website\ndriver.get(DOWNLOAD_OLD_DATA_URL)\n\n# wait for the page to load and the dropdowns to appear\ndropdowns = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".select-box\")))\nlen(dropdowns)\n\n5\n\n\n\ndrop_data_type, drop_frequency, drop_states, drop_cities, drop_stations = dropdowns\n\n\n# Select data type\nselect_dropdown_option(driver, drop_data_type, \"Raw data\")\n\n# Select frequency\nselect_dropdown_option(driver, drop_frequency, \"1 day\")\n\n# Get the states\ndrop_states.click() # Open the dropdown\nstates = drop_states.text.replace(\"\\n\", \"\").split(\"\\n\")\nprint(\"Number of states:\", len(states))\ndrop_states.click() # Close the dropdown\n\nNumber of states: 31\n\n\n\nmetadata_df = pd.DataFrame(columns=[\"State\", \"City\", \"Station\", \"site_id\"])\n\n# This loop took less than a minute to run\nprogress_bar = tqdm(total=600) # as of 2024, 560 stations. update this number if it changes\nfor state in states:\n    select_dropdown_option(driver, drop_states, state)\n    \n    # Get all cities\n    drop_cities.click() # Open the dropdown\n    cities = drop_cities.text.replace(\"\\n\", \"\").split(\"\\n\")\n    drop_cities.click() # Close the dropdown\n    \n    for city in cities:\n        select_dropdown_option(driver, drop_cities, city)\n        \n        # Get all stations\n        drop_stations.click() # Open the dropdown\n        stations = drop_stations.text.replace(\"\\n\", \"\").split(\"\\n\")\n        drop_stations.click() # Close the dropdown\n        \n        for station in stations:\n            # corner cases\n            if station == \"Municipal Corporation Office, Dharuhera - HSPCB\":\n                site_id = \"site_5044\"\n            elif station == \"Civil Lines, Ajmer - RSPCB\":\n                site_id = \"site_1392\"\n            else:\n                try:\n                    select_dropdown_option(driver, drop_stations, station)\n                except:\n                    print(\"Unable to select station\")\n                    print(station)\n                    print(drop_stations.text)\n                    continue\n                site_id = drop_stations.get_attribute(\"ng-reflect-model\")\n            metadata_df.loc[len(metadata_df)] = [state, city, station, site_id]\n            progress_bar.update(1)\n\n\n\n\n\nlen(metadata_df)\n\n560\n\n\n\nmetadata_df.head()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n2\nAndhra Pradesh\nChittoor\nGangineni Cheruvu, Chittoor - APPCB\nsite_5665\n\n\n3\nAndhra Pradesh\nKadapa\nYerramukkapalli, Kadapa - APPCB\nsite_5693\n\n\n4\nAndhra Pradesh\nRajamahendravaram\nAnand Kala Kshetram, Rajamahendravaram - APPCB\nsite_1399\n\n\n\n\n\n\n\n\nmetadata_df.tail()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n555\nWest Bengal\nKolkata\nRabindra Bharati University, Kolkata - WBPCB\nsite_296\n\n\n556\nWest Bengal\nKolkata\nFort William, Kolkata - WBPCB\nsite_5110\n\n\n557\nWest Bengal\nKolkata\nVictoria, Kolkata - WBPCB\nsite_309\n\n\n558\nWest Bengal\nKolkata\nBidhannagar, Kolkata - WBPCB\nsite_5129\n\n\n559\nWest Bengal\nSiliguri\nWard-32 Bapupara, Siliguri - WBPCB\nsite_1419\n\n\n\n\n\n\n\n\nfor site_id, more_than_1 in (metadata_df.site_id.value_counts() &gt; 1).items():\n    if more_than_1:\n        print(metadata_df[metadata_df.site_id == site_id])\n\n           State        City                               Station    site_id\n25         Bihar  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n254  Maharashtra  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n           State        City                              Station   site_id\n26         Bihar  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n255  Maharashtra  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n             State           City                                    Station  \\\n499  Uttar Pradesh  Greater Noida  Knowledge Park - V, Greater Noida - UPPCB   \n526  Uttar Pradesh          Noida  Knowledge Park - V, Greater Noida - UPPCB   \n\n       site_id  \n499  site_5121  \n526  site_5121  \n             State           City  \\\n498  Uttar Pradesh  Greater Noida   \n525  Uttar Pradesh          Noida   \n\n                                         Station    site_id  \n498  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n525  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n           State        City                              Station    site_id\n28         Bihar  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n257  Maharashtra  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n           State        City                           Station    site_id\n27         Bihar  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n256  Maharashtra  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n\n\n\n# clean up\ndrop_items = [metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - III, Greater Noida - UPPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"More Chowk Waluj, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Maharashtra\") & (metadata_df.Station == \"Gurdeo Nagar, Aurangabad - BSPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"Rachnakar Colony, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - V, Greater Noida - UPPCB\")].index.item()]\n\nmetadata_df.drop(drop_items, inplace=True)\nlen(metadata_df)\n\n554\n\n\n\nassert set(metadata_df.site_id.value_counts()) == {1}\n\n\nmetadata_df.to_csv(\"metadata.csv\", index=False)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "href": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "title": "Download CPCB live data",
    "section": "Downloading data",
    "text": "Downloading data\n\n# URL is specific to PM2.5 and PM10 so update it as per your needs\ndef get_url(state, city, site_id):\n    return f\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-view-data-report/%2522%257B%255C%2522parameter_list%255C%2522%253A%255B%257B%255C%2522id%255C%2522%253A0%252C%255C%2522itemName%255C%2522%253A%255C%2522PM2.5%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_193%255C%2522%257D%252C%257B%255C%2522id%255C%2522%253A1%252C%255C%2522itemName%255C%2522%253A%255C%2522PM10%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_215%255C%2522%257D%255D%252C%255C%2522criteria%255C%2522%253A%255C%252224%2520Hours%255C%2522%252C%255C%2522reportFormat%255C%2522%253A%255C%2522Tabular%255C%2522%252C%255C%2522fromDate%255C%2522%253A%255C%252201-01-2024%2520T00%253A00%253A00Z%255C%2522%252C%255C%2522toDate%255C%2522%253A%255C%252211-12-2024%2520T16%253A45%253A59Z%255C%2522%252C%255C%2522state%255C%2522%253A%255C%2522{state.replace(' ', '%2520')}%255C%2522%252C%255C%2522city%255C%2522%253A%255C%2522{city.replace(' ', '%2520')}%255C%2522%252C%255C%2522station%255C%2522%253A%255C%2522{site_id}%255C%2522%252C%255C%2522parameter%255C%2522%253A%255B%255C%2522parameter_193%255C%2522%252C%255C%2522parameter_215%255C%2522%255D%252C%255C%2522parameterNames%255C%2522%253A%255B%255C%2522PM2.5%255C%2522%252C%255C%2522PM10%255C%2522%255D%257D%2522\"\n\n\n# add download directory\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\"prefs\", {\n    \"download.default_directory\": \"/Users/project561/cpcb_downloads\"\n})\n\ndriver = webdriver.Chrome(options=options)\ndriver.get(HOME_URL)\n\nEnter Captcha manually before moving ahead\n\nmetadata_df = pd.read_csv(\"metadata.csv\")\nmetadata_df.head(2)\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n\n\n\n\n\n\nfiles = glob(\"/Users/project561/cpcb_downloads/*.xlsx\")\nprint(\"Number of files in the download directory:\", len(files))\nsite_ids = [re.search(r\"site_\\d+?2024\", file).group()[:-4] for file in files]\n# assert len(set(site_ids)) == len(site_ids), pd.Series(site_ids).value_counts()\nsite_ids = set(site_ids)\n\nfor i in range(len(metadata_df)):\n    state, city, station, site_id = metadata_df.iloc[i]\n    if site_id in site_ids:\n        # print(\"Already downloaded\", i, state, city, station, site_id)\n        continue\n    print(\"Downloading\", i, state, city, station, site_id)\n    url = get_url(state, city, site_id)\n    \n    # open new tab\n    driver.execute_script(\"window.open('');\")\n    driver.switch_to.window(driver.window_handles[-1])\n    driver.get(url)\n    excel_button = WebDriverWait(driver, 20).until(\n    EC.element_to_be_clickable((By.CLASS_NAME, \"fa-file-excel-o\")))\n    click_it(driver, excel_button)\n    sleep(1)\n    \n    if len(driver.window_handles) &gt; 10:\n        # close first 9 windows\n        for _ in range(9):\n            driver.switch_to.window(driver.window_handles[0])\n            driver.close()\n            \n        driver.switch_to.window(driver.window_handles[-1])\n        sleep(1)\n\nNumber of files in the download directory: 302\nDownloading 301 Maharashtra Nagpur Ram Nagar, Nagpur - MPCB site_5793\nDownloading 302 Maharashtra Nagpur Mahal, Nagpur - MPCB site_5796\nDownloading 303 Maharashtra Nagpur Opp GPO Civil Lines, Nagpur - MPCB site_303\nDownloading 304 Maharashtra Nagpur Ambazari, Nagpur - MPCB site_5792\nDownloading 305 Maharashtra Nanded Sneh Nagar, Nanded - MPCB site_5795\nDownloading 306 Maharashtra Nashik Pandav Nagari, Nashik - MPCB site_5779\nDownloading 307 Maharashtra Nashik MIDC Ambad, Nashik - MPCB site_5781\nDownloading 308 Maharashtra Nashik Gangapur Road, Nashik - MPCB site_304\nDownloading 309 Maharashtra Nashik Hirawadi, Nashik - MPCB site_5782\nDownloading 310 Maharashtra Navi Mumbai Tondare-Taloja, Navi Mumbai - MPCB site_5803\nDownloading 311 Maharashtra Navi Mumbai Sanpada, Navi Mumbai - MPCB site_5815\nDownloading 312 Maharashtra Navi Mumbai Airoli, Navi Mumbai - MPCB site_261\nDownloading 313 Maharashtra Navi Mumbai Mahape, Navi Mumbai - MPCB site_5114\nDownloading 314 Maharashtra Navi Mumbai Kopripada-Vashi, Navi Mumbai - MPCB site_5805\nDownloading 315 Maharashtra Navi Mumbai Sector-19A Nerul, Navi Mumbai - IITM site_5401\nDownloading 316 Maharashtra Navi Mumbai Nerul, Navi Mumbai - MPCB site_5103\nDownloading 317 Maharashtra Navi Mumbai Sector-2E Kalamboli, Navi Mumbai - MPCB site_5799\nDownloading 318 Maharashtra Parbhani Masoom Colony, Parbhani - MPCB site_5794\nDownloading 319 Maharashtra Pimpri-Chinchwad Park Street Wakad, Pimpri Chinchwad - MPCB site_5764\nDownloading 320 Maharashtra Pimpri-Chinchwad Savta Mali Nagar, Pimpri-Chinchwad - IITM site_5998\nDownloading 321 Maharashtra Pimpri-Chinchwad Thergaon, Pimpri Chinchwad - MPCB site_5765\nDownloading 322 Maharashtra Pimpri-Chinchwad Gavalinagar, Pimpri Chinchwad - MPCB site_5763\nDownloading 323 Maharashtra Pune Revenue Colony-Shivajinagar, Pune - IITM site_5409\nDownloading 324 Maharashtra Pune Mhada Colony, Pune - IITM site_5404\nDownloading 325 Maharashtra Pune Savitribai Phule Pune University, Pune - MPCB site_5767\nDownloading 326 Maharashtra Pune Bhumkar Nagar, Pune - IITM site_5988\nDownloading 327 Maharashtra Pune Hadapsar, Pune - IITM site_5407\nDownloading 328 Maharashtra Pune Karve Road, Pune - MPCB site_292\nDownloading 329 Maharashtra Pune Alandi, Pune - IITM site_5405"
  },
  {
    "objectID": "posts/CNPs_for_Images.html",
    "href": "posts/CNPs_for_Images.html",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# turn off preallocation by JAX\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nimport distrax as dx\n\nimport optax\n\n# load mnist dataset from tensorflow datasets\nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n# define initializers\ndef first_layer_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-1.0/num_input, maxval=1.0/num_input)\n\ndef other_layers_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-np.sqrt(6 / num_input)/30, maxval=np.sqrt(6 / num_input)/30)\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n    \n    \n    for n_features in self.features[1:]:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n\n    for n_features in self.features:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.output_dim*2)(x)\n    loc, raw_scale = x[:, :self.output_dim], x[:, self.output_dim:]\n    scale = jnp.exp(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features, self.output_dim)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=0.005+scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/CNPs_for_Images.html#load-mnist",
    "href": "posts/CNPs_for_Images.html#load-mnist",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "Load MNIST",
    "text": "Load MNIST\n\nds = tfds.load('mnist')\n\n\ndef dataset_to_arrays(dataset):\n    data = []\n    labels = []\n    stopper = 0\n    end = 100\n    for sample in dataset:\n        data.append(sample[\"image\"].numpy())\n        labels.append(sample[\"label\"].numpy())\n        stopper += 1\n        if stopper == end:\n            break\n    return np.array(data), np.array(labels)[..., None]\n\ntrain_data, train_labels = dataset_to_arrays(ds[\"train\"])\ntest_data, test_labels = dataset_to_arrays(ds[\"test\"])\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n2023-06-02 09:58:48.609001: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2023-06-02 09:58:48.681190: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n((100, 28, 28, 1), (100, 1), (100, 28, 28, 1), (100, 1))\n\n\n\ncoords = np.linspace(-1, 1, 28)\nx, y = np.meshgrid(coords, coords)\ntrain_X = jnp.stack([x, y], axis=-1).reshape(-1, 2)\n\ntrain_y = jax.vmap(lambda x: x.reshape(-1, 1))(train_data) / 255.0\ntrain_X.shape, train_y.shape, type(train_X), type(train_y)\n\n((784, 2),\n (100, 784, 1),\n jaxlib.xla_extension.ArrayImpl,\n jaxlib.xla_extension.ArrayImpl)\n\n\n\niterations = 10000\n\ndef loss_fn(params, context_X, context_y, target_X, target_y):\n  def loss_fn_per_sample(context_X, context_y, target_X, target_y):\n    loc, scale = model.apply(params, context_X, context_y, target_X)\n    # predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    # return -predictive_distribution.log_prob(target_y)\n    return jnp.square(loc.ravel() - target_y.ravel()).mean()\n  \n  return jax.vmap(loss_fn_per_sample, in_axes=(None, 0, None, 0))(context_X, context_y, target_X, target_y).mean()\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nmodel = CNP([256]*2, 128, [256]*4, 1)\nparams = model.init(jax.random.PRNGKey(0), train_X, train_y[0], train_X)\noptimizer = optax.adam(1e-5)\nstate = optimizer.init(params)\n\n# losses = []\n# for iter in tqdm(range(iterations)):\n#   tmp_index = jax.random.permutation(jax.random.PRNGKey(iter), index)\n#   context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n#   context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n#   target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n#   target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  \n#   # print(context_X.shape, context_y.shape, target_X.shape, target_y.shape)\n#   # print(loss_fn(params, context_X, context_y, target_X, target_y).shape)\n  \n#   loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n#   updates, state = optimizer.update(grads, state)\n#   params = optax.apply_updates(params, updates)\n#   losses.append(loss.item())\n\ndef one_step(params_and_state, key):\n  params, state = params_and_state\n  tmp_index = jax.random.permutation(key, train_X.shape[0])\n  context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n  context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n  target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n  target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n  updates, state = optimizer.update(grads, state)\n  params = optax.apply_updates(params, updates)\n  return (params, state), loss\n\n(params, state), loss_history = jax.lax.scan(one_step, (params, state), jax.random.split(jax.random.PRNGKey(0), iterations))\n\n\nplt.plot(loss_history[10:]);\n\n\n\n\n\n\n\n\n\ntest_key = jax.random.PRNGKey(0)\ntmp_index = jax.random.permutation(test_key, train_X.shape[0])\ncontext_X = train_X[tmp_index][:int(train_X.shape[0]*0.5)]\ncontext_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.5), :]\ntarget_X = train_X#[tmp_index][int(train_X.shape[0]*0.5):]\ntarget_y = train_y#[:, tmp_index, :][:, int(train_X.shape[0]*0.5):, :]\n\nid = 91\nplt.imshow(train_y[id].reshape(28, 28), cmap=\"gray\", interpolation=None);\n\nlocs, scales = jax.vmap(model.apply, in_axes=(None, None, 0, None))(params, context_X, context_y, target_X)\n# full_preds = jnp.concatenate([context_y, locs], axis=1)\n# full_preds = full_preds.at[:, tmp_index, :].set(full_preds).__array__()\n\nplt.figure()\nplt.imshow(locs[id].reshape(28, 28), cmap=\"gray\", interpolation=None);"
  },
  {
    "objectID": "posts/Rank1_GPs.html",
    "href": "posts/Rank1_GPs.html",
    "title": "Can Rank 1 GPs represent all GPs?",
    "section": "",
    "text": "from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gpytorch.kernels import RBFKernel, Kernel\n\n\nclass Rank1Kernel(nn.Module):\n    def __init__(self, input_dim, output_dim, n_neurons_per_layer, activation):\n        super().__init__()\n        self.init = nn.Linear(input_dim, n_neurons_per_layer[0])\n        self.n_neurons_per_layer = n_neurons_per_layer\n        self.activation = activation\n        \n        for i in range(1, len(n_neurons_per_layer)):\n            setattr(self, f'fc{i}', nn.Linear(n_neurons_per_layer[i-1], n_neurons_per_layer[i]))\n        \n        self.out = nn.Linear(n_neurons_per_layer[-1], output_dim)\n        \n    def forward(self, x1, x2):\n        def _forward(x):\n            x = self.init(x)\n            for i in range(1, len(self.n_neurons_per_layer)):\n                x = getattr(self, f'fc{i}')(x)\n                x = self.activation(x)\n            return self.out(x)\n        \n        x1 = _forward(x1)\n        x2 = _forward(x2)\n        \n        # print(x1.shape, x2.shape)\n        covar = x1 @ x2.T\n        # print(covar.shape, gt_covar.shape, x1.shape, x2.shape)\n        return covar\n\n\nfixed_kernel = RBFKernel()\nfixed_kernel.lengthscale = 0.3\n\nX1 = torch.linspace(-1, 1, 100).view(-1, 1)\n\n\nepochs = 1000\nn_neurons_per_layer = [64]*4\noutput_dim = 10\nkernel = Rank1Kernel(1, output_dim, n_neurons_per_layer, torch.sin)\noptimizer = torch.optim.Adam(kernel.parameters(), lr=0.001)\n\nlosses = []\nwith torch.no_grad():\n    gt_covar = fixed_kernel(X1, X1).evaluate_kernel().tensor\n    \nbar = tqdm(range(epochs))\nfor epoch in bar:\n    optimizer.zero_grad()\n    pred_covar = kernel(X1, X1)\n    loss = torch.mean((gt_covar - pred_covar)**2)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    bar.set_description(f\"Loss: {loss.item():.4f}\")\n\nLoss: 0.0001: 100%|| 1000/1000 [00:06&lt;00:00, 150.34it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,2,figsize=(8, 3))\n\nsns.heatmap(gt_covar, ax=ax[0], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[0].set_title('Ground Truth Covariance')\n\nX_new = torch.linspace(-1.5, 1.5, 100).view(-1, 1)\nwith torch.no_grad():\n    est_covar = kernel(X_new, X_new)\nsns.heatmap(est_covar, ax=ax[1], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[1].set_title('Estimated Covariance');\n\n\n\n\n\n\n\n\n\n# plt.plot()\n\nX2 = torch.zeros(1, 1) + 1\nwith torch.no_grad():\n    variance = gt_covar[-1, :]\n    plt.plot(X1, variance.numpy(), label=\"fixed kernel\");\n    \n    variance = kernel(X1, X2)\n    plt.plot(X1, variance.numpy(), label=f\"rank-{output_dim} kernel\");\n    \n    plt.legend()\n\n\n\n\n\n\n\n\n\nprint(gt_covar.shape)\ntorch.random.manual_seed(2)\nnorm = dist.MultivariateNormal(torch.zeros(100), gt_covar + 1e-5 * torch.eye(100))\ny = norm.sample()\nplt.plot(X1, y);\n\ntorch.Size([100, 100])\n\n\n\n\n\n\n\n\n\n\nn = 6\n\nfig, ax = plt.subplots(1, n, figsize=(15, 2))\nd_x = X1\nd_y = y\nfor i in range(n):\n    print(f\"{i}: {torch.var(d_y)}\")\n    ax[i].plot(d_x, d_y)\n    d_x = d_x[1:] - d_x[:-1]\n    d_x = torch.cumsum(d_x, dim=0)\n    d_y = d_y[1:] - d_y[:-1]\n    \nf = lambda x: torch.zeros_like(x)\nax[-1].plot(d_x, f(d_x), c=\"r\", label=\"f(x)\")\n\n0: 0.5698477029800415\n1: 0.006691396702080965\n2: 0.0001796285796444863\n3: 0.00022799619182478637\n4: 0.0008216467685997486\n5: 0.00304242386482656"
  },
  {
    "objectID": "posts/py_over_ipynb.html",
    "href": "posts/py_over_ipynb.html",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "",
    "text": "I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files:"
  },
  {
    "objectID": "posts/py_over_ipynb.html#fewer-errors",
    "href": "posts/py_over_ipynb.html#fewer-errors",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Fewer Errors",
    "text": "Fewer Errors\n\n.py files are easier to debug with a VS code like IDE, making it easier to find the errors.\nExecution of .py starts fresh, unlike some left out variables silently getting carried over from the last execution/deleted cells in .ipynb files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "href": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Better Usage of a Shared Server",
    "text": "Better Usage of a Shared Server\n\n.py files release the resources (e.g., GPU memory) once executed. It could be inconvenient to repeatedly remind or be reminded by someone to release the resources manually from a Jupyter notebook."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-productivity",
    "href": "posts/py_over_ipynb.html#increased-productivity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Productivity",
    "text": "Increased Productivity\n\nYou can make use of fantastic auto-complete, syntax-highlighting extensions in VS code to save a lot of time while working with .py files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#boost-collaboration",
    "href": "posts/py_over_ipynb.html#boost-collaboration",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Boost Collaboration",
    "text": "Boost Collaboration\n\n.py do not take time to render on GitHub because they are just plain text files, unlike .ipynb files.\nIt is a lot easier to see the changes made by others in a .py file than a .ipynb file."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-modularity",
    "href": "posts/py_over_ipynb.html#increased-modularity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Modularity",
    "text": "Increased Modularity\n\nFunction and Class calls from other files are seamless with .py files.\n\nFeel free to comment your views/suggestions/additions in the comment box."
  },
  {
    "objectID": "posts/kl-divergence.html",
    "href": "posts/kl-divergence.html",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#ground",
    "href": "posts/kl-divergence.html#ground",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence",
    "href": "posts/kl-divergence.html#kl-divergence",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence",
    "text": "KL divergence\nWe can use KL divergence to check how good is our model. The formula is:\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} p_G(y_i)\\log\\frac{p_G(y_i)}{p_P(y_i)}\n\\]\nFor our example,\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\log\\frac{1}{0.8}\n\\]\nIt is evident that if \\(p_P(y = L2)\\) decreses from \\(0.8\\), \\(D_{KL}(p_G\\;\\rVert\\;p_P)\\) will increase and vice versa. Note that KL divergence is not symmetric which means \\(D_{KL}(p_G\\;\\rVert\\;p_P) \\ne D_{KL}(p_P\\;\\rVert\\;p_G)\\)."
  },
  {
    "objectID": "posts/kl-divergence.html#cross-entory",
    "href": "posts/kl-divergence.html#cross-entory",
    "title": "KL divergence v/s cross-entropy",
    "section": "Cross-entory",
    "text": "Cross-entory\nCross-entropy is another measure for distribution similarity. The formula is:\n\\[\nH(p_G, p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} - p_G(y_i)\\log p_P(y_i)\n\\]\nFor our example:\n\\[\nH(p_G, p_P) = -\\log 0.8 = \\log \\frac{1}{0.8}\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "href": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence v/s cross-entropy",
    "text": "KL divergence v/s cross-entropy\nThis shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence?\nThats because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Object Detection - A how-to guide\n\n\n\n\n\n\nML\n\n\nCV\n\n\n\nBasic operations in object detection task\n\n\n\n\n\nDec 29, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload CPCB CAAQM locations\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB CAAQM locations using Selenium\n\n\n\n\n\nDec 27, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload CPCB live data\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB data with selenium\n\n\n\n\n\nDec 10, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals across ML domains\n\n\n\n\n\n\nML\n\n\n\nKnowledge transfer between ML domains\n\n\n\n\n\nJun 25, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nObject Detection Random Baseline\n\n\n\n\n\n\nML\n\n\nCV\n\n\n\nCompare your performance with a random baseline.\n\n\n\n\n\nFeb 10, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\n\nML\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\nSep 16, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\n\nML\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\nSep 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Basis Regression\n\n\n\n\n\n\nML\n\n\n\nBayesian Basis Regression\n\n\n\n\n\nAug 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow numpy handles day-to-day algebra?\n\n\n\n\n\n\nNumPy, Mathematics\n\n\n\nA deep dive into basic operations of numpy\n\n\n\n\n\nAug 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nCan Rank 1 GPs represent all GPs?\n\n\n\n\n\n\nML, GP\n\n\n\nA trial\n\n\n\n\n\nJul 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload low-cost data from OpenAQ\n\n\n\n\n\n\nML, GP\n\n\n\nA guide to download low-cost sensor data from OpenAQ\n\n\n\n\n\nJul 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with GPs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of GPs for climate modeling\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-class classification with Gaussian Processes\n\n\n\n\n\n\nML, GP\n\n\n\nMulti-class GP classification with different strategies\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with SIRENs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of SIRENs for climate modeling\n\n\n\n\n\nJul 1, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGNNs and GPs\n\n\n\n\n\n\nML\n\n\n\nExploring similarities between GNNs and GPs\n\n\n\n\n\nJun 23, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Neural Networks for Regression\n\n\n\n\n\n\nML\n\n\n\nChallenges in using GNNs for regression using various strategies\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBasis functions\n\n\n\n\n\n\nML\n\n\n\nExploring basis functions\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes for Image Interpolation\n\n\n\n\n\n\nML\n\n\n\nExtreme Image Interpolation with Conditional Neural processes\n\n\n\n\n\nMay 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPasswordless SSH setup for MacOS Hosts\n\n\n\n\n\n\nmacOS\n\n\n\nA tiny handbook to setup passwordless ssh in MacOS\n\n\n\n\n\nMay 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSine Combination Networks\n\n\n\n\n\n\nML\n\n\n\nChallenges in fitting to a combination of sine waves\n\n\n\n\n\nApr 29, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Gaussian Process\n\n\n\n\n\n\nGP\n\n\nML\n\n\n\nExploring NTK kernels + GPJax with toy datasets\n\n\n\n\n\nMar 28, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Variational Gaussian processes in JAX\n\n\n\n\n\n\nGP\n\n\n\nA practical implementation of Hensman et al.2015 from scratch in JAX\n\n\n\n\n\nOct 31, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Output Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nExploring MOGPs from scratch\n\n\n\n\n\nOct 27, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Processes - A no-skip-math version\n\n\n\n\n\n\nML\n\n\n\nEnd-to-end math derivations for Gaussian process regression and classification\n\n\n\n\n\nOct 21, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTrain NN with KFAC-Laplace in JAX\n\n\n\n\n\n\nML\n\n\n\nExploring KFAC-Laplace approximation on simple problems in JAX\n\n\n\n\n\nOct 18, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes in JAX\n\n\n\n\n\n\nML\n\n\n\nImplementing conditional neural processes from scratch in JAX\n\n\n\n\n\nAug 1, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nJAX Optimizers\n\n\n\n\n\n\nML\n\n\n\nPros and cons of several jax optimizers.\n\n\n\n\n\nJun 10, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGet a list of contributors from a repo\n\n\n\n\n\n\nGitHub\n\n\n\nGet contributors list using GitHub API and pandas\n\n\n\n\n\nMay 17, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nIteratively reweighted least squares (IRLS) logistic regression\n\n\n\n\n\n\nML\n\n\n\nImplementation of IRLS from Probabilistic ML book of Dr.Kevin Murphy and its comparison with naive second order implementation.\n\n\n\n\n\nMay 14, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGcloud cheatsheet\n\n\n\n\n\n\nGcloud\n\n\n\nMost used commands while working with gcloud\n\n\n\n\n\nApr 9, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Contrubuting FAQs\n\n\n\n\n\n\nGitHub\n\n\n\nThis is a collection of FAQs/road-blocks/queries/issues I had over the past 2 years of engagement with GitHub.\n\n\n\n\n\nApr 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTorch essentials\n\n\n\n\n\n\nML\n\n\n\nPractical and direct introduction to PyTorch\n\n\n\n\n\nMar 8, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\n\n\n\nML\n\n\n\nA video lecture series from Prof.Philipp Hennig\n\n\n\n\n\nMar 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in Deep Learning\n\n\n\n\n\n\nML\n\n\n\nReview of PhD thesis of Dr.Yarin Gal\n\n\n\n\n\nMar 5, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Tips\n\n\n\n\n\n\nML\n\n\n\nPyTorch zen tips\n\n\n\n\n\nFeb 25, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConference Presentation Tips\n\n\n\n\n\n\nAcademic\n\n\n\nConference Presentation Tips\n\n\n\n\n\nJan 29, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Gaussian Process Regression Frameworks\n\n\n\n\n\n\nML\n\n\n\nA basic comparison among GPy, GPyTorch and TinyGP\n\n\n\n\n\nJan 25, 2022\n\n\nZeel B Patel, Harsh Patel, Shivam Sahni\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Committee\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to QBC with Random Forest Classifier.\n\n\n\n\n\nJan 24, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nKL divergence v/s cross-entropy\n\n\n\n\n\n\nML\n\n\n\nUnderstanding KL divergence\n\n\n\n\n\nJan 20, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nWhy .py files are better than .ipynb files for ML codebase\n\n\n\n\n\n\nPython\n\n\n\nWhere .py files are better than .ipynb files?\n\n\n\n\n\nJan 15, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nAnonymization tips for double-blind submission\n\n\n\n\n\n\nAcademic\n\n\n\nA last-minute help list\n\n\n\n\n\nOct 26, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nInput Warped GPs - A failed idea\n\n\n\n\n\n\nML\n\n\n\nAn idea of input warping GPs\n\n\n\n\n\nOct 23, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSparseGPs in Stheno\n\n\n\n\n\nA simple demo of sparse regression in stheno with VFE and FITC methods.\n\n\n\n\n\nOct 12, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDocker Cheatsheet\n\n\n\n\n\n\nDocker\n\n\n\nMost used command while working with Docker\n\n\n\n\n\nSep 28, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow to apply constraint on parameters in various GP libraries\n\n\n\n\n\nApply constraints in GPy, GPFlow, GPyTorch\n\n\n\n\n\nSep 27, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Kernels in Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nAn exploratory analysis of kernels in GPs\n\n\n\n\n\nMar 22, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nSep 21, 2020\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nActive Learning with Bayesian Linear Regression\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to Active Learning with Bayesian Linear Regression.\n\n\n\n\n\nMar 28, 2020\n\n\nZeel B Patel, Nipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Zeel. This is my blog, where I add coding + other resources related to my research. Head over to this page for my personal website."
  },
  {
    "objectID": "posts/climate-modeling-with-SpecialGP.html",
    "href": "posts/climate-modeling-with-SpecialGP.html",
    "title": "Climate Modeling with GPs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pyproj\nimport numpy as np\nimport xarray as xr\n\nfrom skgpytorch.models import GPRegression\n\nimport matplotlib.pyplot as plt\n\n\n# def haversine(lon1, lat1, lon2, lat2):\n#     \"\"\"\n#     Calculate the great circle distance in kilometers between two points \n#     on the earth (specified in decimal degrees)\n#     \"\"\"\n#     # convert decimal degrees to radians \n#     lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n#     # haversine formula \n#     dlon = lon2 - lon1 \n#     dlat = lat2 - lat1 \n#     a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n#     c = 2 * np.arcsin(np.sqrt(a)) \n#     r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n#     return c * r\n\n# def new_coords(lat1, long1):\n#     new_lat1 = haversine(0, 0, 0, lat1)\n#     new_long1 = haversine(0, 0, long1, 0)\n#     return new_lat1, new_long1\n\ndef lat_long_to_cartesian(latitude, longitude):\n    # Convert latitude and longitude to radians\n    phi = np.radians(latitude)\n    lam = np.radians(longitude)\n\n    # Constants for WGS 84 ellipsoid\n    a = 6378137.0  # equatorial radius in meters\n    e = 0.0818191908426  # eccentricity\n\n    # Calculate Earth's radius at the given latitude\n    R = a / np.sqrt(1 - (e ** 2) * (np.sin(phi) ** 2))\n\n    # Convert to Cartesian coordinates\n    X = R * np.sin(lam)\n    Y = R * np.tan(phi)\n\n    return X, Y\n\ndef wgs84_coords(lat, lon):    \n    # Define coordinate systems\n    wgs84 = pyproj.CRS.from_epsg(4326)  # WGS 84 lat-long system\n    utm_zone_32n = pyproj.CRS.from_string(\"+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n\n    # Create a transformer object\n    transformer = pyproj.Transformer.from_crs(wgs84, utm_zone_32n)\n\n    # Convert lat-long coordinates to UTM coordinates\n    utm_easting, utm_northing = transformer.transform(lon, lat)\n\n    return utm_northing, utm_easting\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------\n# Position embedding utils\n# --------------------------------------------------------\n\n\n# --------------------------------------------------------\n# 2D sine-cosine position embedding\n# References:\n# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n# MoCo v3: https://github.com/facebookresearch/moco-v3\n# --------------------------------------------------------\ndef get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size_h, dtype=np.float32)\n    grid_w = np.arange(grid_size_w, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\n# --------------------------------------------------------\n# Interpolate position embeddings for high-resolution\n# References:\n# DeiT: https://github.com/facebookresearch/deit\n# --------------------------------------------------------\ndef interpolate_pos_embed(model, checkpoint_model, new_size=(64, 128)):\n    if \"net.pos_embed\" in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model[\"net.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        orig_num_patches = pos_embed_checkpoint.shape[-2]\n        patch_size = model.patch_size\n        w_h_ratio = 2\n        orig_h = int((orig_num_patches // w_h_ratio) ** 0.5)\n        orig_w = w_h_ratio * orig_h\n        orig_size = (orig_h, orig_w)\n        new_size = (new_size[0] // patch_size, new_size[1] // patch_size)\n        # print (orig_size)\n        # print (new_size)\n        if orig_size[0] != new_size[0]:\n            print(\"Interpolate PEs from %dx%d to %dx%d\" % (orig_size[0], orig_size[1], new_size[0], new_size[1]))\n            pos_tokens = pos_embed_checkpoint.reshape(-1, orig_size[0], orig_size[1], embedding_size).permute(\n                0, 3, 1, 2\n            )\n            new_pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size[0], new_size[1]), mode=\"bicubic\", align_corners=False\n            )\n            new_pos_tokens = new_pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            checkpoint_model[\"net.pos_embed\"] = new_pos_tokens\n\n\ndef interpolate_channel_embed(checkpoint_model, new_len):\n    if \"net.channel_embed\" in checkpoint_model:\n        channel_embed_checkpoint = checkpoint_model[\"net.channel_embed\"]\n        old_len = channel_embed_checkpoint.shape[1]\n        if new_len &lt;= old_len:\n            checkpoint_model[\"net.channel_embed\"] = channel_embed_checkpoint[:, :new_len]\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=initializers.RandomUniform(-1 / input_dim, 1 / input_dim), activation=tf.sin))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[i-1]) / activation_scale, np.sqrt(6 / features[i-1]) / activation_scale), activation=tf.sin))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[-1]) / activation_scale, np.sqrt(6 / features[-1]) / activation_scale), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../data/2m_temperature_2018_5.625deg_Jan.nc\").to_dataframe().reset_index()\ndata1 = xr.open_dataset(\"../data/2m_temperature_2018_1.40625deg_Jan.nc\").to_dataframe().reset_index()\n\n\ndata5.head()\n\n\n\n\n\n\n\n\nlon\nlat\ntime\nt2m\n\n\n\n\n0\n0.0\n-87.1875\n2018-01-01 00:00:00\n250.728180\n\n\n1\n0.0\n-87.1875\n2018-01-01 01:00:00\n250.468552\n\n\n2\n0.0\n-87.1875\n2018-01-01 02:00:00\n250.250931\n\n\n3\n0.0\n-87.1875\n2018-01-01 03:00:00\n250.040314\n\n\n4\n0.0\n-87.1875\n2018-01-01 04:00:00\n249.993790\n\n\n\n\n\n\n\n\ntime_stamp = \"2018-01-01 01:00:00\"\ntrain_df = data5[data5.time == time_stamp]\ntest_df = data1[data1.time == time_stamp]\n\nX = np.stack([train_df.lat.values, train_df.lon.values], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\nrff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(2048, 2), y.shape=(2048, 1)\nX_test.shape=(32768, 2), y_test.shape=(32768, 1)\nX.shape=(2048, 2), X_test.shape=(32768, 2)\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = MLP(2, 1, [256]*4, 30.0, 0.0)\n# model = ResNet()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n\nhistory = model.fit(X_scaled, y_scaled, epochs=5000, batch_size=X_scaled.shape[0], verbose=0)\n\n\nplt.plot(history.history['loss']);\n\n\n\n\n\n\n\n\n\ny_pred = model.predict(X_test_scaled) * (y_max - y_min) + y_min\nplt.imshow(y_pred.reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n1024/1024 [==============================] - 1s 1ms/step\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\n\n\n\n\n\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\nText(0.5, 1.0, 'Diff')\n\n\n\n\n\n\n\n\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\nrmse = np.sqrt(np.mean((y_pred.ravel() - y_test.ravel())**2))\nprint(f\"{rmse=}\")\n\nrmse=2.7606046\n\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")\n\nmean_bias=0.10866926"
  },
  {
    "objectID": "posts/2021-10-26-anonymization-tips.html",
    "href": "posts/2021-10-26-anonymization-tips.html",
    "title": "Anonymization tips for double-blind submission",
    "section": "",
    "text": "Use following command locally to search for author names, institute name and other terms you think may violate double-blind\n\ngit grep &lt;query&gt;\n\nAbove command matches the query everywhere and thus a safe way. Avoid GitHub search for this purpose, it fails to identify some terms many times and there is no regex there (yet)!\n\n\nDo not use full paths inside README file. If you move content in other repo, the links will either become unusable or may violate double-blind. So follow the example below.\n\nBad practice: [link](https://github.com/patel-zeel/reponame/blob/master/dataset)\nGood practice: [link](dataset)\n\nPoint no. 2 does not work for GitHub pages links (username.github.io/stuff). Thus, keep in mind to manually update those (if you have a better idea, let everyone know in comments below)\nDownload the repo zip locally and create an anonymized repository in your anonymized GitHub account. Open the GitHub web editor by pressing . (dot) at repo homepage.\nNow, you can select and drag all folders to the left pan of the web editor to upload them at once. Finally, commit with a meaningfull message and the changes will automatically be uploaded to the mail branch of your anonymized repo.\nUpdate the link in your manuscipt and submit !!\n\n\nEdit:\nAfter acceptance, transfer the ownership to personal account and delete the ownership of anonymized account from the personal account. This will remove all the traces of repository from the anonymized account. However, repository will still show that the commits were made by anonymized account which is anyway not violation of the doule-blind explicitely."
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html",
    "href": "posts/2023-04-29-sine-combination-netowrks.html",
    "title": "Sine Combination Networks",
    "section": "",
    "text": "We know that any continuous signal can be represented as a sum of sinusoids. The question is, how many sinusoids do we need to represent a signal? In this notebook, we will explore this question.\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "title": "Sine Combination Networks",
    "section": "Random Combination of Sinusoids",
    "text": "Random Combination of Sinusoids\n\nN = 1000\nx = jnp.linspace(-10, 10, N).reshape(-1, 1)\ny = jnp.sin(x) + jnp.sin(2*x) #+ jax.random.normal(jax.random.PRNGKey(0), (N, 1)) * 0.1\nplt.plot(x, y, \"kx\");\nprint(x.shape, y.shape)\n\n(1000, 1) (1000, 1)"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "title": "Sine Combination Networks",
    "section": "Recover the Signal",
    "text": "Recover the Signal\n\ndef get_weights(key):\n    w1 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    key = jax.random.split(key)[0]\n    w2 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    return w1, w2\n    \ndef get_sine(weights, x):\n    w1, w2 = weights\n    return jnp.sin(w1*x) + jnp.sin(w2*x)\n\ndef loss_fn(weights, x, y):\n    output = get_sine(weights, x)\n    w1, w2 = weights\n    return jnp.mean((output.ravel() - y.ravel())**2)\n\n\ndef one_step(weights_and_state, xs):\n    weights, state = weights_and_state\n    loss, grads = value_and_grad_fn(weights, x, y)\n    updates, state = optimizer.update(grads, state)\n    weights = optax.apply_updates(weights, updates)\n    return (weights, state), (loss, weights)\n\nepochs = 1000\noptimizer = optax.adam(1e-2)\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nfig, ax = plt.subplots(4, 3, figsize=(15, 12))\nfig2, ax2 = plt.subplots(4, 3, figsize=(15, 12))\nax = ax.ravel()\nax2 = ax2.ravel()\nfor seed in tqdm(range(12)):\n    key = jax.random.PRNGKey(seed)\n    init_weights = get_weights(key)\n    state = optimizer.init(init_weights)\n    (weights, _), (loss_history, _) = jax.lax.scan(one_step, (init_weights, state), None, length=epochs)\n    y_pred = get_sine(weights, x)\n    ax[seed].plot(x, y, \"kx\")\n    ax[seed].plot(x, y_pred, \"r-\")\n    ax[seed].set_title(f\"w_init=({init_weights[0]:.2f}, {init_weights[1]:.2f}), w_pred=({weights[0]:.2f}, {weights[1]:.2f}), loss={loss_fn(weights, x, y):.2f}\")\n    ax2[seed].plot(loss_history)\nfig.tight_layout()\n\n100%|| 12/12 [00:00&lt;00:00, 15.91it/s]"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "title": "Sine Combination Networks",
    "section": "Plot loss surface",
    "text": "Plot loss surface\n\nw1 = jnp.linspace(0, 3, 100)\nw2 = jnp.linspace(0, 3, 100)\nW1, W2 = jnp.meshgrid(w1, w2)\nloss = jax.vmap(jax.vmap(lambda w1, w2: loss_fn((w1, w2), x, y)))(W1, W2)\n\n# plot the loss surface in 3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(W1, W2, loss, cmap=\"viridis\", alpha=0.9);\nax.set_xlabel(\"w1\");\nax.set_ylabel(\"w2\");\n# top view\nax.view_init(30, 45)"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html",
    "title": "Programatically download OpenAQ data",
    "section": "",
    "text": "# uncomment to install these libraries\n# !pip install boto3 botocore\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport boto3\nimport botocore\nimport os\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "title": "Programatically download OpenAQ data",
    "section": "Setup",
    "text": "Setup\n\ns3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\nbucket_name = 'openaq-fetches'\nprefix = 'realtime-gzipped/'\n\npath = '/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/'\n\nstart_date = '2020/01/01' # start date (inclusive)\nend_date = '2020/12/31' # end date (inclusive)\n\n\nDownload\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  clear_output(wait=True)\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  print('Downloading:', date)\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    f_name = file_obj['Key']\n    tmp_path = '/'.join((path+f_name).split('/')[:-1])\n    \n    if not os.path.exists(tmp_path):\n      os.makedirs(tmp_path)\n    \n    s3.download_file(bucket_name, f_name, path+f_name)\n\nDownloading: 2020-05-04\n\n\n\n\nValidate\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    assert os.path.exists(path+file_obj['Key']), file_obj['Key']\n\n\nprint('Validated')"
  },
  {
    "objectID": "posts/2023-03-28-nngp.html",
    "href": "posts/2023-03-28-nngp.html",
    "title": "Neural Network Gaussian Process",
    "section": "",
    "text": "# %%capture\n# %pip install -U --force-reinstall jaxutils\n# %pip install -U jax jaxlib optax\n\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nfrom jaxutils import Dataset\n\ntry:\n    from neural_tangents import stax\nexcept ModuleNotFoundError:\n    %pip install neural-tangents\n    from neural_tangents import stax\n\ntry:\n    import optax as ox\nexcept ModuleNotFoundError:\n    %pip install optax\n    import optax as ox\n\ntry:\n    import gpjax as gpx\nexcept ModuleNotFoundError:\n    %pip install gpjax\n    import gpjax as gpx\n\ntry:\n    import regdata as rd\nexcept ModuleNotFoundError:\n    %pip install regdata\n    import regdata as rd\n\nimport matplotlib.pyplot as plt\n\n\nclass NTK(gpx.kernels.AbstractKernel):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def __call__(self, params, x, y):\n        params = jax.tree_util.tree_map(jax.nn.softplus, params)\n        init_fn, apply_fn, kernel_fn = stax.serial(\n            stax.Dense(512, W_std=params[\"w1\"], b_std=params[\"b1\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w2\"], b_std=params[\"b2\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w3\"], b_std=params[\"b3\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w4\"], b_std=params[\"b4\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w5\"], b_std=params[\"b5\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w6\"], b_std=params[\"b6\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w7\"], b_std=params[\"b7\"]), stax.Relu(),\n            stax.Dense(1, W_std=params[\"w8\"], b_std=params[\"b8\"])\n        )\n        return kernel_fn(x.reshape(1, 1), y.reshape(1, 1)).nngp.squeeze()\n\n    def init_params(self, key):\n        # return init_fn(key, input_shape=(2,1))\n        return {\"w1\": 0.1, \"w2\": 0.2, \"w3\": 0.3, \"w4\": 0.4, \"w5\": 0.5,  \"w6\": 0.6, \"w7\": 0.7, \"w8\": 0.8,\n                \"b1\": 0.1, \"b2\": 0.2, \"b3\": 0.3, \"b4\": 0.4, \"b5\": 0.5,  \"b6\": 0.6, \"b7\": 0.7, \"b8\": 0.8\n                }\n\n    # This is depreciated. Can be removed once JaxKern is updated.\n    def _initialise_params(self, key):\n        return self.init_params(key)\n\n\nn = 100\nnoise = 0.3\nkey = jr.PRNGKey(123)\n# x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).sort().reshape(-1, 1)\n# f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n# signal = f(x)\n# y = signal + jr.normal(key, shape=signal.shape) * noise\nx, y, xtest = rd.MotorcycleHelmet().get_data()\ny = y.reshape(-1, 1)\n\nD = Dataset(X=x, y=y)\n\n# xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n# ytest = f(xtest)\n\nprint(x.shape, y.shape)\n\n(94, 1) (94, 1)\n\n\n\nkernel = NTK()\nprior = gpx.Prior(kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n\n\nkey = jr.PRNGKey(1234)\nparameter_state = gpx.initialise(posterior, key)\nparams, trainable, bijectors = parameter_state.unpack()\nparams[\"likelihood\"][\"obs_noise\"] = jnp.array(0.1)\nparameter_state = gpx.parameters.ParameterState(params, trainable, bijectors)\nprint(params)\n\n{'kernel': {'w1': 0.1, 'w2': 0.2, 'w3': 0.3, 'w4': 0.4, 'w5': 0.5, 'w6': 0.6, 'w7': 0.7, 'w8': 0.8, 'b1': 0.1, 'b2': 0.2, 'b3': 0.3, 'b4': 0.4, 'b5': 0.5, 'b6': 0.6, 'b7': 0.7, 'b8': 0.8}, 'mean_function': {}, 'likelihood': {'obs_noise': Array(0.1, dtype=float32, weak_type=True)}}\n\n\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n\n\n\nnegative_mll = jax.jit(posterior.marginal_log_likelihood(D, negative=True))\nnegative_mll(params)\n\nArray(415.1062, dtype=float32)\n\n\n\noptimiser = ox.adam(learning_rate=0.01)\n\ninference_state = gpx.fit(\n    objective=negative_mll,\n    parameter_state=parameter_state,\n    optax_optim=optimiser,\n    num_iters=500,\n)\n\nlearned_params, training_history = inference_state.unpack()\n\n100%|| 500/500 [00:02&lt;00:00, 172.53it/s, Objective=76.34]\n\n\n\nplt.plot(training_history);\n\n\n\n\n\n\n\n\n\nlearned_params\n\n{'kernel': {'b1': Array(0.03292831, dtype=float32),\n  'b2': Array(-0.9647168, dtype=float32),\n  'b3': Array(-1.2660046, dtype=float32),\n  'b4': Array(-1.3792713, dtype=float32),\n  'b5': Array(-1.4311961, dtype=float32),\n  'b6': Array(-1.4504426, dtype=float32),\n  'b7': Array(-1.4371448, dtype=float32),\n  'b8': Array(-1.3471106, dtype=float32),\n  'w1': Array(1.0706716, dtype=float32),\n  'w2': Array(1.1768614, dtype=float32),\n  'w3': Array(1.2740505, dtype=float32),\n  'w4': Array(1.3689499, dtype=float32),\n  'w5': Array(1.462641, dtype=float32),\n  'w6': Array(1.5562503, dtype=float32),\n  'w7': Array(1.6506695, dtype=float32),\n  'w8': Array(1.7462935, dtype=float32)},\n 'likelihood': {'obs_noise': Array(0.184795, dtype=float32)},\n 'mean_function': {}}\n\n\n\nlatent_dist = posterior(learned_params, D)(xtest)\npredictive_dist = likelihood(learned_params, latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=\"tab:blue\",\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\n\n# ax.plot(\n#     xtest, ytest, label=\"Latent function\", color=\"black\", linestyle=\"--\", linewidth=1\n# )\n\nax.legend();"
  },
  {
    "objectID": "posts/2025-02-10-object-detection-random-baseline.html",
    "href": "posts/2025-02-10-object-detection-random-baseline.html",
    "title": "Object Detection Random Baseline",
    "section": "",
    "text": "Why Random Baseline?\nGiven a standard dataset with a fixed set of models, it is easier to compare the performance of different models. However, in cases where the models performance is worse compared to the best models, but we need to test if it is better than random predictions, we can use a random baseline.\n\n\nProposed Idea\n\nTo formalize the problem, lets say that for an arbitrary image, model predicts \\(k\\) bounding boxes with sizes \\((h_1, w_1), (h_2, w_2), \\ldots, (h_k, w_k)\\).\nA simple random baseline would be to generate \\(k\\) random bounding boxes for that image with sizes \\((h_1, w_1), (h_2, w_2), \\ldots, (h_k, w_k)\\).\n\n\n\nImports\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport supervision as sv\nfrom roboflow import Roboflow\nfrom dotenv import load_dotenv\nfrom ultralytics import YOLO\nfrom copy import deepcopy\nfrom PIL import Image\n\nload_dotenv()\n\nTrue\n\n\n\n\nDataset\n\ndata_location = \"/tmp/poker-cards-fmjio\"\n\nrf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"yolov8\", location=data_location)\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n\nTrain Model\n\nmodel = YOLO(\"yolo11m\")\n\nmodel.train(data=f\"{data_location}/data.yaml\", epochs=1, project=\"/tmp/poker-cards-fmjio\", exist_ok=True)\n\nNew https://pypi.org/project/ultralytics/8.3.74 available  Update with 'pip install -U ultralytics'\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nengine/trainer: task=detect, mode=train, model=yolo11m.pt, data=/tmp/poker-cards-fmjio/data.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/tmp/poker-cards-fmjio, name=train, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/tmp/poker-cards-fmjio/train\nOverriding model.yaml nc=80 with nc=52\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n 23        [16, 19, 22]  1   1451116  ultralytics.nn.modules.head.Detect           [52, [256, 512, 512]]         \nYOLO11m summary: 409 layers, 20,093,100 parameters, 20,093,084 gradients, 68.4 GFLOPs\n\nTransferred 643/649 items from pretrained weights\nFreezing layer 'model.23.dfl.conv.weight'\nAMP: running Automatic Mixed Precision (AMP) checks...\nAMP: checks passed \n\n\ntrain: Scanning /tmp/poker-cards-fmjio/train/labels.cache... 811 images, 0 backgrounds, 0 corrupt: 100%|| 811/811 [00:00&lt;?, ?it/s]\nval: Scanning /tmp/poker-cards-fmjio/valid/labels.cache... 44 images, 0 backgrounds, 0 corrupt: 100%|| 44/44 [00:00&lt;?, ?it/s]\n\n\nPlotting labels to /tmp/poker-cards-fmjio/train/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000179, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to /tmp/poker-cards-fmjio/train\nStarting training for 1 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n        1/1      8.56G     0.8313      3.612      1.244         66        640: 100%|| 51/51 [00:08&lt;00:00,  5.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:01&lt;00:00,  1.56it/s]\n\n\n                   all         44        197      0.289      0.514      0.256      0.228\n\n1 epochs completed in 0.004 hours.\nOptimizer stripped from /tmp/poker-cards-fmjio/train/weights/last.pt, 40.6MB\nOptimizer stripped from /tmp/poker-cards-fmjio/train/weights/best.pt, 40.6MB\n\nValidating /tmp/poker-cards-fmjio/train/weights/best.pt...\nUltralytics 8.3.55  Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nYOLO11m summary (fused): 303 layers, 20,070,124 parameters, 0 gradients, 67.9 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00&lt;00:00,  6.48it/s]\n\n\n                   all         44        197       0.29      0.516      0.255      0.227\n           10 of clubs          3          3     0.0908          1      0.583      0.541\n        10 of diamonds          7          7     0.0933          1      0.613      0.558\n          10 of hearts          7          7      0.364          1      0.738      0.629\n          10 of spades          4          4          1          0       0.19      0.147\n            2 of clubs          2          2          1          0          0          0\n         2 of diamonds          2          2          1          0          0          0\n           2 of hearts          1          1          0          0     0.0191     0.0153\n           2 of spades          4          4      0.321      0.477      0.314      0.272\n            3 of clubs          2          2          0          0     0.0421     0.0337\n         3 of diamonds          2          2     0.0793      0.238      0.101     0.0993\n           3 of hearts          1          1          1          0     0.0231     0.0208\n           3 of spades          4          4      0.141          1      0.187       0.14\n            4 of clubs          2          2          1          0     0.0288     0.0288\n         4 of diamonds          2          2          1          0     0.0509     0.0509\n           4 of hearts          1          1          1          0     0.0169     0.0135\n           4 of spades          4          4          0          0      0.188      0.161\n            5 of clubs          8          8      0.433       0.25      0.283      0.251\n         5 of diamonds          3          3          0          0      0.123      0.106\n           5 of hearts          3          3       0.11      0.333      0.125      0.109\n           5 of spades          1          1      0.061          1      0.124     0.0995\n            6 of clubs          7          7      0.103      0.857      0.268      0.261\n         6 of diamonds          3          3      0.057          1      0.577      0.554\n           6 of hearts          3          3     0.0564      0.333      0.352       0.35\n           6 of spades          1          1     0.0481          1     0.0622     0.0498\n            7 of clubs          7          7       0.29      0.529      0.399      0.342\n         7 of diamonds          3          3     0.0934          1      0.451      0.451\n           7 of hearts          3          3     0.0495      0.333      0.119     0.0944\n           7 of spades          1          1          0          0     0.0321     0.0257\n            8 of clubs          7          7      0.794      0.286      0.388      0.316\n         8 of diamonds          3          3          1          0      0.355      0.329\n           8 of hearts          3          3     0.0449      0.333     0.0803     0.0579\n           8 of spades          1          1     0.0799          1      0.995      0.895\n            9 of clubs          3          3     0.0509      0.667      0.246       0.22\n         9 of diamonds          7          7      0.454      0.857      0.489      0.464\n           9 of hearts          7          7      0.121          1      0.194      0.165\n           9 of spades          4          4      0.356          1      0.362      0.285\n          ace of clubs          2          2          0          0      0.153      0.142\n       ace of diamonds          2          2          0          0     0.0228     0.0147\n         ace of hearts          1          1     0.0325          1     0.0369     0.0221\n         ace of spades          4          4     0.0943          1      0.769      0.695\n        jack  of clubs          3          3     0.0619          1      0.124      0.124\n      jack of diamonds          7          7     0.0993      0.857       0.32      0.316\n        jack of hearts          6          6      0.173      0.833      0.256      0.196\n        jack of spades          4          4          1          0          0          0\n         king of clubs          3          3      0.053          1      0.301      0.279\n      king of diamonds          7          7      0.154      0.286      0.178      0.171\n        king of hearts          7          7      0.291      0.857       0.46      0.411\n        king of spades          4          4      0.345          1      0.362      0.303\n        queen of clubs          3          3     0.0981      0.667      0.255      0.247\n     queen of diamonds          7          7      0.191      0.857      0.271      0.252\n       queen of hearts          7          7       0.18          1      0.541      0.431\n       queen of spades          4          4          0          0     0.0935     0.0681\nSpeed: 0.1ms preprocess, 1.8ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to /tmp/poker-cards-fmjio/train\n\n\nultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object at 0x7f6242879ae0&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,       0.375,       0.375,           0],\n       [      0.625,       0.625,       0.625, ...,     0.58333,     0.58333,           0],\n       [          1,           1,           1, ...,      0.4375,      0.4375,           0],\n       ...,\n       [        0.4,         0.4,         0.4, ...,     0.18919,     0.18919,           0],\n       [        0.6,         0.6,         0.6, ...,     0.46667,     0.46667,           0],\n       [    0.16667,     0.16667,     0.16667, ...,    0.070175,    0.070175,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.024793,    0.024793,    0.029844, ...,           0,           0,           0],\n       [   0.028747,    0.028747,    0.039205, ...,           0,           0,           0],\n       [   0.077778,    0.077778,    0.093524, ...,           0,           0,           0],\n       ...,\n       [   0.086957,    0.086957,      0.1261, ...,           0,           0,           0],\n       [   0.058091,    0.058091,    0.077243, ...,           0,           0,           0],\n       [   0.040201,    0.040201,    0.042321, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.012552,    0.012552,    0.015148, ...,           1,           1,           1],\n       [   0.014583,    0.014583,    0.019995, ...,           1,           1,           1],\n       [   0.040462,    0.040462,    0.049056, ...,           1,           1,           1],\n       ...,\n       [   0.045455,    0.045455,    0.067293, ...,           1,           1,           1],\n       [   0.029915,    0.029915,    0.040173, ...,           1,           1,           1],\n       [   0.020513,    0.020513,    0.021618, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       ...,\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.2298734327529019\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.54102,     0.55754,     0.62871,     0.14656,           0,           0,    0.015308,     0.27232,    0.033672,    0.099296,    0.020826,     0.13978,    0.028841,    0.050857,    0.013492,     0.16072,      0.2506,     0.10594,     0.10935,      0.0995,     0.26062,     0.55392,     0.35036,     0.04975,\n           0.34158,     0.45147,    0.094422,    0.025677,     0.31576,      0.3291,    0.057884,      0.8955,     0.22025,     0.46372,     0.16495,     0.28542,     0.14242,    0.014657,    0.022111,     0.69542,     0.12437,      0.3157,      0.1958,           0,     0.27879,     0.17111,     0.41131,     0.30336,\n           0.24653,     0.25212,     0.43146,    0.068056])\nnames: {0: '10 of clubs', 1: '10 of diamonds', 2: '10 of hearts', 3: '10 of spades', 4: '2 of clubs', 5: '2 of diamonds', 6: '2 of hearts', 7: '2 of spades', 8: '3 of clubs', 9: '3 of diamonds', 10: '3 of hearts', 11: '3 of spades', 12: '4 of clubs', 13: '4 of diamonds', 14: '4 of hearts', 15: '4 of spades', 16: '5 of clubs', 17: '5 of diamonds', 18: '5 of hearts', 19: '5 of spades', 20: '6 of clubs', 21: '6 of diamonds', 22: '6 of hearts', 23: '6 of spades', 24: '7 of clubs', 25: '7 of diamonds', 26: '7 of hearts', 27: '7 of spades', 28: '8 of clubs', 29: '8 of diamonds', 30: '8 of hearts', 31: '8 of spades', 32: '9 of clubs', 33: '9 of diamonds', 34: '9 of hearts', 35: '9 of spades', 36: 'ace of clubs', 37: 'ace of diamonds', 38: 'ace of hearts', 39: 'ace of spades', 40: 'jack  of clubs', 41: 'jack of diamonds', 42: 'jack of hearts', 43: 'jack of spades', 44: 'king of clubs', 45: 'king of diamonds', 46: 'king of hearts', 47: 'king of spades', 48: 'queen of clubs', 49: 'queen of diamonds', 50: 'queen of hearts', 51: 'queen of spades'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.2897364808086953, 'metrics/recall(B)': 0.5163495844974251, 'metrics/mAP50(B)': 0.25505519041533115, 'metrics/mAP50-95(B)': 0.22707545967929865, 'fitness': 0.2298734327529019}\nsave_dir: PosixPath('/tmp/poker-cards-fmjio/train')\nspeed: {'preprocess': 0.10125745426524768, 'inference': 1.7866654829545454, 'loss': 0.00047141855413263494, 'postprocess': 1.7158822579817339}\ntask: 'detect'\n\n\n\n\nEvaluate Model\n\ntest_dataset = sv.DetectionDataset.from_yolo(f\"{data_location}/test/images\", f\"{data_location}/test/labels\", f\"{data_location}/data.yaml\")\nlen(test_dataset)\n\n44\n\n\n\nannotations_list = []\ndetections_list = []\nfor _, img, annotations in tqdm(test_dataset):\n    results = model.predict(img, verbose=False)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    annotations_list.append(annotations)\n    detections_list.append(detections)\n\n\n\n\n\nmAP = sv.metrics.MeanAveragePrecision().update(detections_list, annotations_list).compute()\nmAP.map50\n\n0.1417744455736285\n\n\n\n\nRandom Baseline\nAs per our assumption, we would simply need to randomly move the existing bounding boxes keeping their sizes constant with the following constraints: * The bounding box should be within the image boundaries.\n\nmin_size = 0\nmax_size = model.args['imgsz']\n\nmAPs = []\nfor random_seed in tqdm(range(100)):\n    np.random.seed(random_seed)\n    random_detections_list = []\n    for detections in detections_list:\n        random_detections = deepcopy(detections)\n        shift = np.random.rand(len(detections))\n        lower_limit = - detections.xyxy.min(axis=1) + 1e-6\n        upper_limit = max_size - detections.xyxy.max(axis=1) - 1e-6\n        transformed_shift = lower_limit + shift * (upper_limit - lower_limit)\n        random_detections.xyxy = random_detections.xyxy + transformed_shift.reshape(-1, 1)\n        random_detections_list.append(random_detections)\n        \n    mAP = sv.metrics.MeanAveragePrecision().update(random_detections_list, annotations_list).compute()\n    mAPs.append(mAP.map50)\n    \nprint(f\"mAP50: {mAP.map50:.2f} +/- {np.std(mAPs):.2f}\")\n\n\n\n\nmAP50: 0.04 +/- 0.01\n\n\nWe can also modify the confidence values.\n\nmin_size = 0\nmax_size = model.args['imgsz']\n\nmAPs = []\nfor random_seed in tqdm(range(100)):\n    np.random.seed(random_seed)\n    random_detections_list = []\n    for detections in detections_list:\n        random_detections = deepcopy(detections)\n        shift = np.random.rand(len(detections))\n        lower_limit = - detections.xyxy.min(axis=1) + 1e-6\n        upper_limit = max_size - detections.xyxy.max(axis=1) - 1e-6\n        transformed_shift = lower_limit + shift * (upper_limit - lower_limit)\n        random_detections.xyxy = random_detections.xyxy + transformed_shift.reshape(-1, 1)\n        random_detections.confidence = np.random.rand(len(detections))\n        random_detections_list.append(random_detections)\n        \n    mAP = sv.metrics.MeanAveragePrecision().update(random_detections_list, annotations_list).compute()\n    mAPs.append(mAP.map50)\n    \nprint(f\"mAP50: {mAP.map50:.2f} +/- {np.std(mAPs):.2f}\")\n\n\n\n\nmAP50: 0.03 +/- 0.01"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport GPy\nimport jax\nimport gpytorch\nimport botorch\nimport tinygp\nimport jax.numpy as jnp\nimport optax\nfrom IPython.display import clear_output\n\nfrom sklearn.preprocessing import StandardScaler\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Data",
    "text": "Data\n\nnp.random.seed(0) # We don't want surprices in a presentation :)\nN = 10\ntrain_x = torch.linspace(0, 1, N)\ntrain_y = torch.sin(train_x * (2 * math.pi)) + torch.normal(0, 0.1, size=(N,))\n \ntest_x = torch.linspace(0, 1, N*10)\ntest_y = torch.sin(test_x * (2 * math.pi))\n\n\nplt.plot(train_x, train_y, 'ko', label='train');\nplt.plot(test_x, test_y, label='test');\nplt.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Defining kernel",
    "text": "Defining kernel\n\\[\\begin{equation}\n\\sigma_f^2 = \\text{variance}\\\\\n\\ell = \\text{lengthscale}\\\\\nk_{RBF}(x_1, x_2) = \\sigma_f^2 \\exp \\left[-\\frac{\\lVert x_1 - x_2 \\rVert^2}{2\\ell^2}\\right]\n\\end{equation}\\]\n\nGPy\n\ngpy_kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\ngpy_kernel\n\n\n\n\n\n\nrbf.\nvalue\nconstraints\npriors\n\n\nvariance\n1.0\n+ve\n\n\n\nlengthscale\n1.0\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\ngpytorch_kernel.outputscale = 1. # variance\ngpytorch_kernel.base_kernel.lengthscale = 1. # lengthscale\n\ngpytorch_kernel\n\nScaleKernel(\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n)\n\n\n\n\nTinyGP\n\ndef RBFKernel(variance, lengthscale):\n    return jnp.exp(variance) * tinygp.kernels.ExpSquared(scale=jnp.exp(lengthscale))\n    \ntinygp_kernel = RBFKernel(variance=1., lengthscale=1.)\ntinygp_kernel\n\n&lt;tinygp.kernels.Product at 0x7f544039d710&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Define model",
    "text": "Define model\n\\[\n\\sigma_n^2 = \\text{noise variance}\n\\]\n\nGPy\n\ngpy_model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], gpy_kernel)\ngpy_model.Gaussian_noise.variance = 0.1\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 16.757933772959404\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n1.0\n+ve\n\n\n\nrbf.lengthscale\n1.0\n+ve\n\n\n\nGaussian_noise.variance\n0.1\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, kernel):\n        super().__init__(train_x, train_y, likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ngpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngpytorch_model = ExactGPModel(train_x, train_y, gpytorch_likelihood, gpytorch_kernel)\n\ngpytorch_model.likelihood.noise = 0.1\ngpytorch_model\n\nExactGPModel(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\n\nTinyGP\n\ndef build_gp(theta, X):\n    mean = theta[0] \n    variance, lengthscale, noise_variance = jnp.exp(theta[1:])\n    \n    kernel = variance * tinygp.kernels.ExpSquared(lengthscale)\n    \n    return tinygp.GaussianProcess(kernel, X, diag=noise_variance, mean=mean)\n\ntinygp_model = build_gp(theta=np.array([0., 1., 1., 0.1]), X=train_x.numpy())\n\ntinygp_model\n# __repr__\n\n&lt;tinygp.gp.GaussianProcess at 0x7f5440401850&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Train the model",
    "text": "Train the model\n\nGPy\n\ngpy_model.optimize(max_iters=50)\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 3.944394423452163\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n0.9376905183253631\n+ve\n\n\n\nrbf.lengthscale\n0.2559000163858406\n+ve\n\n\n\nGaussian_noise.variance\n0.012506184441481319\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(gpytorch_likelihood, gpytorch_model)\nbotorch.fit_gpytorch_model(mll)\n\ndisplay(gpytorch_model.mean_module.constant, # Mean\n        gpytorch_model.covar_module.outputscale, # Variance\n        gpytorch_model.covar_module.base_kernel.lengthscale, # Lengthscale \n        gpytorch_model.likelihood.noise) # Noise variance\n\n /opt/conda/lib/python3.7/site-packages/botorch/fit.py:143: UserWarning:CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/c10/cuda/CUDAFunctions.cpp:112.)\n\n\nParameter containing:\ntensor([0.0923], requires_grad=True)\n\n\ntensor(0.9394, grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([[0.2560]], grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([0.0124], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nTinyGP\n\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(theta, X, y):\n    gp = build_gp(theta, X)\n    return -gp.condition(y)\n\n\nobj = jax.jit(jax.value_and_grad(neg_log_likelihood))\nresult = minimize(obj, [0., 1., 1., 0.1], jac=True, args=(train_x.numpy(), train_y.numpy()))\nresult.x[0], np.exp(result.x[1:])\n\n(0.09213499552879165, array([0.9395271 , 0.25604163, 0.01243025]))"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Inference",
    "text": "Inference\n\ndef plot_gp(pred_y, var_y):\n    std_y = var_y ** 0.5\n    plt.figure()\n    plt.scatter(train_x, train_y, label='train')\n    plt.plot(test_x, pred_y, label='predictive mean')\n    plt.fill_between(test_x.ravel(), \n                     pred_y.ravel() - 2*std_y.ravel(), \n                     pred_y.ravel() + 2*std_y.ravel(), alpha=0.2, label='95% confidence')\n    plt.legend()\n\n\nGPy\n\npred_y, var_y = gpy_model.predict(test_x.numpy()[:, None])\nplot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_model.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = gpytorch_likelihood(gpytorch_model(test_x))\n    pred_y, var_y = pred_dist.mean, pred_dist.variance\n    plot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nTinyGP\n\ntinygp_model = build_gp(result.x, train_x.numpy())\npred_y, var_y = tinygp_model.predict(train_y.numpy(), test_x.numpy(), return_var=True)\n\nplot_gp(pred_y, var_y)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Tiny GP on CO2 dataset",
    "text": "Tiny GP on CO2 dataset\n\ndata = pd.read_csv(\"data/co2.csv\")\n\n# Train test split\nX = data[\"0\"].iloc[:290].values.reshape(-1, 1)\nX_test = data[\"0\"].iloc[290:].values.reshape(-1, 1)\ny = data[\"1\"].iloc[:290].values\ny_test = data[\"1\"].iloc[290:].values\n\n# Scaling the dataset\nXscaler = StandardScaler()\nX = Xscaler.fit_transform(X)\nX_test = Xscaler.transform(X_test)\n\nyscaler = StandardScaler()\ny = yscaler.fit_transform(y.reshape(-1, 1)).ravel()\ny_test = yscaler.transform(y_test.reshape(-1, 1)).ravel()\n\n\nplt.plot(X, y, label='train');\nplt.plot(X_test, y_test, label='test');\nplt.legend();\n\n\n\n\n\n\n\n\n\nclass SpectralMixture(tinygp.kernels.Kernel):\n    def __init__(self, weight, scale, freq):\n        self.weight = jnp.atleast_1d(weight)\n        self.scale = jnp.atleast_1d(scale)\n        self.freq = jnp.atleast_1d(freq)\n\n    def evaluate(self, X1, X2):\n        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n        return jnp.sum(\n            self.weight\n            * jnp.prod(\n                jnp.exp(-2 * jnp.pi ** 2 * tau ** 2 / self.scale ** 2)\n                * jnp.cos(2 * jnp.pi * self.freq * tau),\n                axis=-1,\n            )\n        )\n    \ndef build_spectral_gp(theta):\n    kernel = SpectralMixture(\n        jnp.exp(theta[\"log_weight\"]),\n        jnp.exp(theta[\"log_scale\"]),\n        jnp.exp(theta[\"log_freq\"]),\n    )\n    return tinygp.GaussianProcess(\n        kernel, X, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n    )\n\n\nK = 4 # Number of mixtures\ndiv_factor = 0.4\nnp.random.seed(1)\nparams = {\n    \"log_weight\": np.abs(np.random.rand(K))/div_factor,\n    \"log_scale\": np.abs(np.random.rand(K))/div_factor,\n    \"log_freq\": np.abs(np.random.rand(K))/div_factor,\n    \"log_diag\": np.abs(np.random.rand(1))/div_factor,\n    \"mean\": 0.,\n}\n\n@jax.jit\n@jax.value_and_grad\ndef loss(theta):\n    return -build_spectral_gp(theta).condition(y)\n# opt = optax.sgd(learning_rate=0.001)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\nlosses = []\nfor i in range(100):\n    loss_val, grads = loss(params)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    losses.append(loss_val)\n    clear_output(wait=True)\n    print(f\"iter {i}, loss {loss_val}\")\n\nopt_gp = build_spectral_gp(params)\n\nparams\n\niter 99, loss 27.987701416015625\n\n\n{'log_diag': DeviceArray([-2.7388687], dtype=float32),\n 'log_freq': DeviceArray([-3.6072493, -3.1795945, -3.4490397, -2.373117 ], dtype=float32),\n 'log_scale': DeviceArray([3.9890492, 3.8530042, 4.0878096, 4.4860597], dtype=float32),\n 'log_weight': DeviceArray([-1.3715047, -0.6132469, -2.413771 , -1.6582283], dtype=float32),\n 'mean': DeviceArray(0.38844627, dtype=float32)}\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nmu, var = opt_gp.predict(y, X_test, return_var=True)\n\nplt.plot(X, y, c='k')\nplt.fill_between(\n    X_test.ravel(), mu + np.sqrt(var), mu - np.sqrt(var), color=\"C0\", alpha=0.5\n)\nplt.plot(X_test, mu, color=\"C0\", lw=2)\n\n# plt.xlim(t.min(), 2025)\nplt.xlabel(\"year\")\n_ = plt.ylabel(\"CO$_2$ in ppm\")"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)",
    "text": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)",
    "text": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)\n\nMinimize inverse term fully\nNow, Minimize both togather"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html",
    "href": "posts/2021-10-12-sparsegps.html",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#imports",
    "href": "posts/2021-10-12-sparsegps.html#imports",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#data-preperation",
    "href": "posts/2021-10-12-sparsegps.html#data-preperation",
    "title": "SparseGPs in Stheno",
    "section": "Data preperation",
    "text": "Data preperation\n\n# Define points to predict at.\nx = B.linspace(0, 10, 100)\nx_obs = B.linspace(0, 7, 50_000)\nx_ind = B.linspace(0, 10, 20)\n\n# Construct a prior.\nf = GP(EQ().periodic(2 * B.pi))\n\n# Sample a true, underlying function and observations.\nf_true = B.sin(x)\ny_obs = B.sin(x_obs) + B.sqrt(0.5) * B.randn(*x_obs.shape)"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#plotting-function",
    "href": "posts/2021-10-12-sparsegps.html#plotting-function",
    "title": "SparseGPs in Stheno",
    "section": "Plotting function",
    "text": "Plotting function\n\ndef plot(method):\n    if method == 'VFE':\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            obs.mu(f.measure)[:, 0],\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()\n    else:\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            B.dense(f_post(x_ind).mean),\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "title": "SparseGPs in Stheno",
    "section": "Sparse regression with Variational Free Energy (VFE) method",
    "text": "Sparse regression with Variational Free Energy (VFE) method\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsVFE(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('VFE')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "title": "SparseGPs in Stheno",
    "section": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod",
    "text": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsFITC(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('FITC')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "href": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "title": "SparseGPs in Stheno",
    "section": "Hyperparameter tuning (Noisy Sine data)",
    "text": "Hyperparameter tuning (Noisy Sine data)\n\ndef model(vs):\n    \"\"\"Constuct a model with learnable parameters.\"\"\"\n    return vs['variance']*GP(EQ().stretch(vs['length_scale']))\n\n\ntorch.manual_seed(123)\n\ndataObj = rd.SineNoisy(scale_X=False, scale_y=False, return_test=True, backend='torch')\nx_obs, y_obs, x = dataObj.get_data()\n\n\nplt.scatter(x_obs, y_obs, s=2);\n\n\n\n\n\n\n\n\n\nVFE\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsVFE(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nFITC\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsFITC(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2024-12-27-download_caaqm_locations copy.html",
    "href": "posts/2024-12-27-download_caaqm_locations copy.html",
    "title": "Download CPCB CAAQM locations",
    "section": "",
    "text": "try:\n    import selenium\nexcept ModuleNotFoundError:\n    %pip install selenium\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm, trange\nfrom time import sleep, time\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n!rm log.txt\n\ndef print_it(*args, **kwargs):\n    print(*args, **kwargs)\n    with open('log.txt', 'a') as f:\n        print(*args, **kwargs, file=f)\n\nglobal_init = time()\n\nrm: log.txt: No such file or directory\n\n\n\n# Set up WebDriver\nop = webdriver.ChromeOptions()\n\ndriver = webdriver.Chrome(options=op)\n\n# Navigate to the website and manually solve the CAPTCHA\ndriver.get(\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\")\n\n\nManually solve captcha before moving on to the next cell..\n\n\n# leaflet-marker-icon custom-div-icon map_markers station_status_live leaflet-zoom-animated leaflet-interactive\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\nall_stations_len = len(all_station_markers)\nprint(\"Total stations: \", all_stations_len)\n\nTotal stations:  558\n\n\n\ndef get_after(string, phrase):\n    return string[string.index(phrase) + len(phrase):]\n\ndata = {}\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\nmarker_id = 0\nprogress_bar = tqdm(total=all_stations_len, desc=\"Progress\")\nwhile marker_id &lt; all_stations_len:\n    try:\n        marker = all_station_markers[marker_id]\n        driver.execute_script(\"arguments[0].click();\", marker)\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n        children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n        assert \"Station Name\" in children[3].text\n        \n        # parse it\n    \n        station, address, location = children[3].text.split('\\n')\n        station = get_after(station, \"Station Name: \")\n        address = get_after(address, \"Address: \")\n        latitude, longitude = location.split(\",\")\n        latitude = get_after(latitude, \"Latitude: \")\n        longitude = get_after(longitude, \"Longitude: \")\n        \n        data[station] = {\"address\": address, \"latitude\": float(latitude), \"longitude\": float(longitude)}\n        close = driver.find_element(By.CLASS_NAME, \"close\")\n        close.click()\n        sleep(0.5)\n        marker_id += 1\n        progress_bar.update(1)\n    except Exception as e:\n        driver.refresh()\n        input(\"Please manually solve the Captcha\")\n        all_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 12\n     10 marker = all_station_markers[marker_id]\n     11 driver.execute_script(\"arguments[0].click();\", marker)\n---&gt; 12 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n     13 children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n     14 assert \"Station Name\" in children[3].text\n\nFile /opt/miniconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:102, in WebDriverWait.until(self, method, message)\n    100     screen = getattr(exc, \"screen\", None)\n    101     stacktrace = getattr(exc, \"stacktrace\", None)\n--&gt; 102 time.sleep(self._poll)\n    103 if time.monotonic() &gt; end_time:\n    104     break\n\nKeyboardInterrupt: \n\n\n\n\ndf = pd.DataFrame(data).T\ndf.index.name = \"station\"\ndf.head(2)\n\n\n\n\n\n\n\n\naddress\nlatitude\nlongitude\n\n\nstation\n\n\n\n\n\n\n\nSIDCO Kurichi, Coimbatore - TNPCB\nSIDCO Kurichi, Coimbatore, Tamil Nadu.\n10.942451\n76.978996\n\n\nMuradpur, Patna - BSPCB\nS K Memorial Hall Premises, Near Gandhi Maidan...\n25.619651\n85.147382\n\n\n\n\n\n\n\n\ndf.to_csv(\"station_data.csv\")"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html",
    "href": "posts/2022-04-06-github_faqs.html",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "href": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "href": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q2: What to do if the main (or master) gets updated before I open a PR?",
    "text": "Q2: What to do if the main (or master) gets updated before I open a PR?\nPull the changes directly to your branch with:\ngit pull https://github.com/probml/pyprobml"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "href": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q3: What to do with the forks main when the original main is updated?",
    "text": "Q3: What to do with the forks main when the original main is updated?\nFetch upstream with GitHub GUI or use the same solution given in Q2."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "href": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q4: Why and when keeping the forks main up to date with the original main is important?",
    "text": "Q4: Why and when keeping the forks main up to date with the original main is important?\nWhenever we need to create new branches (usually from the forks main)."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "href": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q5: How to update a change in a PR that is open?",
    "text": "Q5: How to update a change in a PR that is open?\nPush the change to the corresponding branch and PR will get updated automatically."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html",
    "href": "posts/2021-03-22-gp_kernels.html",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "",
    "text": "!pip install -qq GPy\nimport autograd.numpy as np\nimport pandas as pd\nimport GPy\nimport matplotlib.pyplot as plt\nfrom autograd import grad\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "href": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "RBF (Radial basis function) Kernel, Stationarity and Isotropy",
    "text": "RBF (Radial basis function) Kernel, Stationarity and Isotropy\nRBF is one of the most commonly used kernels in GPs due to its infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions \\(X \\to Y\\).\nRBF kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2exp\\left(-\\frac{(x-x')^2}{2l^2}\\right)\n\\end{aligned}\n\\] Where, \\(\\sigma^2\\) is variance and \\(l\\) is known as lengthscale. #### Stationarity RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, \\(\\mathcal{K}(x,x')\\) depends only on \\(x-x'\\).\n\nIsotropy\nRBF is also isotropic kernel, which means that \\(\\mathcal{K}(x,x')\\) depends only on \\(|x-x'|\\). Thus, we have \\(\\mathcal{K}(x,x') = \\mathcal{K}(x',x)\\).\nLets visualize few functions drawn from the RBF kernel\n\ndef K_rbf(X1, X2, sigma=1., l=1.):\n  return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2))\n\n\n\nHelper functions\n\ndef plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)):\n  mean = np.zeros(X.shape[0])\n  cov = kernel_func(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  fig = plt.figure(figsize=(14,8), constrained_layout=True)\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1])\n  ax0.set_ylim(*ax0_ylim)\n  ax1 = fig.add_subplot(gs[1, 0:2])\n  ax1.set_ylim(*ax1_ylim)\n  ax2 = fig.add_subplot(gs[1, 2:4])\n  for func in functions:\n    ax0.plot(X, func,'o-');\n  ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel');\n  ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n  sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\ndef animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), \n                      k_name='',p_name='',symbol=''):\n  fig = plt.figure(figsize=(14,8))\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]);\n  def update(p):\n    ax0.cla();ax1.cla();ax2.cla();\n    ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim)\n    if p_name == 'Lengthscale':\n      cov = kernel_func(X, X, l=p)\n    elif p_name == 'Variance':\n      cov = kernel_func(X, X, sigma=np.sqrt(p))\n    elif p_name == 'Offset':\n      cov = kernel_func(X, X, c=p)\n    elif p_name == 'Period':\n      cov = kernel_func(X, X, p=p)\n    functions = np.random.multivariate_normal(mean, cov, size=5)\n    for func in functions:\n      ax0.plot(X, func,'o-');\n    ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel\\n'+p_name+' ('+symbol+') = '+str(p));\n    ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n    sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False);\n    ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\n  anim = FuncAnimation(fig, update, frames=val_list, blit=False)\n  plt.close()\n  rc('animation', html='jshtml')\n  return anim\n\nVerifying if our kernel is consistent with GPy kernels.\n\nX = np.linspace(101,1001,200).reshape(-1,1)\nsigma, l = 7, 11\nassert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) \n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\nk_name = 'RBF'\nplot_functions(K_rbf, ax0_ylim=(-3.5,3))\n\n\n\n\n\n\n\n\nLets see the effect of varying parameters \\(\\sigma\\) and \\(l\\) of the RBF kernel function.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_rbf, val_list, k_name='RBF', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nl = 1.\nval_list = [1,4,9,16,25]\nanimate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26),\n                  k_name='RBF', p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWith increase in value of \\(l\\), functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in \\(l\\).\nIncreasing \\(\\sigma^2\\) increase the overall uncertainty (width of the space where 95% of the functions live) across all the points."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Matern Kernel",
    "text": "Matern Kernel\nMatern kernels are given by a general formula as following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1, x_2) =  \\sigma^2\\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\n\\Bigg)^\\nu K_\\nu\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\\Bigg)\n\\end{aligned}\n\\] Where, \\(\\Gamma\\) is gamma function and \\(K_\\nu\\) is modified Bessel function of second order.\nThe general formula is not very intuitive about the functionality of this kernel. In practice, Matern with \\(\\nu=\\{0.5,1.5,2.5\\}\\) are used, where GP with each kernel is \\((\\lceil\\nu\\rceil-1)\\) times differentiable.\nMatern functions corresponding to each \\(\\nu\\) values are defined as the following, \\[\n\\begin{aligned}\nMatern12 \\to \\mathcal{K_{\\nu=0.5}}(x_1, x_2) &=  \\sigma^2exp\\left(-\\frac{|x_1-x_2|}{l}\\right)\\\\\nMatern32 \\to \\mathcal{K_{\\nu=1.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)exp\\left(-\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)\\\\\nMatern52 \\to \\mathcal{K_{\\nu=2.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{5}|x_1-x_2|}{l}+\\frac{5(x_1-x_2)^2)}{3l^2}\\right)exp\\left(-\\frac{\\sqrt{5}|x_1-x_2|}{l}\\right)\n\\end{aligned}\n\\] Matern kernels are stationary as well as isotropic. With \\(\\nu \\to \\infty\\) they converge to \\(RBF\\) kernel. \\(Matern12\\) is also known as \\(Exponential\\) kernel in toolkits such as GPy.\nNow, lets draw few functions from each of these versions and try to get intuition behind each of them.\n\ndef K_m12(X1, X2, sigma=1., l=1.): # v = 0.5\n  return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l))\ndef K_m32(X1, X2, sigma=1., l=1.): # v = 1.5\n  return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l))\ndef K_m52(X1, X2, sigma=1., l=1.): # v = 2.5\n  return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))*\\\n                    (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l))\n\nVerifying if our kernels are consistent with GPy kernels.\n\nX = np.linspace(101,1001,50).reshape(-1,1)\nassert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X))\nassert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X))\n\n\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\n\nfig, ax = plt.subplots(3,2,figsize=(14,10))\nnames = ['Matern12', 'Matern32', 'Matern52']\nfor k_i, kernel in enumerate([K_m12, K_m32, K_m52]):\n  mean = np.zeros(X.shape[0])\n  cov = kernel(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  for func in functions:\n    ax[k_i,0].plot(X, func);\n  ax[k_i,0].set_xlabel('X');ax[k_i,0].set_ylabel('Y');ax[k_i,0].set_title('Functions drawn from '+names[k_i]+' kernel');\n  sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax[k_i,1].set_xlabel('X');ax[k_i,1].set_ylabel('X');ax[k_i,1].set_title('Covariance matrix');\nplt.tight_layout();\n\n\n\n\n\n\n\n\nFrom the above plot, we can say that smoothness is increasing in functions as we increase \\(\\nu\\). Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52.\nLet us see effect of varying \\(\\sigma\\) and \\(l\\) on Matern32 which is more popular among the three.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_m32, val_list, k_name='Matern32', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that Matern32 kernel behaves similar to RBF with varying \\(l\\). Though, Matern32 is less smoother than RBF. A quick comparison would clarify this.\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_rbf(X,X, l=3.)[:,50], label='RBF')\nplt.plot(X, K_m32(X,X, l=3.)[:,50], label='Matern32')\nplt.legend();plt.xlabel('X');plt.ylabel('Covariance (K(0,X))');\nplt.title('K(0,X)');"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Periodic Kernel",
    "text": "Periodic Kernel\nPeriodic Kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2\\exp\\left(-\\frac{\\sin^2(\\pi|x_1 - x_2|/p)}{2l^2}\\right)\n\\end{aligned}\n\\] Where \\(p\\) is period. Lets visualize few functions drawn from this kernel.\n\ndef K_periodic(X1, X2, sigma=1., l=1., p=3.):\n  return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2)\n\nX = np.linspace(10,1001,50).reshape(-1,1)\nassert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), \n                   GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X))\n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1\nl = 1.\np = 3.\nk_name = 'Periodic'\nplot_functions(K_periodic)\n\n\n\n\n\n\n\n\nWe will investigate the effect of varying period \\(p\\) now.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1),\n                  k_name='Periodic',p_name='Period')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFrom the above animation we can see that, all points that are \\(p\\) distance apart from each other have exactly same values because they have correlation of exactly 1 (\\(\\sigma=1 \\to covariance=correlation\\)).\nNow, we will investigate effect of lenging lengthscale \\(l\\) while other parameters are constant.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1),\n                  k_name='Periodic',p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that correlation between a pair of locations \\(\\{x_1,x_2|x_1-x_2&lt;p\\}\\) increases as the lengthscale is increased."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Linear Kernel",
    "text": "Linear Kernel\nLinear kernel (a.k.a. dot-product kernel) is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+\\sigma^2\n\\end{aligned}\n\\] Lets visualize few functions drawn from the linear kernel\n\ndef K_lin(X1, X2, sigma=1., c=1.):\n  return (X1-c)@(X2.T-c) + sigma**2\n\n\nnp.random.seed(0)\nsigma = 1.\nc = 1.\n\nplot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7))\n\n\n\n\n\n\n\n\nLets see the effect of varying parameters \\(\\sigma\\) and \\(c\\) of the linear kernel function.\n\nval_list = [-3,-2,-1,0,1,2,3]\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), \n                  p_name='Offset', symbol='c')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nnp.random.seed(1)\nval_list = np.square(np.array([1,2,3,4,5,8]))\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), \n                  p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nVarying \\(c\\) parameter changes position of shallow region in covariance matrix. In other words, as \\(x \\to c\\), points close to \\(x\\) have variance \\(\\to \\sigma^2\\). Distant points have monotonically increasing variance.\nIncreasing \\(\\sigma^2\\) adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points.\n\nNon-stationary behaviour of Linear kernel\nUnlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim.\n\nfig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True)\nkerns = [K_rbf, K_m32, K_periodic, K_lin]\nk_names = ['RBF', 'Matern32', 'Periodic', 'Linear']\nX = np.linspace(-10,10,21).reshape(-1,1)\ndef update(x):\n  count = 0\n  for i in range(2):\n    for j in range(2):\n      ax.ravel()[count].cla()\n      tmp_kern = kerns[count]\n      mean = np.zeros(X.shape[0])\n      cov = tmp_kern(X,X)\n      ax.ravel()[count].plot(X, cov[:,x]);\n      ax.ravel()[count].set_xlim(X[x-3],X[x+3])\n      ax.ravel()[count].set_xlabel('X');\n      ax.ravel()[count].set_ylabel('K('+str(X[x].round(2))+',X)');\n      ax.ravel()[count].set_title('Covariance K('+str(X[x].round(2))+',X) for '+k_names[count]+' kernel');\n      count += 1\n  ax.ravel()[3].set_ylim(-5,80)\n  plt.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False)\nplt.close()\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "href": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Multiplications of kernels",
    "text": "Multiplications of kernels\nIf a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels.\n\nPeriodic * Linear\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label='Periodic')\nplt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label='Linear')\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label='Periodic*Linear')\nplt.legend(bbox_to_anchor=(1,1));plt.xlabel('X');plt.ylabel('Covariance')\nplt.title('K(0,*)');\n\n\n\n\n\n\n\n\n\n\nLinear * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nplt.plot(X, K_lin(X,X,c=-1)[:,50], label='Linear1')\nplt.plot(X, K_lin(X,X,c=1)[:,50], label='Linear2')\nplt.plot(X, K_lin(X,X,c=0.5)[:,50], label='Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label='Linear1*Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label='Linear1*Linear2*Linear3')\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\n\nMatern * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nk1 = K_lin(X,X,c=1)[:,50]\nk2 = K_m32(X,X)[:,50]\nplt.plot(X, k1, label='Linear')\nplt.plot(X, k2, label='Matern32')\nplt.plot(X, k1*k2, label='Matern32*Linear')\nplt.legend(bbox_to_anchor=(1,1));"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "href": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Appendix (Extra material)",
    "text": "Appendix (Extra material)\nAt this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes.\n\n\nPredicting at a single point after observing value at a single point\nLets see how would be a GP prediction after observing value at a single point.\nOur kernel function is given by, * \\(K(x,x')=(x-c) \\cdot (x'-c)+\\sigma^2\\)\nNow, we observe value \\(y\\) at a location \\(x\\) and we want to predict value \\(y^*\\) at location \\(x^*\\). \\[\n\\begin{aligned}\n(y^*|x_1,y_1,x^*) &= K(x^*,x) \\cdot K^{-1}(x,x)\\cdot y \\\\\n&= \\left(\\frac{(x-c)(x^*-c)+\\sigma^2}{(x-c)(x-c)+\\sigma^2}\\right)\\cdot y\n\\end{aligned}\n\\] \\(c\\) and \\(\\sigma^2\\) do not vary in numerator and denominator so, the value of \\(y^* \\propto x^*\\).\n\n\n\nPredicting at a single point after observing values at two points\nNow, well take a case where two values \\({y_1, y_2}\\) are observed at \\({x_1, x_2}\\). Let us try to predict value \\(y^*\\) at \\(x^*\\).\n$$ y^* =\n\\[\\begin{bmatrix}\nK(x_1, x^*) & K(x_2,x^*)\n\\end{bmatrix}\\begin{bmatrix}\nK(x_1, x_1) & K(x_1,x_2) \\\\\nK(x_2, x_1) & K(x_2,x_2)\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_1-c)^2+\\sigma^2 & (x_1-c) (x_2-c)+\\sigma^2 \\\\\n(x_2-c) (x_1-c)+\\sigma^2 & (x_2-c)^2 +\\sigma^2\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix} \\frac{1}{\\sigma^2(x_1-x_2)^2}\n\\begin{bmatrix}\n(x_2-c)^2+\\sigma^2 & -[(x_1-c)(x_2-c)+\\sigma^2] \\\\\n-[(x_2-c) (x_1-c)+\\sigma^2] & (x_1-c)^2 +\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix} \\tag{1}\\]\nFrom Eq. (1) second term, we can say that if \\(\\sigma^2=0\\), matrix is not-invertible because determinant is zero. It means that, if \\(\\sigma^2=0\\), observing a single point is enough, we can infer values at infinite points after observing that single point.\nEvaluating Eq. (1) further, it converges to the following equation, \\[\n\\begin{aligned}\ny^* = \\frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)}\n\\end{aligned}\n\\] Interestingly, we can see that output does not depend on \\(c\\) or \\(\\sigma^2\\) anymore. Let us verify experimentally if this is true for observing more than 2 data points.\n\n\nPrepering useful functions\n\nfrom scipy.optimize import minimize\n\n\ndef cov_func(x, x_prime, sigma, c):\n  return (x-c)@(x_prime-c) + sigma**2\n\ndef neg_log_likelihood(params):\n  n = X.shape[0]\n  sigma, c, noise_std = params\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) \n  return nll_ar[0,0]\n\ndef predict(params):\n  sigma, c, noise_std = params\n  k = cov_func(X, X.T, sigma, c)\n  np.fill_diagonal(k, k.diagonal()+noise_std**2)\n  k_inv = np.linalg.pinv(k)\n  k_star = cov_func(X_test, X.T, sigma, c)\n\n  mean = k_star@k_inv@Y\n  cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T\n  return mean, cov\n\n\n\nObserving more than two points and changing hyperparameters manually\n\nX = np.array([3,4,5,6,7,8]).reshape(-1,1)\nY = np.array([6,9,8,11,10,13]).reshape(-1,1)\nX_test = np.linspace(1,8,20).reshape(-1,1)\nparams_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], \n                [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std\n\nX_extra = np.hstack([np.ones((X.shape[0], 1)), X])\nTheta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y\nX_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\nY_test_ideal = X_test_extra@Theta\n\nfig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True)\nmeans = []\nfor p_i, params in enumerate(params_grid):\n  Y_test_mean, Y_test_cov = predict(params)\n  means.append(Y_test_mean)\n  ax[p_i].scatter(X, Y, label='train')\n  ax[p_i].scatter(X_test, Y_test_mean, label='test')\n  ax[p_i].legend();ax[p_i].set_xlabel('X');ax[p_i].set_ylabel('Y');\n  ax[p_i].set_title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\n\n\n\n\n\n\n\n\n\nnp.allclose(Y_test_ideal, means[0]),\\\nnp.allclose(Y_test_ideal, means[1]),\\\nnp.allclose(Y_test_ideal, means[2]),\\\nnp.allclose(Y_test_ideal, means[3])\n\n(True, True, True, False)\n\n\n\nmodel = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1))\n# model['Gaussian_noise'].fix(10**-10)\n# model.kern.variances.fix(10**-10)\nmodel.optimize()\nmodel.plot()\nplt.plot(X_test, Y_test_ideal, label='Normal Eq. fit')\nplt.plot(X_test,model.predict(X_test)[0], label='Prediction')\nplt.legend()\nmodel\n\n\n\n\nModel: GP regression\nObjective: 13.51314321804978\nNumber of Parameters: 2\nNumber of Optimization Parameters: 2\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nlinear.variances\n2.806515343539501\n+ve\n\n\n\nGaussian_noise.variance\n2.0834221617534134\n+ve\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there is no change in fit with change in \\(c\\) and \\(\\sigma\\). 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood.\n\nparams = [1., 1., 1.]\nresult = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)])\nparams = result.x\nprint(params, result.fun)\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(Y_test_ideal, Y_test_mean)\n\n[9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\ndef neg_log_likelihood(sigma, c, noise_std):\n  n = X.shape[0]\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov))\n  return nll_ar[0,0]\n\n\ngrad_func = grad(neg_log_likelihood, argnum=[0,1,2])\nalpha = 0.01\nloss = []\nsigma, c, noise_std = 1., 1., 1.\nfor _ in range(5000):\n  grads = grad_func(sigma, c, noise_std)\n  # print(grads)\n  sigma = sigma - alpha*grads[0]\n  c = c - alpha*grads[1]\n  noise_std = noise_std - alpha*grads[2]\n  loss.append(neg_log_likelihood(sigma, c, noise_std))\nprint(sigma, c, noise_std)\nplt.plot(loss);\nloss[-1]\n\n7.588989986845149 -2.830840439162303 32.2487569348891\n\n\n31.05187173290998\n\n\n\n\n\n\n\n\n\n\nparams = sigma, c, noise_std\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1)\n\nFalse"
  },
  {
    "objectID": "posts/docker_cheatsheet.html",
    "href": "posts/docker_cheatsheet.html",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#images",
    "href": "posts/docker_cheatsheet.html#images",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#containers",
    "href": "posts/docker_cheatsheet.html#containers",
    "title": "Docker Cheatsheet",
    "section": "Containers",
    "text": "Containers\nCreate a new container from an image with following flags 1. -v: for telling docker to use a shared directory between host and container 2. -p \\&lt;host-port\\&gt;\\&lt;container-port\\&gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. 3. --name generates a name for the container for easy reference in other commands 4. --gpus all tells docker to use all GPUs available on host 5. --memory-swap Restricts RAM+Swap usage 6. -it makes sure container awaits after starting instead of instantly shutting down if no startup scripts are configured.\nTo create new container with default params:\ndocker create tensorflow/tensorflow:2.4.0-gpu-jupyter \nTo create new container with manual params:\ndocker create -it \\\n-v \\path_in_host:\\path_in_container \\\n-p9000:8888 \\\n--name aaai \\\n--cpus 2 \\\n--gpus all \\ # To use specific gpus: --gpu '\"device=0,2\"'\n--memory 90g \\ # Uses 90g memory\n--memory-swap 100g \\ # --memory-swap is a modifier flag that only has meaning if --memory is also set. In this case 10g of swap will be used.\ntensorflow/tensorflow:2.4.0-gpu-jupyter\nUpdate some of the above configurations after container creation:\n# change RAM limit of a container named \"aaai\"\ndocker update --memory-swap 50g aaai\n\nNote: In general, changes made to container persist when container is stopped.\n\nCheck containers:\ndocker ps # shows running containers\ndocker ps -a # shows all containers\nStart a container (default script will be executed with this if any):\n# docker start &lt;container-name&gt;\ndocker start aaai\nStop a container:\n# docker stop &lt;container-name&gt; \ndocker stop aaai\nDelete a container:\n# docker rm &lt;container-name&gt;\ndocker rm aaai\nGo to a running containers shell:\n#docker exec -it &lt;container-name&gt; bash\ndocker exec -it aaai bash # -it stands for interactive\nExecute any command on a running container without opening a shell in container:\n# docker exec -it &lt;container-name&gt; &lt;command&gt;\ndocker exec -it aaai jupyter notebook list\nCheck container logs (including shell commands output):\n# docker logs &lt;container-name&gt;\ndocker logs aaai"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#system",
    "href": "posts/docker_cheatsheet.html#system",
    "title": "Docker Cheatsheet",
    "section": "System",
    "text": "System\nCheck all images, all containers and space occupied by them:\ndocker system df -v"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "href": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "title": "Docker Cheatsheet",
    "section": "Set up rootless docker",
    "text": "Set up rootless docker\nRefer to this guide: https://docs.docker.com/engine/security/rootless/\nMain steps:\n\nRun dockerd-rootless-setuptool.sh install.\nSetup PATH and DOCKER_HOME as suggested by command output.\nsystemctl --user restart docker.\nTry docker images to check if things worked.\nTry docker run --rm hello-world to check if things really worked."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html",
    "href": "posts/numpy-algebra- copy.html",
    "title": "How numpy handles day-to-day algebra?",
    "section": "",
    "text": "import numpy as np\n\nnp.__version__\n\n118 False\n140 False\numath 8 False\numath 10 False\numath 12 True\ncore 73 False\nYes!\nnumeric 29 False\nnumeric 41 False\nnumeric 1128 False\nnumeric 2515 False\nnumeric 2517 True\ncore 75 False\ncore 77 True\ncore 83 True\ncore 94 True\n142 False\n144 True\n\n\n'1.25.2'"
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#motivation",
    "href": "posts/numpy-algebra- copy.html#motivation",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Motivation",
    "text": "Motivation\nIn this blog post, we will try to figure out how numpy handles seemingly simple math operations. The motivation behind this exploration is to figure out if there are a few foundational operations behind most of the frequently used functions in numpy. For the sake of right level of abstraction, we will not look into addition, subtraction, multiplication, and division."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#pi",
    "href": "posts/numpy-algebra- copy.html#pi",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Pi",
    "text": "Pi\n\nBackground\npi is an irrational number and computing its value to a certain precision is a challenging task. This video talks in detail how people used to compute pi in the past. At the time of writing this blog post, Google holds the record for computing pi to the highest precision to 100 trilian digits. They used y-cruncher program (its free. try it!) with Chudnovsky algorithm to compute pi. Here are the first 100 digits of pi:\n\\(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679\\)\n\n\nSource code\nThis is how pi is defined in numpy source code upto 36 digits.\n#define NPY_PI        3.141592653589793238462643383279502884  /* pi */\nLets verify it.\n\nprint(f\"{np.pi:.64f}\")\n\n3.1415926535897931159979634685441851615905761718750000000000000000\n\n\nHmm, that looks off. From 16th digit onwards, the values are different. Lets try to figure out why.\n\npi = 3.141592653589793238462643383279502884\npi = np.array(pi, dtype=np.float64)\npi = f\"{pi:.64f}\"\nnp_pi = f\"{np.pi:.64f}\"\nassert np_pi == pi\n\nOkay, so it seems like converting 36 digits of pi to 64 bit precision went wrong from 16th digit onwards. What a waste of last 20 digits of pi due to floating point errors! Anyways, lets move on."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#power",
    "href": "posts/numpy-algebra- copy.html#power",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Power",
    "text": "Power\nLets find out what happens when you execute the following code in numpy.\n\nnumber = np.float64(1.1)\nnumber**1.2\n\n1.1211693641406024"
  },
  {
    "objectID": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "href": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "title": "Active Learning with Bayesian Linear Regression",
    "section": "",
    "text": "A quick wrap-up for Bayesian Linear Regression (BLR)\nWe have a feature matrix \\(X\\) and a target vector \\(Y\\). We want to obtain \\(\\theta\\) vector in such a way that the error \\(\\epsilon\\) for the following equation is minimum.\n\\[\nY = X^T\\theta + \\epsilon\n\\] Prior PDF for \\(\\theta\\) is,\n\\[\np(\\theta) \\sim \\mathcal{N}(M_0, S_0)\n\\]\nWhere \\(S_0\\) is prior covariance matrix, and \\(M_0\\) is prior mean.\nPosterier PDF can be given as,\n\\[\n\\begin{aligned}\np(\\theta|X,Y) &\\sim \\mathcal{N}(\\theta | M_n, S_n) \\\\\nS_n &= (S_0^{-1} + \\sigma_{mle}^{-2}X^TX) \\\\\nM_n &= S_n(S_0^{-1}M_0+\\sigma_{mle}^{-2}X^TY)\n\\end{aligned}\n\\]\nMaximum likelihood estimation of \\(\\sigma\\) can be calculated as,\n\\[\n\\begin{aligned}\n\\theta_{mle} &= (X^TX)^{-1}X^TY \\\\\n\\sigma_{mle} &= ||Y - X^T\\theta_{mle}||\n\\end{aligned}\n\\]\nFinally, predicted mean \\(\\hat{Y}_{mean}\\) and predicted covariance matrix \\(\\hat{Y}_{cov}\\) can be given as,\n\\[\n\\begin{aligned}\n\\hat{Y} &\\sim \\mathcal{N}(\\hat{Y}_{mean}, \\hat{Y}_{cov}) \\\\\n\\hat{Y}_{mean} &= XM_n \\\\\n\\hat{Y}_{cov} &= X^TS_nX\n\\end{aligned}\n\\]\nNow, lets put everything together and write a class for Bayesian Linear Regression.\n\n\nCreating scikit-learn like class with fit predict methods for BLR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 0 # random seed for train_test_split\n\n\nclass BLR():\n  def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix\n    self.S0 = S0\n    self.M0 = M0\n\n  def fit(self,x,y, return_self = False):\n    self.x = x\n    self.y = y\n\n    # Maximum likelihood estimation for sigma parameter\n    theta_mle = np.linalg.pinv(x.T@x)@(x.T@y)\n    sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2\n    sigma_mle = np.sqrt(sigma_2_mle)\n\n    # Calculating predicted mean and covariance matrix for theta\n    self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x)\n    self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze())\n\n    # Calculating predicted mean and covariance matrix for data\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    if return_self:\n      return (self.y_hat_map, self.pred_var)\n    \n  def predict(self, x):\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    return (self.y_hat_map, self.pred_var)\n\n  def plot(self, s=1): # s -&gt; size of dots for scatter plot\n    individual_var = self.pred_var.diagonal()\n    plt.figure()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.plot(self.x[:,1], self.y_hat_map, color='black', label='model')\n    plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color='black', label='uncertainty')\n    plt.scatter(self.x[:,1], self.y, label='actual data',s=s)\n    plt.title('MAE is '+str(np.mean(np.abs(self.y - self.y_hat_map))))\n    plt.legend()\n\n\n\nCreating & visualizing dataset\nTo start with, lets create a random dataset with degree 3 polynomial function with some added noise.\n\\[\nY = (5X^3 - 4X^2 + 3X - 2) + \\mathcal{N}(0,1)\n\\]\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, )\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\nWell try to fit a degree 5 polynomial function to our data.\n\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nN_features = X.shape[1]\n\n\nplt.scatter(X[:,1], Y, s=0.5, label = 'data points')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLearning a BLR model on the entire data\nWell take \\(M_0\\) (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about \\(M_0\\). Were taking \\(S_0\\) (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other.\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\n\n\n\nVisualising the fit\n\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nThis doesnt look like a good fit, right? Lets set the prior closer to the real values and visualize the fit again.\n\n\nVisualising the fit after changing the prior\n\nnp.random.seed(seed)\nS0 = np.eye(N_features)\nM0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, )\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nHmm, better. Now lets see how it fits after reducing the noise and setting the prior mean to zero vector again.\n\n\nVisualising the fit after reducing the noise\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nWhen the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit.\n\n\nIntuition to Active Learning (Uncertainty Sampling) with an example\nLets take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How?\nWe train our model with existing data and test it on all the suspected patients data. Lets say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing.\nThis method is called Uncertainty Sampling in Active Learning. Now lets formally define Active Learning. From Wikipedia,\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.\nNow, well go through the active learning procedure step by step.\n\n\nTrain set, test set, and pool. What is what?\nThe train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty.\nSo, the algorithm can be represented as the following,\n\nTrain the model with the train set.\nTest the performance on the test set (This is expected to improve).\nTest the model with the pool.\nQuery for the most uncertain datapoint from the pool.\nAdd that datapoint into the train set.\nRepeat step 1 to step 5 for \\(K\\) iterations (\\(K\\) ranges from \\(0\\) to the pool size).\n\n\n\nCreating initial train set, test set, and pool\nLets take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Lets start with 2 data points as the train set.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed)\n\nVisualizing train, test and pool.\n\nplt.scatter(test_X[:,1], test_Y, label='test set',color='r', s=2)\nplt.scatter(train_X[:,1], train_Y, label='train set',marker='s',color='k', s=50)\nplt.scatter(pool_X[:,1], pool_Y, label='pool',color='b', s=2)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLets initialize a few dictionaries to keep track of each iteration.\n\ntrain_X_iter = {} # to store train points at each iteration\ntrain_Y_iter = {} # to store corresponding labels to the train set at each iteration\nmodels = {} # to store the models at each iteration\nestimations = {} # to store the estimations on the test set at each iteration\ntest_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n\n\n\nTraining & testing initial learner on train set (Iteration 0)\nNow we will train the model for the initial train set, which is iteration 0.\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\nCreating a plot method to visualize train, test and pool with estimations and uncertainty.\n\ndef plot(ax, model, init_title=''):\n  # Plotting the pool\n  ax.scatter(pool_X[:,1], pool_Y, label='pool',s=1,color='r',alpha=0.4)\n  \n  # Plotting the test data\n  ax.scatter(test_X[:,1], test_Y, label='test data',s=1, color='b', alpha=0.4)\n  \n  # Combining the test & the pool\n  test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y)\n  \n  # Sorting test_pool for plotting\n  sorted_inds = np.argsort(test_pool_X[:,1])\n  test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n  \n  # Plotting test_pool with uncertainty\n  model.predict(test_pool_X)\n  individual_var = model.pred_var.diagonal()\n  ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n  ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                  , alpha=0.2, color='black', label='uncertainty')\n  \n  # Plotting the train data\n  ax.scatter(model.x[:,1], model.y,s=40, color='k', marker='s', label='train data')\n  ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n  \n  # Plotting MAE on the test set\n  model.predict(test_X)\n  ax.set_title(init_title+' MAE is '+str(np.mean(np.abs(test_Y - model.y_hat_map))))\n  ax.set_xlabel('x')\n  ax.set_ylabel('y')\n  ax.legend()\n\nPlotting the estimations and uncertainty.\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nLets check the maximum uncertainty about any point for the model.\n\nmodels[0].pred_var.diagonal().max()\n\n4.8261426545316604e-29\n\n\nOops!! There is almost no uncertainty in the model. Why? lets try again with more train points.\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed)\n\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nNow uncertainty is visible, and currently, its high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model.\nLets evaluate the performance on the test set.\n\nestimations[0], _ = models[0].predict(test_X)\ntest_mae_error[0] = np.mean(np.abs(test_Y - estimations[0]))\n\nMean Absolute Error (MAE) on the test set is\n\ntest_mae_error[0]\n\n0.5783654195019617\n\n\n\n\nMoving the most uncertain point from the pool to the train set\nIn the previous plot, we saw that the model was least certain about the left-most point. Well move that point from the pool to the train set and see the effect.\n\nesimations_pool, _ = models[0].predict(pool_X)\n\nFinding out a point having the most uncertainty.\n\nin_var = models[0].pred_var.diagonal().argmax()\nto_add_x = pool_X[in_var,:]\nto_add_y = pool_Y[in_var]\n\nAdding the point from the pool to the train set.\n\ntrain_X_iter[1] = np.vstack([train_X_iter[0], to_add_x])\ntrain_Y_iter[1] = np.append(train_Y_iter[0], to_add_y)\n\nDeleting the point from the pool.\n\npool_X = np.delete(pool_X, in_var, axis=0)\npool_Y = np.delete(pool_Y, in_var)\n\n\n\nTraining again and visualising the results (Iteration 1)\nThis time, we will pass previously learnt prior to the next iteration.\n\nS0 = np.eye(N_features)\nmodels[1] = BLR(S0, models[0].MN)\n\n\nmodels[1].fit(train_X_iter[1], train_Y_iter[1])\n\n\nestimations[1], _ = models[1].predict(test_X)\ntest_mae_error[1] = np.mean(np.abs(test_Y - estimations[1]))\n\nMAE on the test set is\n\ntest_mae_error[1]\n\n0.5779411133071186\n\n\nVisualizing the results.\n\nfig, ax = plt.subplots()\nplot(ax, models[1])\n\n\n\n\n\n\n\n\nBefore & after adding most uncertain point\n\nfig, ax = plt.subplots(1,2, figsize=(13.5,4.5))\nplot(ax[0], models[0],'Before')\nplot(ax[1], models[1],'After')\n\n\n\n\n\n\n\n\nWe can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data.\nNow lets do this for few more iterations in a loop and visualise the results.\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x= np.zeros((num_iterations+1, N_features))\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(2, num_iterations+1):\n    # Making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool, var = models[iteration-1].predict(pool_X)\n    \n    # Finding the point from the pool with highest uncertainty\n    in_var = var.diagonal().argmax()\n    to_add_x = pool_X[in_var,:]\n    to_add_y = pool_Y[in_var]\n    points_added_x[iteration-1,:] = to_add_x\n    points_added_y.append(to_add_y)\n    \n    # Adding the point to the train set from the pool\n    train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x])\n    train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var, axis=0)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set\n    models[iteration] = BLR(S0, models[iteration-1].MN)\n    models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration])\n    \n    estimations[iteration], _ = models[iteration].predict(test_X)\n    test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n2 0.49023173501654815\n3 0.4923391714942153\n4 0.49040074812746753\n5 0.49610198614600165\n6 0.5015282102751122\n7 0.5051264429971314\n8 0.5099913097301352\n9 0.504455016053513\n10 0.5029219102020734\n11 0.5009762782262487\n12 0.5004883097883343\n13 0.5005169638980388\n14 0.5002731089932334\n15 0.49927485683909884\n16 0.49698416490822594\n17 0.49355398855432897\n18 0.49191185613804617\n19 0.491164833699368\n20 0.4908067530719673\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Lets visualise fits for all the iterations. Well discuss this behaviour after that.\n\n\nVisualizing active learning procedure\n\nprint('Initial model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[0].MN[::-1]))\nprint('\\nFinal model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[num_iterations].MN[::-1]))\n\nInitial model\nY = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63\n\nFinal model\nY = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58\n\n\n\ndef update(iteration):\n    ax.cla()\n    plot(ax, models[iteration])\n    fig.tight_layout()\n\n\nfig, ax = plt.subplots()\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250)\nplt.close()\nrc('animation', html='jshtml')\n\n\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually.\nNow, lets put everything together and create a class for active learning procedure\n\n\nCreating a class for active learning procedure\n\nclass ActiveL():\n  def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1):\n    self.X_init = X\n    self.y = y\n    self.S0 = S0\n    self.M0 = M0\n    self.train_X_iter = {} # to store train points at each iteration\n    self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration\n    self.models = {} # to store the models at each iteration\n    self.estimations = {} # to store the estimations on the test set at each iteration\n    self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n    self.test_size = test_size\n    self.degree = degree\n    self.iterations = iterations\n    self.seed = seed\n    self.train_size = degree + 2\n\n  def data_preperation(self):\n    # Adding polynomial features\n    self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init)\n    N_features = self.X.shape[1]\n    \n    # Splitting into train, test and pool\n    train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, \n                                                                            test_size=self.test_size,\n                                                                            random_state=self.seed)\n    self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, \n                                                                            train_size=self.train_size, \n                                                                            random_state=self.seed)\n    \n    # Setting BLR prior incase of not given\n    if self.M0 == None:\n      self.M0 = np.zeros((N_features, ))\n    if self.S0 == None:\n      self.S0 = np.eye(N_features)\n    \n  def main(self):\n    # Training for iteration 0\n    self.train_X_iter[0] = self.train_X\n    self.train_Y_iter[0] = self.train_Y\n    self.models[0] = BLR(self.S0, self.M0)\n    self.models[0].fit(self.train_X, self.train_Y)\n\n    # Running loop for all iterations\n    for iteration in range(1, self.iterations+1):\n      # Making predictions on the pool set based on model learnt in the respective train set \n      estimations_pool, var = self.models[iteration-1].predict(self.pool_X)\n      \n      # Finding the point from the pool with highest uncertainty\n      in_var = var.diagonal().argmax()\n      to_add_x = self.pool_X[in_var,:]\n      to_add_y = self.pool_Y[in_var]\n      \n      # Adding the point to the train set from the pool\n      self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x])\n      self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y)\n      \n      # Deleting the point from the pool\n      self.pool_X = np.delete(self.pool_X, in_var, axis=0)\n      self.pool_Y = np.delete(self.pool_Y, in_var)\n      \n      # Training on the new set\n      self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN)\n      self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration])\n      \n      self.estimations[iteration], _ = self.models[iteration].predict(self.test_X)\n      self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean()\n\n  def _plot_iter_MAE(self, ax, iteration):\n    ax.plot(list(self.test_mae_error.values())[:iteration+1], 'ko-')\n    ax.set_title('MAE on test set over iterations')\n    ax.set_xlim((-0.5, self.iterations+0.5))\n    ax.set_ylabel(\"MAE on test set\")\n    ax.set_xlabel(\"# Points Queried\")\n  \n  def _plot(self, ax, model):\n    # Plotting the pool\n    ax.scatter(self.pool_X[:,1], self.pool_Y, label='pool',s=1,color='r',alpha=0.4)\n    \n    # Plotting the test data\n    ax.scatter(self.test_X[:,1], self.test_Y, label='test data',s=1, color='b', alpha=0.4)\n    \n    # Combining test_pool\n    test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y)\n    \n    # Sorting test_pool\n    sorted_inds = np.argsort(test_pool_X[:,1])\n    test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n    \n    # Plotting test_pool with uncertainty\n    preds, var = model.predict(test_pool_X)\n    individual_var = var.diagonal()\n    ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n    ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                    , alpha=0.2, color='black', label='uncertainty')\n    \n    # plotting the train data\n    ax.scatter(model.x[:,1], model.y,s=10, color='k', marker='s', label='train data')\n    ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n    \n    # plotting MAE\n    preds, var = model.predict(self.test_X)\n    ax.set_title('MAE is '+str(np.mean(np.abs(self.test_Y - preds))))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    \n  def visualise_AL(self):\n    fig, ax = plt.subplots(1,2,figsize=(13,5))\n    def update(iteration):\n      ax[0].cla()\n      ax[1].cla()\n      self._plot(ax[0], self.models[iteration])\n      self._plot_iter_MAE(ax[1], iteration)\n      fig.tight_layout()\n\n    print('Initial model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[0].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n    print('\\nFinal model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[self.iterations].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250)\n    plt.close()\n\n    rc('animation', html='jshtml')\n    return anim\n\n\n\nVisualizing a different polynomial fit on the same dataset\nLets try to fit a degree 7 polynomial to the same data now.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nmodel = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7\n\nFinal model\nY = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations.\n\n\nActive learning for diabetes dataset from the Scikit-learn module\nLets run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. Well choose only weight feature, which seems to have more correlation with the target.\nWell try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, lets check the performance of Scikit-learn linear regression model.\n\nX, Y = datasets.load_diabetes(return_X_y=True)\nX = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression\n\n# Normalizing\nX = (X - X.min())/(X.max() - X.min())\nY = (Y - Y.min())/(Y.max() - Y.min())\n\nVisualizing the dataset.\n\nplt.scatter(X, Y)\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.show()\n\n\n\n\n\n\n\n\nLets fit the Scikit-learn linear regression model with 50% train-test split.\n\nfrom sklearn.linear_model import LinearRegression\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed)\n\n\nclf = LinearRegression()\n\n\nclf.fit(train_X, train_Y)\npred_Y = clf.predict(test_X)\n\nVisualizing the fit & MAE.\n\nplt.scatter(X, Y, label='data', s=5)\nplt.plot(test_X, pred_Y, label='model', color='r')\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.title('MAE is '+str(np.mean(np.abs(pred_Y - test_Y))))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow well fit the same data with our BLR model\n\nmodel = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = 0.41 + 0.16 X^1\n\nFinal model\nY = 0.13 + 0.86 X^1\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nInitially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. Its interesting to see that our initial train points tend to make a vertical fit, but the model doesnt get carried away by that and stabilizes the self with prior.\n\nprint('MAE for Scikit-learn Linear Regression is',np.mean(np.abs(pred_Y - test_Y)))\nprint('MAE for Bayesian Linear Regression is', model.test_mae_error[20])\n\nMAE for Scikit-learn Linear Regression is 0.15424985705353944\nMAE for Bayesian Linear Regression is 0.15738001811804758\n\n\nAt the end, results of sklearn linear regression and our active learning based BLR model are comparable even though weve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit."
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html",
    "href": "posts/Multiclass_GP_classification.html",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.gaussian_process import GaussianProcessClassifier, kernels\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#common-functions",
    "href": "posts/Multiclass_GP_classification.html#common-functions",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Common functions",
    "text": "Common functions\n\ndef get_kernel(ard):\n    return GPy.kern.RBF(2, ARD=ard)\n\ndef create_and_fit_model(model_class, X, y, ard, **kwargs):\n    model = model_class(X, y, get_kernel(ard), **kwargs)\n    model.optimize()\n    return model"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "href": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\n\nX, y = make_blobs(n_samples=200, centers=9, random_state=0)\n# X, y = make_moons(n_samples=200, noise=0.1, random_state=0)\ny = y.reshape(-1, 1)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');\n\n\n\n\n\n\n\n\n\ngrid1 = np.linspace(X.min(axis=0)[0]-1, X.max(axis=0)[0]+1, 50)\ngrid2 = np.linspace(X.min(axis=0)[1]-1, X.max(axis=0)[1]+1, 50)\nGrid1, Grid2 = np.meshgrid(grid1, grid2)\nX_grid = np.vstack([Grid1.ravel(), Grid2.ravel()]).T"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#train-test-split",
    "href": "posts/Multiclass_GP_classification.html#train-test-split",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Train-test split",
    "text": "Train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\ny_train_one_hot = encoder.transform(y_train)\ny_test_one_hot = encoder.transform(y_test)"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a regression problem",
    "text": "Treat it as a regression problem\nHere we can regress over the class labels as if they are discrete realizations of a continuous variable. We will round the predictions to the nearest integer to get the class labels.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.67      0.46      0.55        13\n           1       0.33      0.57      0.42         7\n           2       0.57      0.62      0.59        13\n           3       0.71      0.45      0.56        11\n           4       0.71      0.91      0.80        11\n           5       0.67      0.80      0.73        10\n           6       0.33      0.50      0.40         6\n           7       0.85      0.65      0.73        17\n           8       1.00      0.83      0.91        12\n\n    accuracy                           0.65       100\n   macro avg       0.65      0.64      0.63       100\nweighted avg       0.69      0.65      0.66       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.60      0.46      0.52        13\n           1       0.27      0.43      0.33         7\n           2       0.57      0.62      0.59        13\n           3       0.83      0.45      0.59        11\n           4       0.73      1.00      0.85        11\n           5       0.62      0.80      0.70        10\n           6       0.38      0.50      0.43         6\n           7       0.92      0.65      0.76        17\n           8       1.00      0.92      0.96        12\n\n    accuracy                           0.66       100\n   macro avg       0.66      0.65      0.64       100\nweighted avg       0.70      0.66      0.67       100\n\n\n\nWe get the raw predictions as below:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nmappables = []\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0]\n    mappable = ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    mappables.append(mappable)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\n# put a common colorbar for both mappables\nfig.colorbar(mappables[0], ax=ax, cax=fig.add_axes([0.92, 0.1, 0.02, 0.8]));\n\n\n\n\n\n\n\n\nNow, let us see the classification boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].round().astype(int)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a multi-output regression problem",
    "text": "Treat it as a multi-output regression problem\nIn this method, we learn a shared GP model among each class in one v/s rest setting. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\nWhat will happen if we ignore the points where the predictions are below 0.5?\n\nprint(\"ARD\")\npred_probas = ard_model.predict(X_test)[0]\npred_proba = pred_probas.max(axis=1)\n\nmask = pred_proba &gt; 0.5\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\npreds_masked = preds[mask]\nground_truth_masked = y_test[mask]\n\nprint(ground_truth_masked.shape, preds_masked.shape)\nprint(classification_report(ground_truth_masked, preds_masked))\n\nARD\n(98, 1) (98,)\n              precision    recall  f1-score   support\n\n           0       0.78      0.64      0.70        11\n           1       0.67      0.86      0.75         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.92        98\n   macro avg       0.91      0.92      0.91        98\nweighted avg       0.92      0.92      0.92        98\n\n\n\nLets visualize the uncertain points:\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='rainbow', edgecolor='k')\nplt.scatter(X_test[~mask, 0], X_test[~mask, 1], s=150, c=y_test[~mask], cmap='rainbow', edgecolor='k', label='uncertain points');\nplt.legend();\n\n\n\n\n\n\n\n\nWe see that some points close to the decision boundary are uncertain. We can now plot the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].argmax(axis=1) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n    \n# known = set()\n# for i in range(len(y_grid)):\n#     pred = y_grid[i]\n#     if pred in known:\n#         x = np.random.uniform()\n#         if x &lt; 0.2:\n#             known.remove(pred)\n#     else:\n#         known.add(pred)\n#         ax[0].text(X_grid[i, 0], X_grid[i, 1], str(pred), fontsize=10, color='k', ha='center', va='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\npd.Series(y_grid).value_counts()\n\n7    478\n5    396\n8    378\n4    324\n6    297\n1    265\n2    201\n3     98\n0     63\ndtype: int64"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a One v/s Rest classification problem",
    "text": "Treat it as a One v/s Rest classification problem\nHere we learn a separate model for each class. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=[0.01, 0.01])\n# non_ard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=0.1)\nard_model = GaussianProcessClassifier(kernel=ard_kernel, random_state=0)\n# non_ard_model = GaussianProcessClassifier(kernel=non_ard_kernel, random_state=0)\nard_model.fit(X_train, y_train.ravel())\n# non_ard_model.fit(X_train, y_train.ravel())\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)\nprint(classification_report(y_test, preds))\n# print(\"Non-ARD\")\n# preds = non_ard_model.predict(X_test)\n# print(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        13\n           1       0.00      0.00      0.00         7\n           2       1.00      0.08      0.14        13\n           3       0.00      0.00      0.00        11\n           4       0.00      0.00      0.00        11\n           5       0.00      0.00      0.00        10\n           6       0.00      0.00      0.00         6\n           7       0.00      0.00      0.00        17\n           8       0.12      1.00      0.22        12\n\n    accuracy                           0.13       100\n   macro avg       0.12      0.12      0.04       100\nweighted avg       0.14      0.13      0.04       100\n\n\n\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\nVisualizing the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model]:\n    y_grid = model.predict_proba(X_grid).argmax(axis=1)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={bool(i)}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "href": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Try random forest model",
    "text": "Try random forest model\n\nmodel = RandomForestClassifier(n_estimators=1000, random_state=0)\nmodel.fit(X_train, y_train.ravel())\n\nprint(\"Random Forest\")\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))\n\nRandom Forest\n              precision    recall  f1-score   support\n\n           0       0.78      0.54      0.64        13\n           1       0.60      0.86      0.71         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.91      1.00      0.95        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.92      0.90       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\ny_grid = model.predict(X_grid) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\nax.contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom time import time\n\n# Enable high precision\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n# To enable animation inside notebook\nplt.rc(\"animation\", html=\"jshtml\")"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Create dataset",
    "text": "Create dataset\n\nfeatures, labels = make_blobs(100, n_features=2, centers=2, random_state=0)\nplt.scatter(features[:, 0], features[:, 1], c=labels);\n\n\n\n\n\n\n\n\n\nprint(features.shape, features.dtype, labels.shape, labels.dtype)\n\n(100, 2) float64 (100,) int64"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing Newtons method (naive way)",
    "text": "Implementing Newtons method (naive way)\nWe will first try to implement Eq. 10.31 directly from PML book1:\n\\[\n\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t} \\mathbf{H}_{t}^{-1} \\boldsymbol{g}_{t}\n\\]\n\ndef get_logits(params, feature):  # for a single data-point\n  logits = jnp.sum(feature * params[\"w\"]) + params[\"b\"]\n  return logits\n\ndef naive_loss(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n\n  # Check if label is 1 or 0\n  is_one = (label == 1)\n  loss_if_one = lambda: -jnp.log(prob)  # loss if label is 1\n  loss_if_zero = lambda: -jnp.log(1 - prob)  # loss if labels is 0\n\n  # Use lax.cond to convert if..else.. in jittable format\n  loss = jax.lax.cond(is_one, loss_if_one, loss_if_zero)\n\n  return loss\n\ndef naive_loss_batch(params, features, labels):  # for a batch of data-points\n   losses = jax.vmap(naive_loss, in_axes=(None, 0, 0))(params, features, labels)\n   return jnp.mean(losses)\n\nWriting the train function\n\ndef naive_train_step(params, features, labels, learning_rate):\n  # Find gradient\n  loss_value, grads = jax.value_and_grad(naive_loss_batch)(params, features, labels)\n\n  # Find Hessian\n  hess = jax.hessian(naive_loss_batch)(params, features, labels)\n\n  # Adjust Hessian matrix nicely\n  hess_matrix = jnp.block([[hess[\"b\"][\"b\"], hess[\"b\"][\"w\"]],\n                           [hess[\"w\"][\"b\"], hess[\"w\"][\"w\"]]])\n  \n  # Adjust gradient vector nicely\n  grad_vector = jnp.r_[grads[\"b\"], grads[\"w\"]]\n\n  # Find H^-1g\n  h_inv_g = jnp.dot(jnp.linalg.inv(hess_matrix), grad_vector)\n\n  # Get back the structure\n  h_inv_g = {\"b\": h_inv_g[0], \"w\": h_inv_g[1:]}\n\n  # Apply the update\n  params = jax.tree_map(lambda p, g: p - learning_rate*g, params, h_inv_g)\n\n  return params, loss_value\n\n# First order method\n# vg = jax.value_and_grad(naive_loss_batch)\n# def train_step(params, features, labels, learning_rate):\n#   # Find gradient\n#   loss_value, grads = vg(params, features, labels)\n\n#   # Apply the update\n#   params = jax.tree_map(lambda p, g: p - learning_rate*g, params, grads)\n\n#   return params, loss_value\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3, ))\n# \"b\" should have shape (1,) for hessian trick with jnp.block to work\nparams = {\"w\": random_params[:2], \"b\": random_params[2].reshape(1,)}\nlearning_rate = 1.0\nepochs = 20\n\ntrain_step_jitted = jax.jit(naive_train_step)\n\nhistory = {\"loss\": [], \"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels, learning_rate)\n\ninit = time()\nfor _ in range(epochs):\n  history[\"params\"].append(params)\n  params, loss_value = train_step_jitted(params, features, labels, learning_rate)\n  history[\"loss\"].append(loss_value)\nprint(time() - init, \"seconds\")\nprint(params)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n0.0015490055084228516 seconds\n{'b': DeviceArray([13.22076694], dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}\n\n\nA helper function to animate the learning.\n\ndef animate(history):\n  fig, ax = plt.subplots(1, 2, figsize=(10,4))\n  def update(idx):\n    # Clear previous frame\n    ax[0].cla()\n    ax[1].cla()\n\n    # Plot data\n    params = history[\"params\"][idx]\n    losses = history[\"loss\"][:idx]\n    ax[0].scatter(features[:, 0], features[:, 1], c=labels)\n    \n    # Calculate and plot decision boundary\n    x0_min, x0_max = features[:, 0].min(), features[:, 0].max()\n    x1_min = -(params[\"b\"] + params[\"w\"][0] * x0_min)/params[\"w\"][1]\n    x1_max = -(params[\"b\"] + params[\"w\"][0] * x0_max)/params[\"w\"][1]\n\n    ax[0].plot([x0_min, x0_max], [x1_min, x1_max], label='decision boundary')\n\n    # Plot losses\n    ax[1].plot(losses, label=\"loss\")\n    ax[1].set_xlabel(\"Iterations\")\n\n    ax[0].legend()\n    ax[1].legend()\n\n  anim = FuncAnimation(fig, update, range(epochs))\n  plt.close()\n  return anim\n\n\nanimate(history)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing IRLS algorithm",
    "text": "Implementing IRLS algorithm\n\ndef get_s_and_z(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n  s = prob * (1 - prob)\n  z = logits + (label - prob)/s\n  return s, z\n\ndef irls_train_step(params, features, labels):\n  s, z = jax.vmap(get_s_and_z, in_axes=(None, 0, 0))(params, features, labels)\n  S = jnp.diag(s.flatten())  # convert into a diagonal matrix\n\n  # Add column with ones\n  X = jnp.c_[jnp.ones(len(z)), features]\n\n  # Get weights\n  weights = jnp.linalg.inv(X.T@S@X)@X.T@S@z.flatten()\n\n  # get correct format\n  params = {\"b\": weights[0], \"w\": weights[1:]}\n\n  return params\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3,))\nparams = {\"w\": random_params[:2], \"b\": random_params[2]}\nepochs = 20\n\ntrain_step_jitted = jax.jit(irls_train_step)\n\nirls_history = {\"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels)\n\ninit = time()\nfor _ in range(epochs):\n  irls_history[\"params\"].append(params)\n  params = train_step_jitted(params, features, labels)\nprint(time() - init, \"seconds\")\nprint(params)\n\n0.0016303062438964844 seconds\n{'b': DeviceArray(13.22076694, dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Comparison",
    "text": "Comparison\n\nnaive_params_b = list(map(lambda x: x[\"b\"], history[\"params\"]))\nirls_params_b = list(map(lambda x: x[\"b\"], irls_history[\"params\"]))\n\nnaive_params_w = list(map(lambda x: x[\"w\"], history[\"params\"]))\nirls_params_w = list(map(lambda x: x[\"w\"], irls_history[\"params\"]))\n\n\nplt.plot(naive_params_b, \"o-\", label=\"Naive\")\nplt.plot(irls_params_b, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Bias\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(naive_params_w, \"o-\", label=\"Naive\")\nplt.plot(irls_params_w, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Weights\")\nplt.legend();"
  },
  {
    "objectID": "posts/fundamentals_across_domains.html",
    "href": "posts/fundamentals_across_domains.html",
    "title": "Fundamentals across ML domains",
    "section": "",
    "text": "Similarities among ML domains\n\n\n\nNN\nTransformer\nCNN\n\n\n\n\n-\nMulti-head\nMulti-channel\n\n\n-\nSkip-connection\nResNet\n\n\n\n\n\nProgress of Natural Language Processing\n\n\n\n\n\n\n\n\n\nModel\nMain Disadvantage\nSolved by\nHow?\n\n\n\n\nNN\nCant handle dynamic length input\nRNN\nRNN can handle dynamic length input\n\n\nRNN\nVanishing Gradient Problem\nLSTM\nLSTM can handle vanishing gradient problem\n\n\nLSTM\nNon parallelizable\nTransformer\nTransformer can parallelize the computation\n\n\nTrasformer\nlosses sequentiality\nTransformer\nPositional Encoding"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html",
    "href": "posts/pruning_vs_uncertainty.html",
    "title": "Pruning vs Uncertainty",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\n# import pruning library\nimport torch.nn.utils.prune as prune\n\n# import torchvision\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n\ntry:\n    from laplace import Laplace\nexcept ModuleNotFoundError:\n    %pip install laplace-torch\n    from laplace import Laplace\n\n&lt;frozen importlib._bootstrap&gt;:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "href": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "title": "Pruning vs Uncertainty",
    "section": "Train a model on MNIST",
    "text": "Train a model on MNIST\n\n# Define data transformations\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n        # convert dtype to float32\n        # transforms.Lambda(lambda x: x.to(torch.float32)),\n    ]\n)\n\n\n# Load MNIST dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, transform=transform, download=True\n)\nprint(\"Train size\", len(train_dataset))\n\ntrain_dataset = TensorDataset(\n    train_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    train_dataset.targets.to(device),\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\", train=False, transform=transform, download=True\n)\nprint(\"Test size\", len(test_dataset))\ntest_dataset = TensorDataset(\n    test_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    test_dataset.targets.to(device),\n)\n\nUsing cuda device\nTrain size 60000\nTest size 10000\n\n\n\ntrain_dataset[0][0].dtype, train_dataset[0][1].dtype\n\n(torch.float32, torch.int64)\n\n\n\n# Define data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# Load pre-trained ResNet model\nresnet = torchvision.models.resnet18(pretrained=True)\nprint(\"Loaded pre-trained ResNet18 model\")\nprint(resnet.fc.in_features)\n\n# Modify the last fully connected layer to match MNIST's number of classes (10)\nnum_classes = 10\nresnet.fc = nn.Sequential(\n    nn.Linear(resnet.fc.in_features, resnet.fc.in_features),\n    nn.GELU(),\n    nn.Linear(resnet.fc.in_features, num_classes),\n)\n\n# Freeze all layers except the last fully connected layer\nfor name, param in resnet.named_parameters():\n    param.requires_grad = False\nresnet.fc.requires_grad_(True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n\n# Training loop\nnum_epochs = 50\nprint(f\"Training on device {device}\")\nresnet.to(device)\n\nprint(\"Training ResNet18 model\")\nfor epoch in range(num_epochs):\n    resnet.train()\n    epoch_loss = 0.0\n    for images, labels in tqdm(train_loader):\n        optimizer.zero_grad()\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n\n    # Evaluation\n    resnet.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        predicted_list = []\n        for images, labels in test_loader:\n            outputs = resnet(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nLoaded pre-trained ResNet18 model\n512\nTraining on device cuda\nTraining ResNet18 model\n\n\n100%|| 938/938 [00:03&lt;00:00, 242.75it/s]\n\n\nEpoch [1/50] Loss: 1.0877\nAccuracy on the test set: 75.42%\n\n\n100%|| 938/938 [00:03&lt;00:00, 262.53it/s]\n\n\nEpoch [2/50] Loss: 0.8051\nAccuracy on the test set: 76.74%\n\n\n100%|| 938/938 [00:03&lt;00:00, 270.43it/s]\n\n\nEpoch [3/50] Loss: 0.7578\nAccuracy on the test set: 78.27%\n\n\n100%|| 938/938 [00:03&lt;00:00, 265.38it/s]\n\n\nEpoch [4/50] Loss: 0.7290\nAccuracy on the test set: 78.71%\n\n\n100%|| 938/938 [00:03&lt;00:00, 265.51it/s]\n\n\nEpoch [5/50] Loss: 0.7083\nAccuracy on the test set: 79.62%\n\n\n100%|| 938/938 [00:03&lt;00:00, 266.62it/s]\n\n\nEpoch [6/50] Loss: 0.6761\nAccuracy on the test set: 79.82%\n\n\n100%|| 938/938 [00:03&lt;00:00, 268.49it/s]\n\n\nEpoch [7/50] Loss: 0.6627\nAccuracy on the test set: 80.47%\n\n\n100%|| 938/938 [00:03&lt;00:00, 266.33it/s]\n\n\nEpoch [8/50] Loss: 0.6423\nAccuracy on the test set: 80.24%\n\n\n100%|| 938/938 [00:03&lt;00:00, 268.52it/s]\n\n\nEpoch [9/50] Loss: 0.6257\nAccuracy on the test set: 81.11%\n\n\n100%|| 938/938 [00:03&lt;00:00, 269.38it/s]\n\n\nEpoch [10/50] Loss: 0.6131\nAccuracy on the test set: 81.42%\n\n\n100%|| 938/938 [00:03&lt;00:00, 264.77it/s]\n\n\nEpoch [11/50] Loss: 0.5911\nAccuracy on the test set: 82.02%\n\n\n100%|| 938/938 [00:03&lt;00:00, 266.07it/s]\n\n\nEpoch [12/50] Loss: 0.5765\nAccuracy on the test set: 82.32%\n\n\n100%|| 938/938 [00:03&lt;00:00, 262.19it/s]\n\n\nEpoch [13/50] Loss: 0.5611\nAccuracy on the test set: 82.30%\n\n\n100%|| 938/938 [00:04&lt;00:00, 214.62it/s]\n\n\nEpoch [14/50] Loss: 0.5466\nAccuracy on the test set: 82.49%\n\n\n100%|| 938/938 [00:04&lt;00:00, 219.31it/s]\n\n\nEpoch [15/50] Loss: 0.5358\nAccuracy on the test set: 82.81%\n\n\n100%|| 938/938 [00:04&lt;00:00, 226.53it/s]\n\n\nEpoch [16/50] Loss: 0.5266\nAccuracy on the test set: 83.30%\n\n\n100%|| 938/938 [00:05&lt;00:00, 171.25it/s]\n\n\nEpoch [17/50] Loss: 0.5137\nAccuracy on the test set: 83.37%\n\n\n100%|| 938/938 [00:03&lt;00:00, 278.59it/s]\n\n\nEpoch [18/50] Loss: 0.5051\nAccuracy on the test set: 83.17%\n\n\n100%|| 938/938 [00:03&lt;00:00, 248.82it/s]\n\n\nEpoch [19/50] Loss: 0.4969\nAccuracy on the test set: 83.46%\n\n\n100%|| 938/938 [00:05&lt;00:00, 175.56it/s]\n\n\nEpoch [20/50] Loss: 0.4811\nAccuracy on the test set: 83.76%\n\n\n100%|| 938/938 [00:03&lt;00:00, 277.18it/s]\n\n\nEpoch [21/50] Loss: 0.4714\nAccuracy on the test set: 83.57%\n\n\n100%|| 938/938 [00:03&lt;00:00, 273.71it/s]\n\n\nEpoch [22/50] Loss: 0.4624\nAccuracy on the test set: 84.25%\n\n\n100%|| 938/938 [00:03&lt;00:00, 242.18it/s]\n\n\nEpoch [23/50] Loss: 0.4553\nAccuracy on the test set: 84.27%\n\n\n100%|| 938/938 [00:03&lt;00:00, 279.42it/s]\n\n\nEpoch [24/50] Loss: 0.4506\nAccuracy on the test set: 84.62%\n\n\n100%|| 938/938 [00:03&lt;00:00, 269.21it/s]\n\n\nEpoch [25/50] Loss: 0.4394\nAccuracy on the test set: 83.97%\n\n\n100%|| 938/938 [00:04&lt;00:00, 227.36it/s]\n\n\nEpoch [26/50] Loss: 0.4346\nAccuracy on the test set: 84.16%\n\n\n100%|| 938/938 [00:04&lt;00:00, 222.91it/s]\n\n\nEpoch [27/50] Loss: 0.4271\nAccuracy on the test set: 84.38%\n\n\n100%|| 938/938 [00:04&lt;00:00, 223.68it/s]\n\n\nEpoch [28/50] Loss: 0.4193\nAccuracy on the test set: 84.84%\n\n\n100%|| 938/938 [00:03&lt;00:00, 261.50it/s]\n\n\nEpoch [29/50] Loss: 0.4148\nAccuracy on the test set: 85.05%\n\n\n100%|| 938/938 [00:03&lt;00:00, 246.52it/s]\n\n\nEpoch [30/50] Loss: 0.4040\nAccuracy on the test set: 84.49%\n\n\n100%|| 938/938 [00:03&lt;00:00, 281.60it/s]\n\n\nEpoch [31/50] Loss: 0.3990\nAccuracy on the test set: 84.59%\n\n\n100%|| 938/938 [00:03&lt;00:00, 278.41it/s]\n\n\nEpoch [32/50] Loss: 0.4016\nAccuracy on the test set: 84.92%\n\n\n100%|| 938/938 [00:03&lt;00:00, 275.60it/s]\n\n\nEpoch [33/50] Loss: 0.3979\nAccuracy on the test set: 85.01%\n\n\n100%|| 938/938 [00:03&lt;00:00, 250.04it/s]\n\n\nEpoch [34/50] Loss: 0.3844\nAccuracy on the test set: 84.82%\n\n\n100%|| 938/938 [00:03&lt;00:00, 280.53it/s]\n\n\nEpoch [35/50] Loss: 0.3789\nAccuracy on the test set: 85.49%\n\n\n100%|| 938/938 [00:03&lt;00:00, 279.26it/s]\n\n\nEpoch [36/50] Loss: 0.3760\nAccuracy on the test set: 85.26%\n\n\n100%|| 938/938 [00:04&lt;00:00, 207.71it/s]\n\n\nEpoch [37/50] Loss: 0.3733\nAccuracy on the test set: 85.36%\n\n\n100%|| 938/938 [00:03&lt;00:00, 265.92it/s]\n\n\nEpoch [38/50] Loss: 0.3655\nAccuracy on the test set: 84.98%\n\n\n100%|| 938/938 [00:03&lt;00:00, 279.79it/s]\n\n\nEpoch [39/50] Loss: 0.3627\nAccuracy on the test set: 85.19%\n\n\n100%|| 938/938 [00:03&lt;00:00, 276.73it/s]\n\n\nEpoch [40/50] Loss: 0.3517\nAccuracy on the test set: 84.78%\n\n\n100%|| 938/938 [00:03&lt;00:00, 278.32it/s]\n\n\nEpoch [41/50] Loss: 0.3526\nAccuracy on the test set: 85.43%\n\n\n100%|| 938/938 [00:03&lt;00:00, 243.70it/s]\n\n\nEpoch [42/50] Loss: 0.3523\nAccuracy on the test set: 85.55%\n\n\n100%|| 938/938 [00:03&lt;00:00, 240.48it/s]\n\n\nEpoch [43/50] Loss: 0.3457\nAccuracy on the test set: 85.02%\n\n\n100%|| 938/938 [00:03&lt;00:00, 274.70it/s]\n\n\nEpoch [44/50] Loss: 0.3447\nAccuracy on the test set: 85.20%\n\n\n100%|| 938/938 [00:03&lt;00:00, 276.08it/s]\n\n\nEpoch [45/50] Loss: 0.3411\nAccuracy on the test set: 85.47%\n\n\n100%|| 938/938 [00:04&lt;00:00, 215.18it/s]\n\n\nEpoch [46/50] Loss: 0.3312\nAccuracy on the test set: 85.55%\n\n\n100%|| 938/938 [00:03&lt;00:00, 244.20it/s]\n\n\nEpoch [47/50] Loss: 0.3290\nAccuracy on the test set: 85.52%\n\n\n100%|| 938/938 [00:03&lt;00:00, 267.56it/s]\n\n\nEpoch [48/50] Loss: 0.3277\nAccuracy on the test set: 85.35%\n\n\n100%|| 938/938 [00:03&lt;00:00, 267.91it/s]\n\n\nEpoch [49/50] Loss: 0.3241\nAccuracy on the test set: 85.80%\n\n\n100%|| 938/938 [00:03&lt;00:00, 266.04it/s]\n\n\nEpoch [50/50] Loss: 0.3217\nAccuracy on the test set: 84.93%\n\n\n\n# Evaluation\nresnet.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    predicted_list = []\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        softmax_outputs = nn.Softmax(dim=1)(outputs)\n        predicted_list.append(softmax_outputs.data.cpu().numpy())\n\nall_predicted = np.concatenate(predicted_list, axis=0)\nprint(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\nAccuracy on the test set: 84.93%"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#check-calibration",
    "href": "posts/pruning_vs_uncertainty.html#check-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Check calibration",
    "text": "Check calibration\n\ntest_dataset.tensors[1].cpu().numpy().shape, all_predicted.shape\n\n((10000,), (10000, 10))\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = all_predicted[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    # ece_score = compute_ece(predicted_probabilities, true_labels, num_bins=10)\n    # print(f\"Expected Calibration Error (ECE): {ece_score:.4f}\")\n\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\n# Compute expected calibration error (ECE)\nece = compute_ece(all_predicted, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.021885250088572478\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(\n        test_dataset.tensors[1].cpu().numpy(), all_predicted.argmax(axis=1)\n    )\n)\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90       980\n           1       0.91      0.97      0.94      1135\n           2       0.75      0.72      0.73      1032\n           3       0.77      0.74      0.75      1010\n           4       0.82      0.88      0.85       982\n           5       0.68      0.70      0.69       892\n           6       0.85      0.84      0.85       958\n           7       0.80      0.79      0.79      1028\n           8       0.76      0.75      0.75       974\n           9       0.81      0.74      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000\n\n\n\n\ndef compute_ece(predicted_probs, true_labels, num_bins=10):\n    # Ensure predicted_probs is a NumPy array\n    predicted_probs = np.array(predicted_probs)\n    true_labels = np.array(true_labels)\n\n    # Calculate predicted class labels\n    predicted_labels = np.argmax(predicted_probs, axis=1)\n\n    # Calculate confidence scores (maximum predicted probability)\n    confidence_scores = np.max(predicted_probs, axis=1)\n\n    # Create bins for confidence scores\n    bin_edges = np.linspace(0, 1, num_bins + 1)\n\n    ece = 0.0\n    total_samples = len(true_labels)\n\n    for bin_idx in range(num_bins):\n        # Find examples whose confidence scores fall into the current bin\n        bin_mask = (confidence_scores &gt;= bin_edges[bin_idx]) & (\n            confidence_scores &lt; bin_edges[bin_idx + 1]\n        )\n\n        if np.any(bin_mask):\n            # Calculate the accuracy of predictions in this bin\n            bin_accuracy = np.mean(predicted_labels[bin_mask] == true_labels[bin_mask])\n\n            # Calculate the fraction of examples in this bin\n            bin_fraction = np.sum(bin_mask) / total_samples\n\n            # Calculate the calibration error in this bin\n            bin_error = np.abs(bin_accuracy - np.mean(confidence_scores[bin_mask]))\n\n            # Weighted contribution to ECE\n            ece += bin_fraction * bin_error\n\n    return ece"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "href": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Does MC-dropout help with calibration?",
    "text": "Does MC-dropout help with calibration?"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "href": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "title": "Pruning vs Uncertainty",
    "section": "Last layer only",
    "text": "Last layer only\n\nclass MCDropout(nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n        self.dropout = nn.Dropout(p=self.p)\n\n    def forward(self, x):\n        self.train()\n        return self.dropout(x)\n\n\nresnet_with_dropout = torchvision.models.resnet18(pretrained=True)\nresnet_with_dropout.fc = nn.Sequential(\n    nn.Linear(\n        resnet_with_dropout.fc.in_features, resnet_with_dropout.fc.in_features // 2\n    ),\n    nn.GELU(),\n    MCDropout(p=0.33),\n    nn.Linear(resnet_with_dropout.fc.in_features // 2, num_classes),\n)\n\nresnet_with_dropout.load_state_dict(resnet.state_dict())\n\nresnet_with_dropout.to(device)\n\nmc_samples = 1000\n\noutputs = []\nfor _ in tqdm(range(mc_samples)):\n    output = resnet_with_dropout(test_dataset.tensors[0])\n    softmax_output = nn.Softmax(dim=1)(output)\n    outputs.append(softmax_output.data.cpu().numpy())\n\n100%|| 1000/1000 [00:18&lt;00:00, 55.50it/s]\n\n\n\nmc_mean = np.mean(outputs, axis=0)\nmc_std = np.std(outputs, axis=0)\nmc_mean.shape\n\n(10000, 10)\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = mc_mean[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\nece = compute_ece(mc_mean, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.04250686831623317\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(test_dataset.tensors[1].cpu().numpy(), mc_mean.argmax(axis=1))\n)\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.92      0.90       980\n           1       0.91      0.98      0.94      1135\n           2       0.73      0.71      0.72      1032\n           3       0.78      0.72      0.75      1010\n           4       0.83      0.87      0.85       982\n           5       0.68      0.71      0.69       892\n           6       0.82      0.88      0.85       958\n           7       0.80      0.78      0.79      1028\n           8       0.77      0.74      0.75       974\n           9       0.82      0.72      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000"
  },
  {
    "objectID": "posts/gcloud.html",
    "href": "posts/gcloud.html",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I dont know if this is required or not. This command should trigger installation of gcloud Beta Commands automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#initial-setup",
    "href": "posts/gcloud.html#initial-setup",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I dont know if this is required or not. This command should trigger installation of gcloud Beta Commands automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms",
    "href": "posts/gcloud.html#working-with-tpu-vms",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs",
    "text": "Working with TPU VMs\nThere are two different terms here: TPU VMs and TPU nodes. TPU nodes can be connected externally via another VM. TPU VMs are stand-alone systems with TPUs, RAM and CPU (96 core Intel 2 GHz processor and 335 GB RAM). We may be charged via GCP for the VM (CPUs and RAM). (I will update this info once I know for sure):\n\n\nTo create a TPU VM in preferred zone via CLI (be careful about the --zone to avoid charges, check the first email received from TRC team to see what kind of TPUs are free in different zones. if --zone is not passed, VM will be created in the default zone that we set initially. This command triggered installation of gcloud Alpha Commands):\n\ngcloud alpha compute tpus tpu-vm create vm-1 --accelerator-type v2-8 --version tpu-vm-tf-2.8.0 --zone us-central1-f\n\nTo get the list of TPU nodes/VMs:\n\ngcloud compute tpus list\n\nTo delete a TPU node/VM:\n\ngcloud compute tpus delete vm-1\n\nTo connect with a vm via ssh (this automatically creates ssh key pair and places in default ssh config location):\n\ngcloud alpha compute tpus tpu-vm ssh vm-1\n\nFollow this guide to create and attach a persistent disk with the TPU VM"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "href": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs via VS-code",
    "text": "Working with TPU VMs via VS-code\n\nInstall the following extension on VS-code: \nUse the following button to connect to a remote machine (use Connect to Host button): \nManually update the default ssh config file (in my case, C:\\.ssh) to add a VM in VS-code (you can use VS-code command palette to figure out the config file for you and edit it. Please see the screeshot below).\n\n\n\nNote that ssh public-private key pair with name google_compute_engine is automatically generated when you connect with the VM for the first time with gcloud alpha compute tpus tpu-vm ssh command. The VM config for me looks like this:\n\nHost Cloud-TPU-Node-2\n  HostName &lt;External-IP-of-your-TPU-VM&gt;\n  User zeelp\n  Port 22\n  IdentityFile C:\\Users\\zeelp\\.ssh\\google_compute_engine"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "title": "Get a list of contributors from a repo",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "title": "Get a list of contributors from a repo",
    "section": "Config",
    "text": "Config\n\nowner = \"probml\"\nrepo = \"pyprobml\""
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Get all contributors to a repo",
    "text": "Get all contributors to a repo\n\ncontributors = pd.read_json(f\"https://api.github.com/repos/{owner}/{repo}/contributors?per_page=100\")\ncontributors = contributors.set_index(\"login\")\nprint(f\"Number of contributors: {len(contributors.index.unique())}\")\ncontributors.head(2)\n\nNumber of contributors: 47\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nnode_id\navatar_url\ngravatar_id\nurl\nhtml_url\nfollowers_url\nfollowing_url\ngists_url\nstarred_url\nsubscriptions_url\norganizations_url\nrepos_url\nevents_url\nreceived_events_url\ntype\nsite_admin\ncontributions\n\n\nlogin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmurphyk\n4632336\nMDQ6VXNlcjQ2MzIzMzY=\nhttps://avatars.githubusercontent.com/u/463233...\n\nhttps://api.github.com/users/murphyk\nhttps://github.com/murphyk\nhttps://api.github.com/users/murphyk/followers\nhttps://api.github.com/users/murphyk/following...\nhttps://api.github.com/users/murphyk/gists{/gi...\nhttps://api.github.com/users/murphyk/starred{/...\nhttps://api.github.com/users/murphyk/subscript...\nhttps://api.github.com/users/murphyk/orgs\nhttps://api.github.com/users/murphyk/repos\nhttps://api.github.com/users/murphyk/events{/p...\nhttps://api.github.com/users/murphyk/received_...\nUser\nFalse\n1777\n\n\nNeoanarika\n5188337\nMDQ6VXNlcjUxODgzMzc=\nhttps://avatars.githubusercontent.com/u/518833...\n\nhttps://api.github.com/users/Neoanarika\nhttps://github.com/Neoanarika\nhttps://api.github.com/users/Neoanarika/followers\nhttps://api.github.com/users/Neoanarika/follow...\nhttps://api.github.com/users/Neoanarika/gists{...\nhttps://api.github.com/users/Neoanarika/starre...\nhttps://api.github.com/users/Neoanarika/subscr...\nhttps://api.github.com/users/Neoanarika/orgs\nhttps://api.github.com/users/Neoanarika/repos\nhttps://api.github.com/users/Neoanarika/events...\nhttps://api.github.com/users/Neoanarika/receiv...\nUser\nFalse\n184"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Fetch all PRs from a repo",
    "text": "Fetch all PRs from a repo\n\npage_range = range(1, 6)\nget_pr_df = lambda page: pd.read_json(f\"https://api.github.com/repos/probml/pyprobml/pulls?state=all&per_page=100&page={page}\")\npull_requests = pd.concat(map(get_pr_df, page_range))\nprint(f\"Number of PRs: {len(pull_requests)}\")\npull_requests.head(2)\n\nNumber of PRs: 497\n\n\n\n  \n    \n      \n\n\n\n\n\n\nurl\nid\nnode_id\nhtml_url\ndiff_url\npatch_url\nissue_url\nnumber\nstate\nlocked\n...\nreview_comments_url\nreview_comment_url\ncomments_url\nstatuses_url\nhead\nbase\n_links\nauthor_association\nauto_merge\nactive_lock_reason\n\n\n\n\n0\nhttps://api.github.com/repos/probml/pyprobml/p...\n938329819\nPR_kwDOA-3vB8437cbb\nhttps://github.com/probml/pyprobml/pull/841\nhttps://github.com/probml/pyprobml/pull/841.diff\nhttps://github.com/probml/pyprobml/pull/841.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n841\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:posrprocessing', 'ref': ...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n1\nhttps://api.github.com/repos/probml/pyprobml/p...\n938317389\nPR_kwDOA-3vB8437ZZN\nhttps://github.com/probml/pyprobml/pull/840\nhttps://github.com/probml/pyprobml/pull/840.diff\nhttps://github.com/probml/pyprobml/pull/840.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n840\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:master', 'ref': 'master'...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n\n\n2 rows  36 columns"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "title": "Get a list of contributors from a repo",
    "section": "Get a list of contributors sorted by count of PRs",
    "text": "Get a list of contributors sorted by count of PRs\n\npull_requests['login'] = pull_requests['user'].apply(lambda x: x[\"login\"])\nsorted_by_pr_count = pull_requests.groupby(\"login\").agg({'url': len}).sort_values(by='url', ascending=False)\nsorted_by_pr_count.rename(columns={'url': 'Number of PRs'}, inplace=True)\nsorted_by_pr_count.head(5)\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of PRs\n\n\nlogin\n\n\n\n\n\nDrishttii\n79\n\n\ngerdm\n55\n\n\nkaralleyna\n43\n\n\nalways-newbie161\n29\n\n\nkarm-patel\n29"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "title": "Get a list of contributors from a repo",
    "section": "Create a dashboard",
    "text": "Create a dashboard\n\ndef get_href_user(user):\n  username, profile_link = user.split(\"|\")\n  return f\"[{username}]({profile_link})\"\n\ndashboard = pd.DataFrame(index=sorted_by_pr_count.index)\ndashboard[\"Avatar\"] = contributors.avatar_url.apply(lambda url: f'&lt;img width=\"25\" alt=\"image\" src=\"{url}\"&gt;')\ndashboard[\"Contributor\"] = (contributors.index +\"|\"+ contributors['html_url']).apply(get_href_user)\ndashboard[\"Number of PRs\"] = sorted_by_pr_count[\"Number of PRs\"]\nprint(dashboard.dropna().T.to_markdown())\n\n|               | Drishttii                                                                               | gerdm                                                                                  | karalleyna                                                                              | always-newbie161                                                                        | karm-patel                                                                              | Duane321                                                                                | Nirzu97                                                                                 | patel-zeel                                                                              | animesh-007                                                                             | ashishpapanai                                                                           | shivaditya-meduri                                                                       | Neoanarika                                                                             | andrewnc                                                                               | nappaillav                                                                              | Abdelrahman350                                                                          | mjsML                                                                                  | jdf22                                                                                  | kzymgch                                                                                 | nalzok                                                                                  | nitish1295                                                                              | Garvit9000c                                                                             | AnkitaKumariJain14                                                                      | rohit-khoiwal-30                                                                        | shobro                                                                                  | raymondyeh07                                                                           | khanshehjad                                                                             | alenm10                                                                                 | firatoncel                                                                             | AnandShegde                                                                             | Aadesh-1404                                                                             | nealmcb                                                                               | nipunbatra                                                                           | petercerno                                                                             | posgnu                                                                                  | mvervuurt                                                                              | hieuza                                                                                 | Prahitha                                                                                | TripleTop                                                                               | UmarJ                                                                                   | Vishal987595                                                                            | a-fakhri                                                                                | adamnemecek                                                                           | galv                                                                                   | jlh2018                                                                                 | krasserm                                                                              | yuanx749                                                                                |\n|:--------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|\n| Avatar        | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/35187749?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4108759?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/36455180?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/66471669?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59387624?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/19956442?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/28842790?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59758528?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/53366877?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/52123364?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/77324692?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5188337?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7716402?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/43855961?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47902062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7131192?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1637094?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/10054419?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/13443062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/21181046?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68856476?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/62535006?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/87682045?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/54628243?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5696982?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/31896767?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/42214173?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/9141211?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/79975787?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68186100?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/119472?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/60985?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1649209?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/30136201?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/6399881?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1021144?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/44160152?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/48208522?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/34779641?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/97757583?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/65111198?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/182415?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4767568?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/40842099?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/202907?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47032563?v=4\"&gt; |\n| Contributor   | [Drishttii](https://github.com/Drishttii)                                               | [gerdm](https://github.com/gerdm)                                                      | [karalleyna](https://github.com/karalleyna)                                             | [always-newbie161](https://github.com/always-newbie161)                                 | [karm-patel](https://github.com/karm-patel)                                             | [Duane321](https://github.com/Duane321)                                                 | [Nirzu97](https://github.com/Nirzu97)                                                   | [patel-zeel](https://github.com/patel-zeel)                                             | [animesh-007](https://github.com/animesh-007)                                           | [ashishpapanai](https://github.com/ashishpapanai)                                       | [shivaditya-meduri](https://github.com/shivaditya-meduri)                               | [Neoanarika](https://github.com/Neoanarika)                                            | [andrewnc](https://github.com/andrewnc)                                                | [nappaillav](https://github.com/nappaillav)                                             | [Abdelrahman350](https://github.com/Abdelrahman350)                                     | [mjsML](https://github.com/mjsML)                                                      | [jdf22](https://github.com/jdf22)                                                      | [kzymgch](https://github.com/kzymgch)                                                   | [nalzok](https://github.com/nalzok)                                                     | [nitish1295](https://github.com/nitish1295)                                             | [Garvit9000c](https://github.com/Garvit9000c)                                           | [AnkitaKumariJain14](https://github.com/AnkitaKumariJain14)                             | [rohit-khoiwal-30](https://github.com/rohit-khoiwal-30)                                 | [shobro](https://github.com/shobro)                                                     | [raymondyeh07](https://github.com/raymondyeh07)                                        | [khanshehjad](https://github.com/khanshehjad)                                           | [alenm10](https://github.com/alenm10)                                                   | [firatoncel](https://github.com/firatoncel)                                            | [AnandShegde](https://github.com/AnandShegde)                                           | [Aadesh-1404](https://github.com/Aadesh-1404)                                           | [nealmcb](https://github.com/nealmcb)                                                 | [nipunbatra](https://github.com/nipunbatra)                                          | [petercerno](https://github.com/petercerno)                                            | [posgnu](https://github.com/posgnu)                                                     | [mvervuurt](https://github.com/mvervuurt)                                              | [hieuza](https://github.com/hieuza)                                                    | [Prahitha](https://github.com/Prahitha)                                                 | [TripleTop](https://github.com/TripleTop)                                               | [UmarJ](https://github.com/UmarJ)                                                       | [Vishal987595](https://github.com/Vishal987595)                                         | [a-fakhri](https://github.com/a-fakhri)                                                 | [adamnemecek](https://github.com/adamnemecek)                                         | [galv](https://github.com/galv)                                                        | [jlh2018](https://github.com/jlh2018)                                                   | [krasserm](https://github.com/krasserm)                                               | [yuanx749](https://github.com/yuanx749)                                                 |\n| Number of PRs | 79                                                                                      | 55                                                                                     | 43                                                                                      | 29                                                                                      | 29                                                                                      | 29                                                                                      | 25                                                                                      | 23                                                                                      | 18                                                                                      | 17                                                                                      | 16                                                                                      | 10                                                                                     | 10                                                                                     | 10                                                                                      | 8                                                                                       | 7                                                                                      | 7                                                                                      | 6                                                                                       | 6                                                                                       | 5                                                                                       | 4                                                                                       | 4                                                                                       | 3                                                                                       | 3                                                                                       | 2                                                                                      | 2                                                                                       | 2                                                                                       | 2                                                                                      | 2                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                    | 1                                                                                      | 1                                                                                       | 1                                                                                      | 1                                                                                      | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                      | 1                                                                                       | 1                                                                                     | 1                                                                                       |"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "title": "Uncertainty in Deep Learning",
    "section": "",
    "text": "import torch"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "title": "Uncertainty in Deep Learning",
    "section": "1 - Introduction",
    "text": "1 - Introduction\n\nAn online deep learning book from Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n1.1 - Deep Learning\nWe define a single layer network as the following:\n\nclass SingleLayerNetwork(torch.nn.Module):\n    def __init__(self, Q, D, K):\n        \"\"\"\n        Q: number of features\n        D: number of outputs\n        K: number of hidden features\n        \"\"\"\n        super().__init__()\n        self.input = torch.nn.Linear(Q, K) # Transforms Q features into K hidden features\n        self.output = torch.nn.Linear(K, D) # Transforms K hidden features to D output features\n        self.non_lin_transform = torch.nn.ReLU() # A non-linear transformation\n        \n    def forward(self, X):\n        \"\"\"\n        X: input (N x Q)\n        \"\"\"\n        self.linear_transformed_X = self.input(X)  # (N, Q) -&gt; (N, K)\n        self.non_lin_transformed_X = self.non_lin_transform(linear_transformed_X)  # (N, K) -&gt; (N, K)\n        output = self.output(self.non_lin_transformed_X)  # (N, K) -&gt; (N, D)\n        return output\n\n\nQ = 10 # Number of features\nN = 100 # Number of samples\nD = 15 # Number of outputs\nK = 32 # Number of hidden features\n\nX = torch.rand(N, Q) # Input\nY = torch.rand(N, D) # Output\n\n\nmodel = SingleLayerNetwork(Q=Q, D=D, K=K)\nmodel\n\nSingleLayerNetwork(\n  (input): Linear(in_features=10, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=15, bias=True)\n  (non_lin_transform): ReLU()\n)\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value.shape)\n\ninput.weight torch.Size([32, 10])\ninput.bias torch.Size([32])\noutput.weight torch.Size([15, 32])\noutput.bias torch.Size([15])\n\n\nReLU is does not contain any parameters here so it is merely a function.\n\n\n1.2 Model Uncertainty\nIn which cases we want our model to be uncertain?\n\nWhen it encounters a out-of-the-distribution data\nWhen training data is noisy (irreducible/aleatoric uncertainty)\nWhen we have multiple predictors (model/epistemic uncertainty)"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html",
    "href": "posts/2022-06-10-jaxoptimizers.html",
    "title": "JAX Optimizers",
    "section": "",
    "text": "%%capture\n%pip install -U jax\nimport jax\nimport jax.numpy as jnp\ntry:\n  import jaxopt\nexcept ModuleNotFoundError:\n  %pip install -qq jaxopt\n  import jaxopt\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install -qq optax\n  import optax\n\nimport tensorflow_probability.substrates.jax as tfp"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "href": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "title": "JAX Optimizers",
    "section": "Loss function",
    "text": "Loss function\n\ndef loss_fun(x, a):\n  return (((x['param1'] - a) + (x['param2'] - (a+1)))**2).sum()"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "href": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "title": "JAX Optimizers",
    "section": "Initial parameters",
    "text": "Initial parameters\n\nN = 3\ninit_params = lambda: {'param1': jnp.zeros(N), 'param2': jnp.ones(N)}\na = 2.0"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "href": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "title": "JAX Optimizers",
    "section": "Optimizers",
    "text": "Optimizers\n\nJaxOpt ScipyMinimize\n\n%%time\nsolver = jaxopt.ScipyMinimize('L-BFGS-B', fun=loss_fun)\nans = solver.run(init_params(), a)\nprint(ans)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nOptStep(params={'param1': DeviceArray([1.9999999, 1.9999999, 1.9999999], dtype=float32), 'param2': DeviceArray([3., 3., 3.], dtype=float32)}, state=ScipyMinimizeInfo(fun_val=DeviceArray(4.2632564e-14, dtype=float32), success=True, status=0, iter_num=2))\nCPU times: user 78.3 ms, sys: 18.5 ms, total: 96.8 ms\nWall time: 95.8 ms\n\n\n\nPros\n\nTwo lines of code will do it all.\n\n\n\nCons\n\nIt only returns the final parameters and final loss. No option to retrive in-between loss values.\n\n\n\n\nOptax\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nvalue_and_grad_fun = jax.jit(jax.value_and_grad(loss_fun, argnums=0))\nparams = init_params()\nstate = optimizer.init(params)\n\nfor _ in range(100):\n  loss_value, gradients = value_and_grad_fun(params, a)\n  updates, state = optimizer.update(gradients, state)\n  params = optax.apply_updates(params, updates)\n\nprint(params)\n\n{'param1': DeviceArray([2.0084236, 2.0084236, 2.0084236], dtype=float32), 'param2': DeviceArray([3.0084238, 3.0084238, 3.0084238], dtype=float32)}\nCPU times: user 3.09 s, sys: 63.4 ms, total: 3.16 s\nWall time: 4.2 s\n\n\n\nPros:\n\nFull control in users hand. We can save intermediate loss values.\n\n\n\nCons:\n\nIts code is verbose, similar to PyTorch optimizers.\n\n\n\n\nJaxopt OptaxSolver\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nsolver = jaxopt.OptaxSolver(loss_fun, optimizer, maxiter=100)\nans = solver.run(init_params(), a)\nprint(ans)\n\nOptStep(params={'param1': DeviceArray([2.008423, 2.008423, 2.008423], dtype=float32), 'param2': DeviceArray([3.008423, 3.008423, 3.008423], dtype=float32)}, state=OptaxState(iter_num=DeviceArray(100, dtype=int32, weak_type=True), value=DeviceArray(0.00113989, dtype=float32), error=DeviceArray(0.09549397, dtype=float32), internal_state=(ScaleByAdamState(count=DeviceArray(100, dtype=int32), mu={'param1': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32), 'param2': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32)}, nu={'param1': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32), 'param2': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32)}), EmptyState()), aux=None))\nCPU times: user 719 ms, sys: 13.4 ms, total: 732 ms\nWall time: 1.09 s\n\n\n\nPros:\n\nLess lines of code.\nApplies lax.scan internally to make it fast [reference].\n\n\n\nCons:\n\nNot able to get in-between state/loss values\n\n\n\n\ntfp math minimize\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nparams, losses = tfp.math.minimize_stateless(loss_fun, (init_params(), a), num_steps=1000, optimizer=optimizer)\nprint(params)\nprint(losses[:5])\n\n({'param1': DeviceArray([1.0000008, 1.0000008, 1.0000008], dtype=float32), 'param2': DeviceArray([1.9999989, 1.9999989, 1.9999989], dtype=float32)}, DeviceArray(0.9999999, dtype=float32))\n[48.       38.88006  30.751791 23.626852 17.507807]\nCPU times: user 880 ms, sys: 15.2 ms, total: 895 ms\nWall time: 1.53 s\n\n\n\nPros:\n\nOne line of code to optimize the function and return in-between losses.\n\n\n\nCons:\n\nBy default, it optimizes all arguments passed to the loss function. In above example, we can not control if a should be optimized or not. I have raised an issue here for this problem."
  },
  {
    "objectID": "posts/torch-tips.html",
    "href": "posts/torch-tips.html",
    "title": "PyTorch Tips",
    "section": "",
    "text": "Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code.\n\nAll the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy.\n.cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable.\nDo not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead.\nNeed to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f\"name_{zero}\"). They can be accessed with model.name_0.\nHave something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer.\nLet .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as:\n\nmodule.to(deivce) sends all parameters and buffers of model/submodules to the device.\n\nmodule.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively.\nLet .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions.\ntorch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code.\nLink the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are:\n\nsetting module.train() or module.eval() puts all submodules in train mode or eval mode respectively.\nAll submodules parameters can be accesses directly from the parent module with module.parameters().\n\nCreating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters."
  },
  {
    "objectID": "posts/2021-09-27-constraints.html",
    "href": "posts/2021-09-27-constraints.html",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('font', **{'size':18})"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpy",
    "href": "posts/2021-09-27-constraints.html#gpy",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPy",
    "text": "GPy\n\nfrom paramz.transformations import Logexp\n\n\ngpy_trans = Logexp()\n\n\nx = torch.arange(-1000,10000).to(torch.float)\nplt.plot(x, gpy_trans.f(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpytorch",
    "href": "posts/2021-09-27-constraints.html#gpytorch",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPyTorch",
    "text": "GPyTorch\n\nfrom gpytorch.constraints import Positive\n\n\ngpytorch_trans = Positive()\n\n\nplt.plot(x, gpytorch_trans.transform(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpflow",
    "href": "posts/2021-09-27-constraints.html#gpflow",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPFlow",
    "text": "GPFlow\n\nfrom gpflow.utilities.bijectors import positive\n\n\ngpflow_trans = positive()\n\n\nplt.plot(x, gpflow_trans(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');\n\n\n\n\n\n\n\n\n\nnp.allclose(gpy_trans.f(x), gpytorch_trans.transform(x))\n\nTrue\n\n\n\nnp.allclose(gpy_trans.f(x), gpflow_trans(x))\n\nTrue"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "",
    "text": "Roboflow Notebooks"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#setup",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#setup",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Setup",
    "text": "Setup\n\nConfigure your API keys\nTo fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n\nOpen your HuggingFace Settings page. Click Access Tokens then New Token to generate new token.\nGo to your Roboflow Settings page. Click Copy. This will place your private key in the clipboard.\nIn Colab, go to the left pane and click on Secrets ().\n\nStore HuggingFace Access Token under the name HF_TOKEN.\nStore Roboflow API Key under the name ROBOFLOW_API_KEY.\n\n\n\n\nSelect the runtime\nLets make sure that we have access to GPU. We can use nvidia-smi command to do that. In case of any problems navigate to Edit -&gt; Notebook settings -&gt; Hardware accelerator, set it to L4 GPU, and then click Save.\n\n!nvidia-smi\n\nTue Feb 11 14:44:13 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n| N/A   38C    P0             86W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n| N/A   35C    P0             63W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n| N/A   35C    P0             63W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n| N/A   34C    P0             62W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\n\n\n\nDownload example data\nNOTE: Feel free to replace our example image with your own photo.\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!ls -lh\n\ntotal 337M\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023 dog.jpeg\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023 dog.jpeg.1\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023 dog.jpeg.2\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023 dog.jpeg.3\n-rw-rw-r-- 1 patel_zeel patel_zeel 4.7M Jan 20 13:40 example.tiff\n-rw-rw-r-- 1 patel_zeel patel_zeel 4.3M Feb 11 14:44 how-to-finetune-florence-2-on-detection-dataset.ipynb\ndrwxrwxr-x 7 patel_zeel patel_zeel 4.0K Feb 11 14:33 model_checkpoints\ndrwxrwxr-x 5 patel_zeel patel_zeel 4.0K Feb 11 00:54 poker-cards-4\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 20 15:43 runs\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.5M Jan 22 10:51 scratchpad.ipynb\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 20 15:17 trench_width\ndrwxrwxr-x 7 patel_zeel patel_zeel 4.0K Jan 20 15:43 wandb\n-rw-rw-r-- 1 patel_zeel patel_zeel  41M Jan 20 15:36 yolo11m-obb.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel 5.4M Jan 20 15:17 yolo11n.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel  50M Jan 20 15:43 yolov8m.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel  53M Jan 20 15:43 yolov8m-seg.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel  23M Jan 20 15:14 yolov8s-obb.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel  22M Jan 18 23:20 yolov8s.pt\n-rw-rw-r-- 1 patel_zeel patel_zeel 134M Jan 20 15:30 yolov8x-obb.pt\n\n\n\nEXAMPLE_IMAGE_PATH = \"dog.jpeg\""
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#download-and-configure-the-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#download-and-configure-the-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Download and configure the model",
    "text": "Download and configure the model\nLets download the model checkpoint and configure it so that you can fine-tune it later on.\n\n!pip install -q transformers flash_attn timm einops peft\n!pip install -q roboflow git+https://github.com/roboflow/supervision.git\n\n\n# @title Imports\n\nimport io\nimport os\nimport re\nimport json\nimport torch\nimport html\nimport base64\nimport itertools\n\nimport numpy as np\nimport supervision as sv\n\n# from google.colab import userdata\nfrom IPython.core.display import display, HTML\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom roboflow import Roboflow\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n\n\nLoad the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library. Note that you need to pass trust_remote_code as True since this model is not a standard transformers model.\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\n# REVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From v4.50 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#run-inference-with-pre-trained-florence-2-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#run-inference-with-pre-trained-florence-2-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Run inference with pre-trained Florence-2 model",
    "text": "Run inference with pre-trained Florence-2 model\n\n# @title Example object detection inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\n\n# @title Example image captioning inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;DETAILED_CAPTION&gt;\"\ntext = \"&lt;DETAILED_CAPTION&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\nresponse\n\n{'&lt;DETAILED_CAPTION&gt;': 'In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.'}\n\n\n\n# @title Example caption to phrase grounding inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\ntext = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt; Vehicle\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom dataset",
    "text": "Fine-tune Florence-2 on custom dataset\n\nDownload dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 5 {dataset.location}/train/annotations.jsonl\n\n{\"image\": \"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\", \"prefix\": \"&lt;OD&gt;\", \"suffix\": \":!pg!dmvct&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;21!pg!dmvct&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;kbdl!!pg!dmvct&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;rvffo!pg!dmvct&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;ljoh!pg!dmvct&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n{\"image\": \"IMG_20220316_171515_jpg.rf.e3b1932bb375b3b3912027647586daa8.jpg\", \"prefix\": \"&lt;OD&gt;\", \"suffix\": \"6!pg!dmvct&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;7!pg!dmvct&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;8!pg!dmvct&lt;loc_363&gt;&lt;loc_484&gt;&lt;loc_552&gt;&lt;loc_905&gt;9!pg!dmvct&lt;loc_535&gt;&lt;loc_449&gt;&lt;loc_757&gt;&lt;loc_971&gt;\"}\n{\"image\": \"IMG_20220316_165139_jpg.rf.e30257ec169a2bfdfecb693211d37250.jpg\", \"prefix\": \"&lt;OD&gt;\", \"suffix\": \":!pg!ejbnpoet&lt;loc_596&gt;&lt;loc_535&gt;&lt;loc_859&gt;&lt;loc_982&gt;kbdl!pg!ejbnpoet&lt;loc_211&gt;&lt;loc_546&gt;&lt;loc_411&gt;&lt;loc_880&gt;rvffo!pg!ejbnpoet&lt;loc_430&gt;&lt;loc_34&gt;&lt;loc_692&gt;&lt;loc_518&gt;ljoh!pg!ejbnpoet&lt;loc_223&gt;&lt;loc_96&gt;&lt;loc_451&gt;&lt;loc_523&gt;21!pg!ejbnpoet&lt;loc_387&gt;&lt;loc_542&gt;&lt;loc_604&gt;&lt;loc_925&gt;\"}\n{\"image\": \"IMG_20220316_143407_jpg.rf.e1eb3be3efc6c3bbede436cfb5489e7c.jpg\", \"prefix\": \"&lt;OD&gt;\", \"suffix\": \"bdf!pg!ifbsut&lt;loc_345&gt;&lt;loc_315&gt;&lt;loc_582&gt;&lt;loc_721&gt;3!pg!ifbsut&lt;loc_709&gt;&lt;loc_115&gt;&lt;loc_888&gt;&lt;loc_509&gt;4!pg!ifbsut&lt;loc_529&gt;&lt;loc_228&gt;&lt;loc_735&gt;&lt;loc_613&gt;5!pg!ifbsut&lt;loc_98&gt;&lt;loc_421&gt;&lt;loc_415&gt;&lt;loc_845&gt;\"}\n{\"image\": \"IMG_20220316_165139_jpg.rf.e4c229a9128494d17992cbe88af575df.jpg\", \"prefix\": \"&lt;OD&gt;\", \"suffix\": \":!pg!ejbnpoet&lt;loc_141&gt;&lt;loc_18&gt;&lt;loc_404&gt;&lt;loc_465&gt;kbdl!pg!ejbnpoet&lt;loc_589&gt;&lt;loc_120&gt;&lt;loc_789&gt;&lt;loc_454&gt;rvffo!pg!ejbnpoet&lt;loc_308&gt;&lt;loc_482&gt;&lt;loc_570&gt;&lt;loc_966&gt;ljoh!pg!ejbnpoet&lt;loc_549&gt;&lt;loc_477&gt;&lt;loc_777&gt;&lt;loc_904&gt;21!pg!ejbnpoet&lt;loc_396&gt;&lt;loc_75&gt;&lt;loc_613&gt;&lt;loc_458&gt;\"}\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n# read jsonl file\ndef read_jsonl(file_path: str) -&gt; Generator[Dict[str, Any], None, None]:\n    with open(file_path, \"r\") as f:\n        for line in f:\n            yield json.loads(line)\n\nlines = []\nsplit = \"test\"     \nfor line in read_jsonl(dataset.location + f\"/{split}/annotations.jsonl.backup\"):\n    # print(line)\n    # edit = True\n    # copied_line = list(line['suffix'])\n    # for i in range(len(copied_line)):\n    #     if copied_line[i] == \"&lt;\":\n    #         edit = False\n    #     elif copied_line[i] == \"&gt;\":\n    #         edit = True\n    #     else:\n    #         if edit:\n    #             copied_line[i] = chr(ord(copied_line[i]) + 1)\n    # copied_line = \"\".join(copied_line)\n    # line['suffix'] = copied_line\n    \n    line['suffix'] = line['suffix'].replace(\"club\", \"dog\").replace(\"diamond\", \"cat\").replace(\"heart\", \"bird\").replace(\"spade\", \"fish\")\n    print(line)\n    lines.append(line)\n\nwith open(dataset.location + f\"/{split}/annotations.jsonl\", \"w\") as f:\n    for line in lines:\n        f.write(json.dumps(line) + \"\\n\")\n\n{'image': 'IMG_20220316_140255_jpg.rf.0d10768652a0f20bea317e96632d3448.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of fishs&lt;loc_146&gt;&lt;loc_488&gt;&lt;loc_541&gt;&lt;loc_917&gt;6 of fishs&lt;loc_259&gt;&lt;loc_221&gt;&lt;loc_604&gt;&lt;loc_673&gt;7 of fishs&lt;loc_470&gt;&lt;loc_206&gt;&lt;loc_761&gt;&lt;loc_741&gt;8 of fishs&lt;loc_599&gt;&lt;loc_201&gt;&lt;loc_949&gt;&lt;loc_732&gt;'}\n{'image': 'IMG_20220316_140400_jpg.rf.3f21e54bd916b05218202fbf109d8a5f.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of fishs&lt;loc_127&gt;&lt;loc_280&gt;&lt;loc_378&gt;&lt;loc_597&gt;7 of fishs&lt;loc_244&gt;&lt;loc_415&gt;&lt;loc_539&gt;&lt;loc_904&gt;8 of fishs&lt;loc_414&gt;&lt;loc_98&gt;&lt;loc_792&gt;&lt;loc_513&gt;6 of fishs&lt;loc_450&gt;&lt;loc_469&gt;&lt;loc_847&gt;&lt;loc_999&gt;'}\n{'image': 'IMG_20220316_144511_jpg.rf.40ee049c8f854c558e2ca20f90be3787.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_49&gt;&lt;loc_508&gt;&lt;loc_221&gt;&lt;loc_891&gt;6 of birds&lt;loc_205&gt;&lt;loc_391&gt;&lt;loc_384&gt;&lt;loc_832&gt;7 of birds&lt;loc_387&gt;&lt;loc_281&gt;&lt;loc_621&gt;&lt;loc_777&gt;8 of birds&lt;loc_615&gt;&lt;loc_100&gt;&lt;loc_971&gt;&lt;loc_677&gt;'}\n{'image': 'IMG_20220316_144657_jpg.rf.0b7bb9ab4b594b83097af9c4c1ea46c3.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_643&gt;&lt;loc_238&gt;&lt;loc_870&gt;&lt;loc_672&gt;6 of birds&lt;loc_184&gt;&lt;loc_34&gt;&lt;loc_555&gt;&lt;loc_468&gt;7 of birds&lt;loc_137&gt;&lt;loc_531&gt;&lt;loc_527&gt;&lt;loc_989&gt;8 of birds&lt;loc_517&gt;&lt;loc_521&gt;&lt;loc_753&gt;&lt;loc_982&gt;'}\n{'image': 'IMG_20220316_172435_jpg.rf.854fe0c471b03fe3a3894a3d2cbe00d0.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of dogs&lt;loc_76&gt;&lt;loc_146&gt;&lt;loc_420&gt;&lt;loc_702&gt;10 of dogs&lt;loc_353&gt;&lt;loc_111&gt;&lt;loc_586&gt;&lt;loc_570&gt;jack  of dogs&lt;loc_540&gt;&lt;loc_59&gt;&lt;loc_724&gt;&lt;loc_460&gt;queen of dogs&lt;loc_336&gt;&lt;loc_550&gt;&lt;loc_586&gt;&lt;loc_998&gt;king of dogs&lt;loc_525&gt;&lt;loc_456&gt;&lt;loc_717&gt;&lt;loc_852&gt;'}\n{'image': 'IMG_20220316_161455_jpg.rf.635ccd0ee9f7dd762009f539d6b998e9.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of birds&lt;loc_22&gt;&lt;loc_495&gt;&lt;loc_367&gt;&lt;loc_878&gt;10 of birds&lt;loc_423&gt;&lt;loc_33&gt;&lt;loc_653&gt;&lt;loc_428&gt;jack of birds&lt;loc_170&gt;&lt;loc_136&gt;&lt;loc_445&gt;&lt;loc_563&gt;queen of birds&lt;loc_390&gt;&lt;loc_470&gt;&lt;loc_687&gt;&lt;loc_791&gt;king of birds&lt;loc_666&gt;&lt;loc_207&gt;&lt;loc_938&gt;&lt;loc_643&gt;'}\n{'image': 'IMG_20220316_161313_jpg.rf.12498ecbc8985a8ff65bd2033d8f622a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of birds&lt;loc_165&gt;&lt;loc_191&gt;&lt;loc_374&gt;&lt;loc_532&gt;10 of birds&lt;loc_361&gt;&lt;loc_115&gt;&lt;loc_584&gt;&lt;loc_471&gt;jack of birds&lt;loc_527&gt;&lt;loc_212&gt;&lt;loc_912&gt;&lt;loc_646&gt;queen of birds&lt;loc_375&gt;&lt;loc_616&gt;&lt;loc_692&gt;&lt;loc_988&gt;king of birds&lt;loc_216&gt;&lt;loc_451&gt;&lt;loc_463&gt;&lt;loc_846&gt;'}\n{'image': 'IMG_20220316_142643_jpg.rf.13b0a2a65a9d4b17580b39ed19de5bba.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of birds&lt;loc_180&gt;&lt;loc_323&gt;&lt;loc_416&gt;&lt;loc_725&gt;2 of birds&lt;loc_374&gt;&lt;loc_577&gt;&lt;loc_671&gt;&lt;loc_946&gt;3 of birds&lt;loc_300&gt;&lt;loc_16&gt;&lt;loc_572&gt;&lt;loc_485&gt;4 of birds&lt;loc_535&gt;&lt;loc_214&gt;&lt;loc_973&gt;&lt;loc_769&gt;'}\n{'image': 'IMG_20220316_144650_jpg.rf.34b246c8ee646cbb5979a35d68c58901.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_606&gt;&lt;loc_265&gt;&lt;loc_919&gt;&lt;loc_746&gt;6 of birds&lt;loc_258&gt;&lt;loc_2&gt;&lt;loc_689&gt;&lt;loc_439&gt;7 of birds&lt;loc_1&gt;&lt;loc_295&gt;&lt;loc_330&gt;&lt;loc_841&gt;8 of birds&lt;loc_271&gt;&lt;loc_352&gt;&lt;loc_646&gt;&lt;loc_875&gt;'}\n{'image': 'IMG_20220316_161217_jpg.rf.1755e5fefb14ca6df49690604289bb46.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of birds&lt;loc_20&gt;&lt;loc_195&gt;&lt;loc_275&gt;&lt;loc_491&gt;10 of birds&lt;loc_268&gt;&lt;loc_188&gt;&lt;loc_490&gt;&lt;loc_495&gt;jack of birds&lt;loc_491&gt;&lt;loc_152&gt;&lt;loc_735&gt;&lt;loc_452&gt;queen of birds&lt;loc_713&gt;&lt;loc_145&gt;&lt;loc_991&gt;&lt;loc_441&gt;king of birds&lt;loc_298&gt;&lt;loc_480&gt;&lt;loc_607&gt;&lt;loc_935&gt;'}\n{'image': 'IMG_20220316_144711_jpg.rf.19a6cc13f83f27b45e10a6056bb25721.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_738&gt;&lt;loc_233&gt;&lt;loc_997&gt;&lt;loc_663&gt;6 of birds&lt;loc_20&gt;&lt;loc_202&gt;&lt;loc_258&gt;&lt;loc_630&gt;7 of birds&lt;loc_476&gt;&lt;loc_220&gt;&lt;loc_734&gt;&lt;loc_649&gt;8 of birds&lt;loc_264&gt;&lt;loc_205&gt;&lt;loc_491&gt;&lt;loc_630&gt;'}\n{'image': 'IMG_20220316_164257_jpg.rf.0c3abfccf0f7f147946c89251b87f598.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of cats&lt;loc_49&gt;&lt;loc_312&gt;&lt;loc_290&gt;&lt;loc_621&gt;6 of cats&lt;loc_681&gt;&lt;loc_245&gt;&lt;loc_920&gt;&lt;loc_554&gt;7 of cats&lt;loc_242&gt;&lt;loc_202&gt;&lt;loc_492&gt;&lt;loc_559&gt;8 of cats&lt;loc_468&gt;&lt;loc_261&gt;&lt;loc_687&gt;&lt;loc_592&gt;'}\n{'image': 'IMG_20220316_170755_jpg.rf.71804c21e1c7681d5b656427449e0a2a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of dogs&lt;loc_47&gt;&lt;loc_95&gt;&lt;loc_391&gt;&lt;loc_754&gt;2 of dogs&lt;loc_577&gt;&lt;loc_238&gt;&lt;loc_737&gt;&lt;loc_674&gt;3 of dogs&lt;loc_723&gt;&lt;loc_348&gt;&lt;loc_861&gt;&lt;loc_708&gt;4 of dogs&lt;loc_366&gt;&lt;loc_189&gt;&lt;loc_602&gt;&lt;loc_711&gt;'}\n{'image': 'IMG_20220316_141442_jpg.rf.41913768e5d56c57566ee3b45391470d.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of fishs&lt;loc_509&gt;&lt;loc_233&gt;&lt;loc_768&gt;&lt;loc_603&gt;10 of fishs&lt;loc_202&gt;&lt;loc_86&gt;&lt;loc_530&gt;&lt;loc_413&gt;jack of fishs&lt;loc_597&gt;&lt;loc_476&gt;&lt;loc_836&gt;&lt;loc_895&gt;queen of fishs&lt;loc_370&gt;&lt;loc_441&gt;&lt;loc_635&gt;&lt;loc_916&gt;king of fishs&lt;loc_145&gt;&lt;loc_312&gt;&lt;loc_482&gt;&lt;loc_782&gt;'}\n{'image': 'IMG_20220316_140723_jpg.rf.826bc2b212c4001c11115ef28f58d142.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '6 of fishs&lt;loc_23&gt;&lt;loc_341&gt;&lt;loc_329&gt;&lt;loc_713&gt;7 of fishs&lt;loc_228&gt;&lt;loc_259&gt;&lt;loc_442&gt;&lt;loc_527&gt;5 of fishs&lt;loc_438&gt;&lt;loc_254&gt;&lt;loc_709&gt;&lt;loc_555&gt;8 of fishs&lt;loc_631&gt;&lt;loc_284&gt;&lt;loc_999&gt;&lt;loc_652&gt;'}\n{'image': 'IMG_20220316_134701_jpg.rf.27aa29de9d6012ae05c64b156f7c07b8.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of fishs&lt;loc_197&gt;&lt;loc_114&gt;&lt;loc_489&gt;&lt;loc_480&gt;2 of fishs&lt;loc_270&gt;&lt;loc_581&gt;&lt;loc_538&gt;&lt;loc_999&gt;3 of fishs&lt;loc_470&gt;&lt;loc_398&gt;&lt;loc_718&gt;&lt;loc_782&gt;4 of fishs&lt;loc_580&gt;&lt;loc_213&gt;&lt;loc_824&gt;&lt;loc_543&gt;'}\n{'image': 'IMG_20220316_163749_jpg.rf.3c41ec25d2a8390c760eb7fcf2d2466b.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of cats&lt;loc_0&gt;&lt;loc_276&gt;&lt;loc_289&gt;&lt;loc_582&gt;6 of cats&lt;loc_256&gt;&lt;loc_286&gt;&lt;loc_497&gt;&lt;loc_566&gt;7 of cats&lt;loc_485&gt;&lt;loc_282&gt;&lt;loc_751&gt;&lt;loc_559&gt;8 of cats&lt;loc_698&gt;&lt;loc_254&gt;&lt;loc_995&gt;&lt;loc_548&gt;'}\n{'image': 'IMG_20220316_165206_jpg.rf.1e20afbea0b132989e944ffbd800f348.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of cats&lt;loc_57&gt;&lt;loc_220&gt;&lt;loc_401&gt;&lt;loc_691&gt;jack of cats&lt;loc_420&gt;&lt;loc_30&gt;&lt;loc_713&gt;&lt;loc_468&gt;queen of cats&lt;loc_266&gt;&lt;loc_490&gt;&lt;loc_563&gt;&lt;loc_971&gt;king of cats&lt;loc_397&gt;&lt;loc_422&gt;&lt;loc_797&gt;&lt;loc_948&gt;10 of cats&lt;loc_217&gt;&lt;loc_74&gt;&lt;loc_500&gt;&lt;loc_557&gt;'}\n{'image': 'IMG_20220316_144500_jpg.rf.14c3cb5eadd6c3d449916f52d9f9381e.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_43&gt;&lt;loc_27&gt;&lt;loc_396&gt;&lt;loc_608&gt;6 of birds&lt;loc_348&gt;&lt;loc_191&gt;&lt;loc_610&gt;&lt;loc_702&gt;7 of birds&lt;loc_577&gt;&lt;loc_326&gt;&lt;loc_790&gt;&lt;loc_785&gt;8 of birds&lt;loc_795&gt;&lt;loc_452&gt;&lt;loc_973&gt;&lt;loc_883&gt;'}\n{'image': 'IMG_20220316_172537_jpg.rf.8fe076e115c111f04732f8e7f778e51d.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of dogs&lt;loc_635&gt;&lt;loc_348&gt;&lt;loc_998&gt;&lt;loc_738&gt;10 of dogs&lt;loc_1&gt;&lt;loc_445&gt;&lt;loc_320&gt;&lt;loc_852&gt;jack  of dogs&lt;loc_267&gt;&lt;loc_177&gt;&lt;loc_467&gt;&lt;loc_409&gt;queen of dogs&lt;loc_473&gt;&lt;loc_141&gt;&lt;loc_722&gt;&lt;loc_358&gt;king of dogs&lt;loc_340&gt;&lt;loc_380&gt;&lt;loc_634&gt;&lt;loc_788&gt;'}\n{'image': 'IMG_20220316_144652_jpg.rf.316d0dad84d3d696fa8fa3caf53fb700.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_451&gt;&lt;loc_505&gt;&lt;loc_820&gt;&lt;loc_980&gt;6 of birds&lt;loc_519&gt;&lt;loc_101&gt;&lt;loc_773&gt;&lt;loc_495&gt;7 of birds&lt;loc_177&gt;&lt;loc_68&gt;&lt;loc_452&gt;&lt;loc_427&gt;8 of birds&lt;loc_291&gt;&lt;loc_301&gt;&lt;loc_559&gt;&lt;loc_730&gt;'}\n{'image': 'IMG_20220316_173229_jpg.rf.1b93cc9a66ca2d0a28820a7dae74222a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of dogs&lt;loc_514&gt;&lt;loc_426&gt;&lt;loc_769&gt;&lt;loc_895&gt;10 of dogs&lt;loc_650&gt;&lt;loc_168&gt;&lt;loc_978&gt;&lt;loc_680&gt;jack  of dogs&lt;loc_151&gt;&lt;loc_368&gt;&lt;loc_432&gt;&lt;loc_865&gt;queen of dogs&lt;loc_365&gt;&lt;loc_82&gt;&lt;loc_657&gt;&lt;loc_546&gt;king of dogs&lt;loc_67&gt;&lt;loc_272&gt;&lt;loc_327&gt;&lt;loc_786&gt;'}\n{'image': 'IMG_20220316_171916_jpg.rf.7c8b85f64455e815acc30b5111422bf2.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of dogs&lt;loc_402&gt;&lt;loc_294&gt;&lt;loc_713&gt;&lt;loc_806&gt;6 of dogs&lt;loc_175&gt;&lt;loc_587&gt;&lt;loc_334&gt;&lt;loc_982&gt;7 of dogs&lt;loc_509&gt;&lt;loc_5&gt;&lt;loc_893&gt;&lt;loc_552&gt;8 of dogs&lt;loc_273&gt;&lt;loc_414&gt;&lt;loc_491&gt;&lt;loc_867&gt;'}\n{'image': 'IMG_20220316_165737_jpg.rf.11f2e19b001c300ee820e093b135409a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of cats&lt;loc_141&gt;&lt;loc_350&gt;&lt;loc_441&gt;&lt;loc_848&gt;jack of cats&lt;loc_257&gt;&lt;loc_107&gt;&lt;loc_513&gt;&lt;loc_429&gt;queen of cats&lt;loc_354&gt;&lt;loc_405&gt;&lt;loc_688&gt;&lt;loc_926&gt;king of cats&lt;loc_201&gt;&lt;loc_223&gt;&lt;loc_477&gt;&lt;loc_597&gt;10 of cats&lt;loc_519&gt;&lt;loc_114&gt;&lt;loc_783&gt;&lt;loc_467&gt;'}\n{'image': 'IMG_20220316_170523_jpg.rf.a106d534bf3279ec40e771452a142c1e.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of dogs&lt;loc_1&gt;&lt;loc_295&gt;&lt;loc_290&gt;&lt;loc_616&gt;2 of dogs&lt;loc_250&gt;&lt;loc_296&gt;&lt;loc_509&gt;&lt;loc_604&gt;3 of dogs&lt;loc_493&gt;&lt;loc_275&gt;&lt;loc_765&gt;&lt;loc_588&gt;4 of dogs&lt;loc_700&gt;&lt;loc_261&gt;&lt;loc_999&gt;&lt;loc_577&gt;'}\n{'image': 'IMG_20220316_171810_jpg.rf.be96dac3cbdda6973a920a0a787b33f2.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of dogs&lt;loc_546&gt;&lt;loc_83&gt;&lt;loc_699&gt;&lt;loc_530&gt;6 of dogs&lt;loc_301&gt;&lt;loc_438&gt;&lt;loc_610&gt;&lt;loc_906&gt;7 of dogs&lt;loc_363&gt;&lt;loc_66&gt;&lt;loc_638&gt;&lt;loc_515&gt;8 of dogs&lt;loc_238&gt;&lt;loc_163&gt;&lt;loc_604&gt;&lt;loc_547&gt;'}\n{'image': 'IMG_20220316_171557_jpg.rf.9c6f913ba56e578ef4c31cc3faffcf7d.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of dogs&lt;loc_289&gt;&lt;loc_130&gt;&lt;loc_556&gt;&lt;loc_277&gt;6 of dogs&lt;loc_80&gt;&lt;loc_391&gt;&lt;loc_465&gt;&lt;loc_838&gt;7 of dogs&lt;loc_255&gt;&lt;loc_213&gt;&lt;loc_659&gt;&lt;loc_579&gt;8 of dogs&lt;loc_438&gt;&lt;loc_258&gt;&lt;loc_680&gt;&lt;loc_603&gt;'}\n{'image': 'IMG_20220316_140335_jpg.rf.c3310d8f13f66189440daf8419b1ad9c.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '7 of fishs&lt;loc_85&gt;&lt;loc_452&gt;&lt;loc_295&gt;&lt;loc_678&gt;6 of fishs&lt;loc_291&gt;&lt;loc_413&gt;&lt;loc_488&gt;&lt;loc_648&gt;8 of fishs&lt;loc_491&gt;&lt;loc_405&gt;&lt;loc_696&gt;&lt;loc_626&gt;5 of fishs&lt;loc_708&gt;&lt;loc_418&gt;&lt;loc_961&gt;&lt;loc_652&gt;'}\n{'image': 'IMG_20220316_135021_jpg.rf.d038afdef1a927103dae268ff392888f.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of fishs&lt;loc_514&gt;&lt;loc_234&gt;&lt;loc_814&gt;&lt;loc_523&gt;2 of fishs&lt;loc_326&gt;&lt;loc_506&gt;&lt;loc_596&gt;&lt;loc_916&gt;3 of fishs&lt;loc_601&gt;&lt;loc_515&gt;&lt;loc_921&gt;&lt;loc_937&gt;4 of fishs&lt;loc_219&gt;&lt;loc_22&gt;&lt;loc_528&gt;&lt;loc_347&gt;'}\n{'image': 'IMG_20220316_163800_jpg.rf.e1ad0b7b78e379d5f9f0bec787a9050b.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of cats&lt;loc_26&gt;&lt;loc_100&gt;&lt;loc_376&gt;&lt;loc_752&gt;6 of cats&lt;loc_377&gt;&lt;loc_223&gt;&lt;loc_612&gt;&lt;loc_725&gt;7 of cats&lt;loc_591&gt;&lt;loc_290&gt;&lt;loc_779&gt;&lt;loc_712&gt;8 of cats&lt;loc_760&gt;&lt;loc_336&gt;&lt;loc_916&gt;&lt;loc_702&gt;'}\n{'image': 'IMG_20220316_165711_jpg.rf.c64b84b61322947a5a8c7545e214278c.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of cats&lt;loc_437&gt;&lt;loc_86&gt;&lt;loc_609&gt;&lt;loc_363&gt;10 of cats&lt;loc_217&gt;&lt;loc_123&gt;&lt;loc_434&gt;&lt;loc_458&gt;jack of cats&lt;loc_69&gt;&lt;loc_504&gt;&lt;loc_395&gt;&lt;loc_995&gt;queen of cats&lt;loc_366&gt;&lt;loc_530&gt;&lt;loc_610&gt;&lt;loc_951&gt;king of cats&lt;loc_553&gt;&lt;loc_425&gt;&lt;loc_745&gt;&lt;loc_759&gt;'}\n{'image': 'IMG_20220316_141917_jpg.rf.b3075e4161fe5fa2285d75bb2da3bc7a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of fishs&lt;loc_682&gt;&lt;loc_295&gt;&lt;loc_951&gt;&lt;loc_709&gt;10 of fishs&lt;loc_63&gt;&lt;loc_502&gt;&lt;loc_293&gt;&lt;loc_807&gt;jack of fishs&lt;loc_462&gt;&lt;loc_345&gt;&lt;loc_727&gt;&lt;loc_729&gt;queen of fishs&lt;loc_256&gt;&lt;loc_486&gt;&lt;loc_497&gt;&lt;loc_819&gt;king of fishs&lt;loc_113&gt;&lt;loc_191&gt;&lt;loc_411&gt;&lt;loc_488&gt;'}\n{'image': 'IMG_20220316_144507_jpg.rf.ab9b95792bbdbe7694910967641fba38.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of birds&lt;loc_1&gt;&lt;loc_339&gt;&lt;loc_343&gt;&lt;loc_888&gt;6 of birds&lt;loc_327&gt;&lt;loc_326&gt;&lt;loc_563&gt;&lt;loc_773&gt;7 of birds&lt;loc_573&gt;&lt;loc_320&gt;&lt;loc_810&gt;&lt;loc_735&gt;8 of birds&lt;loc_784&gt;&lt;loc_320&gt;&lt;loc_999&gt;&lt;loc_684&gt;'}\n{'image': 'IMG_20220316_161515_jpg.rf.b22dd5d2b8037f009a9e65052d2d3b5c.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of birds&lt;loc_91&gt;&lt;loc_238&gt;&lt;loc_309&gt;&lt;loc_620&gt;10 of birds&lt;loc_708&gt;&lt;loc_179&gt;&lt;loc_900&gt;&lt;loc_532&gt;jack of birds&lt;loc_470&gt;&lt;loc_161&gt;&lt;loc_670&gt;&lt;loc_525&gt;queen of birds&lt;loc_386&gt;&lt;loc_495&gt;&lt;loc_689&gt;&lt;loc_816&gt;king of birds&lt;loc_282&gt;&lt;loc_163&gt;&lt;loc_490&gt;&lt;loc_532&gt;'}\n{'image': 'IMG_20220316_164227_jpg.rf.e42455b79bae041d959f90597b7065bc.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of cats&lt;loc_59&gt;&lt;loc_326&gt;&lt;loc_274&gt;&lt;loc_720&gt;6 of cats&lt;loc_740&gt;&lt;loc_223&gt;&lt;loc_955&gt;&lt;loc_625&gt;7 of cats&lt;loc_260&gt;&lt;loc_206&gt;&lt;loc_523&gt;&lt;loc_639&gt;8 of cats&lt;loc_485&gt;&lt;loc_252&gt;&lt;loc_760&gt;&lt;loc_695&gt;'}\n{'image': 'IMG_20220316_135012_jpg.rf.ee7179374d33235528db011cb5418226.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of fishs&lt;loc_290&gt;&lt;loc_323&gt;&lt;loc_573&gt;&lt;loc_639&gt;2 of fishs&lt;loc_330&gt;&lt;loc_652&gt;&lt;loc_617&gt;&lt;loc_986&gt;3 of fishs&lt;loc_570&gt;&lt;loc_447&gt;&lt;loc_856&gt;&lt;loc_816&gt;4 of fishs&lt;loc_375&gt;&lt;loc_49&gt;&lt;loc_658&gt;&lt;loc_361&gt;'}\n{'image': 'IMG_20220316_161525_jpg.rf.f7962cdc50e0a3cd03e4c081a1d2a67a.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of birds&lt;loc_55&gt;&lt;loc_363&gt;&lt;loc_334&gt;&lt;loc_798&gt;10 of birds&lt;loc_607&gt;&lt;loc_155&gt;&lt;loc_863&gt;&lt;loc_418&gt;jack of birds&lt;loc_400&gt;&lt;loc_204&gt;&lt;loc_670&gt;&lt;loc_516&gt;queen of birds&lt;loc_423&gt;&lt;loc_432&gt;&lt;loc_778&gt;&lt;loc_795&gt;king of birds&lt;loc_224&gt;&lt;loc_244&gt;&lt;loc_499&gt;&lt;loc_602&gt;'}\n{'image': 'IMG_20220316_141940_jpg.rf.e0ca71cfc8dc86a6624c3f90fe6c5e9e.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of fishs&lt;loc_680&gt;&lt;loc_333&gt;&lt;loc_922&gt;&lt;loc_778&gt;10 of fishs&lt;loc_105&gt;&lt;loc_438&gt;&lt;loc_451&gt;&lt;loc_920&gt;jack of fishs&lt;loc_493&gt;&lt;loc_230&gt;&lt;loc_709&gt;&lt;loc_655&gt;queen of fishs&lt;loc_219&gt;&lt;loc_227&gt;&lt;loc_489&gt;&lt;loc_654&gt;king of fishs&lt;loc_366&gt;&lt;loc_412&gt;&lt;loc_728&gt;&lt;loc_796&gt;'}\n{'image': 'IMG_20220316_165221_jpg.rf.a5188caf60a7bd5e8c6dcf92d68f9505.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of cats&lt;loc_43&gt;&lt;loc_241&gt;&lt;loc_334&gt;&lt;loc_550&gt;jack of cats&lt;loc_470&gt;&lt;loc_132&gt;&lt;loc_757&gt;&lt;loc_420&gt;queen of cats&lt;loc_347&gt;&lt;loc_275&gt;&lt;loc_641&gt;&lt;loc_586&gt;king of cats&lt;loc_295&gt;&lt;loc_511&gt;&lt;loc_623&gt;&lt;loc_948&gt;10 of cats&lt;loc_596&gt;&lt;loc_446&gt;&lt;loc_906&gt;&lt;loc_757&gt;'}\n{'image': 'IMG_20220316_162828_jpg.rf.db7557485f5c3e5ce01b2e1ab3d4621e.jpg', 'prefix': '&lt;OD&gt;', 'suffix': 'ace of cats&lt;loc_34&gt;&lt;loc_73&gt;&lt;loc_348&gt;&lt;loc_538&gt;2 of cats&lt;loc_277&gt;&lt;loc_523&gt;&lt;loc_555&gt;&lt;loc_978&gt;3 of cats&lt;loc_318&gt;&lt;loc_91&gt;&lt;loc_659&gt;&lt;loc_498&gt;4 of cats&lt;loc_539&gt;&lt;loc_447&gt;&lt;loc_838&gt;&lt;loc_917&gt;'}\n{'image': 'IMG_20220316_171936_jpg.rf.b6e31b1cc6b5e14dc66462becfa4a63d.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of dogs&lt;loc_329&gt;&lt;loc_499&gt;&lt;loc_623&gt;&lt;loc_984&gt;6 of dogs&lt;loc_353&gt;&lt;loc_2&gt;&lt;loc_748&gt;&lt;loc_355&gt;7 of dogs&lt;loc_390&gt;&lt;loc_238&gt;&lt;loc_699&gt;&lt;loc_731&gt;8 of dogs&lt;loc_38&gt;&lt;loc_416&gt;&lt;loc_420&gt;&lt;loc_784&gt;'}\n{'image': 'IMG_20220316_172800_jpg.rf.e63a3bae897cbf27dccbf969f543bb6f.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '9 of dogs&lt;loc_708&gt;&lt;loc_168&gt;&lt;loc_906&gt;&lt;loc_501&gt;10 of dogs&lt;loc_42&gt;&lt;loc_430&gt;&lt;loc_409&gt;&lt;loc_877&gt;jack  of dogs&lt;loc_481&gt;&lt;loc_126&gt;&lt;loc_678&gt;&lt;loc_445&gt;queen of dogs&lt;loc_232&gt;&lt;loc_158&gt;&lt;loc_440&gt;&lt;loc_488&gt;king of dogs&lt;loc_448&gt;&lt;loc_459&gt;&lt;loc_737&gt;&lt;loc_773&gt;'}\n{'image': 'IMG_20220316_171653_jpg.rf.637e380597f1d844654a33f4a3555471.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of dogs&lt;loc_102&gt;&lt;loc_473&gt;&lt;loc_418&gt;&lt;loc_800&gt;6 of dogs&lt;loc_423&gt;&lt;loc_495&gt;&lt;loc_798&gt;&lt;loc_862&gt;7 of dogs&lt;loc_466&gt;&lt;loc_100&gt;&lt;loc_816&gt;&lt;loc_400&gt;8 of dogs&lt;loc_173&gt;&lt;loc_157&gt;&lt;loc_466&gt;&lt;loc_480&gt;'}\n{'image': 'IMG_20220316_140241_jpg.rf.c44522806b5455bfb03a638aa3ffa896.jpg', 'prefix': '&lt;OD&gt;', 'suffix': '5 of fishs&lt;loc_300&gt;&lt;loc_409&gt;&lt;loc_405&gt;&lt;loc_673&gt;6 of fishs&lt;loc_372&gt;&lt;loc_380&gt;&lt;loc_489&gt;&lt;loc_688&gt;7 of fishs&lt;loc_456&gt;&lt;loc_334&gt;&lt;loc_623&gt;&lt;loc_747&gt;8 of fishs&lt;loc_598&gt;&lt;loc_236&gt;&lt;loc_880&gt;&lt;loc_830&gt;'}\n\n\n\n# @title Define `DetectionsDataset` class\n\nclass JSONLDataset:\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.jsonl_file_path = jsonl_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -&gt; List[Dict[str, Any]]:\n        entries = []\n        with open(self.jsonl_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -&gt; int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Image.Image, Dict[str, Any]]:\n        if idx &lt; 0 or idx &gt;= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n\n\nclass DetectionDataset(Dataset):\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, data = self.dataset[idx]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        return prefix, suffix, image\n\n\n# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n    return inputs, answers\n\ntrain_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/train/\"\n)\nval_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/valid/\"\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n\n# @title Setup LoRA Florence-2 model\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n    bias=\"none\",\n    inference_mode=False,\n    use_rslora=True,\n    init_lora_weights=\"gaussian\",\n    revision=\"asdjasdsa\",\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n\ntrainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n\n\n\ntorch.cuda.empty_cache()\n\n\n# @title Run inference with pre-trained Florence-2 model on validation dataset\n\ndef render_inline(image: Image.Image, resize=(128, 128)):\n    \"\"\"Convert image into inline html.\"\"\"\n    image.resize(resize)\n    with io.BytesIO() as buffer:\n        image.save(buffer, format='jpeg')\n        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n        return f\"data:image/jpeg;base64,{image_b64}\"\n\n\ndef render_example(image: Image.Image, response):\n    try:\n        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n    except:\n        print('failed to redner model response')\n    return f\"\"\"\n&lt;div style=\"display: inline-flex; align-items: center; justify-content: center;\"&gt;\n    &lt;img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" /&gt;\n    &lt;p style=\"width:512px; margin:10px; font-size:small;\"&gt;{html.escape(json.dumps(response))}&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n\ndef render_inference_results(model, dataset: DetectionDataset, count: int):\n    html_out = \"\"\n    count = min(count, len(dataset))\n    for i in range(count):\n        image, data = dataset.dataset[i]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        answer = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n        html_out += render_example(image, answer)\n\n    display(HTML(html_out))\n\nrender_inference_results(peft_model, val_dataset, 4)\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom object detection dataset",
    "text": "Fine-tune Florence-2 on custom object detection dataset\n\n# @title Define train loop\n\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    render_inference_results(peft_model, val_loader.dataset, 6)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n            render_inference_results(peft_model, val_loader.dataset, 6)\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n\n%%time\n\nEPOCHS = 10\nLR = 5e-6\n\ntrain_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[434.8800048828125, 140.47999572753906, 460.47998046875, 178.239990234375]], \"labels\": [\"human face\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[321.6000061035156, 212.1599884033203, 343.3599853515625, 242.87998962402344]], \"labels\": [\"human face\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"trousers\"]}}\n\n\n\nTraining Epoch 1/10: 100%|| 136/136 [01:32&lt;00:00,  1.48it/s]\n\n\nAverage Training Loss: 5.180607767666087\n\n\nValidation Epoch 1/10: 100%|| 8/8 [00:02&lt;00:00,  2.76it/s]\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\nAverage Validation Loss: 3.619225859642029\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 356.79998779296875]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 129.59999084472656, 267.8399963378906, 411.8399963378906]], \"labels\": [\"6 of 6 of 7 of 8 of 9 of 9's of 7's of 8's of 5's of 6's of 4's of 9' of 5' of 4' of 6' of 7' of 8' of 10's of 10' of 12's of 11's of 16's of 17's of 18's of 19's of 20's of 30's of 40's of 50's of 70's of 80's of eight's of ten's of four's of seven's of all sorts of sorts of eight' of sorts's of sorts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[19.520000457763672, 289.6000061035156, 223.0399932861328, 580.7999877929688], [186.55999755859375, 274.239990234375, 395.8399963378906, 511.03997802734375]], \"labels\": [\"queen of spades\", \"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [], \"labels\": []}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[15.039999961853027, 254.39999389648438, 213.44000244140625, 464.9599914550781], [463.67999267578125, 222.39999389648438, 635.8399658203125, 405.44000244140625], [329.2799987792969, 192.3199920654297, 466.8799743652344, 397.1199951171875], [208.95999145507812, 285.1199951171875, 345.91998291015625, 461.7599792480469]], \"labels\": [\"queen of spades\", \"queen of spades\", \"queen of spades\", \"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[294.0799865722656, 176.95999145507812, 624.3200073242188, 398.3999938964844]], \"labels\": [\"6 of spades\"]}}\n\n\n\nSetting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\nTraining Epoch 2/10: 100%|| 136/136 [01:32&lt;00:00,  1.47it/s]\n\n\nAverage Training Loss: 3.457882178180358\n\n\nValidation Epoch 2/10: 100%|| 8/8 [00:02&lt;00:00,  2.75it/s]\n\n\nAverage Validation Loss: 2.489599049091339\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 356.79998779296875], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 166.0800018310547, 469.44000244140625]], \"labels\": [\"queen of cats\", \"king of cats\", \"9 of cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.8399963378906, 411.8399963378906], [198.0800018310547, 82.23999786376953, 381.1199951171875, 323.5199890136719], [330.55999755859375, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of cats\", \"7 of cats\", \"5 of cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[18.8799991607666, 289.6000061035156, 223.0399932861328, 581.4400024414062], [368.3199768066406, 235.1999969482422, 517.4400024414062, 490.55999755859375], [185.27999877929688, 273.6000061035156, 396.47998046875, 511.67999267578125], [86.08000183105469, 164.16000366210938, 255.67999267578125, 403.5199890136719], [254.39999389648438, 167.36000061035156, 389.44000244140625, 317.1199951171875]], \"labels\": [\"queen of cats\", \"9 of cats\", \"queen's of cats\", \"queen cats\", \"queen card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[55.36000061035156, 228.79998779296875, 331.8399963378906, 637.1199951171875], [331.1999816894531, 151.36000061035156, 479.03997802734375, 447.03997802734375], [296.6399841308594, 253.1199951171875, 459.8399963378906, 550.719970703125], [436.79998779296875, 157.1199951171875, 557.760009765625, 392.0]], \"labels\": [\"8 of cats\", \"6 of cats\", \"7 of cats\", \"6 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[329.2799987792969, 192.3199920654297, 466.8799743652344, 397.7599792480469], [208.95999145507812, 285.1199951171875, 345.91998291015625, 461.7599792480469], [14.399999618530273, 254.39999389648438, 214.0800018310547, 464.9599914550781], [463.03997802734375, 222.39999389648438, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of cats\", \"7 of cats\", \"8 of cats\", \"5 of cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [292.1600036621094, 176.3199920654297, 624.9599609375, 399.03997802734375], [10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of cats\", \"9 of cats\", \"7 of cats\"]}}\n\n\n\nTraining Epoch 3/10: 100%|| 136/136 [01:32&lt;00:00,  1.47it/s]\n\n\nAverage Training Loss: 2.4063032251947067\n\n\nValidation Epoch 3/10: 100%|| 8/8 [00:02&lt;00:00,  2.76it/s]\n\n\nAverage Validation Loss: 1.7791805267333984\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 356.79998779296875], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.67999267578125, 166.0800018310547, 469.44000244140625], [310.0799865722656, 360.6399841308594, 445.7599792480469, 616.6400146484375]], \"labels\": [\"queen of cats\", \"king of cats\", \"9 of cats\", \"10 of cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.68000030517578, 267.1999816894531, 410.55999755859375], [200.63999938964844, 82.23999786376953, 381.1199951171875, 323.5199890136719], [333.1199951171875, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of dogs\", \"7 of dogs\", \"5 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[19.520000457763672, 289.6000061035156, 223.0399932861328, 580.7999877929688], [368.3199768066406, 234.55999755859375, 517.4400024414062, 490.55999755859375], [185.9199981689453, 273.6000061035156, 395.8399963378906, 511.03997802734375], [86.08000183105469, 163.51998901367188, 255.0399932861328, 401.6000061035156], [255.0399932861328, 167.36000061035156, 388.79998779296875, 317.1199951171875]], \"labels\": [\"queen of birds\", \"9 of birds\", \"king of birds\", \"queen birds\", \"queen fish\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[55.36000061035156, 228.79998779296875, 333.1199951171875, 637.1199951171875], [437.44000244140625, 157.1199951171875, 557.760009765625, 391.3599853515625], [296.6399841308594, 253.1199951171875, 459.8399963378906, 550.0800170898438], [331.1999816894531, 150.0800018310547, 479.67999267578125, 447.03997802734375]], \"labels\": [\"8 of birds\", \"6 of birds\", \"7 of birds\", \"5 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 397.7599792480469], [208.95999145507812, 285.1199951171875, 346.55999755859375, 461.7599792480469], [14.399999618530273, 254.39999389648438, 214.0800018310547, 465.5999755859375], [462.3999938964844, 221.1199951171875, 636.47998046875, 407.3599853515625]], \"labels\": [\"6 of birds\", \"7 of birds\", \"8 of birds\", \"5 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [308.1600036621094, 423.3599853515625, 548.7999877929688, 563.5199584960938], [10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of cats\", \"9 of cats\", \"7 of cats\"]}}\n\n\n\nTraining Epoch 4/10: 100%|| 136/136 [01:32&lt;00:00,  1.47it/s]\n\n\nAverage Training Loss: 1.849329402341562\n\n\nValidation Epoch 4/10: 100%|| 8/8 [00:02&lt;00:00,  2.72it/s]\n\n\nAverage Validation Loss: 1.6737226694822311\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.239990234375, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 167.36000061035156, 470.0799865722656], [310.0799865722656, 360.6399841308594, 445.7599792480469, 616.6400146484375]], \"labels\": [\"queen of birds\", \"king of birds\", \"9 of birds\", \"10 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 43.20000076293945, 516.7999877929688, 207.0399932861328], [0.3199999928474426, 128.3199920654297, 267.1999816894531, 410.55999755859375], [200.0, 83.5199966430664, 381.1199951171875, 323.5199890136719]], \"labels\": [\"5 of cats\", \"6 of cats\", \"7 of cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [256.32000732421875, 168.0, 388.79998779296875, 317.1199951171875], [18.8799991607666, 289.6000061035156, 223.0399932861328, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.8000030517578, 255.0399932861328, 403.5199890136719]], \"labels\": [\"9 of cats\", \"queen of cats\", \"10 of cats\", \"king of cats\", \"queen OF cats\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.7200012207031], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375], [56.0, 228.79998779296875, 331.8399963378906, 637.1199951171875]], \"labels\": [\"6 of dogs\", \"7 of dogs\", \"5 of dogs\", \"8 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [208.95999145507812, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375], [464.3199768066406, 222.39999389648438, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of birds\", \"7 of birds\", \"8 of birds\", \"5 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of cats\", \"6 of cats\", \"7 of cats\"]}}\n\n\n\nTraining Epoch 5/10: 100%|| 136/136 [01:32&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 1.7156715840101242\n\n\nValidation Epoch 5/10: 100%|| 8/8 [00:02&lt;00:00,  2.74it/s]\n\n\nAverage Validation Loss: 1.6302864849567413\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[371.5199890136719, 112.31999969482422, 512.9599609375, 358.0799865722656], [161.59999084472656, 329.91998291015625, 301.7599792480469, 585.2799682617188], [51.52000045776367, 238.39999389648438, 167.36000061035156, 470.0799865722656], [310.0799865722656, 358.0799865722656, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of birds\", \"king of birds\", \"9 of birds\", \"10 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 43.20000076293945, 516.7999877929688, 207.0399932861328], [0.3199999928474426, 129.59999084472656, 267.1999816894531, 410.55999755859375], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719]], \"labels\": [\"5 of dogs\", \"6 of dogs\", \"7 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [86.08000183105469, 162.87998962402344, 255.67999267578125, 401.6000061035156], [256.32000732421875, 167.36000061035156, 389.44000244140625, 317.1199951171875], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 397.1199951171875, 512.3200073242188]], \"labels\": [\"9 of birds\", \"queen of birds\", \"jack of birds\", \"10 of birds\", \"king of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 556.47998046875, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 458.55999755859375, 550.0800170898438], [56.0, 228.79998779296875, 333.1199951171875, 637.1199951171875]], \"labels\": [\"6 of dogs\", \"5 of dogs\", \"7 of dogs\", \"8 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [208.95999145507812, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375], [464.3199768066406, 222.39999389648438, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of birds\", \"7 of birds\", \"8 of birds\", \"5 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906], [291.5199890136719, 176.3199920654297, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of cats\", \"6 of cats\", \"7 of cats\", \"8 of cats\"]}}\n\n\n\nTraining Epoch 6/10: 100%|| 136/136 [01:32&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 1.6734710993135677\n\n\nValidation Epoch 6/10: 100%|| 8/8 [00:02&lt;00:00,  2.73it/s]\n\n\nAverage Validation Loss: 1.6009675413370132\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[371.5199890136719, 112.31999969482422, 512.9599609375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 585.2799682617188], [51.52000045776367, 239.0399932861328, 168.63999938964844, 470.0799865722656], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of birds\", \"king of birds\", \"9 of birds\", \"10 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 43.20000076293945, 516.7999877929688, 207.0399932861328], [0.3199999928474426, 129.59999084472656, 267.1999816894531, 410.55999755859375], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719]], \"labels\": [\"5 of dogs\", \"6 of dogs\", \"7 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [86.08000183105469, 162.87998962402344, 255.67999267578125, 401.6000061035156], [256.32000732421875, 167.36000061035156, 389.44000244140625, 317.1199951171875], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375]], \"labels\": [\"9 of birds\", \"queen of birds\", \"jack of birds\", \"10 of birds\", \"king of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 556.47998046875, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.0, 228.79998779296875, 333.1199951171875, 637.1199951171875]], \"labels\": [\"6 of dogs\", \"5 of dogs\", \"7 of dogs\", \"8 of dogs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [464.3199768066406, 222.39999389648438, 636.47998046875, 405.44000244140625], [209.59999084472656, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 464.9599914550781]], \"labels\": [\"6 of birds\", \"5 of birds\", \"7 of birds\", \"8 of birds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 424.0, 548.7999877929688, 561.5999755859375], [10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906], [291.5199890136719, 176.3199920654297, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of cats\", \"6 of cats\", \"7 of cats\", \"8 of cats\"]}}\n\n\n\nTraining Epoch 7/10:  34%|      | 46/136 [00:31&lt;01:02,  1.44it/s]\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nFile &lt;timed exec&gt;:4\n\nCell In[29], line 29, in train_model(train_loader, val_loader, model, processor, epochs, lr)\n     21 pixel_values = inputs[\"pixel_values\"]\n     22 labels = processor.tokenizer(\n     23     text=answers,\n     24     return_tensors=\"pt\",\n     25     padding=True,\n     26     return_token_type_ids=False\n     27 ).input_ids.to(DEVICE)\n---&gt; 29 outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n     30 loss = outputs.loss\n     32 loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/peft_model.py:1719, in PeftModelForCausalLM.forward(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\n   1717     with self._enable_peft_forward_hooks(**kwargs):\n   1718         kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}\n-&gt; 1719         return self.base_model(\n   1720             input_ids=input_ids,\n   1721             attention_mask=attention_mask,\n   1722             inputs_embeds=inputs_embeds,\n   1723             labels=labels,\n   1724             output_attentions=output_attentions,\n   1725             output_hidden_states=output_hidden_states,\n   1726             return_dict=return_dict,\n   1727             **kwargs,\n   1728         )\n   1730 batch_size = _get_batch_size(input_ids, inputs_embeds)\n   1731 if attention_mask is not None:\n   1732     # concat prompt attention mask\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197, in BaseTuner.forward(self, *args, **kwargs)\n    196 def forward(self, *args: Any, **kwargs: Any):\n--&gt; 197     return self.model.forward(*args, **kwargs)\n\nFile ~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base-ft/9803f52844ec1ae5df004e6089262e9a23e527fd/modeling_florence2.py:2741, in Florence2ForConditionalGeneration.forward(self, input_ids, pixel_values, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n   2739 if inputs_embeds is not None:\n   2740     attention_mask = attention_mask.to(inputs_embeds.dtype)\n-&gt; 2741 outputs = self.language_model(\n   2742     attention_mask=attention_mask,\n   2743     labels=labels,\n   2744     inputs_embeds=inputs_embeds,\n   2745     decoder_input_ids=decoder_input_ids,\n   2746     encoder_outputs=encoder_outputs,\n   2747     decoder_attention_mask=decoder_attention_mask,\n   2748     head_mask=head_mask,\n   2749     decoder_head_mask=decoder_head_mask,\n   2750     cross_attn_head_mask=cross_attn_head_mask,\n   2751     past_key_values=past_key_values,\n   2752     decoder_inputs_embeds=decoder_inputs_embeds,\n   2753     use_cache=use_cache,\n   2754     output_attentions=output_attentions,\n   2755     output_hidden_states=output_hidden_states,\n   2756     return_dict=return_dict,\n   2757 )\n   2759 logits = outputs.logits\n   2760 logits = logits.float()\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base-ft/9803f52844ec1ae5df004e6089262e9a23e527fd/modeling_florence2.py:2140, in Florence2LanguageForConditionalGeneration.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n   2135     if decoder_input_ids is None and decoder_inputs_embeds is None:\n   2136         decoder_input_ids = shift_tokens_right(\n   2137             labels, self.config.pad_token_id, self.config.decoder_start_token_id\n   2138         )\n-&gt; 2140 outputs = self.model(\n   2141     input_ids,\n   2142     attention_mask=attention_mask,\n   2143     decoder_input_ids=decoder_input_ids,\n   2144     encoder_outputs=encoder_outputs,\n   2145     decoder_attention_mask=decoder_attention_mask,\n   2146     head_mask=head_mask,\n   2147     decoder_head_mask=decoder_head_mask,\n   2148     cross_attn_head_mask=cross_attn_head_mask,\n   2149     past_key_values=past_key_values,\n   2150     inputs_embeds=inputs_embeds,\n   2151     decoder_inputs_embeds=decoder_inputs_embeds,\n   2152     use_cache=use_cache,\n   2153     output_attentions=output_attentions,\n   2154     output_hidden_states=output_hidden_states,\n   2155     return_dict=return_dict,\n   2156 )\n   2158 lm_logits = self.lm_head(outputs[0])\n   2159 lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base-ft/9803f52844ec1ae5df004e6089262e9a23e527fd/modeling_florence2.py:2014, in Florence2LanguageModel.forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\n   2011 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n   2013 if encoder_outputs is None:\n-&gt; 2014     encoder_outputs = self.encoder(\n   2015         input_ids=input_ids,\n   2016         attention_mask=attention_mask,\n   2017         head_mask=head_mask,\n   2018         inputs_embeds=inputs_embeds,\n   2019         output_attentions=output_attentions,\n   2020         output_hidden_states=output_hidden_states,\n   2021         return_dict=return_dict,\n   2022     )\n   2023 # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n   2024 elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base-ft/9803f52844ec1ae5df004e6089262e9a23e527fd/modeling_florence2.py:1593, in Florence2Encoder.forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\n   1588     attention_mask = attention_mask if 0 in attention_mask else None\n   1589 elif self._use_sdpa and head_mask is None and not output_attentions:\n   1590     # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n   1591     # the manual implementation that requires a 4D causal mask in all cases.\n   1592     # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n-&gt; 1593     attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n   1594 else:\n   1595     # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n   1596     attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tuned-model-evaluation",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tuned-model-evaluation",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tuned model evaluation",
    "text": "Fine-tuned model evaluation\n\n# @title Check if the model can still detect objects outside of the custom dataset\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = peft_model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\n\n\n\n\n\n\n\nNOTE: It seems that the model can still detect classes that dont belong to our custom dataset.\n\n# @title Collect predictions\n\nPATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)&lt;loc_\\d+&gt;'\n\ndef extract_classes(dataset: DetectionDataset):\n    class_set = set()\n    for i in range(len(dataset.dataset)):\n        image, data = dataset.dataset[i]\n        suffix = data[\"suffix\"]\n        classes = re.findall(PATTERN, suffix)\n        class_set.update(classes)\n    return sorted(class_set)\n\nCLASSES = extract_classes(train_dataset)\n\ntargets = []\npredictions = []\n\nfor i in range(len(val_dataset.dataset)):\n    image, data = val_dataset.dataset[i]\n    prefix = data['prefix']\n    suffix = data['suffix']\n\n    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    prediction = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n    prediction.confidence = np.ones(len(prediction))\n\n    target = processor.post_process_generation(suffix, task='&lt;OD&gt;', image_size=image.size)\n    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n    targets.append(target)\n    predictions.append(prediction)\n\n\n# @title Calculate mAP\nmean_average_precision = sv.MeanAveragePrecision.from_detections(\n    predictions=predictions,\n    targets=targets,\n)\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n\nmap50_95: 0.49\nmap50: 0.53\nmap75: 0.53\n\n\n\n# @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=CLASSES\n)\n\n_ = confusion_matrix.plot()"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#save-fine-tuned-model-on-hard-drive",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#save-fine-tuned-model-on-hard-drive",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Save fine-tuned model on hard drive",
    "text": "Save fine-tuned model on hard drive\n\npeft_model.save_pretrained(\"/content/florence2-lora\")\nprocessor.save_pretrained(\"/content/florence2-lora/\")\n!ls -la /content/florence2-lora/\n\ntotal 11432\ndrwxr-xr-x 2 root root    4096 Jun 26 21:43 .\ndrwxr-xr-x 1 root root    4096 Jun 26 21:43 ..\n-rw-r--r-- 1 root root     746 Jun 26 21:43 adapter_config.json\n-rw-r--r-- 1 root root 7747264 Jun 26 21:43 adapter_model.safetensors\n-rw-r--r-- 1 root root   22410 Jun 26 21:43 added_tokens.json\n-rw-r--r-- 1 root root  456318 Jun 26 21:43 merges.txt\n-rw-r--r-- 1 root root     947 Jun 26 21:43 preprocessor_config.json\n-rw-r--r-- 1 root root    5102 Jun 26 21:43 README.md\n-rw-r--r-- 1 root root  146627 Jun 26 21:43 special_tokens_map.json\n-rw-r--r-- 1 root root  197658 Jun 26 21:43 tokenizer_config.json\n-rw-r--r-- 1 root root 2297961 Jun 26 21:43 tokenizer.json\n-rw-r--r-- 1 root root  798293 Jun 26 21:43 vocab.json"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#upload-model-to-roboflow-optional",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#upload-model-to-roboflow-optional",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Upload model to Roboflow (optional)",
    "text": "Upload model to Roboflow (optional)\nYou can deploy your Florence-2 object detection model on your own hardware (i.e.a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\nTo deploy your model, you will need a free Roboflow account.\nTo get started, create a new Project in Roboflow if you dont already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\nYou can read our full Deploy Florence-2 with Roboflow guide for step-by-step instructions of these steps.\nOnce you have trained your model A, you can upload it to Roboflow using the following code:\n\nimport roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace(\"workspace-id\").project(\"project-id\")\nversion = project.version(VERSION)\n\nversion.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")\n\nAbove, replace:\n\nAPI_KEY with your Roboflow API key.\nworkspace-id and project-id with your workspace and project IDs.\nVERSION with your project version.\n\nIf you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\nWhen you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\nYour model will be uploaded to Roboflow."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#deploy-to-your-hardware",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#deploy-to-your-hardware",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Deploy to your hardware",
    "text": "Deploy to your hardware\nOnce your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\nInference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\nFor this guide, we will show how to deploy the model with the Python SDK.\nFirst, install inference:\n\n!pip install inference\n\nThen, create a new Python file and add the following code:\n\nimport os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)\n\nIn the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\nWhen you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
  }
]