[
  {
    "objectID": "posts/2021-09-28-docker_cheatsheet.html",
    "href": "posts/2021-09-28-docker_cheatsheet.html",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/2021-09-28-docker_cheatsheet.html#images",
    "href": "posts/2021-09-28-docker_cheatsheet.html#images",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/2021-09-28-docker_cheatsheet.html#containers",
    "href": "posts/2021-09-28-docker_cheatsheet.html#containers",
    "title": "Docker Cheatsheet",
    "section": "Containers",
    "text": "Containers\nCreate a new container from an image with following flags 1. -v: for telling docker to use a shared directory between host and container 2. -p \\&lt;host-port\\&gt;\\&lt;container-port\\&gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. 3. --name generates a name for the container for easy reference in other commands 4. --gpus all tells docker to use all GPUs available on host 5. --memory-swap Restricts RAM+Swap usage 6. -it makes sure container awaits after starting instead of instantly shutting down if no startup scripts are configured.\nTo create new container with default params:\ndocker create tensorflow/tensorflow:2.4.0-gpu-jupyter \nTo create new container with manual params:\ndocker create -it \\\n-v \\path_in_host:\\path_in_container \\\n-p9000:8888 \\\n--name aaai \\\n--cpus 2 \\\n--gpus all \\ # To use specific gpus: --gpu '\"device=0,2\"'\n--memory 90g \\ # Uses 90g memory\n--memory-swap 100g \\ # --memory-swap is a modifier flag that only has meaning if --memory is also set. In this case 10g of swap will be used.\ntensorflow/tensorflow:2.4.0-gpu-jupyter\nUpdate some of the above configurations after container creation:\n# change RAM limit of a container named \"aaai\"\ndocker update --memory-swap 50g aaai\n\nNote: In general, changes made to container persist when container is stopped.\n\nCheck containers:\ndocker ps # shows running containers\ndocker ps -a # shows all containers\nStart a container (default script will be executed with this if any):\n# docker start &lt;container-name&gt;\ndocker start aaai\nStop a container:\n# docker stop &lt;container-name&gt; \ndocker stop aaai\nDelete a container:\n# docker rm &lt;container-name&gt;\ndocker rm aaai\nGo to a running container’s shell:\n#docker exec -it &lt;container-name&gt; bash\ndocker exec -it aaai bash # -it stands for interactive\nExecute any command on a running container without opening a shell in container:\n# docker exec -it &lt;container-name&gt; &lt;command&gt;\ndocker exec -it aaai jupyter notebook list\nCheck container logs (including shell commands output):\n# docker logs &lt;container-name&gt;\ndocker logs aaai"
  },
  {
    "objectID": "posts/2021-09-28-docker_cheatsheet.html#system",
    "href": "posts/2021-09-28-docker_cheatsheet.html#system",
    "title": "Docker Cheatsheet",
    "section": "System",
    "text": "System\nCheck all images, all containers and space occupied by them:\ndocker system df -v"
  },
  {
    "objectID": "posts/2021-09-28-docker_cheatsheet.html#set-up-rootless-docker",
    "href": "posts/2021-09-28-docker_cheatsheet.html#set-up-rootless-docker",
    "title": "Docker Cheatsheet",
    "section": "Set up rootless docker",
    "text": "Set up rootless docker\nRefer to this guide: https://docs.docker.com/engine/security/rootless/\nMain steps:\n\nRun dockerd-rootless-setuptool.sh install.\nSetup PATH and DOCKER_HOME as suggested by command output.\nsystemctl --user restart docker.\nTry docker images to check if things worked.\nTry docker run --rm hello-world to check if things really worked."
  },
  {
    "objectID": "posts/2023-03-28-nngp.html",
    "href": "posts/2023-03-28-nngp.html",
    "title": "Neural Network Gaussian Process",
    "section": "",
    "text": "# %%capture\n# %pip install -U --force-reinstall jaxutils\n# %pip install -U jax jaxlib optax\n\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nfrom jaxutils import Dataset\n\ntry:\n    from neural_tangents import stax\nexcept ModuleNotFoundError:\n    %pip install neural-tangents\n    from neural_tangents import stax\n\ntry:\n    import optax as ox\nexcept ModuleNotFoundError:\n    %pip install optax\n    import optax as ox\n\ntry:\n    import gpjax as gpx\nexcept ModuleNotFoundError:\n    %pip install gpjax\n    import gpjax as gpx\n\ntry:\n    import regdata as rd\nexcept ModuleNotFoundError:\n    %pip install regdata\n    import regdata as rd\n\nimport matplotlib.pyplot as plt\n\n\nclass NTK(gpx.kernels.AbstractKernel):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def __call__(self, params, x, y):\n        params = jax.tree_util.tree_map(jax.nn.softplus, params)\n        init_fn, apply_fn, kernel_fn = stax.serial(\n            stax.Dense(512, W_std=params[\"w1\"], b_std=params[\"b1\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w2\"], b_std=params[\"b2\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w3\"], b_std=params[\"b3\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w4\"], b_std=params[\"b4\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w5\"], b_std=params[\"b5\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w6\"], b_std=params[\"b6\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w7\"], b_std=params[\"b7\"]), stax.Relu(),\n            stax.Dense(1, W_std=params[\"w8\"], b_std=params[\"b8\"])\n        )\n        return kernel_fn(x.reshape(1, 1), y.reshape(1, 1)).nngp.squeeze()\n\n    def init_params(self, key):\n        # return init_fn(key, input_shape=(2,1))\n        return {\"w1\": 0.1, \"w2\": 0.2, \"w3\": 0.3, \"w4\": 0.4, \"w5\": 0.5,  \"w6\": 0.6, \"w7\": 0.7, \"w8\": 0.8,\n                \"b1\": 0.1, \"b2\": 0.2, \"b3\": 0.3, \"b4\": 0.4, \"b5\": 0.5,  \"b6\": 0.6, \"b7\": 0.7, \"b8\": 0.8\n                }\n\n    # This is depreciated. Can be removed once JaxKern is updated.\n    def _initialise_params(self, key):\n        return self.init_params(key)\n\n\nn = 100\nnoise = 0.3\nkey = jr.PRNGKey(123)\n# x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).sort().reshape(-1, 1)\n# f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n# signal = f(x)\n# y = signal + jr.normal(key, shape=signal.shape) * noise\nx, y, xtest = rd.MotorcycleHelmet().get_data()\ny = y.reshape(-1, 1)\n\nD = Dataset(X=x, y=y)\n\n# xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n# ytest = f(xtest)\n\nprint(x.shape, y.shape)\n\n(94, 1) (94, 1)\n\n\n\nkernel = NTK()\nprior = gpx.Prior(kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n\n\nkey = jr.PRNGKey(1234)\nparameter_state = gpx.initialise(posterior, key)\nparams, trainable, bijectors = parameter_state.unpack()\nparams[\"likelihood\"][\"obs_noise\"] = jnp.array(0.1)\nparameter_state = gpx.parameters.ParameterState(params, trainable, bijectors)\nprint(params)\n\n{'kernel': {'w1': 0.1, 'w2': 0.2, 'w3': 0.3, 'w4': 0.4, 'w5': 0.5, 'w6': 0.6, 'w7': 0.7, 'w8': 0.8, 'b1': 0.1, 'b2': 0.2, 'b3': 0.3, 'b4': 0.4, 'b5': 0.5, 'b6': 0.6, 'b7': 0.7, 'b8': 0.8}, 'mean_function': {}, 'likelihood': {'obs_noise': Array(0.1, dtype=float32, weak_type=True)}}\n\n\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n\n\n\nnegative_mll = jax.jit(posterior.marginal_log_likelihood(D, negative=True))\nnegative_mll(params)\n\nArray(415.1062, dtype=float32)\n\n\n\noptimiser = ox.adam(learning_rate=0.01)\n\ninference_state = gpx.fit(\n    objective=negative_mll,\n    parameter_state=parameter_state,\n    optax_optim=optimiser,\n    num_iters=500,\n)\n\nlearned_params, training_history = inference_state.unpack()\n\n100%|██████████| 500/500 [00:02&lt;00:00, 172.53it/s, Objective=76.34]\n\n\n\nplt.plot(training_history);\n\n\n\n\n\nlearned_params\n\n{'kernel': {'b1': Array(0.03292831, dtype=float32),\n  'b2': Array(-0.9647168, dtype=float32),\n  'b3': Array(-1.2660046, dtype=float32),\n  'b4': Array(-1.3792713, dtype=float32),\n  'b5': Array(-1.4311961, dtype=float32),\n  'b6': Array(-1.4504426, dtype=float32),\n  'b7': Array(-1.4371448, dtype=float32),\n  'b8': Array(-1.3471106, dtype=float32),\n  'w1': Array(1.0706716, dtype=float32),\n  'w2': Array(1.1768614, dtype=float32),\n  'w3': Array(1.2740505, dtype=float32),\n  'w4': Array(1.3689499, dtype=float32),\n  'w5': Array(1.462641, dtype=float32),\n  'w6': Array(1.5562503, dtype=float32),\n  'w7': Array(1.6506695, dtype=float32),\n  'w8': Array(1.7462935, dtype=float32)},\n 'likelihood': {'obs_noise': Array(0.184795, dtype=float32)},\n 'mean_function': {}}\n\n\n\nlatent_dist = posterior(learned_params, D)(xtest)\npredictive_dist = likelihood(learned_params, latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=\"tab:blue\",\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\n\n# ax.plot(\n#     xtest, ytest, label=\"Latent function\", color=\"black\", linestyle=\"--\", linewidth=1\n# )\n\nax.legend();"
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html",
    "href": "posts/2023-06-12-GNN_for_regression.html",
    "title": "Graph Neural Networks for Regression",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport GPy\n\nimport torch\nimport torch.nn as nn\n\nfrom tqdm import trange\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ndevice = \"cuda\""
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html#create-a-synthetic-dataset",
    "href": "posts/2023-06-12-GNN_for_regression.html#create-a-synthetic-dataset",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a synthetic dataset",
    "text": "Create a synthetic dataset\n\nnp.random.seed(0)\ntorch.random.manual_seed(4)\n\nN = 50\nx = np.linspace(-1, 1, N).reshape(-1, 1)\nkernel = GPy.kern.RBF(input_dim=1, variance=1, lengthscale=0.1)\ny = np.random.multivariate_normal(np.zeros(N), kernel.K(x)).reshape(-1, 1)\ny_noisy = y + np.random.normal(0, 0.1, N).reshape(-1, 1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y_noisy, test_size=0.4, random_state=0)\n\nplt.plot(x, y, label=\"True\");\nplt.plot(train_x, train_y, 'o', label='train')\nplt.plot(test_x, test_y, 'o', label='test')\nplt.legend();\n\nx, y, y_noisy = map(lambda x: torch.tensor(x).float().to(device), (x, y, y_noisy))\ntrain_x, test_x, train_y, test_y = map(lambda x: torch.tensor(x).float().to(device), (train_x, test_x, train_y, test_y))\nprint(x.shape, y.shape, y_noisy.shape)\n\ntorch.Size([50, 1]) torch.Size([50, 1]) torch.Size([50, 1])"
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html#fit-with-a-simple-mlp",
    "href": "posts/2023-06-12-GNN_for_regression.html#fit-with-a-simple-mlp",
    "title": "Graph Neural Networks for Regression",
    "section": "Fit with a simple MLP",
    "text": "Fit with a simple MLP\n\ndef fit(model, x, y, A=None, lr=0.01, epochs=100):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n    \n    if A is None:\n        inputs = (x,)\n    else:\n        inputs = (x, A)\n    \n    losses = []\n    pbar = trange(epochs)\n    for epoch in pbar:\n        optimizer.zero_grad()\n        y_hat = model(*inputs)\n        loss = loss_fn(y_hat, y)\n        losses.append(loss.item())\n        pbar.set_description(f\"Epoch {epoch} Loss: {loss.item()}\")\n        loss.backward()\n        optimizer.step()\n            \n    return losses\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [nn.Linear(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\ntorch.manual_seed(0)\nmodel = SimpleMLP([10, 10, 10]).to(device)\nfit(model, train_x, train_y, lr=0.01, epochs=1000);\n\npred_y = model(x)\n\n(x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\nplt.plot(x_, y_, label=\"True\");\nplt.plot(train_x_, train_y_, 'o', label='train')\nplt.plot(test_x_, test_y_, 'o', label='test')\nplt.plot(x_, pred_y_, label='pred')\nplt.legend();\n\nEpoch 999 Loss: 0.07143261283636093: 100%|██████████| 1000/1000 [00:02&lt;00:00, 448.34it/s]"
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html#create-a-gcn-layer",
    "href": "posts/2023-06-12-GNN_for_regression.html#create-a-gcn-layer",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a GCN layer",
    "text": "Create a GCN layer\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x, A):    \n        return self.linear(A @ x)\n    \n    \nclass GCN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [GCNLayer(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(GCNLayer(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(GCNLayer(features[-1], 1))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x, A):\n        for layer in self.layers:\n            if isinstance(layer, GCNLayer):\n                x = layer(x, A)\n            else:\n                x = layer(x)\n        return x\n    \ndef get_eucledean_A(x, exponent):\n    d = ((x - x.T)**2)**0.5\n    d = torch.where(d==0, torch.min(d[d!=0])/2, d)  # self distance is 0, so replace it with half of the min distance\n    A = 1/(d**exponent)\n    return A/A.sum(dim=1, keepdim=True)\n\ndef get_KNN_A(x, k):\n    d = torch.abs(x - x.T)\n    A = torch.zeros_like(d)\n    _, indices = torch.topk(d, k, dim=1, largest=False)\n    for i, index in enumerate(indices):\n        A[i, index] = 1\n    return A/A.sum(dim=1, keepdim=True)\n\ndef fit_and_plot(title):\n    model = GCN([10, 10, 10]).to(device)\n    losses = fit(model, train_x, train_y, A=A_train, lr=0.001, epochs=3000);\n\n    pred_y = model(x, A_all)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    axes = ax[0]\n    axes.plot(losses)\n    axes.set_title(\"Losses\")\n\n    (x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\n    axes = ax[1]\n    axes.plot(x_, y_, label=\"True\");\n    axes.plot(train_x_, train_y_, 'o', label='train')\n    axes.plot(test_x_, test_y_, 'o', label='test')\n    axes.plot(x_, pred_y_, label='pred')\n    axes.set_title(title)\n    axes.legend();"
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html#idw-setting",
    "href": "posts/2023-06-12-GNN_for_regression.html#idw-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "IDW setting",
    "text": "IDW setting\n\nexponent = 1\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.07731884717941284: 100%|██████████| 3000/3000 [00:07&lt;00:00, 401.90it/s]\n\n\n\n\n\n\nexponent = 2\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.0778026208281517: 100%|██████████| 3000/3000 [00:07&lt;00:00, 386.31it/s] \n\n\n\n\n\n\nexponent = 3\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.04420239478349686: 100%|██████████| 3000/3000 [00:08&lt;00:00, 369.61it/s]"
  },
  {
    "objectID": "posts/2023-06-12-GNN_for_regression.html#knn-setting",
    "href": "posts/2023-06-12-GNN_for_regression.html#knn-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "KNN Setting",
    "text": "KNN Setting\n\nK = 1\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.04107221961021423: 100%|██████████| 3000/3000 [00:08&lt;00:00, 374.39it/s] \n\n\n\n\n\n\nK = 3\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.15448586642742157: 100%|██████████| 3000/3000 [00:07&lt;00:00, 386.62it/s]\n\n\n\n\n\n\nK = 7\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.18613268435001373: 100%|██████████| 3000/3000 [00:08&lt;00:00, 374.51it/s]\n\n\n\n\n\n\nK = 15\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.797140896320343: 100%|██████████| 3000/3000 [00:07&lt;00:00, 377.66it/s]"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html",
    "href": "posts/2022-06-10-jaxoptimizers.html",
    "title": "JAX Optimizers",
    "section": "",
    "text": "%%capture\n%pip install -U jax\nimport jax\nimport jax.numpy as jnp\ntry:\n  import jaxopt\nexcept ModuleNotFoundError:\n  %pip install -qq jaxopt\n  import jaxopt\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install -qq optax\n  import optax\n\nimport tensorflow_probability.substrates.jax as tfp"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "href": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "title": "JAX Optimizers",
    "section": "Loss function",
    "text": "Loss function\n\ndef loss_fun(x, a):\n  return (((x['param1'] - a) + (x['param2'] - (a+1)))**2).sum()"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "href": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "title": "JAX Optimizers",
    "section": "Initial parameters",
    "text": "Initial parameters\n\nN = 3\ninit_params = lambda: {'param1': jnp.zeros(N), 'param2': jnp.ones(N)}\na = 2.0"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "href": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "title": "JAX Optimizers",
    "section": "Optimizers",
    "text": "Optimizers\n\nJaxOpt ScipyMinimize\n\n%%time\nsolver = jaxopt.ScipyMinimize('L-BFGS-B', fun=loss_fun)\nans = solver.run(init_params(), a)\nprint(ans)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nOptStep(params={'param1': DeviceArray([1.9999999, 1.9999999, 1.9999999], dtype=float32), 'param2': DeviceArray([3., 3., 3.], dtype=float32)}, state=ScipyMinimizeInfo(fun_val=DeviceArray(4.2632564e-14, dtype=float32), success=True, status=0, iter_num=2))\nCPU times: user 78.3 ms, sys: 18.5 ms, total: 96.8 ms\nWall time: 95.8 ms\n\n\n\nPros\n\nTwo lines of code will do it all.\n\n\n\nCons\n\nIt only returns the final parameters and final loss. No option to retrive in-between loss values.\n\n\n\n\nOptax\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nvalue_and_grad_fun = jax.jit(jax.value_and_grad(loss_fun, argnums=0))\nparams = init_params()\nstate = optimizer.init(params)\n\nfor _ in range(100):\n  loss_value, gradients = value_and_grad_fun(params, a)\n  updates, state = optimizer.update(gradients, state)\n  params = optax.apply_updates(params, updates)\n\nprint(params)\n\n{'param1': DeviceArray([2.0084236, 2.0084236, 2.0084236], dtype=float32), 'param2': DeviceArray([3.0084238, 3.0084238, 3.0084238], dtype=float32)}\nCPU times: user 3.09 s, sys: 63.4 ms, total: 3.16 s\nWall time: 4.2 s\n\n\n\nPros:\n\nFull control in user’s hand. We can save intermediate loss values.\n\n\n\nCons:\n\nIts code is verbose, similar to PyTorch optimizers.\n\n\n\n\nJaxopt OptaxSolver\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nsolver = jaxopt.OptaxSolver(loss_fun, optimizer, maxiter=100)\nans = solver.run(init_params(), a)\nprint(ans)\n\nOptStep(params={'param1': DeviceArray([2.008423, 2.008423, 2.008423], dtype=float32), 'param2': DeviceArray([3.008423, 3.008423, 3.008423], dtype=float32)}, state=OptaxState(iter_num=DeviceArray(100, dtype=int32, weak_type=True), value=DeviceArray(0.00113989, dtype=float32), error=DeviceArray(0.09549397, dtype=float32), internal_state=(ScaleByAdamState(count=DeviceArray(100, dtype=int32), mu={'param1': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32), 'param2': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32)}, nu={'param1': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32), 'param2': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32)}), EmptyState()), aux=None))\nCPU times: user 719 ms, sys: 13.4 ms, total: 732 ms\nWall time: 1.09 s\n\n\n\nPros:\n\nLess lines of code.\nApplies lax.scan internally to make it fast [reference].\n\n\n\nCons:\n\nNot able to get in-between state/loss values\n\n\n\n\ntfp math minimize\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nparams, losses = tfp.math.minimize_stateless(loss_fun, (init_params(), a), num_steps=1000, optimizer=optimizer)\nprint(params)\nprint(losses[:5])\n\n({'param1': DeviceArray([1.0000008, 1.0000008, 1.0000008], dtype=float32), 'param2': DeviceArray([1.9999989, 1.9999989, 1.9999989], dtype=float32)}, DeviceArray(0.9999999, dtype=float32))\n[48.       38.88006  30.751791 23.626852 17.507807]\nCPU times: user 880 ms, sys: 15.2 ms, total: 895 ms\nWall time: 1.53 s\n\n\n\nPros:\n\nOne line of code to optimize the function and return in-between losses.\n\n\n\nCons:\n\nBy default, it optimizes all arguments passed to the loss function. In above example, we can not control if a should be optimized or not. I have raised an issue here for this problem."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html",
    "href": "posts/2022-01-24-query_by_committee.html",
    "title": "Query by Committee",
    "section": "",
    "text": "# Common imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\n\nplt.style.use('fivethirtyeight')\nrc('animation', html='jshtml')\n\n# Copy the models\nfrom copy import deepcopy\n\n# Sklearn imports\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Entropy function\nfrom scipy.stats import entropy\n\n# Progress helper\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "title": "Query by Committee",
    "section": "QBC by posterior sampling",
    "text": "QBC by posterior sampling\n\nInteresting fact: For probabilistic models, QBC is similar to uncertainty sampling. How?\n\nDraw \\(k\\) parameter sets from the posterior distribution representing \\(k\\) different models.\nQuery a point which shows maximum disagreement among the points."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "href": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "title": "Query by Committee",
    "section": "An example: Bayesian linear regression",
    "text": "An example: Bayesian linear regression\n\nnp.random.seed(0)\nN = 10\nX = np.linspace(-1,1,N).reshape(-1,1)\n\nt0 = 3\nt1 = 2\n\ny = X * t1 + t0 + np.random.rand(N,1)\n\nplt.scatter(X, y);\n\n\n\n\n\nAssume a posterior\n\nn_samples = 50\n\nt0_dist_samples = np.random.normal(t0, 0.1, size=n_samples)\nt1_dist_samples = np.random.normal(t1, 1, size=n_samples)\n\n\n\nPlot the models\n\nplt.scatter(X, y)\n\nfor i in range(len(t0_dist_samples)):\n    sample_t0 = t0_dist_samples[i]\n    sample_t1 = t1_dist_samples[i]\n    \n    plt.plot(X, X * sample_t1 + sample_t0,alpha=0.1)"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "title": "Query by Committee",
    "section": "QBC by bootstrapping",
    "text": "QBC by bootstrapping\n\n2 class dataset\n\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=3, shuffle=True)\n\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=y);\n\n\n\n\n\n\nFull data fit with RF\n\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X, y);\n\nRandomForestClassifier(random_state=0)\n\n\n\n\nVisualize decision boundary\n\ngrid_X1, grid_X2 = np.meshgrid(np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100), \n                    np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100))\n\ngrid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())]\n\ngrid_pred = model.predict(grid_X)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], c=y);\nplt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2);\n\n\n\n\n\n\nTrain, pool, test split\n\nX_train_pool, X_test, y_train_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nX_train, X_pool, y_train, y_pool = train_test_split(X_train_pool, y_train_pool, train_size=20, random_state=0)\n\nX_list = [X_train, X_pool, X_test]\ny_list = [y_train, y_pool, y_test]\nt_list = ['Train', 'Pool', 'Test']\n\nfig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True)\nfor i in range(3):\n    ax[i].scatter(X_list[i][:,0], X_list[i][:,1], c=y_list[i])\n    ax[i].set_title(t_list[i])\n    \n\n\n\n\n\n\nFitting a model on initial train data\n\nAL_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\nAL_model.fit(X_train, y_train);\n\nRandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\n\nGet the votes from trees on pool dataset\n\nvotes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n\nfor learner_idx, learner in enumerate(AL_model.estimators_):\n    votes[:, learner_idx] = learner.predict(X_pool)\n\n\nvotes.shape\n\n(780, 100)\n\n\n\nvotes\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 1., 1., ..., 0., 1., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nConvert to probabilities\n\np_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n\nfor vote_idx, vote in enumerate(votes):\n    vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()}\n\n    for class_idx, class_label in enumerate(range(X.shape[1])):\n        p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n\n\np_vote\n\narray([[1.  , 0.  ],\n       [0.89, 0.11],\n       [0.06, 0.94],\n       ...,\n       [0.93, 0.07],\n       [1.  , 0.  ],\n       [1.  , 0.  ]])\n\n\n\n\nCalculate dissimilarity (entropy)\n\nexample_id = 2\n\n\nans = 0\nfor category in range(X_pool.shape[1]):\n    ans += (-p_vote[example_id][category] * np.log(p_vote[example_id][category]))\n\nans\n\n0.22696752250060448\n\n\n\nentr = entropy(p_vote, axis=1)\n\n\nentr[example_id]\n\n0.22696752250060448\n\n\n\n\nActive Learning Flow\n\ndef get_query_idx():\n    # Gather the votes\n    votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n    for learner_idx, learner in enumerate(AL_model.estimators_):\n        votes[:, learner_idx] = learner.predict(X_pool)\n    \n    # Calcuate probability of votes\n    p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n    for vote_idx, vote in enumerate(votes):\n        vote_counter = {0 : (1-vote).sum(), \n                    1 : vote.sum()}\n\n        for class_idx, class_label in enumerate(range(X.shape[1])):\n            p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n    \n    # Calculate entropy for each example\n    entr = entropy(p_vote, axis=1)\n    \n    # Choose example with highest entropy (disagreement)\n    return entr.argmax()\n\n\n\nPrepare data for random sampling\n\nX_train_rand = X_train.copy()\ny_train_rand = y_train.copy()\nX_pool_rand = X_pool.copy()\ny_pool_rand = y_pool.copy()\n\nrandom_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\nRun active learning\n\nAL_iters = 100\nnp.random.seed(0)\n\nAL_inds = []\nAL_models = []\nrandom_inds = []\nrandom_models = []\n\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    ######## Active Learning ############\n    # Fit the model\n    AL_model.fit(X_train, y_train)\n    AL_models.append(deepcopy(AL_model))\n    \n    # Query a point\n    query_idx = get_query_idx()\n    AL_inds.append(query_idx)\n    \n    # Add it to the train data\n    X_train = np.concatenate([X_train, X_pool[query_idx:query_idx+1, :]], axis=0)\n    y_train = np.concatenate([y_train, y_pool[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool = np.delete(X_pool, query_idx, axis=0)\n    y_pool = np.delete(y_pool, query_idx, axis=0)\n    \n    ######## Random Sampling ############\n     # Fit the model\n    random_model.fit(X_train_rand, y_train_rand)\n    random_models.append(deepcopy(random_model))\n    \n    # Query a point\n    query_idx = np.random.choice(len(X_pool))\n    random_inds.append(query_idx)\n    # Add it to the train data\n    X_train_rand = np.concatenate([X_train_rand, X_pool_rand[query_idx:query_idx+1, :]], axis=0)\n    y_train_rand = np.concatenate([y_train_rand, y_pool_rand[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool_rand = np.delete(X_pool_rand, query_idx, axis=0)\n    y_pool_rand = np.delete(y_pool_rand, query_idx, axis=0)\n\niteration 99\n\n\n\n\nPlot accuracy\n\nrandom_scores = []\nAL_scores = []\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test)))\n    random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test)))\n    \nplt.plot(AL_scores, label='Active Learning');\nplt.plot(random_scores, label='Random Sampling');\nplt.legend();\nplt.xlabel('Iterations');\nplt.ylabel('Accuracy\\n(Higher is better)');\n\niteration 99\n\n\n\n\n\n\n\nPlot decision boundary\n\ndef update(i):\n    for each in ax:\n        each.cla()\n        \n    AL_grid_preds = AL_models[i].predict(grid_X)\n    random_grid_preds = random_models[i].predict(grid_X)\n    \n    # Active learning\n    ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label='initial_train', alpha=0.2)\n    ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], \n                  c=y_train[n_train:n_train+i], label='new_points')\n    ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[0].set_title('New points')\n    \n    ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[1].set_title('Test points');\n    ax[0].text(locs[0],locs[1],'Active Learning')\n    \n    # Random sampling\n    ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label='initial_train', alpha=0.2)\n    ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], \n                  c=y_train_rand[n_train:n_train+i], label='new_points')\n    ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[2].set_title('New points')\n    \n    ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[3].set_title('Test points');\n    ax[2].text(locs[0],locs[1],'Random Sampling');\n\n\nlocs = (2.7, 4)\nfig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True)\nax = ax.ravel()\nn_train = X_train.shape[0]-AL_iters\n\nanim = FuncAnimation(fig, func=update, frames=range(100))\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html",
    "href": "posts/2021-09-27-constraints.html",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('font', **{'size':18})"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpy",
    "href": "posts/2021-09-27-constraints.html#gpy",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPy",
    "text": "GPy\n\nfrom paramz.transformations import Logexp\n\n\ngpy_trans = Logexp()\n\n\nx = torch.arange(-1000,10000).to(torch.float)\nplt.plot(x, gpy_trans.f(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpytorch",
    "href": "posts/2021-09-27-constraints.html#gpytorch",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPyTorch",
    "text": "GPyTorch\n\nfrom gpytorch.constraints import Positive\n\n\ngpytorch_trans = Positive()\n\n\nplt.plot(x, gpytorch_trans.transform(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpflow",
    "href": "posts/2021-09-27-constraints.html#gpflow",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPFlow",
    "text": "GPFlow\n\nfrom gpflow.utilities.bijectors import positive\n\n\ngpflow_trans = positive()\n\n\nplt.plot(x, gpflow_trans(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');\n\n\n\n\n\nnp.allclose(gpy_trans.f(x), gpytorch_trans.transform(x))\n\nTrue\n\n\n\nnp.allclose(gpy_trans.f(x), gpflow_trans(x))\n\nTrue"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html",
    "href": "posts/2022-10-27-mogp.html",
    "title": "Multi-Output Gaussian Processes",
    "section": "",
    "text": "Inspired from this GPSS video.\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\n\nimport optax\n\nimport matplotlib.pyplot as plt\nfrom tinygp import kernels"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#helper-functions",
    "href": "posts/2022-10-27-mogp.html#helper-functions",
    "title": "Multi-Output Gaussian Processes",
    "section": "Helper functions",
    "text": "Helper functions\n\ndef random_fill(key, params):\n    values, unravel_fn = ravel_pytree(params)\n    random_values = jax.random.normal(key, shape=values.shape)\n    return unravel_fn(random_values)\n\ndef get_real_params(params):\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = params[f'a{i}'].reshape(n_outputs, rank)\n    if method == 'icm':\n        params['var'] = jnp.exp(params['log_var'])\n        params['scale'] = jnp.exp(params['log_scale'])\n        params['noise'] = jnp.exp(params['log_noise'])\n    elif method == 'lmc':\n        for i in range(1, q_len+1):\n            params[f'var{i}'] = jnp.exp(params[f'log_var{i}'])\n            params[f'scale{i}'] = jnp.exp(params[f'log_scale{i}'])\n            params[f'noise{i}'] = jnp.exp(params[f'log_noise{i}'])\n    return params\n\ndef kron_cov_fn(params, x1, x2, add_noise=False):\n    params = get_real_params(params)\n    a_list = [params[f'a{i}'] for i in range(1, q_len+1)]\n\n    if method == 'icm':\n        kernel_fn = params['var'] * kernels.ExpSquared(scale=params['scale'])\n        cov = kernel_fn(x1, x2)\n        if add_noise:\n            cov = cov + jnp.eye(cov.shape[0])*params['noise']\n\n        B = jax.tree_util.tree_reduce(lambda x1, x2: x1@x1.T+x2@x2.T, a_list)\n#         print(B.shape, cov.shape)\n        return jnp.kron(B, cov)\n\n    elif method == 'lmc':\n        cov_list = []\n        for idx in range(1, q_len+1):\n            kernel_fn = params[f'var{idx}'] * kernels.ExpSquared(scale=params[f'scale{idx}'])\n            cov = kernel_fn(x1, x2)\n            if add_noise:\n                cov = cov + jnp.eye(cov.shape[0])*params[f'noise{idx}']\n\n            B = a_list[idx-1]@a_list[idx-1].T\n            cov_list.append(jnp.kron(B, cov))\n            \n        return jax.tree_util.tree_reduce(lambda x1, x2: x1+x2, cov_list)"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#configuration",
    "href": "posts/2022-10-27-mogp.html#configuration",
    "title": "Multi-Output Gaussian Processes",
    "section": "Configuration",
    "text": "Configuration\n\nq_len = 2\nrank = 2 # if 1, slfm\nn_outputs = 2\n\nmethod = 'lmc' # lmc, icm\n\nif rank = 1, lmc becomes slfm.\n\nGenerative process\n\nx_key = jax.random.PRNGKey(4)\n\nx = jax.random.uniform(x_key, shape=(40, 1)).sort(axis=0)\nx_test = jnp.linspace(0,1,100).reshape(-1, 1)\n\ne1_key, e2_key = jax.random.split(x_key)\n\ne1 = jax.random.normal(e1_key, shape=(x.shape[0],))\ne2 = jax.random.normal(e2_key, shape=(x.shape[0],))\n\nif method == 'icm':\n    noise = 0.01\n    gen_kernel = 1.2*kernels.ExpSquared(scale=0.2)\n    gen_covariance = gen_kernel(x, x) + jnp.eye(x.shape[0])*noise\n    gen_chol = jnp.linalg.cholesky(gen_covariance)\n    \n    y1 = gen_chol@e1\n    y2 = gen_chol@e2\n\n    y = jnp.concatenate([y1, y2])\n    \nelif method == 'lmc':\n    noise1 = 0.01\n    noise2 = 0.1\n    gen_kernel1 = 1.2*kernels.ExpSquared(scale=0.1)\n    gen_covariance1 = gen_kernel1(x, x) + jnp.eye(x.shape[0])*noise1\n    gen_chol1 = jnp.linalg.cholesky(gen_covariance1)\n\n    gen_kernel2 = 0.8*kernels.ExpSquared(scale=0.2)\n    gen_covariance2 = gen_kernel2(x, x) + jnp.eye(x.shape[0])*noise2\n    gen_chol2 = jnp.linalg.cholesky(gen_covariance2)\n    \n    y1 = gen_chol1@e1\n    y2 = gen_chol2@e2\n\n    y = jnp.concatenate([y1, y2])\n    \n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.legend();\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\ndef loss_fn(params):\n    mo_cov = kron_cov_fn(params, x, x, add_noise=True)\n#     print(y.shape, mo_cov.shape)\n    return -jax.scipy.stats.multivariate_normal.logpdf(y, jnp.zeros_like(y), mo_cov)\n\n\nkey = jax.random.PRNGKey(1)\nif method == 'icm':\n    params = {'log_var':0.0, 'log_scale':0.0, 'log_noise':0.0}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\nelif method == 'lmc':\n    params = {}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\n        params[f'log_var{i}'] = 0.0\n        params[f'log_scale{i}'] = 0.0\n        params[f'log_noise{i}'] = 0.0\n\nparams = random_fill(key, params)\nparams\n\n{'a1': DeviceArray([[-0.764527 ,  1.0286916],\n              [-1.0690447, -0.7921495]], dtype=float32),\n 'a2': DeviceArray([[ 0.8845895, -1.1941622],\n              [-1.7434924,  1.5159688]], dtype=float32),\n 'log_noise1': DeviceArray(-1.1254696, dtype=float32),\n 'log_noise2': DeviceArray(-0.22446911, dtype=float32),\n 'log_scale1': DeviceArray(0.39719132, dtype=float32),\n 'log_scale2': DeviceArray(-0.22453257, dtype=float32),\n 'log_var1': DeviceArray(-0.7590596, dtype=float32),\n 'log_var2': DeviceArray(-0.08601531, dtype=float32)}\n\n\n\nloss_fn(params)\n\nDeviceArray(116.04026, dtype=float32)\n\n\n\nkey = jax.random.PRNGKey(3)\nparams = random_fill(key, params)\n\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), (params, loss)\n\n(tuned_params, state), (params_history, loss_history) = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(loss_history);\n\n\n\n\n\ndef predict_fn(params, x_test):\n    cov = kron_cov_fn(params, x, x, add_noise=True)\n    test_cov = kron_cov_fn(params, x_test, x_test, add_noise=True)\n    cross_cov = kron_cov_fn(params, x_test, x, add_noise=False)\n    \n    chol = jnp.linalg.cholesky(cov)\n    k_inv_y = jax.scipy.linalg.cho_solve((chol, True), y)\n    k_inv_cross_cov = jax.scipy.linalg.cho_solve((chol, True), cross_cov.T)\n\n    pred_mean = cross_cov@k_inv_y\n    pred_cov = test_cov - cross_cov@k_inv_cross_cov\n    return pred_mean, pred_cov\n\n\npred_mean, pred_cov = predict_fn(tuned_params, x_test)\npred_conf = 2 * jnp.diag(pred_cov)**0.5\n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.plot(x_test, pred_mean[:x_test.shape[0]], label='pred_y1')\nplt.plot(x_test, pred_mean[x_test.shape[0]:], label='pred_y2')\nplt.fill_between(x_test.ravel(), pred_mean[:x_test.shape[0]] - pred_conf[:x_test.shape[0]], pred_mean[:x_test.shape[0]] + pred_conf[:x_test.shape[0]], label='pred_conf_y1', alpha=0.3)\nplt.fill_between(x_test.ravel(), pred_mean[x_test.shape[0]:] - pred_conf[x_test.shape[0]:], pred_mean[x_test.shape[0]:] + pred_conf[x_test.shape[0]:], label='pred_conf_y2', alpha=0.3)\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\nfor name, value in get_real_params(tuned_params).items():\n    if not name.startswith('log_'):\n        print(name, value)\n\na1 [[0.03664799 0.00039898]\n [0.3191718  0.00344488]]\na2 [[ 0.1351072   0.00248941]\n [-0.05392759 -0.04239884]]\nnoise1 0.6797133\nnoise2 0.4154678\nscale1 5.048228\nscale2 0.10743636\nvar1 0.016275918\nvar2 41.034225"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes’ theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes’ theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom time import time\n\n# Enable high precision\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n# To enable animation inside notebook\nplt.rc(\"animation\", html=\"jshtml\")"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Create dataset",
    "text": "Create dataset\n\nfeatures, labels = make_blobs(100, n_features=2, centers=2, random_state=0)\nplt.scatter(features[:, 0], features[:, 1], c=labels);\n\n\n\n\n\nprint(features.shape, features.dtype, labels.shape, labels.dtype)\n\n(100, 2) float64 (100,) int64"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing Newton’s method (naive way)",
    "text": "Implementing Newton’s method (naive way)\nWe will first try to implement Eq. 10.31 directly from PML book1:\n\\[\n\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t} \\mathbf{H}_{t}^{-1} \\boldsymbol{g}_{t}\n\\]\n\ndef get_logits(params, feature):  # for a single data-point\n  logits = jnp.sum(feature * params[\"w\"]) + params[\"b\"]\n  return logits\n\ndef naive_loss(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n\n  # Check if label is 1 or 0\n  is_one = (label == 1)\n  loss_if_one = lambda: -jnp.log(prob)  # loss if label is 1\n  loss_if_zero = lambda: -jnp.log(1 - prob)  # loss if labels is 0\n\n  # Use lax.cond to convert if..else.. in jittable format\n  loss = jax.lax.cond(is_one, loss_if_one, loss_if_zero)\n\n  return loss\n\ndef naive_loss_batch(params, features, labels):  # for a batch of data-points\n   losses = jax.vmap(naive_loss, in_axes=(None, 0, 0))(params, features, labels)\n   return jnp.mean(losses)\n\nWriting the train function\n\ndef naive_train_step(params, features, labels, learning_rate):\n  # Find gradient\n  loss_value, grads = jax.value_and_grad(naive_loss_batch)(params, features, labels)\n\n  # Find Hessian\n  hess = jax.hessian(naive_loss_batch)(params, features, labels)\n\n  # Adjust Hessian matrix nicely\n  hess_matrix = jnp.block([[hess[\"b\"][\"b\"], hess[\"b\"][\"w\"]],\n                           [hess[\"w\"][\"b\"], hess[\"w\"][\"w\"]]])\n  \n  # Adjust gradient vector nicely\n  grad_vector = jnp.r_[grads[\"b\"], grads[\"w\"]]\n\n  # Find H^-1g\n  h_inv_g = jnp.dot(jnp.linalg.inv(hess_matrix), grad_vector)\n\n  # Get back the structure\n  h_inv_g = {\"b\": h_inv_g[0], \"w\": h_inv_g[1:]}\n\n  # Apply the update\n  params = jax.tree_map(lambda p, g: p - learning_rate*g, params, h_inv_g)\n\n  return params, loss_value\n\n# First order method\n# vg = jax.value_and_grad(naive_loss_batch)\n# def train_step(params, features, labels, learning_rate):\n#   # Find gradient\n#   loss_value, grads = vg(params, features, labels)\n\n#   # Apply the update\n#   params = jax.tree_map(lambda p, g: p - learning_rate*g, params, grads)\n\n#   return params, loss_value\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3, ))\n# \"b\" should have shape (1,) for hessian trick with jnp.block to work\nparams = {\"w\": random_params[:2], \"b\": random_params[2].reshape(1,)}\nlearning_rate = 1.0\nepochs = 20\n\ntrain_step_jitted = jax.jit(naive_train_step)\n\nhistory = {\"loss\": [], \"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels, learning_rate)\n\ninit = time()\nfor _ in range(epochs):\n  history[\"params\"].append(params)\n  params, loss_value = train_step_jitted(params, features, labels, learning_rate)\n  history[\"loss\"].append(loss_value)\nprint(time() - init, \"seconds\")\nprint(params)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n0.0015490055084228516 seconds\n{'b': DeviceArray([13.22076694], dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}\n\n\nA helper function to animate the learning.\n\ndef animate(history):\n  fig, ax = plt.subplots(1, 2, figsize=(10,4))\n  def update(idx):\n    # Clear previous frame\n    ax[0].cla()\n    ax[1].cla()\n\n    # Plot data\n    params = history[\"params\"][idx]\n    losses = history[\"loss\"][:idx]\n    ax[0].scatter(features[:, 0], features[:, 1], c=labels)\n    \n    # Calculate and plot decision boundary\n    x0_min, x0_max = features[:, 0].min(), features[:, 0].max()\n    x1_min = -(params[\"b\"] + params[\"w\"][0] * x0_min)/params[\"w\"][1]\n    x1_max = -(params[\"b\"] + params[\"w\"][0] * x0_max)/params[\"w\"][1]\n\n    ax[0].plot([x0_min, x0_max], [x1_min, x1_max], label='decision boundary')\n\n    # Plot losses\n    ax[1].plot(losses, label=\"loss\")\n    ax[1].set_xlabel(\"Iterations\")\n\n    ax[0].legend()\n    ax[1].legend()\n\n  anim = FuncAnimation(fig, update, range(epochs))\n  plt.close()\n  return anim\n\n\nanimate(history)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing IRLS algorithm",
    "text": "Implementing IRLS algorithm\n\ndef get_s_and_z(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n  s = prob * (1 - prob)\n  z = logits + (label - prob)/s\n  return s, z\n\ndef irls_train_step(params, features, labels):\n  s, z = jax.vmap(get_s_and_z, in_axes=(None, 0, 0))(params, features, labels)\n  S = jnp.diag(s.flatten())  # convert into a diagonal matrix\n\n  # Add column with ones\n  X = jnp.c_[jnp.ones(len(z)), features]\n\n  # Get weights\n  weights = jnp.linalg.inv(X.T@S@X)@X.T@S@z.flatten()\n\n  # get correct format\n  params = {\"b\": weights[0], \"w\": weights[1:]}\n\n  return params\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3,))\nparams = {\"w\": random_params[:2], \"b\": random_params[2]}\nepochs = 20\n\ntrain_step_jitted = jax.jit(irls_train_step)\n\nirls_history = {\"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels)\n\ninit = time()\nfor _ in range(epochs):\n  irls_history[\"params\"].append(params)\n  params = train_step_jitted(params, features, labels)\nprint(time() - init, \"seconds\")\nprint(params)\n\n0.0016303062438964844 seconds\n{'b': DeviceArray(13.22076694, dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Comparison",
    "text": "Comparison\n\nnaive_params_b = list(map(lambda x: x[\"b\"], history[\"params\"]))\nirls_params_b = list(map(lambda x: x[\"b\"], irls_history[\"params\"]))\n\nnaive_params_w = list(map(lambda x: x[\"w\"], history[\"params\"]))\nirls_params_w = list(map(lambda x: x[\"w\"], irls_history[\"params\"]))\n\n\nplt.plot(naive_params_b, \"o-\", label=\"Naive\")\nplt.plot(irls_params_b, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Bias\")\nplt.legend();\n\n\n\n\n\nplt.plot(naive_params_w, \"o-\", label=\"Naive\")\nplt.plot(irls_params_w, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Weights\")\nplt.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport GPy\nimport jax\nimport gpytorch\nimport botorch\nimport tinygp\nimport jax.numpy as jnp\nimport optax\nfrom IPython.display import clear_output\n\nfrom sklearn.preprocessing import StandardScaler\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Data",
    "text": "Data\n\nnp.random.seed(0) # We don't want surprices in a presentation :)\nN = 10\ntrain_x = torch.linspace(0, 1, N)\ntrain_y = torch.sin(train_x * (2 * math.pi)) + torch.normal(0, 0.1, size=(N,))\n \ntest_x = torch.linspace(0, 1, N*10)\ntest_y = torch.sin(test_x * (2 * math.pi))\n\n\nplt.plot(train_x, train_y, 'ko', label='train');\nplt.plot(test_x, test_y, label='test');\nplt.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Defining kernel",
    "text": "Defining kernel\n\\[\\begin{equation}\n\\sigma_f^2 = \\text{variance}\\\\\n\\ell = \\text{lengthscale}\\\\\nk_{RBF}(x_1, x_2) = \\sigma_f^2 \\exp \\left[-\\frac{\\lVert x_1 - x_2 \\rVert^2}{2\\ell^2}\\right]\n\\end{equation}\\]\n\nGPy\n\ngpy_kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\ngpy_kernel\n\n\n\n\n\n\nrbf.\nvalue\nconstraints\npriors\n\n\nvariance\n1.0\n+ve\n\n\n\nlengthscale\n1.0\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\ngpytorch_kernel.outputscale = 1. # variance\ngpytorch_kernel.base_kernel.lengthscale = 1. # lengthscale\n\ngpytorch_kernel\n\nScaleKernel(\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n)\n\n\n\n\nTinyGP\n\ndef RBFKernel(variance, lengthscale):\n    return jnp.exp(variance) * tinygp.kernels.ExpSquared(scale=jnp.exp(lengthscale))\n    \ntinygp_kernel = RBFKernel(variance=1., lengthscale=1.)\ntinygp_kernel\n\n&lt;tinygp.kernels.Product at 0x7f544039d710&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Define model",
    "text": "Define model\n\\[\n\\sigma_n^2 = \\text{noise variance}\n\\]\n\nGPy\n\ngpy_model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], gpy_kernel)\ngpy_model.Gaussian_noise.variance = 0.1\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 16.757933772959404\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n1.0\n+ve\n\n\n\nrbf.lengthscale\n1.0\n+ve\n\n\n\nGaussian_noise.variance\n0.1\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, kernel):\n        super().__init__(train_x, train_y, likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ngpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngpytorch_model = ExactGPModel(train_x, train_y, gpytorch_likelihood, gpytorch_kernel)\n\ngpytorch_model.likelihood.noise = 0.1\ngpytorch_model\n\nExactGPModel(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\n\nTinyGP\n\ndef build_gp(theta, X):\n    mean = theta[0] \n    variance, lengthscale, noise_variance = jnp.exp(theta[1:])\n    \n    kernel = variance * tinygp.kernels.ExpSquared(lengthscale)\n    \n    return tinygp.GaussianProcess(kernel, X, diag=noise_variance, mean=mean)\n\ntinygp_model = build_gp(theta=np.array([0., 1., 1., 0.1]), X=train_x.numpy())\n\ntinygp_model\n# __repr__\n\n&lt;tinygp.gp.GaussianProcess at 0x7f5440401850&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Train the model",
    "text": "Train the model\n\nGPy\n\ngpy_model.optimize(max_iters=50)\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 3.944394423452163\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n0.9376905183253631\n+ve\n\n\n\nrbf.lengthscale\n0.2559000163858406\n+ve\n\n\n\nGaussian_noise.variance\n0.012506184441481319\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(gpytorch_likelihood, gpytorch_model)\nbotorch.fit_gpytorch_model(mll)\n\ndisplay(gpytorch_model.mean_module.constant, # Mean\n        gpytorch_model.covar_module.outputscale, # Variance\n        gpytorch_model.covar_module.base_kernel.lengthscale, # Lengthscale \n        gpytorch_model.likelihood.noise) # Noise variance\n\n /opt/conda/lib/python3.7/site-packages/botorch/fit.py:143: UserWarning:CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/c10/cuda/CUDAFunctions.cpp:112.)\n\n\nParameter containing:\ntensor([0.0923], requires_grad=True)\n\n\ntensor(0.9394, grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([[0.2560]], grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([0.0124], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nTinyGP\n\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(theta, X, y):\n    gp = build_gp(theta, X)\n    return -gp.condition(y)\n\n\nobj = jax.jit(jax.value_and_grad(neg_log_likelihood))\nresult = minimize(obj, [0., 1., 1., 0.1], jac=True, args=(train_x.numpy(), train_y.numpy()))\nresult.x[0], np.exp(result.x[1:])\n\n(0.09213499552879165, array([0.9395271 , 0.25604163, 0.01243025]))"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Inference",
    "text": "Inference\n\ndef plot_gp(pred_y, var_y):\n    std_y = var_y ** 0.5\n    plt.figure()\n    plt.scatter(train_x, train_y, label='train')\n    plt.plot(test_x, pred_y, label='predictive mean')\n    plt.fill_between(test_x.ravel(), \n                     pred_y.ravel() - 2*std_y.ravel(), \n                     pred_y.ravel() + 2*std_y.ravel(), alpha=0.2, label='95% confidence')\n    plt.legend()\n\n\nGPy\n\npred_y, var_y = gpy_model.predict(test_x.numpy()[:, None])\nplot_gp(pred_y, var_y)\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_model.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = gpytorch_likelihood(gpytorch_model(test_x))\n    pred_y, var_y = pred_dist.mean, pred_dist.variance\n    plot_gp(pred_y, var_y)\n\n\n\n\n\n\nTinyGP\n\ntinygp_model = build_gp(result.x, train_x.numpy())\npred_y, var_y = tinygp_model.predict(train_y.numpy(), test_x.numpy(), return_var=True)\n\nplot_gp(pred_y, var_y)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Tiny GP on CO2 dataset",
    "text": "Tiny GP on CO2 dataset\n\ndata = pd.read_csv(\"data/co2.csv\")\n\n# Train test split\nX = data[\"0\"].iloc[:290].values.reshape(-1, 1)\nX_test = data[\"0\"].iloc[290:].values.reshape(-1, 1)\ny = data[\"1\"].iloc[:290].values\ny_test = data[\"1\"].iloc[290:].values\n\n# Scaling the dataset\nXscaler = StandardScaler()\nX = Xscaler.fit_transform(X)\nX_test = Xscaler.transform(X_test)\n\nyscaler = StandardScaler()\ny = yscaler.fit_transform(y.reshape(-1, 1)).ravel()\ny_test = yscaler.transform(y_test.reshape(-1, 1)).ravel()\n\n\nplt.plot(X, y, label='train');\nplt.plot(X_test, y_test, label='test');\nplt.legend();\n\n\n\n\n\nclass SpectralMixture(tinygp.kernels.Kernel):\n    def __init__(self, weight, scale, freq):\n        self.weight = jnp.atleast_1d(weight)\n        self.scale = jnp.atleast_1d(scale)\n        self.freq = jnp.atleast_1d(freq)\n\n    def evaluate(self, X1, X2):\n        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n        return jnp.sum(\n            self.weight\n            * jnp.prod(\n                jnp.exp(-2 * jnp.pi ** 2 * tau ** 2 / self.scale ** 2)\n                * jnp.cos(2 * jnp.pi * self.freq * tau),\n                axis=-1,\n            )\n        )\n    \ndef build_spectral_gp(theta):\n    kernel = SpectralMixture(\n        jnp.exp(theta[\"log_weight\"]),\n        jnp.exp(theta[\"log_scale\"]),\n        jnp.exp(theta[\"log_freq\"]),\n    )\n    return tinygp.GaussianProcess(\n        kernel, X, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n    )\n\n\nK = 4 # Number of mixtures\ndiv_factor = 0.4\nnp.random.seed(1)\nparams = {\n    \"log_weight\": np.abs(np.random.rand(K))/div_factor,\n    \"log_scale\": np.abs(np.random.rand(K))/div_factor,\n    \"log_freq\": np.abs(np.random.rand(K))/div_factor,\n    \"log_diag\": np.abs(np.random.rand(1))/div_factor,\n    \"mean\": 0.,\n}\n\n@jax.jit\n@jax.value_and_grad\ndef loss(theta):\n    return -build_spectral_gp(theta).condition(y)\n# opt = optax.sgd(learning_rate=0.001)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\nlosses = []\nfor i in range(100):\n    loss_val, grads = loss(params)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    losses.append(loss_val)\n    clear_output(wait=True)\n    print(f\"iter {i}, loss {loss_val}\")\n\nopt_gp = build_spectral_gp(params)\n\nparams\n\niter 99, loss 27.987701416015625\n\n\n{'log_diag': DeviceArray([-2.7388687], dtype=float32),\n 'log_freq': DeviceArray([-3.6072493, -3.1795945, -3.4490397, -2.373117 ], dtype=float32),\n 'log_scale': DeviceArray([3.9890492, 3.8530042, 4.0878096, 4.4860597], dtype=float32),\n 'log_weight': DeviceArray([-1.3715047, -0.6132469, -2.413771 , -1.6582283], dtype=float32),\n 'mean': DeviceArray(0.38844627, dtype=float32)}\n\n\n\nplt.plot(losses);\n\n\n\n\n\nmu, var = opt_gp.predict(y, X_test, return_var=True)\n\nplt.plot(X, y, c='k')\nplt.fill_between(\n    X_test.ravel(), mu + np.sqrt(var), mu - np.sqrt(var), color=\"C0\", alpha=0.5\n)\nplt.plot(X_test, mu, color=\"C0\", lw=2)\n\n# plt.xlim(t.min(), 2025)\nplt.xlabel(\"year\")\n_ = plt.ylabel(\"CO$_2$ in ppm\")"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)",
    "text": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)",
    "text": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)\n\nMinimize inverse term fully\nNow, Minimize both togather"
  },
  {
    "objectID": "posts/2023-05-31-CNPs_for_Images.html",
    "href": "posts/2023-05-31-CNPs_for_Images.html",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# turn off preallocation by JAX\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nimport distrax as dx\n\nimport optax\n\n# load mnist dataset from tensorflow datasets\nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n# define initializers\ndef first_layer_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-1.0/num_input, maxval=1.0/num_input)\n\ndef other_layers_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-np.sqrt(6 / num_input)/30, maxval=np.sqrt(6 / num_input)/30)\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n    \n    \n    for n_features in self.features[1:]:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n\n    for n_features in self.features:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.output_dim*2)(x)\n    loc, raw_scale = x[:, :self.output_dim], x[:, self.output_dim:]\n    scale = jnp.exp(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features, self.output_dim)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=0.005+scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/2023-05-31-CNPs_for_Images.html#load-mnist",
    "href": "posts/2023-05-31-CNPs_for_Images.html#load-mnist",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "Load MNIST",
    "text": "Load MNIST\n\nds = tfds.load('mnist')\n\n\ndef dataset_to_arrays(dataset):\n    data = []\n    labels = []\n    stopper = 0\n    end = 100\n    for sample in dataset:\n        data.append(sample[\"image\"].numpy())\n        labels.append(sample[\"label\"].numpy())\n        stopper += 1\n        if stopper == end:\n            break\n    return np.array(data), np.array(labels)[..., None]\n\ntrain_data, train_labels = dataset_to_arrays(ds[\"train\"])\ntest_data, test_labels = dataset_to_arrays(ds[\"test\"])\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n2023-06-02 09:58:48.609001: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2023-06-02 09:58:48.681190: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n((100, 28, 28, 1), (100, 1), (100, 28, 28, 1), (100, 1))\n\n\n\ncoords = np.linspace(-1, 1, 28)\nx, y = np.meshgrid(coords, coords)\ntrain_X = jnp.stack([x, y], axis=-1).reshape(-1, 2)\n\ntrain_y = jax.vmap(lambda x: x.reshape(-1, 1))(train_data) / 255.0\ntrain_X.shape, train_y.shape, type(train_X), type(train_y)\n\n((784, 2),\n (100, 784, 1),\n jaxlib.xla_extension.ArrayImpl,\n jaxlib.xla_extension.ArrayImpl)\n\n\n\niterations = 10000\n\ndef loss_fn(params, context_X, context_y, target_X, target_y):\n  def loss_fn_per_sample(context_X, context_y, target_X, target_y):\n    loc, scale = model.apply(params, context_X, context_y, target_X)\n    # predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    # return -predictive_distribution.log_prob(target_y)\n    return jnp.square(loc.ravel() - target_y.ravel()).mean()\n  \n  return jax.vmap(loss_fn_per_sample, in_axes=(None, 0, None, 0))(context_X, context_y, target_X, target_y).mean()\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nmodel = CNP([256]*2, 128, [256]*4, 1)\nparams = model.init(jax.random.PRNGKey(0), train_X, train_y[0], train_X)\noptimizer = optax.adam(1e-5)\nstate = optimizer.init(params)\n\n# losses = []\n# for iter in tqdm(range(iterations)):\n#   tmp_index = jax.random.permutation(jax.random.PRNGKey(iter), index)\n#   context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n#   context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n#   target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n#   target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  \n#   # print(context_X.shape, context_y.shape, target_X.shape, target_y.shape)\n#   # print(loss_fn(params, context_X, context_y, target_X, target_y).shape)\n  \n#   loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n#   updates, state = optimizer.update(grads, state)\n#   params = optax.apply_updates(params, updates)\n#   losses.append(loss.item())\n\ndef one_step(params_and_state, key):\n  params, state = params_and_state\n  tmp_index = jax.random.permutation(key, train_X.shape[0])\n  context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n  context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n  target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n  target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n  updates, state = optimizer.update(grads, state)\n  params = optax.apply_updates(params, updates)\n  return (params, state), loss\n\n(params, state), loss_history = jax.lax.scan(one_step, (params, state), jax.random.split(jax.random.PRNGKey(0), iterations))\n\n\nplt.plot(loss_history[10:]);\n\n\n\n\n\ntest_key = jax.random.PRNGKey(0)\ntmp_index = jax.random.permutation(test_key, train_X.shape[0])\ncontext_X = train_X[tmp_index][:int(train_X.shape[0]*0.5)]\ncontext_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.5), :]\ntarget_X = train_X#[tmp_index][int(train_X.shape[0]*0.5):]\ntarget_y = train_y#[:, tmp_index, :][:, int(train_X.shape[0]*0.5):, :]\n\nid = 91\nplt.imshow(train_y[id].reshape(28, 28), cmap=\"gray\", interpolation=None);\n\nlocs, scales = jax.vmap(model.apply, in_axes=(None, None, 0, None))(params, context_X, context_y, target_X)\n# full_preds = jnp.concatenate([context_y, locs], axis=1)\n# full_preds = full_preds.at[:, tmp_index, :].set(full_preds).__array__()\n\nplt.figure()\nplt.imshow(locs[id].reshape(28, 28), cmap=\"gray\", interpolation=None);"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html",
    "title": "Programatically download OpenAQ data",
    "section": "",
    "text": "# uncomment to install these libraries\n# !pip install boto3 botocore\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport boto3\nimport botocore\nimport os\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "title": "Programatically download OpenAQ data",
    "section": "Setup",
    "text": "Setup\n\ns3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\nbucket_name = 'openaq-fetches'\nprefix = 'realtime-gzipped/'\n\npath = '/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/'\n\nstart_date = '2020/01/01' # start date (inclusive)\nend_date = '2020/12/31' # end date (inclusive)\n\n\nDownload\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  clear_output(wait=True)\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  print('Downloading:', date)\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    f_name = file_obj['Key']\n    tmp_path = '/'.join((path+f_name).split('/')[:-1])\n    \n    if not os.path.exists(tmp_path):\n      os.makedirs(tmp_path)\n    \n    s3.download_file(bucket_name, f_name, path+f_name)\n\nDownloading: 2020-05-04\n\n\n\n\nValidate\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    assert os.path.exists(path+file_obj['Key']), file_obj['Key']\n\n\nprint('Validated')"
  },
  {
    "objectID": "posts/2021-10-23-warped-gp.html",
    "href": "posts/2021-10-23-warped-gp.html",
    "title": "Input Warped GPs - A failed idea",
    "section": "",
    "text": "Comments\n\nWe are warping inputs \\(\\mathbf{x}\\) into \\(\\mathbf{w}\\cdot\\mathbf{x}\\)\nLearning second level GP over \\(\\mathbf{w}\\).\nAppling penalty over \\(\\mathbf{w}\\) if varies too much unnecessary.\nSee problems at the end of the notebook.\nWe need to check mathematical concerns related to this transformation.\n\n\nimport math\nimport numpy as np\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport regdata as rd\nfrom sklearn.cluster import KMeans\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nclass ExactNSGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_latent):\n        super(ExactNSGPModel, self).__init__(train_x, train_y, likelihood)\n#         inds = np.random.choice(train_x.shape[0], size=num_latent, replace=False)\n#         self.x_bar = train_x[inds]\n        self.x_bar = torch.tensor(KMeans(n_clusters=num_latent).fit(train_x).cluster_centers_).to(train_x)\n        self.w_bar = torch.nn.Parameter(torch.ones(num_latent,).to(self.x_bar))\n        self.bias = torch.nn.Parameter(torch.zeros(1,).to(self.x_bar))\n        self.latent_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n#       We can fix noise to be minimum but it is not ideal. Ideally, noise should automatically reduce to reasonable value.\n#         self.latent_likelihood.raw_noise.requires_grad = False\n#         self.latent_likelihood.raw_noise = torch.tensor(-10.)\n        self.latent_model = ExactGPModel(self.x_bar, self.w_bar, self.latent_likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        self.latent_model.eval()\n        with gpytorch.settings.detach_test_caches(False):  # needed to back propagate thru predictive posterior\n            self.latent_model.set_train_data(self.x_bar, self.w_bar, strict=False)\n            self.w = self.latent_likelihood(self.latent_model(x))  # predictive posterior\n        x_warped = x*self.w.mean[:, None] + self.bias\n        mean_x = self.mean_module(x_warped)\n        covar_x = self.covar_module(x_warped)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\ndef training(model, likelihood):\n    training_iter = 100\n\n    # Find optimal model hyperparameters\n    model.train()\n    likelihood.train()\n\n    # Use the adam optimizer\n    optimizer = torch.optim.Adam([\n        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n    ], lr=0.1)\n\n    # \"Loss\" for GPs - the marginal log likelihood\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    for i in range(training_iter):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        output = model(train_x)\n        # Calc loss and backprop gradients\n        try:\n            loss = -mll(output, train_y) + torch.square(model.w.mean-1).mean()\n#             print(model.latent_likelihood.noise)\n        except AttributeError:\n            loss = -mll(output, train_y)\n        loss.backward()\n#         print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n#             i + 1, training_iter, loss.item(),\n#             model.covar_module.base_kernel.lengthscale.item(),\n#             model.likelihood.noise.item()\n#         ))\n        optimizer.step()\n    \ndef predict_plot(model, likelihood, title):\n    # Get into evaluation (predictive posterior) mode\n    model.eval()\n    likelihood.eval()\n\n    # Test points are regularly spaced along [0,1]\n    # Make predictions by feeding model through likelihood\n    with torch.no_grad():\n        observed_pred = likelihood(model(test_x))\n\n    with torch.no_grad():\n        # Initialize plot\n        f, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n        # Get upper and lower confidence bounds\n        lower, upper = observed_pred.confidence_region()\n        # Plot training data as black stars\n        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n        # Plot predictive means as blue line\n        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n        # Shade between the lower and upper confidence bounds\n        ax.fill_between(test_x.numpy().ravel(), lower.numpy(), upper.numpy(), alpha=0.5)\n        ax.legend(['Observed Data', 'Mean', 'Confidence'])\n        ax.set_title(title)\n    return observed_pred\n\n\ndef GP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactGPModel(train_x, train_y, likelihood)\n    \n    training(model, likelihood)\n    predict_plot(model, likelihood, 'GP')\n\ndef NSGP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactNSGPModel(train_x, train_y, likelihood, num_latent)\n    \n    training(model, likelihood)\n    observed_pred = predict_plot(model, likelihood, 'NSGP')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x*model.w.mean[:, None], observed_pred.mean.numpy())\n        plt.title('Warped test inputs v/s test outputs')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x, model.w.mean, label='interpolated')\n        plt.scatter(model.x_bar, model.w_bar, label='learned')\n        plt.ylim(0,2)\n        plt.title('Test input v/s weights')\n        plt.legend()\n\n\n\nTesting over various datasets\n\ntrain_x, train_y, test_x = rd.DellaGattaGene(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Heinonen4(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Jump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.MotorcycleHelmet(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Olympic(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineJump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineNoisy(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblems\n\nTransformation from x to x_warped is not monotonic."
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html",
    "href": "posts/2022-08-01-conditional_neural_processes.html",
    "title": "Conditional Neural Processes in JAX",
    "section": "",
    "text": "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n# https://github.com/tensorflow/probability/issues/1523\nimport logging\n\nlogger = logging.getLogger()\n\n\nclass CheckTypesFilter(logging.Filter):\n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n  import flax.linen as nn\nexcept ModuleNotFoundError:\n  %pip install flax\n  import flax.linen as nn\n\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install optax\n  import optax\n\ntry:\n  import tensorflow_probability.substrates.jax as tfp\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#model",
    "href": "posts/2022-08-01-conditional_neural_processes.html#model",
    "title": "Conditional Neural Processes in JAX",
    "section": "Model",
    "text": "Model\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(2)(x)\n    loc, raw_scale = x[:, 0], x[:, 1]\n    scale = jax.nn.softplus(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#data",
    "href": "posts/2022-08-01-conditional_neural_processes.html#data",
    "title": "Conditional Neural Processes in JAX",
    "section": "Data",
    "text": "Data\n\nN = 100\nseed = jax.random.PRNGKey(0)\nx = jnp.linspace(-1, 1, N).reshape(-1, 1)\nf = lambda x: (jnp.sin(10*x) + x).flatten()\nnoise = jax.random.normal(seed, shape=(N,)) * 0.2\ny = f(x) + noise\n\nx_test = jnp.linspace(-2, 2, N*2+10).reshape(-1, 1)\ny_test = f(x_test) \n\nplt.scatter(x, y, label='train', zorder=5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.legend();"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#training",
    "href": "posts/2022-08-01-conditional_neural_processes.html#training",
    "title": "Conditional Neural Processes in JAX",
    "section": "Training",
    "text": "Training\n\ndef train_fn(model, optimizer, seed, n_iterations, n_context):\n  params = model.init(seed, x, y, x)\n  value_and_grad_fn = jax.value_and_grad(model.loss_fn)\n  state = optimizer.init(params)\n  indices = jnp.arange(N)\n  \n  def one_step(params_and_state, seed):\n    params, state = params_and_state\n    shuffled_indices = jax.random.permutation(seed, indices)\n    context_indices = shuffled_indices[:n_context]\n    target_indices = shuffled_indices[n_context:]\n    x_context, y_context = x[context_indices], y[context_indices]\n    x_target, y_target = x[target_indices], y[target_indices]\n    loss, grads = value_and_grad_fn(params, x_context, y_context, x_target, y_target)\n    updates, state = optimizer.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n\n  seeds = jax.random.split(seed, num=n_iterations)\n  (params, state), losses = jax.lax.scan(one_step, (params, state), seeds)\n  return params, losses\n\n\nencoder_features = [64, 16, 8]\nencoding_dims = 1\ndecoder_features = [16, 8]\nmodel = CNP(encoder_features, encoding_dims, decoder_features)\noptimizer = optax.adam(learning_rate=0.001)\n\nseed = jax.random.PRNGKey(2)\nn_context = int(0.7 * N)\nn_iterations = 20000\n\nparams, losses = train_fn(model, optimizer, seed, n_iterations=n_iterations, n_context=n_context)\n\n\nplt.plot(losses);"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "href": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "title": "Conditional Neural Processes in JAX",
    "section": "Predict",
    "text": "Predict\n\nloc, scale = model.apply(params, x, y, x_test)\nlower, upper = loc - 2*scale, loc + 2*scale\n\nplt.scatter(x, y, label='train', alpha=0.5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.plot(x_test, loc);\nplt.fill_between(x_test.flatten(), lower, upper, alpha=0.4);\nplt.ylim(-5, 5);"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html",
    "href": "posts/2022-03-08-torch-essentials.html",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "href": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "href": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "title": "Torch essentials",
    "section": "Gradient is all you need",
    "text": "Gradient is all you need\n\nimport matplotlib.pyplot as plt\n\n\nx = torch.rand(5,1)\ny = 3 * x + 2 + torch.randn_like(x)*0.1\n\nplt.scatter(x, y);\n\n\n\n\n\nx_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\nx_plus_ones.shape\n\ntorch.Size([5, 2])\n\n\n\ntheta = torch.zeros(2,1, requires_grad=True)\ntheta\n\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\ntheta.grad\n\n\ntheta.grad_fn\n\n\nlr = 0.1\n\ny_pred = x_plus_ones@theta\nloss = ((y_pred - y)**2).mean()\nloss.backward()\n# y_pred = torch.matmul(x_plus_ones, theta)\n# y_pred = torch.mm(x_plus_ones, theta)\n\n\ntheta.grad # dloss/dtheta\n\ntensor([[-6.3681],\n        [-2.8128]])\n\n\n\ntheta.grad_fn\n\n\ntheta.data -= lr * theta.grad.data\n\n\ntheta\n\ntensor([[0.6368],\n        [0.2813]], requires_grad=True)\n\n\n\ntheta.grad_fn\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)\n\n\n\n\n\nfor i in range(10):\n    theta.grad.data.zero_()\n    y_pred = x_plus_ones@theta\n    loss = ((y_pred - y)**2).mean()\n    loss.backward()\n    theta.data -= lr * theta.grad\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#advanced",
    "href": "posts/2022-03-08-torch-essentials.html#advanced",
    "title": "Torch essentials",
    "section": "Advanced",
    "text": "Advanced\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = torch.nn.Parameter(torch.zeros(2,1))\n#         self.register_parameter(theta, torch.zeros(2,1))\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = x_plus_ones@self.theta\n        return y_pred\n\n\nmodel = LinearRegression()\nmodel\n\nLinearRegression()\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value)\n\ntheta Parameter containing:\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    \n    optimizer.step()\n\n\nmodel.state_dict()\n\nOrderedDict([('theta',\n              tensor([[0.9799],\n                      [0.9808]]))])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "href": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "title": "Torch essentials",
    "section": "Wanna run on GPU?",
    "text": "Wanna run on GPU?\n\nx_gpu = x.to(device)\ny_gpu = y.to(device)\n\n\nprint(model.theta)\nmodel.to(device)\nprint(model.theta)\n\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], requires_grad=True)\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], device='cuda:0', requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x_gpu)\n    loss = loss_fn(y_pred, y_gpu)\n    loss.backward()\n    \n    optimizer.step()"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "href": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "title": "Torch essentials",
    "section": "State dictionary",
    "text": "State dictionary\n\n# torch.save(model.state_dict(), path)\n# model.load_state_dict(torch.load(path))"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#nn-way",
    "href": "posts/2022-03-08-torch-essentials.html#nn-way",
    "title": "Torch essentials",
    "section": "NN way",
    "text": "NN way\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(2, 1) # torch.nn.Linear(128, 64)\n        # What else? \n#         self.activation = torch.nn.ReLU()\n#         torch.nn.LSTM()\n#         torch.nn.Conv2d()\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = self.layer(x_plus_ones)\n        return y_pred"
  },
  {
    "objectID": "posts/2022-01-20-kl-divergence.html",
    "href": "posts/2022-01-20-kl-divergence.html",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/2022-01-20-kl-divergence.html#ground",
    "href": "posts/2022-01-20-kl-divergence.html#ground",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/2022-01-20-kl-divergence.html#kl-divergence",
    "href": "posts/2022-01-20-kl-divergence.html#kl-divergence",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence",
    "text": "KL divergence\nWe can use KL divergence to check how good is our model. The formula is:\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} p_G(y_i)\\log\\frac{p_G(y_i)}{p_P(y_i)}\n\\]\nFor our example,\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\log\\frac{1}{0.8}\n\\]\nIt is evident that if \\(p_P(y = L2)\\) decreses from \\(0.8\\), \\(D_{KL}(p_G\\;\\rVert\\;p_P)\\) will increase and vice versa. Note that KL divergence is not symmetric which means \\(D_{KL}(p_G\\;\\rVert\\;p_P) \\ne D_{KL}(p_P\\;\\rVert\\;p_G)\\)."
  },
  {
    "objectID": "posts/2022-01-20-kl-divergence.html#cross-entory",
    "href": "posts/2022-01-20-kl-divergence.html#cross-entory",
    "title": "KL divergence v/s cross-entropy",
    "section": "Cross-entory",
    "text": "Cross-entory\nCross-entropy is another measure for distribution similarity. The formula is:\n\\[\nH(p_G, p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} - p_G(y_i)\\log p_P(y_i)\n\\]\nFor our example:\n\\[\nH(p_G, p_P) = -\\log 0.8 = \\log \\frac{1}{0.8}\n\\]"
  },
  {
    "objectID": "posts/2022-01-20-kl-divergence.html#kl-divergence-vs-cross-entropy",
    "href": "posts/2022-01-20-kl-divergence.html#kl-divergence-vs-cross-entropy",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence v/s cross-entropy",
    "text": "KL divergence v/s cross-entropy\nThis shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence?\nThat’s because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization."
  },
  {
    "objectID": "posts/2021-10-26-anonymization-tips.html",
    "href": "posts/2021-10-26-anonymization-tips.html",
    "title": "Anonymization tips for double-blind submission",
    "section": "",
    "text": "Use following command locally to search for author names, institute name and other terms you think may violate double-blind\n\ngit grep &lt;query&gt;\n\nAbove command matches the query everywhere and thus a safe way. Avoid GitHub search for this purpose, it fails to identify some terms many times and there is no regex there (yet)!\n\n\nDo not use full paths inside README file. If you move content in other repo, the links will either become unusable or may violate double-blind. So follow the example below.\n\nBad practice: [link](https://github.com/patel-zeel/reponame/blob/master/dataset)\nGood practice: [link](dataset)\n\nPoint no. 2 does not work for GitHub pages links (username.github.io/stuff). Thus, keep in mind to manually update those (if you have a better idea, let everyone know in comments below)\nDownload the repo zip locally and create an anonymized repository in your anonymized GitHub account. Open the GitHub web editor by pressing “.” (dot) at repo homepage.\nNow, you can select and drag all folders to the left pan of the web editor to upload them at once. Finally, commit with a meaningfull message and the changes will automatically be uploaded to the mail branch of your anonymized repo.\nUpdate the link in your manuscipt and submit !!\n\n\nEdit:\nAfter acceptance, transfer the ownership to personal account and delete the ownership of anonymized account from the personal account. This will remove all the traces of repository from the anonymized account. However, repository will still show that the commits were made by anonymized account which is anyway not violation of the doule-blind explicitely."
  },
  {
    "objectID": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "href": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "title": "Active Learning with Bayesian Linear Regression",
    "section": "",
    "text": "A quick wrap-up for Bayesian Linear Regression (BLR)\nWe have a feature matrix \\(X\\) and a target vector \\(Y\\). We want to obtain \\(\\theta\\) vector in such a way that the error \\(\\epsilon\\) for the following equation is minimum.\n\\[\nY = X^T\\theta + \\epsilon\n\\] Prior PDF for \\(\\theta\\) is,\n\\[\np(\\theta) \\sim \\mathcal{N}(M_0, S_0)\n\\]\nWhere \\(S_0\\) is prior covariance matrix, and \\(M_0\\) is prior mean.\nPosterier PDF can be given as,\n\\[\n\\begin{aligned}\np(\\theta|X,Y) &\\sim \\mathcal{N}(\\theta | M_n, S_n) \\\\\nS_n &= (S_0^{-1} + \\sigma_{mle}^{-2}X^TX) \\\\\nM_n &= S_n(S_0^{-1}M_0+\\sigma_{mle}^{-2}X^TY)\n\\end{aligned}\n\\]\nMaximum likelihood estimation of \\(\\sigma\\) can be calculated as,\n\\[\n\\begin{aligned}\n\\theta_{mle} &= (X^TX)^{-1}X^TY \\\\\n\\sigma_{mle} &= ||Y - X^T\\theta_{mle}||\n\\end{aligned}\n\\]\nFinally, predicted mean \\(\\hat{Y}_{mean}\\) and predicted covariance matrix \\(\\hat{Y}_{cov}\\) can be given as,\n\\[\n\\begin{aligned}\n\\hat{Y} &\\sim \\mathcal{N}(\\hat{Y}_{mean}, \\hat{Y}_{cov}) \\\\\n\\hat{Y}_{mean} &= XM_n \\\\\n\\hat{Y}_{cov} &= X^TS_nX\n\\end{aligned}\n\\]\nNow, let’s put everything together and write a class for Bayesian Linear Regression.\n\n\nCreating scikit-learn like class with fit predict methods for BLR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 0 # random seed for train_test_split\n\n\nclass BLR():\n  def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix\n    self.S0 = S0\n    self.M0 = M0\n\n  def fit(self,x,y, return_self = False):\n    self.x = x\n    self.y = y\n\n    # Maximum likelihood estimation for sigma parameter\n    theta_mle = np.linalg.pinv(x.T@x)@(x.T@y)\n    sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2\n    sigma_mle = np.sqrt(sigma_2_mle)\n\n    # Calculating predicted mean and covariance matrix for theta\n    self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x)\n    self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze())\n\n    # Calculating predicted mean and covariance matrix for data\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    if return_self:\n      return (self.y_hat_map, self.pred_var)\n    \n  def predict(self, x):\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    return (self.y_hat_map, self.pred_var)\n\n  def plot(self, s=1): # s -&gt; size of dots for scatter plot\n    individual_var = self.pred_var.diagonal()\n    plt.figure()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.plot(self.x[:,1], self.y_hat_map, color='black', label='model')\n    plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color='black', label='uncertainty')\n    plt.scatter(self.x[:,1], self.y, label='actual data',s=s)\n    plt.title('MAE is '+str(np.mean(np.abs(self.y - self.y_hat_map))))\n    plt.legend()\n\n\n\nCreating & visualizing dataset\nTo start with, let’s create a random dataset with degree 3 polynomial function with some added noise.\n\\[\nY = (5X^3 - 4X^2 + 3X - 2) + \\mathcal{N}(0,1)\n\\]\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, )\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\nWe’ll try to fit a degree 5 polynomial function to our data.\n\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nN_features = X.shape[1]\n\n\nplt.scatter(X[:,1], Y, s=0.5, label = 'data points')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\nLearning a BLR model on the entire data\nWe’ll take \\(M_0\\) (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about \\(M_0\\). We’re taking \\(S_0\\) (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other.\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\n\n\n\nVisualising the fit\n\nmodel.plot(s=0.5)\n\n\n\n\nThis doesn’t look like a good fit, right? Let’s set the prior closer to the real values and visualize the fit again.\n\n\nVisualising the fit after changing the prior\n\nnp.random.seed(seed)\nS0 = np.eye(N_features)\nM0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, )\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\nHmm, better. Now let’s see how it fits after reducing the noise and setting the prior mean to zero vector again.\n\n\nVisualising the fit after reducing the noise\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\nWhen the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit.\n\n\nIntuition to Active Learning (Uncertainty Sampling) with an example\nLet’s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How?\nWe train our model with existing data and test it on all the suspected patients’ data. Let’s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing.\nThis method is called Uncertainty Sampling in Active Learning. Now let’s formally define Active Learning. From Wikipedia,\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.\nNow, we’ll go through the active learning procedure step by step.\n\n\nTrain set, test set, and pool. What is what?\nThe train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty.\nSo, the algorithm can be represented as the following,\n\nTrain the model with the train set.\nTest the performance on the test set (This is expected to improve).\nTest the model with the pool.\nQuery for the most uncertain datapoint from the pool.\nAdd that datapoint into the train set.\nRepeat step 1 to step 5 for \\(K\\) iterations (\\(K\\) ranges from \\(0\\) to the pool size).\n\n\n\nCreating initial train set, test set, and pool\nLet’s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let’s start with 2 data points as the train set.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed)\n\nVisualizing train, test and pool.\n\nplt.scatter(test_X[:,1], test_Y, label='test set',color='r', s=2)\nplt.scatter(train_X[:,1], train_Y, label='train set',marker='s',color='k', s=50)\nplt.scatter(pool_X[:,1], pool_Y, label='pool',color='b', s=2)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n\n\n\n\nLet’s initialize a few dictionaries to keep track of each iteration.\n\ntrain_X_iter = {} # to store train points at each iteration\ntrain_Y_iter = {} # to store corresponding labels to the train set at each iteration\nmodels = {} # to store the models at each iteration\nestimations = {} # to store the estimations on the test set at each iteration\ntest_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n\n\n\nTraining & testing initial learner on train set (Iteration 0)\nNow we will train the model for the initial train set, which is iteration 0.\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\nCreating a plot method to visualize train, test and pool with estimations and uncertainty.\n\ndef plot(ax, model, init_title=''):\n  # Plotting the pool\n  ax.scatter(pool_X[:,1], pool_Y, label='pool',s=1,color='r',alpha=0.4)\n  \n  # Plotting the test data\n  ax.scatter(test_X[:,1], test_Y, label='test data',s=1, color='b', alpha=0.4)\n  \n  # Combining the test & the pool\n  test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y)\n  \n  # Sorting test_pool for plotting\n  sorted_inds = np.argsort(test_pool_X[:,1])\n  test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n  \n  # Plotting test_pool with uncertainty\n  model.predict(test_pool_X)\n  individual_var = model.pred_var.diagonal()\n  ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n  ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                  , alpha=0.2, color='black', label='uncertainty')\n  \n  # Plotting the train data\n  ax.scatter(model.x[:,1], model.y,s=40, color='k', marker='s', label='train data')\n  ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n  \n  # Plotting MAE on the test set\n  model.predict(test_X)\n  ax.set_title(init_title+' MAE is '+str(np.mean(np.abs(test_Y - model.y_hat_map))))\n  ax.set_xlabel('x')\n  ax.set_ylabel('y')\n  ax.legend()\n\nPlotting the estimations and uncertainty.\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\nLet’s check the maximum uncertainty about any point for the model.\n\nmodels[0].pred_var.diagonal().max()\n\n4.8261426545316604e-29\n\n\nOops!! There is almost no uncertainty in the model. Why? let’s try again with more train points.\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed)\n\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\nNow uncertainty is visible, and currently, it’s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model.\nLet’s evaluate the performance on the test set.\n\nestimations[0], _ = models[0].predict(test_X)\ntest_mae_error[0] = np.mean(np.abs(test_Y - estimations[0]))\n\nMean Absolute Error (MAE) on the test set is\n\ntest_mae_error[0]\n\n0.5783654195019617\n\n\n\n\nMoving the most uncertain point from the pool to the train set\nIn the previous plot, we saw that the model was least certain about the left-most point. We’ll move that point from the pool to the train set and see the effect.\n\nesimations_pool, _ = models[0].predict(pool_X)\n\nFinding out a point having the most uncertainty.\n\nin_var = models[0].pred_var.diagonal().argmax()\nto_add_x = pool_X[in_var,:]\nto_add_y = pool_Y[in_var]\n\nAdding the point from the pool to the train set.\n\ntrain_X_iter[1] = np.vstack([train_X_iter[0], to_add_x])\ntrain_Y_iter[1] = np.append(train_Y_iter[0], to_add_y)\n\nDeleting the point from the pool.\n\npool_X = np.delete(pool_X, in_var, axis=0)\npool_Y = np.delete(pool_Y, in_var)\n\n\n\nTraining again and visualising the results (Iteration 1)\nThis time, we will pass previously learnt prior to the next iteration.\n\nS0 = np.eye(N_features)\nmodels[1] = BLR(S0, models[0].MN)\n\n\nmodels[1].fit(train_X_iter[1], train_Y_iter[1])\n\n\nestimations[1], _ = models[1].predict(test_X)\ntest_mae_error[1] = np.mean(np.abs(test_Y - estimations[1]))\n\nMAE on the test set is\n\ntest_mae_error[1]\n\n0.5779411133071186\n\n\nVisualizing the results.\n\nfig, ax = plt.subplots()\nplot(ax, models[1])\n\n\n\n\nBefore & after adding most uncertain point\n\nfig, ax = plt.subplots(1,2, figsize=(13.5,4.5))\nplot(ax[0], models[0],'Before')\nplot(ax[1], models[1],'After')\n\n\n\n\nWe can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data.\nNow let’s do this for few more iterations in a loop and visualise the results.\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x= np.zeros((num_iterations+1, N_features))\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(2, num_iterations+1):\n    # Making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool, var = models[iteration-1].predict(pool_X)\n    \n    # Finding the point from the pool with highest uncertainty\n    in_var = var.diagonal().argmax()\n    to_add_x = pool_X[in_var,:]\n    to_add_y = pool_Y[in_var]\n    points_added_x[iteration-1,:] = to_add_x\n    points_added_y.append(to_add_y)\n    \n    # Adding the point to the train set from the pool\n    train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x])\n    train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var, axis=0)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set\n    models[iteration] = BLR(S0, models[iteration-1].MN)\n    models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration])\n    \n    estimations[iteration], _ = models[iteration].predict(test_X)\n    test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n2 0.49023173501654815\n3 0.4923391714942153\n4 0.49040074812746753\n5 0.49610198614600165\n6 0.5015282102751122\n7 0.5051264429971314\n8 0.5099913097301352\n9 0.504455016053513\n10 0.5029219102020734\n11 0.5009762782262487\n12 0.5004883097883343\n13 0.5005169638980388\n14 0.5002731089932334\n15 0.49927485683909884\n16 0.49698416490822594\n17 0.49355398855432897\n18 0.49191185613804617\n19 0.491164833699368\n20 0.4908067530719673\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\nplt.show()\n\n\n\n\nThe plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let’s visualise fits for all the iterations. We’ll discuss this behaviour after that.\n\n\nVisualizing active learning procedure\n\nprint('Initial model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[0].MN[::-1]))\nprint('\\nFinal model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[num_iterations].MN[::-1]))\n\nInitial model\nY = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63\n\nFinal model\nY = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58\n\n\n\ndef update(iteration):\n    ax.cla()\n    plot(ax, models[iteration])\n    fig.tight_layout()\n\n\nfig, ax = plt.subplots()\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250)\nplt.close()\nrc('animation', html='jshtml')\n\n\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually.\nNow, let’s put everything together and create a class for active learning procedure\n\n\nCreating a class for active learning procedure\n\nclass ActiveL():\n  def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1):\n    self.X_init = X\n    self.y = y\n    self.S0 = S0\n    self.M0 = M0\n    self.train_X_iter = {} # to store train points at each iteration\n    self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration\n    self.models = {} # to store the models at each iteration\n    self.estimations = {} # to store the estimations on the test set at each iteration\n    self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n    self.test_size = test_size\n    self.degree = degree\n    self.iterations = iterations\n    self.seed = seed\n    self.train_size = degree + 2\n\n  def data_preperation(self):\n    # Adding polynomial features\n    self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init)\n    N_features = self.X.shape[1]\n    \n    # Splitting into train, test and pool\n    train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, \n                                                                            test_size=self.test_size,\n                                                                            random_state=self.seed)\n    self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, \n                                                                            train_size=self.train_size, \n                                                                            random_state=self.seed)\n    \n    # Setting BLR prior incase of not given\n    if self.M0 == None:\n      self.M0 = np.zeros((N_features, ))\n    if self.S0 == None:\n      self.S0 = np.eye(N_features)\n    \n  def main(self):\n    # Training for iteration 0\n    self.train_X_iter[0] = self.train_X\n    self.train_Y_iter[0] = self.train_Y\n    self.models[0] = BLR(self.S0, self.M0)\n    self.models[0].fit(self.train_X, self.train_Y)\n\n    # Running loop for all iterations\n    for iteration in range(1, self.iterations+1):\n      # Making predictions on the pool set based on model learnt in the respective train set \n      estimations_pool, var = self.models[iteration-1].predict(self.pool_X)\n      \n      # Finding the point from the pool with highest uncertainty\n      in_var = var.diagonal().argmax()\n      to_add_x = self.pool_X[in_var,:]\n      to_add_y = self.pool_Y[in_var]\n      \n      # Adding the point to the train set from the pool\n      self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x])\n      self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y)\n      \n      # Deleting the point from the pool\n      self.pool_X = np.delete(self.pool_X, in_var, axis=0)\n      self.pool_Y = np.delete(self.pool_Y, in_var)\n      \n      # Training on the new set\n      self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN)\n      self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration])\n      \n      self.estimations[iteration], _ = self.models[iteration].predict(self.test_X)\n      self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean()\n\n  def _plot_iter_MAE(self, ax, iteration):\n    ax.plot(list(self.test_mae_error.values())[:iteration+1], 'ko-')\n    ax.set_title('MAE on test set over iterations')\n    ax.set_xlim((-0.5, self.iterations+0.5))\n    ax.set_ylabel(\"MAE on test set\")\n    ax.set_xlabel(\"# Points Queried\")\n  \n  def _plot(self, ax, model):\n    # Plotting the pool\n    ax.scatter(self.pool_X[:,1], self.pool_Y, label='pool',s=1,color='r',alpha=0.4)\n    \n    # Plotting the test data\n    ax.scatter(self.test_X[:,1], self.test_Y, label='test data',s=1, color='b', alpha=0.4)\n    \n    # Combining test_pool\n    test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y)\n    \n    # Sorting test_pool\n    sorted_inds = np.argsort(test_pool_X[:,1])\n    test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n    \n    # Plotting test_pool with uncertainty\n    preds, var = model.predict(test_pool_X)\n    individual_var = var.diagonal()\n    ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n    ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                    , alpha=0.2, color='black', label='uncertainty')\n    \n    # plotting the train data\n    ax.scatter(model.x[:,1], model.y,s=10, color='k', marker='s', label='train data')\n    ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n    \n    # plotting MAE\n    preds, var = model.predict(self.test_X)\n    ax.set_title('MAE is '+str(np.mean(np.abs(self.test_Y - preds))))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    \n  def visualise_AL(self):\n    fig, ax = plt.subplots(1,2,figsize=(13,5))\n    def update(iteration):\n      ax[0].cla()\n      ax[1].cla()\n      self._plot(ax[0], self.models[iteration])\n      self._plot_iter_MAE(ax[1], iteration)\n      fig.tight_layout()\n\n    print('Initial model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[0].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n    print('\\nFinal model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[self.iterations].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250)\n    plt.close()\n\n    rc('animation', html='jshtml')\n    return anim\n\n\n\nVisualizing a different polynomial fit on the same dataset\nLet’s try to fit a degree 7 polynomial to the same data now.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nmodel = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7\n\nFinal model\nY = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations.\n\n\nActive learning for diabetes dataset from the Scikit-learn module\nLet’s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We’ll choose only ‘weight’ feature, which seems to have more correlation with the target.\nWe’ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let’s check the performance of Scikit-learn linear regression model.\n\nX, Y = datasets.load_diabetes(return_X_y=True)\nX = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression\n\n# Normalizing\nX = (X - X.min())/(X.max() - X.min())\nY = (Y - Y.min())/(Y.max() - Y.min())\n\nVisualizing the dataset.\n\nplt.scatter(X, Y)\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.show()\n\n\n\n\nLet’s fit the Scikit-learn linear regression model with 50% train-test split.\n\nfrom sklearn.linear_model import LinearRegression\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed)\n\n\nclf = LinearRegression()\n\n\nclf.fit(train_X, train_Y)\npred_Y = clf.predict(test_X)\n\nVisualizing the fit & MAE.\n\nplt.scatter(X, Y, label='data', s=5)\nplt.plot(test_X, pred_Y, label='model', color='r')\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.title('MAE is '+str(np.mean(np.abs(pred_Y - test_Y))))\nplt.legend()\nplt.show()\n\n\n\n\nNow we’ll fit the same data with our BLR model\n\nmodel = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = 0.41 + 0.16 X^1\n\nFinal model\nY = 0.13 + 0.86 X^1\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nInitially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It’s interesting to see that our initial train points tend to make a vertical fit, but the model doesn’t get carried away by that and stabilizes the self with prior.\n\nprint('MAE for Scikit-learn Linear Regression is',np.mean(np.abs(pred_Y - test_Y)))\nprint('MAE for Bayesian Linear Regression is', model.test_mae_error[20])\n\nMAE for Scikit-learn Linear Regression is 0.15424985705353944\nMAE for Bayesian Linear Regression is 0.15738001811804758\n\n\nAt the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we’ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit."
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "title": "Get a list of contributors from a repo",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "title": "Get a list of contributors from a repo",
    "section": "Config",
    "text": "Config\n\nowner = \"probml\"\nrepo = \"pyprobml\""
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Get all contributors to a repo",
    "text": "Get all contributors to a repo\n\ncontributors = pd.read_json(f\"https://api.github.com/repos/{owner}/{repo}/contributors?per_page=100\")\ncontributors = contributors.set_index(\"login\")\nprint(f\"Number of contributors: {len(contributors.index.unique())}\")\ncontributors.head(2)\n\nNumber of contributors: 47\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nnode_id\navatar_url\ngravatar_id\nurl\nhtml_url\nfollowers_url\nfollowing_url\ngists_url\nstarred_url\nsubscriptions_url\norganizations_url\nrepos_url\nevents_url\nreceived_events_url\ntype\nsite_admin\ncontributions\n\n\nlogin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmurphyk\n4632336\nMDQ6VXNlcjQ2MzIzMzY=\nhttps://avatars.githubusercontent.com/u/463233...\n\nhttps://api.github.com/users/murphyk\nhttps://github.com/murphyk\nhttps://api.github.com/users/murphyk/followers\nhttps://api.github.com/users/murphyk/following...\nhttps://api.github.com/users/murphyk/gists{/gi...\nhttps://api.github.com/users/murphyk/starred{/...\nhttps://api.github.com/users/murphyk/subscript...\nhttps://api.github.com/users/murphyk/orgs\nhttps://api.github.com/users/murphyk/repos\nhttps://api.github.com/users/murphyk/events{/p...\nhttps://api.github.com/users/murphyk/received_...\nUser\nFalse\n1777\n\n\nNeoanarika\n5188337\nMDQ6VXNlcjUxODgzMzc=\nhttps://avatars.githubusercontent.com/u/518833...\n\nhttps://api.github.com/users/Neoanarika\nhttps://github.com/Neoanarika\nhttps://api.github.com/users/Neoanarika/followers\nhttps://api.github.com/users/Neoanarika/follow...\nhttps://api.github.com/users/Neoanarika/gists{...\nhttps://api.github.com/users/Neoanarika/starre...\nhttps://api.github.com/users/Neoanarika/subscr...\nhttps://api.github.com/users/Neoanarika/orgs\nhttps://api.github.com/users/Neoanarika/repos\nhttps://api.github.com/users/Neoanarika/events...\nhttps://api.github.com/users/Neoanarika/receiv...\nUser\nFalse\n184"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Fetch all PRs from a repo",
    "text": "Fetch all PRs from a repo\n\npage_range = range(1, 6)\nget_pr_df = lambda page: pd.read_json(f\"https://api.github.com/repos/probml/pyprobml/pulls?state=all&per_page=100&page={page}\")\npull_requests = pd.concat(map(get_pr_df, page_range))\nprint(f\"Number of PRs: {len(pull_requests)}\")\npull_requests.head(2)\n\nNumber of PRs: 497\n\n\n\n  \n    \n      \n\n\n\n\n\n\nurl\nid\nnode_id\nhtml_url\ndiff_url\npatch_url\nissue_url\nnumber\nstate\nlocked\n...\nreview_comments_url\nreview_comment_url\ncomments_url\nstatuses_url\nhead\nbase\n_links\nauthor_association\nauto_merge\nactive_lock_reason\n\n\n\n\n0\nhttps://api.github.com/repos/probml/pyprobml/p...\n938329819\nPR_kwDOA-3vB8437cbb\nhttps://github.com/probml/pyprobml/pull/841\nhttps://github.com/probml/pyprobml/pull/841.diff\nhttps://github.com/probml/pyprobml/pull/841.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n841\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:posrprocessing', 'ref': ...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n1\nhttps://api.github.com/repos/probml/pyprobml/p...\n938317389\nPR_kwDOA-3vB8437ZZN\nhttps://github.com/probml/pyprobml/pull/840\nhttps://github.com/probml/pyprobml/pull/840.diff\nhttps://github.com/probml/pyprobml/pull/840.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n840\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:master', 'ref': 'master'...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n\n\n\n2 rows × 36 columns"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "title": "Get a list of contributors from a repo",
    "section": "Get a list of contributors sorted by count of PRs",
    "text": "Get a list of contributors sorted by count of PRs\n\npull_requests['login'] = pull_requests['user'].apply(lambda x: x[\"login\"])\nsorted_by_pr_count = pull_requests.groupby(\"login\").agg({'url': len}).sort_values(by='url', ascending=False)\nsorted_by_pr_count.rename(columns={'url': 'Number of PRs'}, inplace=True)\nsorted_by_pr_count.head(5)\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of PRs\n\n\nlogin\n\n\n\n\n\nDrishttii\n79\n\n\ngerdm\n55\n\n\nkaralleyna\n43\n\n\nalways-newbie161\n29\n\n\nkarm-patel\n29"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "title": "Get a list of contributors from a repo",
    "section": "Create a dashboard",
    "text": "Create a dashboard\n\ndef get_href_user(user):\n  username, profile_link = user.split(\"|\")\n  return f\"[{username}]({profile_link})\"\n\ndashboard = pd.DataFrame(index=sorted_by_pr_count.index)\ndashboard[\"Avatar\"] = contributors.avatar_url.apply(lambda url: f'&lt;img width=\"25\" alt=\"image\" src=\"{url}\"&gt;')\ndashboard[\"Contributor\"] = (contributors.index +\"|\"+ contributors['html_url']).apply(get_href_user)\ndashboard[\"Number of PRs\"] = sorted_by_pr_count[\"Number of PRs\"]\nprint(dashboard.dropna().T.to_markdown())\n\n|               | Drishttii                                                                               | gerdm                                                                                  | karalleyna                                                                              | always-newbie161                                                                        | karm-patel                                                                              | Duane321                                                                                | Nirzu97                                                                                 | patel-zeel                                                                              | animesh-007                                                                             | ashishpapanai                                                                           | shivaditya-meduri                                                                       | Neoanarika                                                                             | andrewnc                                                                               | nappaillav                                                                              | Abdelrahman350                                                                          | mjsML                                                                                  | jdf22                                                                                  | kzymgch                                                                                 | nalzok                                                                                  | nitish1295                                                                              | Garvit9000c                                                                             | AnkitaKumariJain14                                                                      | rohit-khoiwal-30                                                                        | shobro                                                                                  | raymondyeh07                                                                           | khanshehjad                                                                             | alenm10                                                                                 | firatoncel                                                                             | AnandShegde                                                                             | Aadesh-1404                                                                             | nealmcb                                                                               | nipunbatra                                                                           | petercerno                                                                             | posgnu                                                                                  | mvervuurt                                                                              | hieuza                                                                                 | Prahitha                                                                                | TripleTop                                                                               | UmarJ                                                                                   | Vishal987595                                                                            | a-fakhri                                                                                | adamnemecek                                                                           | galv                                                                                   | jlh2018                                                                                 | krasserm                                                                              | yuanx749                                                                                |\n|:--------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|\n| Avatar        | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/35187749?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4108759?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/36455180?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/66471669?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59387624?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/19956442?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/28842790?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59758528?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/53366877?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/52123364?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/77324692?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5188337?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7716402?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/43855961?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47902062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7131192?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1637094?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/10054419?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/13443062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/21181046?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68856476?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/62535006?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/87682045?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/54628243?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5696982?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/31896767?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/42214173?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/9141211?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/79975787?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68186100?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/119472?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/60985?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1649209?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/30136201?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/6399881?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1021144?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/44160152?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/48208522?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/34779641?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/97757583?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/65111198?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/182415?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4767568?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/40842099?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/202907?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47032563?v=4\"&gt; |\n| Contributor   | [Drishttii](https://github.com/Drishttii)                                               | [gerdm](https://github.com/gerdm)                                                      | [karalleyna](https://github.com/karalleyna)                                             | [always-newbie161](https://github.com/always-newbie161)                                 | [karm-patel](https://github.com/karm-patel)                                             | [Duane321](https://github.com/Duane321)                                                 | [Nirzu97](https://github.com/Nirzu97)                                                   | [patel-zeel](https://github.com/patel-zeel)                                             | [animesh-007](https://github.com/animesh-007)                                           | [ashishpapanai](https://github.com/ashishpapanai)                                       | [shivaditya-meduri](https://github.com/shivaditya-meduri)                               | [Neoanarika](https://github.com/Neoanarika)                                            | [andrewnc](https://github.com/andrewnc)                                                | [nappaillav](https://github.com/nappaillav)                                             | [Abdelrahman350](https://github.com/Abdelrahman350)                                     | [mjsML](https://github.com/mjsML)                                                      | [jdf22](https://github.com/jdf22)                                                      | [kzymgch](https://github.com/kzymgch)                                                   | [nalzok](https://github.com/nalzok)                                                     | [nitish1295](https://github.com/nitish1295)                                             | [Garvit9000c](https://github.com/Garvit9000c)                                           | [AnkitaKumariJain14](https://github.com/AnkitaKumariJain14)                             | [rohit-khoiwal-30](https://github.com/rohit-khoiwal-30)                                 | [shobro](https://github.com/shobro)                                                     | [raymondyeh07](https://github.com/raymondyeh07)                                        | [khanshehjad](https://github.com/khanshehjad)                                           | [alenm10](https://github.com/alenm10)                                                   | [firatoncel](https://github.com/firatoncel)                                            | [AnandShegde](https://github.com/AnandShegde)                                           | [Aadesh-1404](https://github.com/Aadesh-1404)                                           | [nealmcb](https://github.com/nealmcb)                                                 | [nipunbatra](https://github.com/nipunbatra)                                          | [petercerno](https://github.com/petercerno)                                            | [posgnu](https://github.com/posgnu)                                                     | [mvervuurt](https://github.com/mvervuurt)                                              | [hieuza](https://github.com/hieuza)                                                    | [Prahitha](https://github.com/Prahitha)                                                 | [TripleTop](https://github.com/TripleTop)                                               | [UmarJ](https://github.com/UmarJ)                                                       | [Vishal987595](https://github.com/Vishal987595)                                         | [a-fakhri](https://github.com/a-fakhri)                                                 | [adamnemecek](https://github.com/adamnemecek)                                         | [galv](https://github.com/galv)                                                        | [jlh2018](https://github.com/jlh2018)                                                   | [krasserm](https://github.com/krasserm)                                               | [yuanx749](https://github.com/yuanx749)                                                 |\n| Number of PRs | 79                                                                                      | 55                                                                                     | 43                                                                                      | 29                                                                                      | 29                                                                                      | 29                                                                                      | 25                                                                                      | 23                                                                                      | 18                                                                                      | 17                                                                                      | 16                                                                                      | 10                                                                                     | 10                                                                                     | 10                                                                                      | 8                                                                                       | 7                                                                                      | 7                                                                                      | 6                                                                                       | 6                                                                                       | 5                                                                                       | 4                                                                                       | 4                                                                                       | 3                                                                                       | 3                                                                                       | 2                                                                                      | 2                                                                                       | 2                                                                                       | 2                                                                                      | 2                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                    | 1                                                                                      | 1                                                                                       | 1                                                                                      | 1                                                                                      | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                      | 1                                                                                       | 1                                                                                     | 1                                                                                       |"
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html",
    "href": "posts/2022-01-15-py_over_ipynb.html",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "",
    "text": "I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files:"
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html#fewer-errors",
    "href": "posts/2022-01-15-py_over_ipynb.html#fewer-errors",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Fewer Errors",
    "text": "Fewer Errors\n\n.py files are easier to debug with a VS code like IDE, making it easier to find the errors.\nExecution of .py starts fresh, unlike some left out variables silently getting carried over from the last execution/deleted cells in .ipynb files."
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html#better-usage-of-a-shared-server",
    "href": "posts/2022-01-15-py_over_ipynb.html#better-usage-of-a-shared-server",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Better Usage of a Shared Server",
    "text": "Better Usage of a Shared Server\n\n.py files release the resources (e.g., GPU memory) once executed. It could be inconvenient to repeatedly remind or be reminded by someone to release the resources manually from a Jupyter notebook."
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html#increased-productivity",
    "href": "posts/2022-01-15-py_over_ipynb.html#increased-productivity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Productivity",
    "text": "Increased Productivity\n\nYou can make use of fantastic auto-complete, syntax-highlighting extensions in VS code to save a lot of time while working with .py files."
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html#boost-collaboration",
    "href": "posts/2022-01-15-py_over_ipynb.html#boost-collaboration",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Boost Collaboration",
    "text": "Boost Collaboration\n\n.py do not take time to render on GitHub because they are just plain text files, unlike .ipynb files.\nIt is a lot easier to see the changes made by others in a .py file than a .ipynb file."
  },
  {
    "objectID": "posts/2022-01-15-py_over_ipynb.html#increased-modularity",
    "href": "posts/2022-01-15-py_over_ipynb.html#increased-modularity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Modularity",
    "text": "Increased Modularity\n\nFunction and Class calls from other files are seamless with .py files.\n\nFeel free to comment your views/suggestions/additions in the comment box."
  },
  {
    "objectID": "posts/2023-05-14-ssh-macos.html",
    "href": "posts/2023-05-14-ssh-macos.html",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you’d like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE."
  },
  {
    "objectID": "posts/2023-05-14-ssh-macos.html#terminology",
    "href": "posts/2023-05-14-ssh-macos.html#terminology",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you’d like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE."
  },
  {
    "objectID": "posts/2023-05-14-ssh-macos.html#what-is-the-problem",
    "href": "posts/2023-05-14-ssh-macos.html#what-is-the-problem",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "What is the problem?",
    "text": "What is the problem?\nSimilar to Windows machines, one can run the following commands on a macOS HOST for setting up the passwordless ssh:\nssh-keygen\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP\nBut this does not work out of the box without the following command which lets your HOST know about the private key.\nssh-add ~/.ssh/id_rsa\nAfter this, connection works fine from macOS CLI. However, if you are trying to connect to REMOTE from VS code, make sure you restart VS code before attempting to connect (quit from the Dock as well).\nSo far so good. But this setup fails when you reboot your HOST since ssh-add is not perstistently adding the pirvate key to HOST.\nSo, what to do now?"
  },
  {
    "objectID": "posts/2023-05-14-ssh-macos.html#permenant-solution",
    "href": "posts/2023-05-14-ssh-macos.html#permenant-solution",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "Permenant solution",
    "text": "Permenant solution\nI found a permenant and full-proof solution here. For each REMOTE you add in your HOST’s ~/.ssh/config, after generating a key pair and copying it to REMOTE with ssh-copy-id command, modify its entry in ~/.ssh/config like the following and the issue should be permenently resolved.\nHost REMOTE\n  UseKeychain yes\n  AddKeysToAgent yes\n  IdentityFile ~/.ssh/id_rsa\n  HostName REMOTE-IP\n  Port PORT\n  User USERNAME"
  },
  {
    "objectID": "posts/2022-04-09-gcloud.html",
    "href": "posts/2022-04-09-gcloud.html",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don’t know if this is required or not. This command should trigger installation of “gcloud Beta Commands” automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/2022-04-09-gcloud.html#initial-setup",
    "href": "posts/2022-04-09-gcloud.html#initial-setup",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don’t know if this is required or not. This command should trigger installation of “gcloud Beta Commands” automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/2022-04-09-gcloud.html#working-with-tpu-vms",
    "href": "posts/2022-04-09-gcloud.html#working-with-tpu-vms",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs",
    "text": "Working with TPU VMs\nThere are two different terms here: “TPU VMs” and “TPU nodes”. TPU nodes can be connected externally via another VM. TPU VMs are stand-alone systems with TPUs, RAM and CPU (96 core Intel 2 GHz processor and 335 GB RAM). We may be charged via GCP for the VM (CPUs and RAM). (I will update this info once I know for sure):\n\n\nTo create a TPU VM in preferred zone via CLI (be careful about the --zone to avoid charges, check the first email received from TRC team to see what kind of TPUs are free in different zones. if --zone is not passed, VM will be created in the default zone that we set initially. This command triggered installation of “gcloud Alpha Commands”):\n\ngcloud alpha compute tpus tpu-vm create vm-1 --accelerator-type v2-8 --version tpu-vm-tf-2.8.0 --zone us-central1-f\n\nTo get the list of TPU nodes/VMs:\n\ngcloud compute tpus list\n\nTo delete a TPU node/VM:\n\ngcloud compute tpus delete vm-1\n\nTo connect with a vm via ssh (this automatically creates ssh key pair and places in default ssh config location):\n\ngcloud alpha compute tpus tpu-vm ssh vm-1\n\nFollow this guide to create and attach a persistent disk with the TPU VM"
  },
  {
    "objectID": "posts/2022-04-09-gcloud.html#working-with-tpu-vms-via-vs-code",
    "href": "posts/2022-04-09-gcloud.html#working-with-tpu-vms-via-vs-code",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs via VS-code",
    "text": "Working with TPU VMs via VS-code\n\nInstall the following extension on VS-code: \nUse the following button to connect to a remote machine (use “Connect to Host…” button): \nManually update the default ssh config file (in my case, “C:\\.ssh”) to add a VM in VS-code (you can use VS-code command palette to figure out the config file for you and edit it. Please see the screeshot below).\n\n\n\nNote that ssh public-private key pair with name google_compute_engine is automatically generated when you connect with the VM for the first time with gcloud alpha compute tpus tpu-vm ssh command. The VM config for me looks like this:\n\nHost Cloud-TPU-Node-2\n  HostName &lt;External-IP-of-your-TPU-VM&gt;\n  User zeelp\n  Port 22\n  IdentityFile C:\\Users\\zeelp\\.ssh\\google_compute_engine"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html",
    "href": "posts/2021-10-12-sparsegps.html",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#imports",
    "href": "posts/2021-10-12-sparsegps.html#imports",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#data-preperation",
    "href": "posts/2021-10-12-sparsegps.html#data-preperation",
    "title": "SparseGPs in Stheno",
    "section": "Data preperation",
    "text": "Data preperation\n\n# Define points to predict at.\nx = B.linspace(0, 10, 100)\nx_obs = B.linspace(0, 7, 50_000)\nx_ind = B.linspace(0, 10, 20)\n\n# Construct a prior.\nf = GP(EQ().periodic(2 * B.pi))\n\n# Sample a true, underlying function and observations.\nf_true = B.sin(x)\ny_obs = B.sin(x_obs) + B.sqrt(0.5) * B.randn(*x_obs.shape)"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#plotting-function",
    "href": "posts/2021-10-12-sparsegps.html#plotting-function",
    "title": "SparseGPs in Stheno",
    "section": "Plotting function",
    "text": "Plotting function\n\ndef plot(method):\n    if method == 'VFE':\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            obs.mu(f.measure)[:, 0],\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()\n    else:\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            B.dense(f_post(x_ind).mean),\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "title": "SparseGPs in Stheno",
    "section": "Sparse regression with Variational Free Energy (VFE) method",
    "text": "Sparse regression with Variational Free Energy (VFE) method\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsVFE(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('VFE')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "title": "SparseGPs in Stheno",
    "section": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod",
    "text": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsFITC(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('FITC')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "href": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "title": "SparseGPs in Stheno",
    "section": "Hyperparameter tuning (Noisy Sine data)",
    "text": "Hyperparameter tuning (Noisy Sine data)\n\ndef model(vs):\n    \"\"\"Constuct a model with learnable parameters.\"\"\"\n    return vs['variance']*GP(EQ().stretch(vs['length_scale']))\n\n\ntorch.manual_seed(123)\n\ndataObj = rd.SineNoisy(scale_X=False, scale_y=False, return_test=True, backend='torch')\nx_obs, y_obs, x = dataObj.get_data()\n\n\nplt.scatter(x_obs, y_obs, s=2);\n\n\n\n\n\nVFE\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsVFE(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nFITC\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsFITC(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "title": "Uncertainty in Deep Learning",
    "section": "",
    "text": "import torch"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "title": "Uncertainty in Deep Learning",
    "section": "1 - Introduction",
    "text": "1 - Introduction\n\nAn online deep learning book from Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n1.1 - Deep Learning\nWe define a single layer network as the following:\n\nclass SingleLayerNetwork(torch.nn.Module):\n    def __init__(self, Q, D, K):\n        \"\"\"\n        Q: number of features\n        D: number of outputs\n        K: number of hidden features\n        \"\"\"\n        super().__init__()\n        self.input = torch.nn.Linear(Q, K) # Transforms Q features into K hidden features\n        self.output = torch.nn.Linear(K, D) # Transforms K hidden features to D output features\n        self.non_lin_transform = torch.nn.ReLU() # A non-linear transformation\n        \n    def forward(self, X):\n        \"\"\"\n        X: input (N x Q)\n        \"\"\"\n        self.linear_transformed_X = self.input(X)  # (N, Q) -&gt; (N, K)\n        self.non_lin_transformed_X = self.non_lin_transform(linear_transformed_X)  # (N, K) -&gt; (N, K)\n        output = self.output(self.non_lin_transformed_X)  # (N, K) -&gt; (N, D)\n        return output\n\n\nQ = 10 # Number of features\nN = 100 # Number of samples\nD = 15 # Number of outputs\nK = 32 # Number of hidden features\n\nX = torch.rand(N, Q) # Input\nY = torch.rand(N, D) # Output\n\n\nmodel = SingleLayerNetwork(Q=Q, D=D, K=K)\nmodel\n\nSingleLayerNetwork(\n  (input): Linear(in_features=10, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=15, bias=True)\n  (non_lin_transform): ReLU()\n)\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value.shape)\n\ninput.weight torch.Size([32, 10])\ninput.bias torch.Size([32])\noutput.weight torch.Size([15, 32])\noutput.bias torch.Size([15])\n\n\nReLU is does not contain any parameters here so it is merely a function.\n\n\n1.2 Model Uncertainty\nIn which cases we want our model to be uncertain?\n\nWhen it encounters a out-of-the-distribution data\nWhen training data is noisy (irreducible/aleatoric uncertainty)\nWhen we have multiple predictors (model/epistemic uncertainty)"
  },
  {
    "objectID": "posts/2022-10-18-kfac-laplace.html",
    "href": "posts/2022-10-18-kfac-laplace.html",
    "title": "Train NN with KFAC-Laplace in JAX",
    "section": "",
    "text": "from math import prod\nfrom functools import partial\nfrom time import time\n\nimport blackjax\nimport flax.linen as nn\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.tree_util as jtu\nimport jax.numpy as jnp\n# jnp.set_printoptions(linewidth=2000)\n\nimport optax\nfrom tqdm import trange\n\nimport arviz as az\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\njax.config.update(\"jax_enable_x64\", False)\n\n%reload_ext watermark\n\nSome helper functions:\n\njitter = 1e-6\n\ndef get_shapes(params):\n    return jtu.tree_map(lambda x:x.shape, params)\n\ndef svd_inverse(matrix):\n    U, S, V = jnp.linalg.svd(matrix+jnp.eye(matrix.shape[0])*jitter)\n    \n    return V.T/S@U.T\n\n\nDataset\nWe take XOR dataset to begin with:\n\nX = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = jnp.array([0, 1, 1, 0])\n\nX.shape, y.shape\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n((4, 2), (4,))\n\n\n\n\nNN model\n\nclass MLP(nn.Module):\n    features: []\n\n    @nn.compact\n    def __call__(self, x):\n        for n_features in self.features[:-1]:\n            x = nn.Dense(n_features, kernel_init=jax.nn.initializers.glorot_normal(), bias_init=jax.nn.initializers.normal())(x)\n            x = nn.relu(x)\n        \n        x = nn.Dense(features[-1])(x)\n        return x.ravel()\n\nLet us initialize the weights of NN and inspect shapes of the parameters:\n\nfeatures = [2, 1]\nkey = jax.random.PRNGKey(0)\n\nmodel = MLP(features)\nparams = model.init(key, X).unfreeze()\n\nget_shapes(params)\n\n{'params': {'Dense_0': {'bias': (2,), 'kernel': (2, 2)},\n  'Dense_1': {'bias': (1,), 'kernel': (2, 1)}}}\n\n\n\nmodel.apply(params, X)\n\nDeviceArray([ 0.00687164, -0.01380461,  0.        ,  0.        ], dtype=float32)\n\n\n\n\nNegative Log Joint\n\nnoise_var = 0.1\n\ndef neg_log_joint(params):\n    y_pred = model.apply(params, X)\n    flat_params = ravel_pytree(params)[0]\n    log_prior = jax.scipy.stats.norm.logpdf(flat_params).sum()\n    log_likelihood = jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=noise_var).sum()\n    \n    return -(log_prior + log_likelihood)\n\nTesting if it works:\n\nneg_log_joint(params)\n\nDeviceArray(105.03511, dtype=float32)\n\n\n\n\nFind MAP\n\nkey = jax.random.PRNGKey(0)\nparams = model.init(key, X).unfreeze()\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(neg_log_joint))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n    \n(params, state), losses = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(losses);\n\n\n\n\n\ny_map = model.apply(params, X)\ny_map\n\nDeviceArray([0.01383345, 0.98666817, 0.98563665, 0.01507111], dtype=float32)\n\n\n\nx = jnp.linspace(-0.1,1.1,100)\nX1, X2 = jnp.meshgrid(x, x)\n\ndef predict_fn(x1, x2):\n    return model.apply(params, jnp.array([x1,x2]).reshape(1,2))\n\npredict_fn_vec = jax.jit(jax.vmap(jax.vmap(predict_fn)))\n\nZ = predict_fn_vec(X1, X2).squeeze()\n\nplt.contourf(X1, X2, Z)\nplt.colorbar();\n\n\n\n\n\n\nFull Hessian Laplace\n\nflat_params, unravel_fn = ravel_pytree(params)\n\ndef neg_log_joint_flat(flat_params):\n    return neg_log_joint(unravel_fn(flat_params))\n\nH = jax.hessian(neg_log_joint_flat)(flat_params)\n\nsns.heatmap(H);\n\n\n\n\n\nposterior_cov = svd_inverse(H)\n\nsns.heatmap(posterior_cov);\n\n\n\n\nNote that we can sample parameters from the posterior and revert them to correct structure with the unravel_fn. Here is a class to do it all:\n\nclass FullHessianLaplace:\n    def __init__(self, map_params, model):\n        flat_params, self.unravel_fn = ravel_pytree(map_params)\n\n        def neg_log_joint_flat(flat_params):\n            params = unravel_fn(flat_params)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(flat_params)\n        \n        self.mean = flat_params\n        self.cov = svd_inverse(self.H)\n        self.model = model\n\n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample(self, seed):\n        sample = jax.random.multivariate_normal(seed, mean=self.mean, cov=self.cov)\n        return self.unravel_fn(sample)\n    \n    def sample(self, seed, shape):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nposterior = FullHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 100000\ny_pred_full = posterior.predict(X, seed=seed, shape=(n_samples,))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i]);\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_pred_mean={y_pred_full[:, i].mean():.3f}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\nKFAC-Laplace\nWe need to invert partial Hessians to do KFAC-Laplace. We can use tree_flatten with ravel_pytree to ease the workflow. We need to: 1. pick up partial Hessians in pure matrix form to be able to invert them. 2. Create layer-wise distributions and sample them. These samples will be 1d arrays. 3. We need to convert those 1d arrays to params dictionary form so that we can plug it into the flax model and get posterior predictions.\nFirst we need to segregate the parameters layer-wise. We will use is_leaf condition to stop traversing the parameter PyTree at a perticular depth. See how it is different from vanilla tree_flatten:\n\nflat_params, tree_def = jtu.tree_flatten(params)\ndisplay(flat_params, tree_def)\n\n[DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n DeviceArray([[ 0.8275324 , -0.8314813 ],\n              [-0.8276633 ,  0.83254045]], dtype=float32),\n DeviceArray([0.01351773], dtype=float32),\n DeviceArray([[1.1750739],\n              [1.1685134]], dtype=float32)]\n\n\nPyTreeDef({'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}})\n\n\n\nis_leaf = lambda param: 'bias' in param\nlayers, tree_def = jtu.tree_flatten(params, is_leaf=is_leaf)\ndisplay(layers, tree_def)\n\n[{'bias': DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n  'kernel': DeviceArray([[ 0.8275324 , -0.8314813 ],\n               [-0.8276633 ,  0.83254045]], dtype=float32)},\n {'bias': DeviceArray([0.01351773], dtype=float32),\n  'kernel': DeviceArray([[1.1750739],\n               [1.1685134]], dtype=float32)}]\n\n\nPyTreeDef({'params': {'Dense_0': *, 'Dense_1': *}})\n\n\nThe difference is clearly evident. Now, we need to flatten the inner dictionaries to get 1d arrays.\n\nflat_params = list(map(lambda x: ravel_pytree(x)[0], layers))\nunravel_fn_list = list(map(lambda x: ravel_pytree(x)[1], layers))\ndisplay(flat_params, unravel_fn_list)\n\n[DeviceArray([-2.4912864e-04,  2.7019347e-04,  8.2753241e-01,\n              -8.3148128e-01, -8.2766330e-01,  8.3254045e-01],            dtype=float32),\n DeviceArray([0.01351773, 1.1750739 , 1.1685134 ], dtype=float32)]\n\n\n[&lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;,\n &lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;]\n\n\n\ndef modified_neg_log_joint_fn(flat_params):\n    layers = jtu.tree_map(lambda unravel_fn, flat_param: unravel_fn(flat_param), unravel_fn_list, flat_params)\n    params = tree_def.unflatten(layers)\n    return neg_log_joint(params)\n\nfull_hessian = jax.hessian(modified_neg_log_joint_fn)(flat_params)\n\n# Pick diagonal entries from the Hessian\nuseful_hessians = [full_hessian[i][i] for i in range(len(full_hessian))]\nuseful_hessians\n\n[DeviceArray([[139.07985,   0.     , 138.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 410.62708,   0.     , 136.54236,   0.     ,\n               273.08472],\n              [138.07985,   0.     , 139.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 136.54236,   0.     , 137.54236,   0.     ,\n               136.54236],\n              [  0.     ,   0.     ,   0.     ,   0.     ,   1.     ,\n                 0.     ],\n              [  0.     , 273.08472,   0.     , 136.54236,   0.     ,\n               274.08472]], dtype=float32),\n DeviceArray([[400.99997,  82.72832,  83.44101],\n              [ 82.72832,  69.43975,   0.     ],\n              [ 83.44101,   0.     ,  70.35754]], dtype=float32)]\n\n\nEach entry in above list corresponds to layer-wise hessian matrices. Now, we need to create layer-wise distributions, sample from them and reconstruct params using the similar tricks we used above:\n\nclass KFACHessianLaplace:\n    def __init__(self, map_params, model):\n        self.model = model\n        layers, self.tree_def = jtu.tree_flatten(map_params, is_leaf=lambda x: 'bias' in x)\n        flat_layers = [ravel_pytree(layer) for layer in layers]\n        self.means = list(map(lambda x: x[0], flat_layers))\n        self.unravel_fn_list = list(map(lambda x: x[1], flat_layers))\n\n        def neg_log_joint_flat(flat_params):\n            flat_layers = [self.unravel_fn_list[i](flat_params[i]) for i in range(len(flat_params))]\n            params = self.tree_def.unflatten(flat_layers)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(self.means)\n        self.useful_H = [self.H[i][i] for i in range(len(self.H))]\n        \n        self.covs = [svd_inverse(matrix) for matrix in self.useful_H]\n        \n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample_partial(self, seed, unravel_fn, mean, cov):\n        sample = jax.random.multivariate_normal(seed, mean=mean, cov=cov)\n        return unravel_fn(sample)\n    \n    def _sample(self, seed):\n        seeds = [seed for seed in jax.random.split(seed, num=len(self.means))]\n        flat_sample = jtu.tree_map(self._sample_partial, seeds, self.unravel_fn_list, self.means, self.covs)\n        sample = self.tree_def.unflatten(flat_sample)\n        return sample\n    \n    def sample(self, seed, n_samples=1):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nkfac_posterior = KFACHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 1000000\ny_pred_kfac = kfac_posterior.predict(X, seed=seed, shape=(n_samples, ))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\nWe can see that KFAC is approximating the trend of Full Hessian Laplace. We can visualize the Covariance matrices as below.\n\nfig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\n\n\n\n\n\n\nComparison with MCMC\nInspired from a blackjax docs example.\n\nkey = jax.random.PRNGKey(0)\nwarmup_key, inference_key = jax.random.split(key, 2)\nnum_warmup = 5000\nnum_samples = n_samples\n\ninitial_position = model.init(key, X)\ndef logprob(params): \n    return -neg_log_joint(params)\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, _ = kernel(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n\ninit = time()\nadapt = blackjax.window_adaptation(blackjax.nuts, logprob, num_warmup)\nfinal_state, kernel, _ = adapt.run(warmup_key, initial_position)\nstates = inference_loop(inference_key, kernel, final_state, num_samples)\nsamples = states.position.unfreeze()\nprint(f\"Sampled {n_samples} samples in {time()-init:.2f} seconds\")\n\nSampled 1000000 samples in 27.85 seconds\n\n\n\ny_pred_mcmc = jax.vmap(model.apply, in_axes=(0, None))(samples, X)\n\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    az.plot_dist(y_pred_mcmc[:, i], ax=ax[i], label='mcmc', color='k')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\nfig, ax = plt.subplots(1,3,figsize=(18,5))\nfig.subplots_adjust(wspace=0.1)\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\nmcmc_cov = jnp.cov(jax.vmap(lambda x: ravel_pytree(x)[0])(samples).T)\n\nsns.heatmap(mcmc_cov, ax=ax[2], annot=True, fmt = '.2f')\nax[2].set_title('MCMC');\n\n\n\n\n\n\nLibrary versions\n\n%watermark --iversions\n\nflax      : 0.6.1\nblackjax  : 0.8.2\noptax     : 0.1.3\nmatplotlib: 3.5.1\njax       : 0.3.23\narviz     : 0.12.1\nseaborn   : 0.11.2\njson      : 2.0.9"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html",
    "href": "posts/2022-10-31-stochastic-variational-gp.html",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "",
    "text": "I recently read a compact and clean explanation of SVGP in the following blog post by Dr. Martin Ingram:\nNow, I am attempting to implement a practical code from scratch for the same (What is practical about it? Sometimes math does not simply translate to code without careful modifications). I am assuming that you have read the blog post cited above before moving further. Let’s go for coding!"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Imports",
    "text": "Imports\n\n# JAX\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\n# Partially initialize functions\nfrom functools import partial\n\n# TFP\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\n# GP Kernels\nfrom tinygp import kernels\n\n# sklearn\nfrom sklearn.datasets import make_moons, make_blobs, make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Optimization\nimport optax\n\n# Plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['scatter.edgecolors'] = \"k\"\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Jitter\nJITTER = 1e-6\n\n# Enable JAX 64bit\njax.config.update(\"jax_enable_x64\", True)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Dataset",
    "text": "Dataset\nFor this blog post, we will stick to the classification problem and pick a reasonable classification dataset.\n\nn_samples = 100\nnoise = 0.1\nrandom_state = 0\nshuffle = True\n\nX, y = make_moons(\n    n_samples=n_samples, random_state=random_state, noise=noise, shuffle=shuffle\n)\nX = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n\nX, y = map(jnp.array, (X, y))\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Methodology",
    "text": "Methodology\nTo define a GP, we need a kernel function. Let us use the RBF or Exponentiated Quadratic or Squared Exponential kernel.\n\nlengthscale = 1.0\nvariance = 1.0\n\nkernel_fn = variance * kernels.ExpSquared(scale=lengthscale)\n\nkernel_fn(X, X).shape\n\n(100, 100)\n\n\nAs explained in the blog post, we want to minimize the following loss function:\n\\[\nKL[q(u|\\eta) || p(u|y, \\theta)] = KL[q(u|\\eta) || p(u | \\theta)] - \\mathbb{E}_{u \\sim q(u|\\eta)} \\log p(y | u, \\theta) + const\n\\]\nLet us break down the loss and discuss each componant.\n\nKL divergence\nIn the first term, we want to compute the KL divergence between prior and variational distribution of GP at inducing points. First, we need to define the inducing points.\n\nkey = jax.random.PRNGKey(0)\nn_inducing = 10\nn_dim = X.shape[1]\n\nX_inducing = jax.random.normal(key, shape=(n_inducing, n_dim))\nX_inducing.shape\n\n(10, 2)\n\n\nNow, defining the prior and variational distributions.\n\ngp_mean = 0.43  # a scalar parameter to train\n\nprior_mean = gp_mean * jnp.zeros(n_inducing)\nprior_cov = kernel_fn(X_inducing, X_inducing)\n\nprior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n\nvariational_mean = jax.random.uniform(key, shape=(n_inducing,)) # a vector parameter to train\n\nA covariance matrix can not be learned directly due to positive definite constraint. We can decompose a covariance matrix in a following way:\n\\[\n\\begin{aligned}\nK &= diag(\\boldsymbol{\\sigma})\\Sigma diag(\\boldsymbol{\\sigma})\\\\\n  &= diag(\\boldsymbol{\\sigma})LL^T diag(\\boldsymbol{\\sigma})\n\\end{aligned}\n\\]\nWhere, \\(\\Sigma\\) is a correlation matrix, \\(L\\) is a lower triangular cholesky decomposition of \\(\\Sigma\\) and \\(\\boldsymbol{\\sigma}\\) is the variance vector. We can use tfb.CorrelationCholesky to generate \\(L\\) from an unconstrained vector:\n\nrandom_vector = jax.random.normal(key, shape=(3,))\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\ncorrelation = corr_chol@corr_chol.T\ncorrelation\n\nDeviceArray([[ 1.        ,  0.54464529, -0.7835968 ],\n             [ 0.54464529,  1.        , -0.33059078],\n             [-0.7835968 , -0.33059078,  1.        ]], dtype=float64)\n\n\nTo constrain \\(\\boldsymbol{\\sigma}\\), any positivity constraint would suffice. So, combining these tricks, we can model the covariance as following:\n\nrandom_vector = jax.random.normal(\n    key, shape=(n_inducing * (n_inducing - 1) // 2,)\n)  # a trainable parameter\nlog_sigma = jax.random.normal(key, shape=(n_inducing, 1))  # a trainable parameter\n\n\nsigma = jnp.exp(log_sigma)\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\nvariational_cov = sigma * sigma.T * (corr_chol @ corr_chol.T)\nprint(variational_cov.shape)\n\nvariational_distribution = tfd.MultivariateNormalFullCovariance(variational_mean, variational_cov\n)\n\n(10, 10)\n\n\nNow, we can compute the KL divergence:\n\nvariational_distribution.kl_divergence(prior_distribution)\n\nDeviceArray(416.89357355, dtype=float64)\n\n\n\n\nExpectation over the likelihood\nWe want to compute the following expectation:\n\\[\n-\\sum_{i=1}^N \\mathbb{E}_{f_i \\sim q(f_i | \\eta, \\theta)} \\log p(y_i| f_i, \\theta)\n\\]\nNote that, \\(p(y_i| f_i, \\theta)\\) can be any likelihood depending upon the problem, but for classification, we may use a Bernoulli likelihood.\n\nf = jax.random.normal(key, shape=y.shape)\nlikelihood_distribution = tfd.Bernoulli(logits=f)\n\nlog_likelihood = likelihood_distribution.log_prob(y).sum()\nlog_likelihood\n\nDeviceArray(-72.04665624, dtype=float64)\n\n\nWe need to sample \\(f_i\\) from \\(q(f_i | \\eta, \\theta)\\) which has the following form:\n\\[\n\\begin{aligned}\nq(u) &\\sim \\mathcal{N}(\\boldsymbol{m}, S)\\\\\nq(f_i | \\eta, \\theta) &\\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\\\\n\\mu_i &= A\\boldsymbol{m}\\\\\n\\sigma_i^2 &= K_{ii} + A(S - K_{mm})A^T\\\\\nA &= K_{im}K_{mm}^{-1}\n\\end{aligned}\n\\]\nNote that matrix inversion is often unstable with jnp.linalg.inv and thus we will use cholesky tricks to compute \\(A\\).\n\ndef q_f(x_i):\n    x_i = x_i.reshape(1, -1) # ensure correct shape\n    K_im = kernel_fn(x_i, X_inducing)\n    K_mm = kernel_fn(X_inducing, X_inducing)\n    chol_mm = jnp.linalg.cholesky(K_mm + jnp.eye(K_mm.shape[0])*JITTER)\n    A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n    \n    mu_i = A@variational_mean\n    sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_cov - prior_cov)@A.T\n    \n    return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n\nHere is a function to compute log likelihood for a single data-point:\n\ndef log_likelihood(x_i, y_i, seed):\n    sample = q_f(x_i).sample(seed=seed)\n    log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n    return log_likelihood.squeeze()\n\n\nlog_likelihood(X[0], y[0], seed=key)\n\nDeviceArray(-0.17831203, dtype=float64)\n\n\nWe can use jax.vmap to compute log_likelihood over a batch. With that, we can leverage the stochastic variational inference following section 10.3.1 (Eq. 10.108) from pml book2. Basically, in each iteration, we need to multiply the batch log likelihood with \\(\\frac{N}{B}\\) to get an unbiased minibatch approximation where \\(N\\) is size of the full dataset and \\(B\\) is the batch size.\n\nbatch_size = 10\n\nseeds = jax.random.split(key, num=batch_size)\n\nll = len(y)/batch_size * jax.vmap(log_likelihood)(X[:batch_size], y[:batch_size], seeds).sum()\nll\n\nDeviceArray(-215.46520331, dtype=float64)\n\n\nNote that, once the parameters are optimized, we can use the derivations of \\(q(f_i | \\eta, \\theta)\\) to compute the posterior distribution. We have figured out all the pieces by now so it is the time to put it togather in a single class. Some pointers to note are the following:\n\nWe define a single function get_constrained_params to transform all unconstrained parameters.\njax.lax.scan gives a huge boost to a training loop.\nThere is some repeatation of code due to lack of super code optimization. You can do it at your end if needed."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "All in one",
    "text": "All in one\n\nclass SVGP:\n    def __init__(self, X_inducing, data_size):\n        self.X_inducing = X_inducing\n        self.n_inducing = len(X_inducing)\n        self.data_size = data_size\n        \n    def init_params(self, seed):\n        variational_corr_chol_param = tfb.CorrelationCholesky().inverse(jnp.eye(self.n_inducing))\n        \n        dummy_params = {\"log_variance\": jnp.zeros(()),\n               \"log_scale\": jnp.zeros(()), \n               \"mean\": jnp.zeros(()),\n               \"X_inducing\": self.X_inducing,\n               \"variational_mean\": jnp.zeros(self.n_inducing),\n               \"variational_corr_chol_param\": variational_corr_chol_param,\n               \"log_variational_sigma\": jnp.zeros((self.n_inducing, 1)),\n               }\n        \n        flat_params, unravel_fn = ravel_pytree(dummy_params)\n        random_params = jax.random.normal(key, shape=(len(flat_params), ))\n        params = unravel_fn(random_params)\n        return params\n    \n    @staticmethod\n    def get_constrained_params(params):\n        return {\"mean\": params[\"mean\"],\n                \"variance\": jnp.exp(params['log_variance']), \n                \"scale\": jnp.exp(params['log_scale']), \n                \"X_inducing\": params[\"X_inducing\"],\n                \"variational_mean\": params[\"variational_mean\"],\n                \"variational_corr_chol_param\": params[\"variational_corr_chol_param\"],\n                \"variational_sigma\": jnp.exp(params[\"log_variational_sigma\"])}\n    \n    @staticmethod\n    def get_q_f(params, x_i, prior_distribution, variational_distribution):\n        x_i = x_i.reshape(1, -1) # ensure correct shape\n        \n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        K_im = kernel_fn(x_i, params[\"X_inducing\"])\n        K_mm = prior_distribution.covariance()\n        chol_mm = jnp.linalg.cholesky(K_mm)\n        A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n\n        mu_i = A@params[\"variational_mean\"]\n        sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_distribution.covariance() - K_mm)@A.T\n\n        return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n    \n    def get_distributions(self, params):\n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        prior_mean = params[\"mean\"]\n        prior_cov = kernel_fn(params[\"X_inducing\"], params[\"X_inducing\"]) + jnp.eye(self.n_inducing)*JITTER\n        prior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n        corr_chol = tfb.CorrelationCholesky()(params[\"variational_corr_chol_param\"])\n        sigma = jnp.diag(params[\"variational_sigma\"])\n        variational_cov = sigma*sigma.T*(corr_chol@corr_chol.T) + jnp.eye(self.n_inducing)*JITTER\n        variational_distribution = tfd.MultivariateNormalFullCovariance(params[\"variational_mean\"], variational_cov)\n        \n        return prior_distribution, variational_distribution\n    \n    def loss_fn(self, params, X_batch, y_batch, seed):\n        params = self.get_constrained_params(params)\n        \n        # Get distributions\n        prior_distribution, variational_distribution = self.get_distributions(params)\n        \n        # Compute kl\n        kl = variational_distribution.kl_divergence(prior_distribution)\n\n        # Compute log likelihood\n        def log_likelihood_fn(x_i, y_i, seed):\n            q_f = self.get_q_f(params, x_i, prior_distribution, variational_distribution)\n            sample = q_f.sample(seed=seed)\n            log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n            return log_likelihood.squeeze()\n        \n        seeds = jax.random.split(seed, num=len(y_batch))\n        log_likelihood = jax.vmap(log_likelihood_fn)(X_batch, y_batch, seeds).sum() * self.data_size/len(y_batch)\n\n        return kl - log_likelihood\n    \n    def fit_fn(self, X, y, init_params, optimizer, n_iters, batch_size, seed):\n        state = optimizer.init(init_params)\n        value_and_grad_fn = jax.value_and_grad(self.loss_fn)\n        \n        def one_step(params_and_state, seed):\n            params, state = params_and_state\n            idx = jax.random.choice(seed, self.data_size, (batch_size,), replace=False)\n            X_batch, y_batch = X[idx], y[idx]\n            \n            seed2 = jax.random.split(seed, 1)[0]\n            loss, grads = value_and_grad_fn(params, X_batch, y_batch, seed2)\n            updates, state = optimizer.update(grads, state)\n            params = optax.apply_updates(params, updates)\n            return (params, state), (loss, params)\n        \n        seeds = jax.random.split(seed, num=n_iters)\n        (best_params, _), (loss_history, params_history) = jax.lax.scan(one_step, (init_params, state), xs=seeds)\n        return best_params, loss_history, params_history\n\n    def predict_fn(self, params, X_new):\n        constrained_params = self.get_constrained_params(params)\n        prior_distribution, variational_distribution = self.get_distributions(constrained_params)\n        \n        def _predict_fn(x_i):    \n            # Get posterior\n            q_f = self.get_q_f(constrained_params, x_i, prior_distribution, variational_distribution)\n            return q_f.mean().squeeze(), q_f.variance().squeeze()\n        \n        mean, var = jax.vmap(_predict_fn)(X_new)\n        return mean.squeeze(), var.squeeze()"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Train and predict",
    "text": "Train and predict\n\nn_inducing = 20\nn_epochs = 100\nbatch_size = 10\ndata_size = len(y)\nn_iters = n_epochs*(data_size/batch_size)\nn_iters\n\n1000.0\n\n\n\nkey = jax.random.PRNGKey(0)\nkey2, subkey = jax.random.split(key)\noptimizer = optax.adam(learning_rate=0.01)\n\nX_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\nmodel = SVGP(X_inducing, data_size)\n\ninit_params = model.init_params(key2)\n\nmodel.loss_fn(init_params, X, y, key)\nbest_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\nplt.figure()\nplt.plot(loss_history);\nplt.title(\"Loss\");\n\n\n\n\n\nx = jnp.linspace(-3.5, 3.5, 100)\nseed = jax.random.PRNGKey(123)\n\nX1, X2 = jnp.meshgrid(x, x)\nf = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\npred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\nlogits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\nproba = jax.nn.sigmoid(logits)\n\nproba_mean = proba.mean(axis=0)\nproba_std2 = proba.std(axis=0)*2\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12,4))\ncplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot1, ax=ax[0])\n\ncplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot2, ax=ax[1])\n\nax[0].scatter(X[:, 0], X[:, 1], c=y);\nax[1].scatter(X[:, 0], X[:, 1], c=y);\n\nax[0].set_title(\"Posterior $\\mu$\");\nax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Some more datasets",
    "text": "Some more datasets\n\ndef fit_and_plot(X, y):\n    X = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n    X, y = map(jnp.array, (X, y))\n\n    X_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\n    model = SVGP(X_inducing, data_size)\n\n    init_params = model.init_params(key2)\n\n    model.loss_fn(init_params, X, y, key)\n    best_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\n    plt.figure()\n    plt.plot(loss_history);\n    plt.title(\"Loss\");\n    \n    f = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\n    pred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\n    logits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\n    proba = jax.nn.sigmoid(logits)\n\n    proba_mean = proba.mean(axis=0)\n    proba_std2 = proba.std(axis=0)*2\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    cplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot1, ax=ax[0])\n\n    cplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot2, ax=ax[1])\n\n    ax[0].scatter(X[:, 0], X[:, 1], c=y);\n    ax[1].scatter(X[:, 0], X[:, 1], c=y);\n\n    ax[0].set_title(\"Posterior $\\mu$\");\n    ax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");\n\n\nmake_blobs\n\nX, y = make_blobs(n_samples=n_samples, random_state=random_state, centers=2)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)\n\n\n\n\n\n\n\n\n\n\n\n\nmake_circles\n\nX, y = make_circles(n_samples=n_samples, random_state=random_state, noise=noise, factor=0.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)"
  },
  {
    "objectID": "posts/2022-02-25-torch-tips.html",
    "href": "posts/2022-02-25-torch-tips.html",
    "title": "PyTorch Tips",
    "section": "",
    "text": "Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code.\n\nAll the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy.\n.cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable.\nDo not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead.\nNeed to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f\"name_{zero}\"). They can be accessed with model.name_0.\nHave something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer.\nLet .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as:\n\nmodule.to(deivce) sends all parameters and buffers of model/submodules to the device.\n\nmodule.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively.\nLet .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions.\ntorch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code.\nLink the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are:\n\nsetting module.train() or module.eval() puts all submodules in train mode or eval mode respectively.\nAll submodules parameters can be accesses directly from the parent module with module.parameters().\n\nCreating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters."
  },
  {
    "objectID": "posts/2022-01-29-presentation_tips.html",
    "href": "posts/2022-01-29-presentation_tips.html",
    "title": "Conference Presentation Tips",
    "section": "",
    "text": "General\n\nFirst page goes like this:\n\nTitle\nAuthors (Underline presenting author, no need to put * in case of equal contribution)\nAffiliations\nConference name\n\nIf importing figures from paper, avoid including the captions.\nInclude lot of images and less maths\nTalk should end with summary and not the future work or thank you slide or something.\nCite the references on the same slide in bottom.\n\nRefer to “Giving talks” section of this blog.\n\n\nDos and Don’ts\n\nNever put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html",
    "href": "posts/2022-04-06-github_faqs.html",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "href": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "href": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q2: What to do if the main (or master) gets updated before I open a PR?",
    "text": "Q2: What to do if the main (or master) gets updated before I open a PR?\nPull the changes directly to your branch with:\ngit pull https://github.com/probml/pyprobml"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "href": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q3: What to do with the fork’s main when the original main is updated?",
    "text": "Q3: What to do with the fork’s main when the original main is updated?\nFetch upstream with GitHub GUI or use the same solution given in Q2."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "href": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q4: Why and when keeping the fork’s main up to date with the original main is important?",
    "text": "Q4: Why and when keeping the fork’s main up to date with the original main is important?\nWhenever we need to create new branches (usually from the fork’s main)."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "href": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q5: How to update a change in a PR that is open?",
    "text": "Q5: How to update a change in a PR that is open?\nPush the change to the corresponding branch and PR will get updated automatically."
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html",
    "href": "posts/2023-04-29-sine-combination-netowrks.html",
    "title": "Sine Combination Networks",
    "section": "",
    "text": "We know that any continuous signal can be represented as a sum of sinusoids. The question is, how many sinusoids do we need to represent a signal? In this notebook, we will explore this question.\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "title": "Sine Combination Networks",
    "section": "Random Combination of Sinusoids",
    "text": "Random Combination of Sinusoids\n\nN = 1000\nx = jnp.linspace(-10, 10, N).reshape(-1, 1)\ny = jnp.sin(x) + jnp.sin(2*x) #+ jax.random.normal(jax.random.PRNGKey(0), (N, 1)) * 0.1\nplt.plot(x, y, \"kx\");\nprint(x.shape, y.shape)\n\n(1000, 1) (1000, 1)"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "title": "Sine Combination Networks",
    "section": "Recover the Signal",
    "text": "Recover the Signal\n\ndef get_weights(key):\n    w1 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    key = jax.random.split(key)[0]\n    w2 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    return w1, w2\n    \ndef get_sine(weights, x):\n    w1, w2 = weights\n    return jnp.sin(w1*x) + jnp.sin(w2*x)\n\ndef loss_fn(weights, x, y):\n    output = get_sine(weights, x)\n    w1, w2 = weights\n    return jnp.mean((output.ravel() - y.ravel())**2)\n\n\ndef one_step(weights_and_state, xs):\n    weights, state = weights_and_state\n    loss, grads = value_and_grad_fn(weights, x, y)\n    updates, state = optimizer.update(grads, state)\n    weights = optax.apply_updates(weights, updates)\n    return (weights, state), (loss, weights)\n\nepochs = 1000\noptimizer = optax.adam(1e-2)\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nfig, ax = plt.subplots(4, 3, figsize=(15, 12))\nfig2, ax2 = plt.subplots(4, 3, figsize=(15, 12))\nax = ax.ravel()\nax2 = ax2.ravel()\nfor seed in tqdm(range(12)):\n    key = jax.random.PRNGKey(seed)\n    init_weights = get_weights(key)\n    state = optimizer.init(init_weights)\n    (weights, _), (loss_history, _) = jax.lax.scan(one_step, (init_weights, state), None, length=epochs)\n    y_pred = get_sine(weights, x)\n    ax[seed].plot(x, y, \"kx\")\n    ax[seed].plot(x, y_pred, \"r-\")\n    ax[seed].set_title(f\"w_init=({init_weights[0]:.2f}, {init_weights[1]:.2f}), w_pred=({weights[0]:.2f}, {weights[1]:.2f}), loss={loss_fn(weights, x, y):.2f}\")\n    ax2[seed].plot(loss_history)\nfig.tight_layout()\n\n100%|██████████| 12/12 [00:00&lt;00:00, 15.91it/s]"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "title": "Sine Combination Networks",
    "section": "Plot loss surface",
    "text": "Plot loss surface\n\nw1 = jnp.linspace(0, 3, 100)\nw2 = jnp.linspace(0, 3, 100)\nW1, W2 = jnp.meshgrid(w1, w2)\nloss = jax.vmap(jax.vmap(lambda w1, w2: loss_fn((w1, w2), x, y)))(W1, W2)\n\n# plot the loss surface in 3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(W1, W2, loss, cmap=\"viridis\", alpha=0.9);\nax.set_xlabel(\"w1\");\nax.set_ylabel(\"w2\");\n# top view\nax.view_init(30, 45)"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html",
    "href": "posts/2021-03-22-gp_kernels.html",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "",
    "text": "!pip install -qq GPy\nimport autograd.numpy as np\nimport pandas as pd\nimport GPy\nimport matplotlib.pyplot as plt\nfrom autograd import grad\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "href": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "RBF (Radial basis function) Kernel, Stationarity and Isotropy",
    "text": "RBF (Radial basis function) Kernel, Stationarity and Isotropy\nRBF is one of the most commonly used kernels in GPs due to it’s infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions \\(X \\to Y\\).\nRBF kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2exp\\left(-\\frac{(x-x')^2}{2l^2}\\right)\n\\end{aligned}\n\\] Where, \\(\\sigma^2\\) is variance and \\(l\\) is known as lengthscale. #### Stationarity RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, \\(\\mathcal{K}(x,x')\\) depends only on \\(x-x'\\).\n\nIsotropy\nRBF is also isotropic kernel, which means that \\(\\mathcal{K}(x,x')\\) depends only on \\(|x-x'|\\). Thus, we have \\(\\mathcal{K}(x,x') = \\mathcal{K}(x',x)\\).\nLet’s visualize few functions drawn from the RBF kernel\n\ndef K_rbf(X1, X2, sigma=1., l=1.):\n  return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2))\n\n\n\nHelper functions\n\ndef plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)):\n  mean = np.zeros(X.shape[0])\n  cov = kernel_func(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  fig = plt.figure(figsize=(14,8), constrained_layout=True)\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1])\n  ax0.set_ylim(*ax0_ylim)\n  ax1 = fig.add_subplot(gs[1, 0:2])\n  ax1.set_ylim(*ax1_ylim)\n  ax2 = fig.add_subplot(gs[1, 2:4])\n  for func in functions:\n    ax0.plot(X, func,'o-');\n  ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel');\n  ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n  sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\ndef animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), \n                      k_name='',p_name='',symbol=''):\n  fig = plt.figure(figsize=(14,8))\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]);\n  def update(p):\n    ax0.cla();ax1.cla();ax2.cla();\n    ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim)\n    if p_name == 'Lengthscale':\n      cov = kernel_func(X, X, l=p)\n    elif p_name == 'Variance':\n      cov = kernel_func(X, X, sigma=np.sqrt(p))\n    elif p_name == 'Offset':\n      cov = kernel_func(X, X, c=p)\n    elif p_name == 'Period':\n      cov = kernel_func(X, X, p=p)\n    functions = np.random.multivariate_normal(mean, cov, size=5)\n    for func in functions:\n      ax0.plot(X, func,'o-');\n    ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel\\n'+p_name+' ('+symbol+') = '+str(p));\n    ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n    sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False);\n    ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\n  anim = FuncAnimation(fig, update, frames=val_list, blit=False)\n  plt.close()\n  rc('animation', html='jshtml')\n  return anim\n\nVerifying if our kernel is consistent with GPy kernels.\n\nX = np.linspace(101,1001,200).reshape(-1,1)\nsigma, l = 7, 11\nassert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) \n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\nk_name = 'RBF'\nplot_functions(K_rbf, ax0_ylim=(-3.5,3))\n\n\n\n\nLet’s see the effect of varying parameters \\(\\sigma\\) and \\(l\\) of the RBF kernel function.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_rbf, val_list, k_name='RBF', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nl = 1.\nval_list = [1,4,9,16,25]\nanimate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26),\n                  k_name='RBF', p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWith increase in value of \\(l\\), functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in \\(l\\).\nIncreasing \\(\\sigma^2\\) increase the overall uncertainty (width of the space where 95% of the functions live) across all the points."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Matern Kernel",
    "text": "Matern Kernel\nMatern kernels are given by a general formula as following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1, x_2) =  \\sigma^2\\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\n\\Bigg)^\\nu K_\\nu\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\\Bigg)\n\\end{aligned}\n\\] Where, \\(\\Gamma\\) is gamma function and \\(K_\\nu\\) is modified Bessel function of second order.\nThe general formula is not very intuitive about the functionality of this kernel. In practice, Matern with \\(\\nu=\\{0.5,1.5,2.5\\}\\) are used, where GP with each kernel is \\((\\lceil\\nu\\rceil-1)\\) times differentiable.\nMatern functions corresponding to each \\(\\nu\\) values are defined as the following, \\[\n\\begin{aligned}\nMatern12 \\to \\mathcal{K_{\\nu=0.5}}(x_1, x_2) &=  \\sigma^2exp\\left(-\\frac{|x_1-x_2|}{l}\\right)\\\\\nMatern32 \\to \\mathcal{K_{\\nu=1.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)exp\\left(-\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)\\\\\nMatern52 \\to \\mathcal{K_{\\nu=2.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{5}|x_1-x_2|}{l}+\\frac{5(x_1-x_2)^2)}{3l^2}\\right)exp\\left(-\\frac{\\sqrt{5}|x_1-x_2|}{l}\\right)\n\\end{aligned}\n\\] Matern kernels are stationary as well as isotropic. With \\(\\nu \\to \\infty\\) they converge to \\(RBF\\) kernel. \\(Matern12\\) is also known as \\(Exponential\\) kernel in toolkits such as GPy.\nNow, let’s draw few functions from each of these versions and try to get intuition behind each of them.\n\ndef K_m12(X1, X2, sigma=1., l=1.): # v = 0.5\n  return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l))\ndef K_m32(X1, X2, sigma=1., l=1.): # v = 1.5\n  return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l))\ndef K_m52(X1, X2, sigma=1., l=1.): # v = 2.5\n  return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))*\\\n                    (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l))\n\nVerifying if our kernels are consistent with GPy kernels.\n\nX = np.linspace(101,1001,50).reshape(-1,1)\nassert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X))\nassert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X))\n\n\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\n\nfig, ax = plt.subplots(3,2,figsize=(14,10))\nnames = ['Matern12', 'Matern32', 'Matern52']\nfor k_i, kernel in enumerate([K_m12, K_m32, K_m52]):\n  mean = np.zeros(X.shape[0])\n  cov = kernel(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  for func in functions:\n    ax[k_i,0].plot(X, func);\n  ax[k_i,0].set_xlabel('X');ax[k_i,0].set_ylabel('Y');ax[k_i,0].set_title('Functions drawn from '+names[k_i]+' kernel');\n  sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax[k_i,1].set_xlabel('X');ax[k_i,1].set_ylabel('X');ax[k_i,1].set_title('Covariance matrix');\nplt.tight_layout();\n\n\n\n\nFrom the above plot, we can say that smoothness is increasing in functions as we increase \\(\\nu\\). Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52.\nLet us see effect of varying \\(\\sigma\\) and \\(l\\) on Matern32 which is more popular among the three.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_m32, val_list, k_name='Matern32', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that Matern32 kernel behaves similar to RBF with varying \\(l\\). Though, Matern32 is less smoother than RBF. A quick comparison would clarify this.\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_rbf(X,X, l=3.)[:,50], label='RBF')\nplt.plot(X, K_m32(X,X, l=3.)[:,50], label='Matern32')\nplt.legend();plt.xlabel('X');plt.ylabel('Covariance (K(0,X))');\nplt.title('K(0,X)');"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Periodic Kernel",
    "text": "Periodic Kernel\nPeriodic Kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2\\exp\\left(-\\frac{\\sin^2(\\pi|x_1 - x_2|/p)}{2l^2}\\right)\n\\end{aligned}\n\\] Where \\(p\\) is period. Let’s visualize few functions drawn from this kernel.\n\ndef K_periodic(X1, X2, sigma=1., l=1., p=3.):\n  return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2)\n\nX = np.linspace(10,1001,50).reshape(-1,1)\nassert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), \n                   GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X))\n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1\nl = 1.\np = 3.\nk_name = 'Periodic'\nplot_functions(K_periodic)\n\n\n\n\nWe will investigate the effect of varying period \\(p\\) now.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1),\n                  k_name='Periodic',p_name='Period')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFrom the above animation we can see that, all points that are \\(p\\) distance apart from each other have exactly same values because they have correlation of exactly 1 (\\(\\sigma=1 \\to covariance=correlation\\)).\nNow, we will investigate effect of lenging lengthscale \\(l\\) while other parameters are constant.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1),\n                  k_name='Periodic',p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that correlation between a pair of locations \\(\\{x_1,x_2|x_1-x_2&lt;p\\}\\) increases as the lengthscale is increased."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Linear Kernel",
    "text": "Linear Kernel\nLinear kernel (a.k.a. dot-product kernel) is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+\\sigma^2\n\\end{aligned}\n\\] Let’s visualize few functions drawn from the linear kernel\n\ndef K_lin(X1, X2, sigma=1., c=1.):\n  return (X1-c)@(X2.T-c) + sigma**2\n\n\nnp.random.seed(0)\nsigma = 1.\nc = 1.\n\nplot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7))\n\n\n\n\nLet’s see the effect of varying parameters \\(\\sigma\\) and \\(c\\) of the linear kernel function.\n\nval_list = [-3,-2,-1,0,1,2,3]\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), \n                  p_name='Offset', symbol='c')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nnp.random.seed(1)\nval_list = np.square(np.array([1,2,3,4,5,8]))\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), \n                  p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nVarying \\(c\\) parameter changes position of shallow region in covariance matrix. In other words, as \\(x \\to c\\), points close to \\(x\\) have variance \\(\\to \\sigma^2\\). Distant points have monotonically increasing variance.\nIncreasing \\(\\sigma^2\\) adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points.\n\nNon-stationary behaviour of Linear kernel\nUnlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim.\n\nfig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True)\nkerns = [K_rbf, K_m32, K_periodic, K_lin]\nk_names = ['RBF', 'Matern32', 'Periodic', 'Linear']\nX = np.linspace(-10,10,21).reshape(-1,1)\ndef update(x):\n  count = 0\n  for i in range(2):\n    for j in range(2):\n      ax.ravel()[count].cla()\n      tmp_kern = kerns[count]\n      mean = np.zeros(X.shape[0])\n      cov = tmp_kern(X,X)\n      ax.ravel()[count].plot(X, cov[:,x]);\n      ax.ravel()[count].set_xlim(X[x-3],X[x+3])\n      ax.ravel()[count].set_xlabel('X');\n      ax.ravel()[count].set_ylabel('K('+str(X[x].round(2))+',X)');\n      ax.ravel()[count].set_title('Covariance K('+str(X[x].round(2))+',X) for '+k_names[count]+' kernel');\n      count += 1\n  ax.ravel()[3].set_ylim(-5,80)\n  plt.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False)\nplt.close()\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "href": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Multiplications of kernels",
    "text": "Multiplications of kernels\nIf a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels.\n\nPeriodic * Linear\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label='Periodic')\nplt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label='Linear')\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label='Periodic*Linear')\nplt.legend(bbox_to_anchor=(1,1));plt.xlabel('X');plt.ylabel('Covariance')\nplt.title('K(0,*)');\n\n\n\n\n\n\nLinear * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nplt.plot(X, K_lin(X,X,c=-1)[:,50], label='Linear1')\nplt.plot(X, K_lin(X,X,c=1)[:,50], label='Linear2')\nplt.plot(X, K_lin(X,X,c=0.5)[:,50], label='Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label='Linear1*Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label='Linear1*Linear2*Linear3')\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\nMatern * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nk1 = K_lin(X,X,c=1)[:,50]\nk2 = K_m32(X,X)[:,50]\nplt.plot(X, k1, label='Linear')\nplt.plot(X, k2, label='Matern32')\nplt.plot(X, k1*k2, label='Matern32*Linear')\nplt.legend(bbox_to_anchor=(1,1));"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "href": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Appendix (Extra material)",
    "text": "Appendix (Extra material)\nAt this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes.\n\n\nPredicting at a single point after observing value at a single point\nLet’s see how would be a GP prediction after observing value at a single point.\nOur kernel function is given by, * \\(K(x,x')=(x-c) \\cdot (x'-c)+\\sigma^2\\)\nNow, we observe value \\(y\\) at a location \\(x\\) and we want to predict value \\(y^*\\) at location \\(x^*\\). \\[\n\\begin{aligned}\n(y^*|x_1,y_1,x^*) &= K(x^*,x) \\cdot K^{-1}(x,x)\\cdot y \\\\\n&= \\left(\\frac{(x-c)(x^*-c)+\\sigma^2}{(x-c)(x-c)+\\sigma^2}\\right)\\cdot y\n\\end{aligned}\n\\] \\(c\\) and \\(\\sigma^2\\) do not vary in numerator and denominator so, the value of \\(y^* \\propto x^*\\).\n\n\n\nPredicting at a single point after observing values at two points\nNow, we’ll take a case where two values \\({y_1, y_2}\\) are observed at \\({x_1, x_2}\\). Let us try to predict value \\(y^*\\) at \\(x^*\\).\n$$ y^* =\n\\[\\begin{bmatrix}\nK(x_1, x^*) & K(x_2,x^*)\n\\end{bmatrix}\\begin{bmatrix}\nK(x_1, x_1) & K(x_1,x_2) \\\\\nK(x_2, x_1) & K(x_2,x_2)\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_1-c)^2+\\sigma^2 & (x_1-c) (x_2-c)+\\sigma^2 \\\\\n(x_2-c) (x_1-c)+\\sigma^2 & (x_2-c)^2 +\\sigma^2\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix} \\frac{1}{\\sigma^2(x_1-x_2)^2}\n\\begin{bmatrix}\n(x_2-c)^2+\\sigma^2 & -[(x_1-c)(x_2-c)+\\sigma^2] \\\\\n-[(x_2-c) (x_1-c)+\\sigma^2] & (x_1-c)^2 +\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix} \\tag{1}\\]\nFrom Eq. (1) second term, we can say that if \\(\\sigma^2=0\\), matrix is not-invertible because determinant is zero. It means that, if \\(\\sigma^2=0\\), observing a single point is enough, we can infer values at infinite points after observing that single point.\nEvaluating Eq. (1) further, it converges to the following equation, \\[\n\\begin{aligned}\ny^* = \\frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)}\n\\end{aligned}\n\\] Interestingly, we can see that output does not depend on \\(c\\) or \\(\\sigma^2\\) anymore. Let us verify experimentally if this is true for observing more than 2 data points.\n\n\nPrepering useful functions\n\nfrom scipy.optimize import minimize\n\n\ndef cov_func(x, x_prime, sigma, c):\n  return (x-c)@(x_prime-c) + sigma**2\n\ndef neg_log_likelihood(params):\n  n = X.shape[0]\n  sigma, c, noise_std = params\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) \n  return nll_ar[0,0]\n\ndef predict(params):\n  sigma, c, noise_std = params\n  k = cov_func(X, X.T, sigma, c)\n  np.fill_diagonal(k, k.diagonal()+noise_std**2)\n  k_inv = np.linalg.pinv(k)\n  k_star = cov_func(X_test, X.T, sigma, c)\n\n  mean = k_star@k_inv@Y\n  cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T\n  return mean, cov\n\n\n\nObserving more than two points and changing hyperparameters manually\n\nX = np.array([3,4,5,6,7,8]).reshape(-1,1)\nY = np.array([6,9,8,11,10,13]).reshape(-1,1)\nX_test = np.linspace(1,8,20).reshape(-1,1)\nparams_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], \n                [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std\n\nX_extra = np.hstack([np.ones((X.shape[0], 1)), X])\nTheta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y\nX_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\nY_test_ideal = X_test_extra@Theta\n\nfig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True)\nmeans = []\nfor p_i, params in enumerate(params_grid):\n  Y_test_mean, Y_test_cov = predict(params)\n  means.append(Y_test_mean)\n  ax[p_i].scatter(X, Y, label='train')\n  ax[p_i].scatter(X_test, Y_test_mean, label='test')\n  ax[p_i].legend();ax[p_i].set_xlabel('X');ax[p_i].set_ylabel('Y');\n  ax[p_i].set_title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\n\n\n\n\n\nnp.allclose(Y_test_ideal, means[0]),\\\nnp.allclose(Y_test_ideal, means[1]),\\\nnp.allclose(Y_test_ideal, means[2]),\\\nnp.allclose(Y_test_ideal, means[3])\n\n(True, True, True, False)\n\n\n\nmodel = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1))\n# model['Gaussian_noise'].fix(10**-10)\n# model.kern.variances.fix(10**-10)\nmodel.optimize()\nmodel.plot()\nplt.plot(X_test, Y_test_ideal, label='Normal Eq. fit')\nplt.plot(X_test,model.predict(X_test)[0], label='Prediction')\nplt.legend()\nmodel\n\n\n\n\nModel: GP regression\nObjective: 13.51314321804978\nNumber of Parameters: 2\nNumber of Optimization Parameters: 2\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nlinear.variances\n2.806515343539501\n+ve\n\n\n\nGaussian_noise.variance\n2.0834221617534134\n+ve\n\n\n\n\n\n\n\n\n\nWe can see that there is no change in fit with change in \\(c\\) and \\(\\sigma\\). 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood.\n\nparams = [1., 1., 1.]\nresult = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)])\nparams = result.x\nprint(params, result.fun)\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(Y_test_ideal, Y_test_mean)\n\n[9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541\n\n\nFalse\n\n\n\n\n\n\ndef neg_log_likelihood(sigma, c, noise_std):\n  n = X.shape[0]\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov))\n  return nll_ar[0,0]\n\n\ngrad_func = grad(neg_log_likelihood, argnum=[0,1,2])\nalpha = 0.01\nloss = []\nsigma, c, noise_std = 1., 1., 1.\nfor _ in range(5000):\n  grads = grad_func(sigma, c, noise_std)\n  # print(grads)\n  sigma = sigma - alpha*grads[0]\n  c = c - alpha*grads[1]\n  noise_std = noise_std - alpha*grads[2]\n  loss.append(neg_log_likelihood(sigma, c, noise_std))\nprint(sigma, c, noise_std)\nplt.plot(loss);\nloss[-1]\n\n7.588989986845149 -2.830840439162303 32.2487569348891\n\n\n31.05187173290998\n\n\n\n\n\n\nparams = sigma, c, noise_std\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1)\n\nFalse"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html",
    "href": "posts/2022-10-21-gaussian-processes.html",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\n\nfrom tinygp.kernels import ExpSquared\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#regression",
    "href": "posts/2022-10-21-gaussian-processes.html#regression",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Regression",
    "text": "Regression\nIn this post, we will consider the regression problem of finding a reasonable map \\(X \\to \\boldsymbol{y}\\) along with uncertainty. We can do this in a simplest setting with Bayesian linear regression assuming a MultiVariate Normal (MVN) prior \\(\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\theta, \\Sigma_\\theta)\\) (why MVN? because \\(\\theta \\in (-\\infty, \\infty)\\)) and Normal likelihood \\(y \\sim \\mathcal{N}(\\boldsymbol{x}^T\\theta, \\sigma_n^2)\\) with i.i.d. assumption.\nTo start with Gaussian process regression, let us first focus on \\(\\boldsymbol{y}\\) (and ignore \\(X\\)). We assume \\(\\boldsymbol{f}\\) as a random variable and \\(\\boldsymbol{y}\\) as a realization of \\(\\boldsymbol{f}\\) with some noise. It would be a natural probabilistic assumption to assume \\(\\boldsymbol{f}\\) to be MVN distributed since its range is \\((-\\infty, \\infty)\\).\n\\[\np(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\n\\tag{prior}\n\\]\nNow, we need to bring in \\(X\\) in a reasonable way to this formulation. A core assumption connecting \\(X\\) with \\(\\boldsymbol{y}\\) is the following: &gt; if two inputs \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{x}'\\) are close to each other (how to define the closeness? kernels!), corresponding \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{y}'\\) are likely to be similar.\nWe use something known as covariance function or kernel (later is more prevalent) to define this closeness. For example, RBF or squared exponential is a well-known kernel:\n\\[\nk_{RBF}(\\boldsymbol{x}, \\boldsymbol{x}') = \\sigma^2 \\exp \\left(-{\\frac {\\|\\boldsymbol{x} -\\boldsymbol{x}' \\|^{2}}{2\\ell ^{2}}}\\right)\n\\tag{kernel}\n\\]\n\nx = jnp.array(0.0).reshape(1, 1)\nx_prime = jnp.linspace(-5,5,100).reshape(-1, 1)\n\nplt.plot(x_prime, ExpSquared()(x_prime, x));\nplt.xlabel(\"$x'$\")\nplt.title(f\"$k(x,x')$ where $x={x[0][0]}$ and $x' \\in ${plt.xlim()}\");\n\n\n\n\nThe plot above shows that value of \\(k(\\boldsymbol{x}, \\boldsymbol{x}')\\) increases as \\(\\boldsymbol{x}'\\) approaches \\(\\boldsymbol{x}\\) and reduces as it moves far from \\(\\boldsymbol{x}\\). Now, we will connect \\(X\\) with \\(\\boldsymbol{f}\\) (and thus with \\(\\boldsymbol{y}\\)) through kernel \\(k\\) with two following assumptions:\n\nDiagonal entries of \\(K_{ff}\\) represent variance of \\(f_i\\), which can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_i)\\).\nNon-diagonal entries of \\(K_{ff}\\) represent covariance between \\(f_i\\) and \\(f_j\\) and can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\).\n\nAt this point, we have made everything clear about prior \\(p(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\\). Now, we will look at the likelihood. As mentioned earlier, \\(\\boldsymbol{y}\\) is noisy realization of \\(f\\) so the following likelihood would be a simple and natural choice.\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{f}, \\sigma_n^2I)\n\\tag{likelihood}\n\\]\nTill now, we followed bottom-up approach and defined prior and likelihood for this problem. Now we will explore the top-down approach.\nOur ultimate goal is derive \\(p(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X)\\) at new inputs \\(X^*\\). This can be written as:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) = \\int p(\\boldsymbol{y}^*|\\boldsymbol{f}^*)p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)d\\boldsymbol{f}^*\n\\tag{pred post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) is the posterior distribution at inputs \\(X^*\\). Once we derive posterior \\(p(\\boldsymbol{f}|\\boldsymbol{y},X)\\), We can find \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) like following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) = \\int p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)p(\\boldsymbol{f}|\\boldsymbol{y}, X)d\\boldsymbol{f}\n\\tag{post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)\\) is a conditional Gaussian distribution with the following closed form:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{f}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*})\n\\tag{cond}\n\\]\nPosterior \\(p(\\boldsymbol{f}|\\boldsymbol{y}, X)\\) can be derived following “Bayes’ rule for Gaussians” (section 2.2.6.2 in pml book2):\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff})\n\\tag{post}\n\\]\nWe can now substitute Eq. (post) and Eq. (cond) in Eq. (post new). The integral can be solved with using Eq. 2.90 in section 2.2.6.2 in pml book2 and also mentioned in Eq. (int gaussians) in Appendix.\n\\[\n\\begin{aligned}\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{\\mu}^*, \\Sigma^*)\\\\\n\\boldsymbol{\\mu}^* &= \\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\left[\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\right]-\\boldsymbol{m}_{f})\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f))\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\\\\n\\\\\n\\Sigma^* &= K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}\\left[K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[I - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[K_{ff}^{-1} - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}\\right]K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}K_{ff^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*})\n\\end{aligned}\n\\]\nNow, we are almost there. Plugging in the above formula in Eq. (pred post) and using known result in Eq. (int gaussians), we get the predictive posterior as following:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*} + \\sigma_n^2I)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe did not exploit the special structure of likelihood variance \\(\\sigma_n^2I\\) anywhere, so, these derivations hold true for full rank likelihood covariance matrices also.\n\n\n\nOptimization\nWe perform type-II likelihood estimation (in other words, minimize log marginal likelihood or evidence term). Our goal is to find optimal model \\(\\mathcal{M}\\) represented by prior (or kernel) hyperparameters and likelihood hyperparameters. We can get the log marginal likelihood using Eq. (int gaussians):\n\\[\n\\begin{aligned}\np(\\boldsymbol{y}|X, \\mathcal{M}) &= \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\\\\n&\\sim \\int \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{f}, \\sigma_n^2I) \\mathcal{N}(\\boldsymbol{f}|\\boldsymbol{m}_f, K_{ff})\\\\\n&\\sim \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{m}_f, K_{ff}+\\sigma_n^2I)\n\\end{aligned}\n\\]\nFor case of RBF kernel, \\(\\mathcal{M}\\) parameters will be \\(\\{\\sigma, \\ell, \\sigma_n\\}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "href": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Classification (with Laplace approximation)",
    "text": "Classification (with Laplace approximation)\nWe will derive a GP predictive posterior for binary case only because for multi-class, it gets a bit complex. Our assumption for prior over the \\(\\boldsymbol{f}\\) can still be the same but likelihood needs to be changed because \\(\\boldsymbol{y}\\) is no more a real number but rather a binary value e.g. 0 or 1. From Bayesian point-of-view, Bernoulli likelihood would be the most appropriate as a likelihood here:\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) = \\prod_{i=1}^{N} \\sigma(f_i)^{y_i=1}(1-\\sigma(f_i))^{y_i=0}\n\\tag{class likelihood}\n\\]\nSince, MVN prior and Bernoulli likelihood are not conjugate, we need to use an approximate method of inference here. We use Laplace approximation to get the MAP estimate \\(\\boldsymbol{\\hat{f}}\\) and by computing the Hessian \\(H\\) of negative log joint (log prior + log likelihood) with respect to \\(\\boldsymbol{\\hat{f}}\\), we can get the posterior distribution as the following:\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{\\hat{f}}, H^{-1})\n\\tag{class post}\n\\]\nEq. (cond) will be the same in this case, and thus, we can solve Eq. (post new) as we did for regression case, like the following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{\\hat{f}}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}H^{-1}K_{ff}^{-1}K_{ff^*})\n\\]\n\nOptimization\nTo perform Type-II likelihood estimation for binary classification, we first need to derive the log marginal likelihood which can be approximated with Laplace approximation. First, we define the following quantity:\n\\[\n\\boldsymbol{\\psi}(\\boldsymbol{f}) \\triangleq \\log p(\\boldsymbol{y}|\\boldsymbol{f}) + \\log p(\\boldsymbol{f})\n\\]\nNow, computing the log marginal likelihood as suggested in section 3.4.4 of GPML book:\n\\[\n\\begin{aligned}\n\\log p(\\boldsymbol{y}|X, \\mathcal{M}) &\\sim \\log \\left[ \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\right]\\\\\n&= \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{f})\\right)d\\boldsymbol{f} \\right]\\\\\n&\\thickapprox \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) -\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f} \\right]\\\\\n&= \\log \\left[ \\exp \\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) \\int exp\\left(-\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f}\\right]\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) + \\log p(\\boldsymbol{\\hat{f}}) - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}|\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) -\\frac{1}{2}\\boldsymbol{\\hat{f}}^TK_{ff}^{-1}\\boldsymbol{\\hat{f}} - \\frac{1}{2}\\log|K_{ff}| - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}| - \\frac{N}{2}\\log(2\\pi)\n\\end{aligned}\n\\]\nOur final optimization algorithm would be as following: 1. For N iterations do 2. to 4. 2. Optimize for \\(\\boldsymbol{\\hat{f}}\\) with M iterations using standard MAP estimation (maybe use non-centered parametrization). 3. Compute gradient of parameters of \\(\\mathcal{M}\\) w.r.t. log marginal likelihood 4. Update parameters of \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#appendix",
    "href": "posts/2022-10-21-gaussian-processes.html#appendix",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Appendix",
    "text": "Appendix\n\\[\n\\int \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{x}+\\boldsymbol{b}, \\Sigma) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}, K) = \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{\\mu}+b, WKW^T+\\Sigma)\n\\tag{int gaussians}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Graph Neural Networks for Regression\n\n\n\n\n\n\n\nML\n\n\n\n\nChallenges in using GNNs for regression using various strategies\n\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nConditional Neural Processes for Image Interpolation\n\n\n\n\n\n\n\nML\n\n\n\n\nExtreme Image Interpolation with Conditional Neural processes\n\n\n\n\n\n\nMay 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nPasswordless SSH setup for MacOS Hosts\n\n\n\n\n\n\n\nmacOS\n\n\n\n\nA tiny handbook to setup passwordless ssh in MacOS\n\n\n\n\n\n\nMay 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nSine Combination Networks\n\n\n\n\n\n\n\nML\n\n\n\n\nChallenges in fitting to a combination of sine waves\n\n\n\n\n\n\nApr 29, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network Gaussian Process\n\n\n\n\n\n\n\nGP\n\n\nML\n\n\n\n\nExploring NTK kernels + GPJax with toy datasets\n\n\n\n\n\n\nMar 28, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nStochastic Variational Gaussian processes in JAX\n\n\n\n\n\n\n\nGP\n\n\n\n\nA practical implementation of Hensman et al. 2015 from scratch in JAX\n\n\n\n\n\n\nOct 31, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nMulti-Output Gaussian Processes\n\n\n\n\n\n\n\nML\n\n\n\n\nExploring MOGPs from scratch\n\n\n\n\n\n\nOct 27, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Processes - A no-skip-math version\n\n\n\n\n\n\n\nML\n\n\n\n\nEnd-to-end math derivations for Gaussian process regression and classification\n\n\n\n\n\n\nOct 21, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nTrain NN with KFAC-Laplace in JAX\n\n\n\n\n\n\n\nML\n\n\n\n\nExploring KFAC-Laplace approximation on simple problems in JAX\n\n\n\n\n\n\nOct 18, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nConditional Neural Processes in JAX\n\n\n\n\n\n\n\nML\n\n\n\n\nImplementing conditional neural processes from scratch in JAX\n\n\n\n\n\n\nAug 1, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nJAX Optimizers\n\n\n\n\n\n\n\nML\n\n\n\n\nPros and cons of several jax optimizers.\n\n\n\n\n\n\nJun 10, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nGet a list of contributors from a repo\n\n\n\n\n\n\n\nGitHub\n\n\n\n\nGet contributors’ list using GitHub API and pandas\n\n\n\n\n\n\nMay 17, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nIteratively reweighted least squares (IRLS) logistic regression\n\n\n\n\n\n\n\nML\n\n\n\n\nImplementation of IRLS from Probabilistic ML book of Dr. Kevin Murphy and its comparison with naive second order implementation.\n\n\n\n\n\n\nMay 14, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nGcloud cheatsheet\n\n\n\n\n\n\n\nGcloud\n\n\n\n\nMost used commands while working with gcloud\n\n\n\n\n\n\nApr 9, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nGitHub Contrubuting FAQs\n\n\n\n\n\n\n\nGitHub\n\n\n\n\nThis is a collection of FAQs/road-blocks/queries/issues I had over the past 2 years of engagement with GitHub.\n\n\n\n\n\n\nApr 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nTorch essentials\n\n\n\n\n\n\n\nML\n\n\n\n\nPractical and direct introduction to PyTorch\n\n\n\n\n\n\nMar 8, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nProbabilistic Machine Learning\n\n\n\n\n\n\n\nML\n\n\n\n\nA video lecture series from Prof. Philipp Hennig\n\n\n\n\n\n\nMar 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nUncertainty in Deep Learning\n\n\n\n\n\n\n\nML\n\n\n\n\nReview of PhD thesis of Dr. Yarin Gal\n\n\n\n\n\n\nMar 5, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch Tips\n\n\n\n\n\n\n\nML\n\n\n\n\nPyTorch zen tips\n\n\n\n\n\n\nFeb 25, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nConference Presentation Tips\n\n\n\n\n\n\n\nAcademic\n\n\n\n\nConference Presentation Tips\n\n\n\n\n\n\nJan 29, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nComparing Gaussian Process Regression Frameworks\n\n\n\n\n\n\n\nML\n\n\n\n\nA basic comparison among GPy, GPyTorch and TinyGP\n\n\n\n\n\n\nJan 25, 2022\n\n\nZeel B Patel, Harsh Patel, Shivam Sahni\n\n\n\n\n\n\n  \n\n\n\n\nQuery by Committee\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to QBC with Random Forest Classifier.\n\n\n\n\n\n\nJan 24, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nKL divergence v/s cross-entropy\n\n\n\n\n\n\n\nML\n\n\n\n\nUnderstanding KL divergence\n\n\n\n\n\n\nJan 20, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nWhy .py files are better than .ipynb files for ML codebase\n\n\n\n\n\n\n\nPython\n\n\n\n\nWhere .py files are better than .ipynb files?\n\n\n\n\n\n\nJan 15, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nAnonymization tips for double-blind submission\n\n\n\n\n\n\n\nAcademic\n\n\n\n\nA last-minute help list\n\n\n\n\n\n\nOct 26, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nInput Warped GPs - A failed idea\n\n\n\n\n\n\n\nML\n\n\n\n\nAn idea of input warping GPs\n\n\n\n\n\n\nOct 23, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nSparseGPs in Stheno\n\n\n\n\n\nA simple demo of sparse regression in stheno with VFE and FITC methods.\n\n\n\n\n\n\nOct 12, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nDocker Cheatsheet\n\n\n\n\n\n\n\nDocker\n\n\n\n\nMost used command while working with Docker\n\n\n\n\n\n\nSep 28, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nHow to apply constraint on parameters in various GP libraries\n\n\n\n\n\nApply constraints in GPy, GPFlow, GPyTorch\n\n\n\n\n\n\nSep 27, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Kernels in Gaussian Processes\n\n\n\n\n\n\n\nML\n\n\n\n\nAn exploratory analysis of kernels in GPs\n\n\n\n\n\n\nMar 22, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\n\nSep 21, 2020\n\n\nZeel B Patel\n\n\n\n\n\n\n  \n\n\n\n\nActive Learning with Bayesian Linear Regression\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to Active Learning with Bayesian Linear Regression.\n\n\n\n\n\n\nMar 28, 2020\n\n\nZeel B Patel, Nipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Zeel. This is my personal academic blog where I add coding + other resources related to my research. Head over to this page for my personal website."
  }
]