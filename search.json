[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Machine Learning Cheat Sheet\n\n\n\n\n\n\nML\n\n\n\nPreparing for ML Breadth and Depth for Interviews\n\n\n\n\n\nApr 24, 2025\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of Class Names on VLM Object Detection\n\n\n\n\n\n\nML\n\n\nCV\n\n\nVLM\n\n\n\nExploring the ‚Äúlanguage‚Äù part of VLM (Florence-2) for object detection\n\n\n\n\n\nFeb 15, 2025\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nObject Detection Random Baseline\n\n\n\n\n\n\nML\n\n\nCV\n\n\n\nCompare your performance with a random baseline.\n\n\n\n\n\nFeb 10, 2025\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nObject Detection - A how-to guide\n\n\n\n\n\n\nML\n\n\nCV\n\n\n\nBasic operations in object detection task\n\n\n\n\n\nDec 29, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload CPCB CAAQM locations\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB CAAQM locations using Selenium\n\n\n\n\n\nDec 27, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload CPCB live data\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB data with selenium\n\n\n\n\n\nDec 10, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals across ML domains\n\n\n\n\n\n\nML\n\n\n\nKnowledge transfer between ML domains\n\n\n\n\n\nJun 25, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\n\nML\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\nSep 16, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\n\nML\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\nSep 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Basis Regression\n\n\n\n\n\n\nML\n\n\n\nBayesian Basis Regression\n\n\n\n\n\nAug 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow numpy handles day-to-day algebra?\n\n\n\n\n\n\nNumPy, Mathematics\n\n\n\nA deep dive into basic operations of numpy\n\n\n\n\n\nAug 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nCan Rank 1 GPs represent all GPs?\n\n\n\n\n\n\nML, GP\n\n\n\nA trial\n\n\n\n\n\nJul 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload low-cost data from OpenAQ\n\n\n\n\n\n\nML, GP\n\n\n\nA guide to download low-cost sensor data from OpenAQ\n\n\n\n\n\nJul 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with GPs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of GPs for climate modeling\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-class classification with Gaussian Processes\n\n\n\n\n\n\nML, GP\n\n\n\nMulti-class GP classification with different strategies\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with SIRENs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of SIRENs for climate modeling\n\n\n\n\n\nJul 1, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGNNs and GPs\n\n\n\n\n\n\nML\n\n\n\nExploring similarities between GNNs and GPs\n\n\n\n\n\nJun 23, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Neural Networks for Regression\n\n\n\n\n\n\nML\n\n\n\nChallenges in using GNNs for regression using various strategies\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBasis functions\n\n\n\n\n\n\nML\n\n\n\nExploring basis functions\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes for Image Interpolation\n\n\n\n\n\n\nML\n\n\n\nExtreme Image Interpolation with Conditional Neural processes\n\n\n\n\n\nMay 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPasswordless SSH setup for MacOS Hosts\n\n\n\n\n\n\nmacOS\n\n\n\nA tiny handbook to setup passwordless ssh in MacOS\n\n\n\n\n\nMay 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSine Combination Networks\n\n\n\n\n\n\nML\n\n\n\nChallenges in fitting to a combination of sine waves\n\n\n\n\n\nApr 29, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Gaussian Process\n\n\n\n\n\n\nGP\n\n\nML\n\n\n\nExploring NTK kernels + GPJax with toy datasets\n\n\n\n\n\nMar 28, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Variational Gaussian processes in JAX\n\n\n\n\n\n\nGP\n\n\n\nA practical implementation of Hensman et al.¬†2015 from scratch in JAX\n\n\n\n\n\nOct 31, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Output Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nExploring MOGPs from scratch\n\n\n\n\n\nOct 27, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Processes - A no-skip-math version\n\n\n\n\n\n\nML\n\n\n\nEnd-to-end math derivations for Gaussian process regression and classification\n\n\n\n\n\nOct 21, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTrain NN with KFAC-Laplace in JAX\n\n\n\n\n\n\nML\n\n\n\nExploring KFAC-Laplace approximation on simple problems in JAX\n\n\n\n\n\nOct 18, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes in JAX\n\n\n\n\n\n\nML\n\n\n\nImplementing conditional neural processes from scratch in JAX\n\n\n\n\n\nAug 1, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nJAX Optimizers\n\n\n\n\n\n\nML\n\n\n\nPros and cons of several jax optimizers.\n\n\n\n\n\nJun 10, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGet a list of contributors from a repo\n\n\n\n\n\n\nGitHub\n\n\n\nGet contributors‚Äô list using GitHub API and pandas\n\n\n\n\n\nMay 17, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nIteratively reweighted least squares (IRLS) logistic regression\n\n\n\n\n\n\nML\n\n\n\nImplementation of IRLS from Probabilistic ML book of Dr.¬†Kevin Murphy and its comparison with naive second order implementation.\n\n\n\n\n\nMay 14, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGcloud cheatsheet\n\n\n\n\n\n\nGcloud\n\n\n\nMost used commands while working with gcloud\n\n\n\n\n\nApr 9, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Contrubuting FAQs\n\n\n\n\n\n\nGitHub\n\n\n\nThis is a collection of FAQs/road-blocks/queries/issues I had over the past 2 years of engagement with GitHub.\n\n\n\n\n\nApr 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTorch essentials\n\n\n\n\n\n\nML\n\n\n\nPractical and direct introduction to PyTorch\n\n\n\n\n\nMar 8, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\n\n\n\nML\n\n\n\nA video lecture series from Prof.¬†Philipp Hennig\n\n\n\n\n\nMar 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in Deep Learning\n\n\n\n\n\n\nML\n\n\n\nReview of PhD thesis of Dr.¬†Yarin Gal\n\n\n\n\n\nMar 5, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Tips\n\n\n\n\n\n\nML\n\n\n\nPyTorch zen tips\n\n\n\n\n\nFeb 25, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConference Presentation Tips\n\n\n\n\n\n\nAcademic\n\n\n\nConference Presentation Tips\n\n\n\n\n\nJan 29, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Gaussian Process Regression Frameworks\n\n\n\n\n\n\nML\n\n\n\nA basic comparison among GPy, GPyTorch and TinyGP\n\n\n\n\n\nJan 25, 2022\n\n\nZeel B Patel, Harsh Patel, Shivam Sahni\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Committee\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to QBC with Random Forest Classifier.\n\n\n\n\n\nJan 24, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nKL divergence v/s cross-entropy\n\n\n\n\n\n\nML\n\n\n\nUnderstanding KL divergence\n\n\n\n\n\nJan 20, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nWhy .py files are better than .ipynb files for ML codebase\n\n\n\n\n\n\nPython\n\n\n\nWhere .py files are better than .ipynb files?\n\n\n\n\n\nJan 15, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nAnonymization tips for double-blind submission\n\n\n\n\n\n\nAcademic\n\n\n\nA last-minute help list\n\n\n\n\n\nOct 26, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nInput Warped GPs - A failed idea\n\n\n\n\n\n\nML\n\n\n\nAn idea of input warping GPs\n\n\n\n\n\nOct 23, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSparseGPs in Stheno\n\n\n\n\n\nA simple demo of sparse regression in stheno with VFE and FITC methods.\n\n\n\n\n\nOct 12, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDocker Cheatsheet\n\n\n\n\n\n\nDocker\n\n\n\nMost used command while working with Docker\n\n\n\n\n\nSep 28, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow to apply constraint on parameters in various GP libraries\n\n\n\n\n\nApply constraints in GPy, GPFlow, GPyTorch\n\n\n\n\n\nSep 27, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Kernels in Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nAn exploratory analysis of kernels in GPs\n\n\n\n\n\nMar 22, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nSep 21, 2020\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nActive Learning with Bayesian Linear Regression\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to Active Learning with Bayesian Linear Regression.\n\n\n\n\n\nMar 28, 2020\n\n\nZeel B Patel, Nipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#setup",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#setup",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Setup",
    "text": "Setup\n\nConfigure your API keys\nTo fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n\nOpen your HuggingFace Settings page. Click Access Tokens then New Token to generate new token.\nGo to your Roboflow Settings page. Click Copy. This will place your private key in the clipboard.\nIn Colab, go to the left pane and click on Secrets (üîë).\n\nStore HuggingFace Access Token under the name HF_TOKEN.\nStore Roboflow API Key under the name ROBOFLOW_API_KEY.\n\n\n\n\nSelect the runtime\nLet‚Äôs make sure that we have access to GPU. We can use nvidia-smi command to do that. In case of any problems navigate to Edit -&gt; Notebook settings -&gt; Hardware accelerator, set it to L4 GPU, and then click Save.\n\n!nvidia-smi\n\nSun Feb 16 17:43:09 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n| N/A   58C    P0            357W /  500W |   19831MiB /  81920MiB |     21%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n| N/A   37C    P0             76W /  500W |   12139MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n| N/A   42C    P0             87W /  500W |   19831MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n| N/A   34C    P0             62W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   2623860      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n|    1   N/A  N/A   2616743      C   ...onda3/envs/shataxi_space/bin/python      12128MiB |\n|    2   N/A  N/A   2626486      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n\nDownload example data\nNOTE: Feel free to replace our example image with your own photo.\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!ls -lh\n\ntotal 12M\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.1\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.2\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.3\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.4\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.5\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:42 'how-to-finetune-florence-2-on-detection-dataset copy 2.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:42 'how-to-finetune-florence-2-on-detection-dataset copy.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:41  how-to-finetune-florence-2-on-detection-dataset.ipynb\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Feb 16 17:34  model_checkpoints\ndrwxrwxr-x 5 patel_zeel patel_zeel 4.0K Feb 16 17:29  poker-cards-4\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.5M Jan 22 10:51  scratchpad.ipynb\n\n\n\nEXAMPLE_IMAGE_PATH = \"dog.jpeg\""
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#download-and-configure-the-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#download-and-configure-the-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Download and configure the model",
    "text": "Download and configure the model\nLet‚Äôs download the model checkpoint and configure it so that you can fine-tune it later on.\n\n# !pip install -q transformers flash_attn timm einops peft\n# !pip install -q roboflow git+https://github.com/roboflow/supervision.git\n\n\n# @title Imports\n\nimport io\nimport os\nimport re\nimport json\nimport torch\nimport html\nimport base64\nimport itertools\n\nimport numpy as np\nimport supervision as sv\n\n# from google.colab import userdata\nfrom IPython.core.display import display, HTML\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom roboflow import Roboflow\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n\n\nLoad the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library. Note that you need to pass trust_remote_code as True since this model is not a standard transformers model.\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\n# REVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#run-inference-with-pre-trained-florence-2-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#run-inference-with-pre-trained-florence-2-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Run inference with pre-trained Florence-2 model",
    "text": "Run inference with pre-trained Florence-2 model\n\n# @title Example object detection inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\n\n# @title Example image captioning inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;DETAILED_CAPTION&gt;\"\ntext = \"&lt;DETAILED_CAPTION&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\nresponse\n\n{'&lt;DETAILED_CAPTION&gt;': 'In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.'}\n\n\n\n# @title Example caption to phrase grounding inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\ntext = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt; Vehicle\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tune-florence-2-on-custom-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tune-florence-2-on-custom-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom dataset",
    "text": "Fine-tune Florence-2 on custom dataset\n\nDownload dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 5 {dataset.location}/train/annotations.jsonl\n\n{\"image\":\"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;jack  of clubs&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;queen of clubs&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;king of clubs&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n{\"image\":\"IMG_20220316_171515_jpg.rf.e3b1932bb375b3b3912027647586daa8.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;7 of clubs&lt;loc_363&gt;&lt;loc_484&gt;&lt;loc_552&gt;&lt;loc_905&gt;8 of clubs&lt;loc_535&gt;&lt;loc_449&gt;&lt;loc_757&gt;&lt;loc_971&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e30257ec169a2bfdfecb693211d37250.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_596&gt;&lt;loc_535&gt;&lt;loc_859&gt;&lt;loc_982&gt;jack of diamonds&lt;loc_211&gt;&lt;loc_546&gt;&lt;loc_411&gt;&lt;loc_880&gt;queen of diamonds&lt;loc_430&gt;&lt;loc_34&gt;&lt;loc_692&gt;&lt;loc_518&gt;king of diamonds&lt;loc_223&gt;&lt;loc_96&gt;&lt;loc_451&gt;&lt;loc_523&gt;10 of diamonds&lt;loc_387&gt;&lt;loc_542&gt;&lt;loc_604&gt;&lt;loc_925&gt;\"}\n{\"image\":\"IMG_20220316_143407_jpg.rf.e1eb3be3efc6c3bbede436cfb5489e7c.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"ace of hearts&lt;loc_345&gt;&lt;loc_315&gt;&lt;loc_582&gt;&lt;loc_721&gt;2 of hearts&lt;loc_709&gt;&lt;loc_115&gt;&lt;loc_888&gt;&lt;loc_509&gt;3 of hearts&lt;loc_529&gt;&lt;loc_228&gt;&lt;loc_735&gt;&lt;loc_613&gt;4 of hearts&lt;loc_98&gt;&lt;loc_421&gt;&lt;loc_415&gt;&lt;loc_845&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e4c229a9128494d17992cbe88af575df.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_141&gt;&lt;loc_18&gt;&lt;loc_404&gt;&lt;loc_465&gt;jack of diamonds&lt;loc_589&gt;&lt;loc_120&gt;&lt;loc_789&gt;&lt;loc_454&gt;queen of diamonds&lt;loc_308&gt;&lt;loc_482&gt;&lt;loc_570&gt;&lt;loc_966&gt;king of diamonds&lt;loc_549&gt;&lt;loc_477&gt;&lt;loc_777&gt;&lt;loc_904&gt;10 of diamonds&lt;loc_396&gt;&lt;loc_75&gt;&lt;loc_613&gt;&lt;loc_458&gt;\"}\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n# # read jsonl file\n# def read_jsonl(file_path: str) -&gt; Generator[Dict[str, Any], None, None]:\n#     with open(file_path, \"r\") as f:\n#         for line in f:\n#             yield json.loads(line)\n\n# lines = []\n# split = \"test\"     \n# for line in read_jsonl(dataset.location + f\"/{split}/annotations.jsonl\"):\n#     # print(line)\n#     # edit = True\n#     # copied_line = list(line['suffix'])\n#     # for i in range(len(copied_line)):\n#     #     if copied_line[i] == \"&lt;\":\n#     #         edit = False\n#     #     elif copied_line[i] == \"&gt;\":\n#     #         edit = True\n#     #     else:\n#     #         if edit:\n#     #             copied_line[i] = chr(ord(copied_line[i]) + 1)\n#     # copied_line = \"\".join(copied_line)\n#     # line['suffix'] = copied_line\n    \n#     line['suffix'] = line['suffix'].replace(\"club\", \"dog\").replace(\"diamond\", \"cat\").replace(\"heart\", \"bird\").replace(\"spade\", \"fish\")\n#     print(line)\n#     lines.append(line)\n\n# with open(dataset.location + f\"/{split}/annotations.jsonl\", \"w\") as f:\n#     for line in lines:\n#         f.write(json.dumps(line) + \"\\n\")\n\n\n# @title Define `DetectionsDataset` class\n\nclass JSONLDataset:\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.jsonl_file_path = jsonl_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -&gt; List[Dict[str, Any]]:\n        entries = []\n        with open(self.jsonl_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -&gt; int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Image.Image, Dict[str, Any]]:\n        if idx &lt; 0 or idx &gt;= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n\n\nclass DetectionDataset(Dataset):\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, data = self.dataset[idx]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        return prefix, suffix, image\n\n\n# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n    return inputs, answers\n\ntrain_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/train/\"\n)\nval_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/valid/\"\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n\n# @title Setup LoRA Florence-2 model\n\n# config = LoraConfig(\n#     r=8,\n#     lora_alpha=8,\n#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n#     task_type=\"CAUSAL_LM\",\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     inference_mode=False,\n#     use_rslora=True,\n#     init_lora_weights=\"gaussian\",\n# )\nconfig = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n        task_type=\"CAUSAL_LM\",\n        lora_dropout=0.05,\n        bias=\"none\",\n        inference_mode=False,\n        use_rslora=True,\n        init_lora_weights=\"gaussian\",\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n\ntrainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n\n\n\ntorch.cuda.empty_cache()\n\n\n# @title Run inference with pre-trained Florence-2 model on validation dataset\n\ndef render_inline(image: Image.Image, resize=(128, 128)):\n    \"\"\"Convert image into inline html.\"\"\"\n    image.resize(resize)\n    with io.BytesIO() as buffer:\n        image.save(buffer, format='jpeg')\n        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n        return f\"data:image/jpeg;base64,{image_b64}\"\n\n\ndef render_example(image: Image.Image, response):\n    try:\n        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n    except:\n        print('failed to redner model response')\n    return f\"\"\"\n&lt;div style=\"display: inline-flex; align-items: center; justify-content: center;\"&gt;\n    &lt;img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" /&gt;\n    &lt;p style=\"width:512px; margin:10px; font-size:small;\"&gt;{html.escape(json.dumps(response))}&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n\ndef render_inference_results(model, dataset: DetectionDataset, count: int):\n    html_out = \"\"\n    count = min(count, len(dataset))\n    for i in range(count):\n        image, data = dataset.dataset[i]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        answer = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n        html_out += render_example(image, answer)\n\n    display(HTML(html_out))\n\nrender_inference_results(peft_model, val_dataset, 4)\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom object detection dataset",
    "text": "Fine-tune Florence-2 on custom object detection dataset\n\n# @title Define train loop\n\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    render_inference_results(peft_model, val_loader.dataset, 6)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n            render_inference_results(peft_model, val_loader.dataset, 6)\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n\n%%time\n\nEPOCHS = 10\nLR = 5e-6\n\ntrain_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"tablecloth\"]}}\n\n\n\nTraining Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:30&lt;00:00,  1.11s/it]\n\n\nAverage Training Loss: 4.254726015469608\n\n\nValidation Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08&lt;00:00,  1.04s/it]\n\n\nAverage Validation Loss: 2.7121481597423553\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625], [164.16000366210938, 330.55999755859375, 301.1199951171875, 584.6400146484375], [173.1199951171875, 14.399999618530273, 303.03997802734375, 252.47999572753906], [53.439998626708984, 239.0399932861328, 166.0800018310547, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"queen spades\", \"9 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.68000030517578, 267.1999816894531, 411.8399963378906], [200.63999938964844, 82.23999786376953, 381.1199951171875, 323.5199890136719], [332.47998046875, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [86.72000122070312, 163.51998901367188, 255.67999267578125, 401.6000061035156]], \"labels\": [\"queen of spades\", \"queen spades\", \"queen card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062], [331.1999816894531, 151.36000061035156, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"6 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [209.59999084472656, 285.7599792480469, 345.91998291015625, 461.7599792480469]], \"labels\": [\"queen of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 427.8399963378906], [97.5999984741211, 433.5999755859375, 314.55999755859375, 564.7999877929688], [309.44000244140625, 424.0, 547.5199584960938, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"6 of spoons\"]}}\n\n\n\nSetting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\nTraining Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:30&lt;00:00,  1.10s/it]\n\n\nAverage Training Loss: 2.237232381806654\n\n\nValidation Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.78it/s]\n\n\nAverage Validation Loss: 1.658259004354477\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625], [311.3599853515625, 360.6399841308594, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.68000030517578, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.0399932861328]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[256.9599914550781, 167.36000061035156, 388.79998779296875, 317.1199951171875], [86.08000183105469, 164.16000366210938, 255.0399932861328, 403.5199890136719], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375]], \"labels\": [\"queen of spades\", \"jack of spade\", \"queen spades\", \"9 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.75999450683594, 556.47998046875, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062], [333.1199951171875, 152.0, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 466.8799743652344, 397.1199951171875], [208.95999145507812, 285.7599792480469, 345.91998291015625, 461.7599792480469], [14.399999618530273, 254.39999389648438, 214.0800018310547, 464.9599914550781], [464.3199768066406, 223.0399932861328, 635.8399658203125, 405.44000244140625]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 564.7999877929688], [310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [292.79998779296875, 176.3199920654297, 625.5999755859375, 399.03997802734375], [11.199999809265137, 228.79998779296875, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:32&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 1.6488525174996431\n\n\nValidation Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.77it/s]\n\n\nAverage Validation Loss: 1.5677409619092941\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 168.63999938964844, 470.0799865722656], [310.0799865722656, 360.6399841308594, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"king of hearts\", \"9 of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 129.59999084472656, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 43.84000015258789, 516.1599731445312, 207.0399932861328], [198.0800018310547, 175.0399932861328, 488.6399841308594, 497.5999755859375]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [87.36000061035156, 164.8000030517578, 254.39999389648438, 401.6000061035156], [257.6000061035156, 168.63999938964844, 388.1600036621094, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 152.0, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 637.1199951171875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [209.59999084472656, 285.7599792480469, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 564.7999877929688], [310.0799865722656, 424.0, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:32&lt;00:00,  1.47it/s]\n\n\nAverage Training Loss: 1.5562010801890318\n\n\nValidation Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.78it/s]\n\n\nAverage Validation Loss: 1.5180256962776184\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[371.5199890136719, 111.04000091552734, 512.9599609375, 358.0799865722656], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 168.63999938964844, 470.0799865722656], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"jack of hearts\", \"queen spades\", \"9 of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[1.5999999046325684, 129.59999084472656, 266.55999755859375, 411.8399963378906], [200.63999938964844, 86.08000183105469, 379.8399963378906, 323.5199890136719], [335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [87.36000061035156, 164.8000030517578, 254.39999389648438, 401.6000061035156]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"10 of space of sp hearts\", \"king of spates\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 152.0, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 464.3199768066406], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 564.7999877929688], [310.0799865722656, 424.0, 548.7999877929688, 561.5999755859375], [291.5199890136719, 176.3199920654297, 625.5999755859375, 399.03997802734375], [10.559999465942383, 228.79998779296875, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:47&lt;00:00,  1.27it/s]\n\n\nAverage Training Loss: 1.4909407706821667\n\n\nValidation Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.12it/s]\n\n\nAverage Validation Loss: 1.4898371398448944\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 168.63999938964844, 470.0799865722656], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875], [371.5199890136719, 112.31999969482422, 512.3200073242188, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"9 of hearts\", \"jack of hearts\", \"queen of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.63999938964844, 86.08000183105469, 379.8399963378906, 323.5199890136719], [335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [87.36000061035156, 164.8000030517578, 254.39999389648438, 401.6000061035156]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"10 of space of sp hearts\", \"king of spates\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 461.7599792480469], [13.119999885559082, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:28&lt;00:00,  1.09s/it]\n\n\nAverage Training Loss: 1.4595082191859974\n\n\nValidation Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.75it/s]\n\n\nAverage Validation Loss: 1.4713604301214218\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 169.27999877929688, 470.0799865722656], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875], [371.5199890136719, 112.31999969482422, 512.9599609375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 585.2799682617188], [310.0799865722656, 360.6399841308594, 447.03997802734375, 616.6400146484375]], \"labels\": [\"9 of hearts\", \"jack of hearts\", \"queen of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 45.119998931884766, 515.5199584960938, 207.0399932861328], [0.9599999785423279, 129.59999084472656, 266.55999755859375, 410.55999755859375], [202.55999755859375, 86.08000183105469, 379.8399963378906, 323.5199890136719], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 167.36000061035156, 388.79998779296875, 317.1199951171875], [87.36000061035156, 164.8000030517578, 254.39999389648438, 401.6000061035156], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"king of space\", \"10 of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [438.0799865722656, 159.0399932861328, 555.8399658203125, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.63999938964844, 230.0800018310547, 331.8399963378906, 637.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 464.3199768066406], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:33&lt;00:00,  1.13s/it]\n\n\nAverage Training Loss: 1.427160604911692\n\n\nValidation Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.73it/s]\n\n\nAverage Validation Loss: 1.4624250680208206\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 169.9199981689453, 470.0799865722656], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875], [371.5199890136719, 112.31999969482422, 512.9599609375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 585.2799682617188], [310.0799865722656, 358.0799865722656, 447.03997802734375, 616.6400146484375]], \"labels\": [\"9 of hearts\", \"jack of hearts\", \"queen of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [0.9599999785423279, 129.59999084472656, 266.55999755859375, 410.55999755859375], [202.55999755859375, 86.08000183105469, 379.8399963378906, 323.5199890136719], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 233.27999877929688, 518.0800170898438, 491.8399963378906], [257.6000061035156, 166.0800018310547, 390.0799865722656, 317.1199951171875], [87.36000061035156, 162.87998962402344, 254.39999389648438, 401.6000061035156], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"king of space\", \"10 of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [438.0799865722656, 159.0399932861328, 555.8399658203125, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.63999938964844, 230.0800018310547, 331.1999816894531, 637.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 461.7599792480469], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:34&lt;00:00,  1.13s/it]\n\n\nAverage Training Loss: 1.4242495412335676\n\n\nValidation Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.76it/s]\n\n\nAverage Validation Loss: 1.4549922198057175\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 169.9199981689453, 474.55999755859375], [173.1199951171875, 13.119999885559082, 303.03997802734375, 254.39999389648438], [371.5199890136719, 112.31999969482422, 513.5999755859375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 586.5599975585938], [310.0799865722656, 358.0799865722656, 447.03997802734375, 616.6400146484375]], \"labels\": [\"9 of hearts\", \"jack of hearts\", \"queen of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [0.9599999785423279, 129.59999084472656, 266.55999755859375, 410.55999755859375], [202.55999755859375, 86.08000183105469, 379.8399963378906, 323.5199890136719], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 233.27999877929688, 518.0800170898438, 491.8399963378906], [257.6000061035156, 166.0800018310547, 390.0799865722656, 317.1199951171875], [88.63999938964844, 164.8000030517578, 254.39999389648438, 401.6000061035156], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [22.079999923706055, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of clubs\", \"king of sp hearts\", \"10 of sp diamonds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [438.0799865722656, 159.0399932861328, 555.8399658203125, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.63999938964844, 230.0800018310547, 331.1999816894531, 637.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 461.7599792480469], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:33&lt;00:00,  1.13s/it]\n\n\nAverage Training Loss: 1.3993427350240595\n\n\nValidation Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.76it/s]\n\n\nAverage Validation Loss: 1.4506149142980576\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 169.9199981689453, 470.0799865722656], [307.5199890136719, 358.0799865722656, 447.03997802734375, 616.6400146484375], [369.6000061035156, 111.04000091552734, 513.5999755859375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 585.2799682617188], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875]], \"labels\": [\"9 of hearts\", \"10 of hearts\", \"queen of hearts\", \"king of hearts\", \"jack of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [0.9599999785423279, 129.59999084472656, 266.55999755859375, 410.55999755859375], [202.55999755859375, 86.08000183105469, 379.8399963378906, 323.5199890136719], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 233.27999877929688, 518.0800170898438, 491.8399963378906], [257.6000061035156, 166.0800018310547, 390.0799865722656, 317.1199951171875], [87.36000061035156, 162.87998962402344, 254.39999389648438, 401.6000061035156], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"king of space\", \"10 of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [438.0799865722656, 159.0399932861328, 555.8399658203125, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.63999938964844, 230.0800018310547, 331.8399963378906, 637.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 461.7599792480469], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:36&lt;00:00,  1.15s/it]\n\n\nAverage Training Loss: 1.3834586292505264\n\n\nValidation Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.68it/s]\n\n\nAverage Validation Loss: 1.4498661905527115\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[51.52000045776367, 237.1199951171875, 169.9199981689453, 470.0799865722656], [307.5199890136719, 358.0799865722656, 447.03997802734375, 616.6400146484375], [369.6000061035156, 111.04000091552734, 513.5999755859375, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.7599792480469, 585.2799682617188], [173.1199951171875, 13.119999885559082, 303.03997802734375, 253.1199951171875]], \"labels\": [\"9 of hearts\", \"10 of hearts\", \"queen of hearts\", \"king of hearts\", \"jack of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 45.119998931884766, 514.8800048828125, 207.0399932861328], [0.9599999785423279, 129.59999084472656, 266.55999755859375, 410.55999755859375], [202.55999755859375, 86.08000183105469, 379.8399963378906, 323.5199890136719], [198.0800018310547, 175.0399932861328, 487.3599853515625, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 233.27999877929688, 518.0800170898438, 491.8399963378906], [257.6000061035156, 166.0800018310547, 390.0799865722656, 317.1199951171875], [87.36000061035156, 162.87998962402344, 254.39999389648438, 401.6000061035156], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"king of space\", \"10 of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[333.1199951171875, 152.63999938964844, 479.03997802734375, 447.03997802734375], [438.0799865722656, 159.0399932861328, 555.8399658203125, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [56.63999938964844, 230.0800018310547, 331.8399963378906, 637.1199951171875]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[461.7599792480469, 218.55999755859375, 637.1199951171875, 408.6399841308594], [326.0799865722656, 191.0399932861328, 467.5199890136719, 399.03997802734375], [207.0399932861328, 285.1199951171875, 346.55999755859375, 461.7599792480469], [13.119999885559082, 254.39999389648438, 214.0800018310547, 467.5199890136719]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [291.5199890136719, 176.3199920654297, 625.5999755859375, 399.03997802734375], [10.559999465942383, 228.79998779296875, 275.5199890136719, 427.8399963378906]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"8 of clubs\", \"7 of clubs\"]}}\n\n\n\nCPU times: user 19min 42s, sys: 5min 34s, total: 25min 17s\nWall time: 24min 12s"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tuned-model-evaluation",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#fine-tuned-model-evaluation",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tuned model evaluation",
    "text": "Fine-tuned model evaluation\n\n# @title Check if the model can still detect objects outside of the custom dataset\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = peft_model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\nNOTE: It seems that the model can still detect classes that don‚Äôt belong to our custom dataset.\n\n# @title Collect predictions\n\nPATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)&lt;loc_\\d+&gt;'\n\ndef extract_classes(dataset: DetectionDataset):\n    class_set = set()\n    for i in range(len(dataset.dataset)):\n        image, data = dataset.dataset[i]\n        suffix = data[\"suffix\"]\n        classes = re.findall(PATTERN, suffix)\n        class_set.update(classes)\n    return sorted(class_set)\n\nCLASSES = extract_classes(train_dataset)\n\ntargets = []\npredictions = []\n\nfor i in range(len(val_dataset.dataset)):\n    image, data = val_dataset.dataset[i]\n    prefix = data['prefix']\n    suffix = data['suffix']\n\n    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    prediction = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n    prediction.confidence = np.ones(len(prediction))\n\n    target = processor.post_process_generation(suffix, task='&lt;OD&gt;', image_size=image.size)\n    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n    targets.append(target)\n    predictions.append(prediction)\n\n\n# @title Calculate mAP\n# mean_average_precision = sv.MeanAveragePrecision.from_detections(\n#     predictions=predictions,\n#     targets=targets,\n# )\nmean_average_precision = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n\nmap50_95: 0.65\nmap50: 0.71\nmap75: 0.71\n\n\n\np = sv.metrics.Precision()\np = p.update(predictions, targets).compute()\nprint(p.precision_at_50)\n\nr = sv.metrics.Recall()\nr = r.update(predictions, targets).compute()\nprint(r.recall_at_50)\n\n0.7759729272419627\n0.751269035532995\n\n\ninvalid value encountered in divide\n\n\n\n# @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=CLASSES\n)\n\n_ = confusion_matrix.plot()"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#save-fine-tuned-model-on-hard-drive",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#save-fine-tuned-model-on-hard-drive",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Save fine-tuned model on hard drive",
    "text": "Save fine-tuned model on hard drive\n\npeft_model.save_pretrained(\"/content/florence2-lora\")\nprocessor.save_pretrained(\"/content/florence2-lora/\")\n!ls -la /content/florence2-lora/\n\n\n---------------------------------------------------------------------------\nPermissionError                           Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 peft_model.save_pretrained(\"/content/florence2-lora\")\n      2 processor.save_pretrained(\"/content/florence2-lora/\")\n      3 get_ipython().system('ls -la /content/florence2-lora/')\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/peft_model.py:320, in PeftModel.save_pretrained(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, path_initial_model_for_weight_conversion, **kwargs)\n    317     return output_state_dict\n    319 if is_main_process:\n--&gt; 320     os.makedirs(save_directory, exist_ok=True)\n    321     self.create_or_update_model_card(save_directory)\n    323 for adapter_name in selected_adapters:\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--&gt; 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:225, in makedirs(name, mode, exist_ok)\n    223         return\n    224 try:\n--&gt; 225     mkdir(name, mode)\n    226 except OSError:\n    227     # Cannot rely on checking for EEXIST, since the operating system\n    228     # could give priority to other errors like EACCES or EROFS\n    229     if not exist_ok or not path.isdir(name):\n\nPermissionError: [Errno 13] Permission denied: '/content'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#upload-model-to-roboflow-optional",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#upload-model-to-roboflow-optional",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Upload model to Roboflow (optional)",
    "text": "Upload model to Roboflow (optional)\nYou can deploy your Florence-2 object detection model on your own hardware (i.e.¬†a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\nTo deploy your model, you will need a free Roboflow account.\nTo get started, create a new Project in Roboflow if you don‚Äôt already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\nYou can read our full Deploy Florence-2 with Roboflow guide for step-by-step instructions of these steps.\nOnce you have trained your model A, you can upload it to Roboflow using the following code:\n\nimport roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace(\"workspace-id\").project(\"project-id\")\nversion = project.version(VERSION)\n\nversion.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")\n\nAbove, replace:\n\nAPI_KEY with your Roboflow API key.\nworkspace-id and project-id with your workspace and project IDs.\nVERSION with your project version.\n\nIf you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\nWhen you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\nYour model will be uploaded to Roboflow."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#deploy-to-your-hardware",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 2.html#deploy-to-your-hardware",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Deploy to your hardware",
    "text": "Deploy to your hardware\nOnce your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\nInference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\nFor this guide, we will show how to deploy the model with the Python SDK.\nFirst, install inference:\n\n!pip install inference\n\nThen, create a new Python file and add the following code:\n\nimport os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)\n\nIn the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\nWhen you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#setup",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#setup",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Setup",
    "text": "Setup\n\nConfigure your API keys\nTo fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n\nOpen your HuggingFace Settings page. Click Access Tokens then New Token to generate new token.\nGo to your Roboflow Settings page. Click Copy. This will place your private key in the clipboard.\nIn Colab, go to the left pane and click on Secrets (üîë).\n\nStore HuggingFace Access Token under the name HF_TOKEN.\nStore Roboflow API Key under the name ROBOFLOW_API_KEY.\n\n\n\n\nSelect the runtime\nLet‚Äôs make sure that we have access to GPU. We can use nvidia-smi command to do that. In case of any problems navigate to Edit -&gt; Notebook settings -&gt; Hardware accelerator, set it to L4 GPU, and then click Save.\n\n!nvidia-smi\n\nSun Feb 16 19:03:06 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n| N/A   37C    P0             76W /  500W |   19377MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n| N/A   37C    P0             75W /  500W |   68735MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n| N/A   36C    P0             75W /  500W |   39657MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n| N/A   35C    P0             75W /  500W |     807MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   2629326      C   ...naconda3/envs/zeel_py310/bin/python      19366MiB |\n|    1   N/A  N/A   2667285      C   ...da3/envs/deepseekvl2_env/bin/python      68724MiB |\n|    2   N/A  N/A   2627487      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n|    2   N/A  N/A   2630980      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n|    3   N/A  N/A   2696543      C   ...aconda3/envs/supervision/bin/python        796MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n\nDownload example data\nNOTE: Feel free to replace our example image with your own photo.\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!ls -lh\n\ntotal 29M\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.1\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.2\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.3\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.4\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.5\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.6\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.7\n-rw-rw-r--  1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.8\n-rw-rw-r--  1 patel_zeel patel_zeel 6.5M Feb 16 18:35 'how-to-finetune-florence-2-on-detection-dataset copy 2.ipynb'\n-rw-rw-r--  1 patel_zeel patel_zeel 6.2M Feb 16 19:03 'how-to-finetune-florence-2-on-detection-dataset copy 3.ipynb'\n-rw-rw-r--  1 patel_zeel patel_zeel 6.2M Feb 16 18:37 'how-to-finetune-florence-2-on-detection-dataset copy.ipynb'\n-rw-rw-r--  1 patel_zeel patel_zeel 6.4M Feb 16 18:36  how-to-finetune-florence-2-on-detection-dataset.ipynb\ndrwxrwxr-x 12 patel_zeel patel_zeel 4.0K Feb 16 18:07  model_checkpoints\ndrwxrwxr-x  5 patel_zeel patel_zeel 4.0K Feb 16 17:29  poker-cards-4\n-rw-rw-r--  1 patel_zeel patel_zeel 2.5M Jan 22 10:51  scratchpad.ipynb\n\n\n\nEXAMPLE_IMAGE_PATH = \"dog.jpeg\""
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#download-and-configure-the-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#download-and-configure-the-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Download and configure the model",
    "text": "Download and configure the model\nLet‚Äôs download the model checkpoint and configure it so that you can fine-tune it later on.\n\n# !pip install -q transformers flash_attn timm einops peft\n# !pip install -q roboflow git+https://github.com/roboflow/supervision.git\n\n\n# @title Imports\n\nimport io\nimport os\nimport re\nimport json\nimport torch\nimport html\nimport base64\nimport itertools\n\nimport numpy as np\nimport supervision as sv\n\n# from google.colab import userdata\nfrom IPython.core.display import display, HTML\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom roboflow import Roboflow\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n\n\nLoad the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library. Note that you need to pass trust_remote_code as True since this model is not a standard transformers model.\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\n# REVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#run-inference-with-pre-trained-florence-2-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#run-inference-with-pre-trained-florence-2-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Run inference with pre-trained Florence-2 model",
    "text": "Run inference with pre-trained Florence-2 model\n\n# @title Example object detection inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\n\n# @title Example image captioning inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;DETAILED_CAPTION&gt;\"\ntext = \"&lt;DETAILED_CAPTION&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\nresponse\n\n{'&lt;DETAILED_CAPTION&gt;': 'In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.'}\n\n\n\n# @title Example caption to phrase grounding inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\ntext = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt; Vehicle\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tune-florence-2-on-custom-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tune-florence-2-on-custom-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom dataset",
    "text": "Fine-tune Florence-2 on custom dataset\n\nDownload dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 5 {dataset.location}/train/annotations.jsonl\n\n{\"image\":\"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;jack  of clubs&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;queen of clubs&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;king of clubs&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n{\"image\":\"IMG_20220316_171515_jpg.rf.e3b1932bb375b3b3912027647586daa8.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;7 of clubs&lt;loc_363&gt;&lt;loc_484&gt;&lt;loc_552&gt;&lt;loc_905&gt;8 of clubs&lt;loc_535&gt;&lt;loc_449&gt;&lt;loc_757&gt;&lt;loc_971&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e30257ec169a2bfdfecb693211d37250.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_596&gt;&lt;loc_535&gt;&lt;loc_859&gt;&lt;loc_982&gt;jack of diamonds&lt;loc_211&gt;&lt;loc_546&gt;&lt;loc_411&gt;&lt;loc_880&gt;queen of diamonds&lt;loc_430&gt;&lt;loc_34&gt;&lt;loc_692&gt;&lt;loc_518&gt;king of diamonds&lt;loc_223&gt;&lt;loc_96&gt;&lt;loc_451&gt;&lt;loc_523&gt;10 of diamonds&lt;loc_387&gt;&lt;loc_542&gt;&lt;loc_604&gt;&lt;loc_925&gt;\"}\n{\"image\":\"IMG_20220316_143407_jpg.rf.e1eb3be3efc6c3bbede436cfb5489e7c.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"ace of hearts&lt;loc_345&gt;&lt;loc_315&gt;&lt;loc_582&gt;&lt;loc_721&gt;2 of hearts&lt;loc_709&gt;&lt;loc_115&gt;&lt;loc_888&gt;&lt;loc_509&gt;3 of hearts&lt;loc_529&gt;&lt;loc_228&gt;&lt;loc_735&gt;&lt;loc_613&gt;4 of hearts&lt;loc_98&gt;&lt;loc_421&gt;&lt;loc_415&gt;&lt;loc_845&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e4c229a9128494d17992cbe88af575df.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_141&gt;&lt;loc_18&gt;&lt;loc_404&gt;&lt;loc_465&gt;jack of diamonds&lt;loc_589&gt;&lt;loc_120&gt;&lt;loc_789&gt;&lt;loc_454&gt;queen of diamonds&lt;loc_308&gt;&lt;loc_482&gt;&lt;loc_570&gt;&lt;loc_966&gt;king of diamonds&lt;loc_549&gt;&lt;loc_477&gt;&lt;loc_777&gt;&lt;loc_904&gt;10 of diamonds&lt;loc_396&gt;&lt;loc_75&gt;&lt;loc_613&gt;&lt;loc_458&gt;\"}\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n# # read jsonl file\n# def read_jsonl(file_path: str) -&gt; Generator[Dict[str, Any], None, None]:\n#     with open(file_path, \"r\") as f:\n#         for line in f:\n#             yield json.loads(line)\n\n# lines = []\n# split = \"test\"     \n# for line in read_jsonl(dataset.location + f\"/{split}/annotations.jsonl\"):\n#     # print(line)\n#     # edit = True\n#     # copied_line = list(line['suffix'])\n#     # for i in range(len(copied_line)):\n#     #     if copied_line[i] == \"&lt;\":\n#     #         edit = False\n#     #     elif copied_line[i] == \"&gt;\":\n#     #         edit = True\n#     #     else:\n#     #         if edit:\n#     #             copied_line[i] = chr(ord(copied_line[i]) + 1)\n#     # copied_line = \"\".join(copied_line)\n#     # line['suffix'] = copied_line\n    \n#     line['suffix'] = line['suffix'].replace(\"club\", \"dog\").replace(\"diamond\", \"cat\").replace(\"heart\", \"bird\").replace(\"spade\", \"fish\")\n#     print(line)\n#     lines.append(line)\n\n# with open(dataset.location + f\"/{split}/annotations.jsonl\", \"w\") as f:\n#     for line in lines:\n#         f.write(json.dumps(line) + \"\\n\")\n\n\n# @title Define `DetectionsDataset` class\n\nclass JSONLDataset:\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.jsonl_file_path = jsonl_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -&gt; List[Dict[str, Any]]:\n        entries = []\n        with open(self.jsonl_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -&gt; int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Image.Image, Dict[str, Any]]:\n        if idx &lt; 0 or idx &gt;= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n\n\nclass DetectionDataset(Dataset):\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, data = self.dataset[idx]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        return prefix, suffix, image\n\n\n# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n    return inputs, answers\n\ntrain_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/train/\"\n)\nval_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/valid/\"\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n\n# @title Setup LoRA Florence-2 model\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n    bias=\"none\",\n    inference_mode=False,\n    use_rslora=True,\n    init_lora_weights=\"gaussian\",\n)\n# config = LoraConfig(\n#         r=8,\n#         lora_alpha=16,\n#         target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n#         task_type=\"CAUSAL_LM\",\n#         lora_dropout=0.05,\n#         bias=\"none\",\n#         # inference_mode=False,\n#         # use_rslora=True,\n# )\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n\ntrainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n\n\n\ntorch.cuda.empty_cache()\n\n\n# @title Run inference with pre-trained Florence-2 model on validation dataset\n\ndef render_inline(image: Image.Image, resize=(128, 128)):\n    \"\"\"Convert image into inline html.\"\"\"\n    image.resize(resize)\n    with io.BytesIO() as buffer:\n        image.save(buffer, format='jpeg')\n        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n        return f\"data:image/jpeg;base64,{image_b64}\"\n\n\ndef render_example(image: Image.Image, response):\n    try:\n        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n    except:\n        print('failed to redner model response')\n    return f\"\"\"\n&lt;div style=\"display: inline-flex; align-items: center; justify-content: center;\"&gt;\n    &lt;img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" /&gt;\n    &lt;p style=\"width:512px; margin:10px; font-size:small;\"&gt;{html.escape(json.dumps(response))}&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n\ndef render_inference_results(model, dataset: DetectionDataset, count: int):\n    html_out = \"\"\n    count = min(count, len(dataset))\n    for i in range(count):\n        image, data = dataset.dataset[i]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        answer = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n        html_out += render_example(image, answer)\n\n    display(HTML(html_out))\n\nrender_inference_results(peft_model, val_dataset, 4)\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom object detection dataset",
    "text": "Fine-tune Florence-2 on custom object detection dataset\n\n# @title Define train loop\n\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    render_inference_results(peft_model, val_loader.dataset, 6)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n            render_inference_results(peft_model, val_loader.dataset, 6)\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n\n%%time\n\nEPOCHS = 10\nLR = 5e-6\n\ntrain_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"tablecloth\"]}}\n\n\n\nTraining Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 4.934313402456396\n\n\nValidation Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.52it/s]\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\nAverage Validation Loss: 3.509212762117386\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.95999145507812, 267.1999816894531, 411.1999816894531], [197.44000244140625, 173.75999450683594, 487.3599853515625, 496.3199768066406], [198.72000122070312, 83.5199966430664, 379.8399963378906, 322.8800048828125], [333.1199951171875, 43.20000076293945, 516.1599731445312, 207.0399932861328]], \"labels\": [\"6 of spades\", \"7 of spade\", \"7 of clubs\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 235.83999633789062, 517.4400024414062, 490.55999755859375]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[56.63999938964844, 229.44000244140625, 331.1999816894531, 639.0399780273438]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[15.039999961853027, 255.0399932861328, 213.44000244140625, 464.9599914550781], [463.67999267578125, 222.39999389648438, 635.8399658203125, 406.0799865722656]], \"labels\": [\"queen of spades\", \"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[294.7200012207031, 176.95999145507812, 624.9599609375, 398.3999938964844]], \"labels\": [\"6 of spades\"]}}\n\n\n\nSetting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\nTraining Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.37it/s]\n\n\nAverage Training Loss: 3.446582743350197\n\n\nValidation Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.43it/s]\n\n\nAverage Validation Loss: 2.687975436449051\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 356.79998779296875], [164.8000030517578, 330.55999755859375, 301.1199951171875, 584.6400146484375], [173.1199951171875, 13.75999927520752, 303.03997802734375, 251.83999633789062], [53.439998626708984, 239.0399932861328, 166.0800018310547, 468.79998779296875]], \"labels\": [\"queen of spades\", \"king of spade\", \"queen spades\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [198.72000122070312, 82.23999786376953, 381.1199951171875, 323.5199890136719], [332.47998046875, 43.20000076293945, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375]], \"labels\": [\"queen of spades\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [298.55999755859375, 253.75999450683594, 457.91998291015625, 549.4400024414062], [331.1999816894531, 150.72000122070312, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"6 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[13.119999885559082, 254.39999389648438, 214.0800018310547, 466.239990234375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375]], \"labels\": [\"queen of spades\", \"7 of spade\", \"6 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.6399841308594, 547.5199584960938, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"6 of spoons\"]}}\n\n\n\nTraining Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:40&lt;00:00,  1.36it/s]\n\n\nAverage Training Loss: 2.6310989400919746\n\n\nValidation Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.50it/s]\n\n\nAverage Validation Loss: 1.8358672708272934\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [164.16000366210938, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.04000091552734, 266.55999755859375, 411.8399963378906], [198.0800018310547, 82.23999786376953, 381.1199951171875, 323.5199890136719], [331.1999816894531, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [87.36000061035156, 163.51998901367188, 256.32000732421875, 401.6000061035156]], \"labels\": [\"queen of spades\", \"king of spade\", \"queen spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 466.8799743652344, 397.7599792480469], [210.239990234375, 285.7599792480469, 345.2799987792969, 461.7599792480469], [14.399999618530273, 254.39999389648438, 214.0800018310547, 465.5999755859375], [463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[98.23999786376953, 433.5999755859375, 314.55999755859375, 563.5199584960938], [309.44000244140625, 423.3599853515625, 548.7999877929688, 562.239990234375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"9 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:40&lt;00:00,  1.35it/s]\n\n\nAverage Training Loss: 1.8770294934511185\n\n\nValidation Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.50it/s]\n\n\nAverage Validation Loss: 1.652064099907875\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 168.63999938964844, 470.0799865722656]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [331.1999816894531, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [87.36000061035156, 163.51998901367188, 255.0399932861328, 402.8800048828125]], \"labels\": [\"queen of spades\", \"jack of spade\", \"queen spades\", \"king of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[436.79998779296875, 157.1199951171875, 557.1199951171875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [333.1199951171875, 152.0, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [329.2799987792969, 192.3199920654297, 467.5199890136719, 397.7599792480469], [210.239990234375, 285.1199951171875, 346.55999755859375, 461.7599792480469]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[309.44000244140625, 423.3599853515625, 548.7999877929688, 562.239990234375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875]], \"labels\": [\"9 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.39it/s]\n\n\nAverage Training Loss: 1.698344780241742\n\n\nValidation Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.48it/s]\n\n\nAverage Validation Loss: 1.6035814881324768\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 168.63999938964844, 470.0799865722656]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.04000091552734, 266.55999755859375, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [331.1999816894531, 42.55999755859375, 516.7999877929688, 207.0399932861328], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438]], \"labels\": [\"9 of spades\", \"queen of clubs\", \"king of clubs\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[436.79998779296875, 157.1199951171875, 557.1199951171875, 391.3599853515625], [332.47998046875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.7599792480469], [210.239990234375, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 424.0, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 1.640886310268851\n\n\nValidation Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.51it/s]\n\n\nAverage Validation Loss: 1.5716023743152618\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 168.63999938964844, 470.0799865722656]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 44.47999954223633, 515.5199584960938, 207.0399932861328], [1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.63999938964844, 83.5199966430664, 379.8399963378906, 322.8800048828125], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [87.36000061035156, 163.51998901367188, 255.0399932861328, 402.8800048828125]], \"labels\": [\"9 of spades\", \"queen of clubs\", \"king of clubs\", \"10 of sp clubs\", \"queen Of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.7599792480469], [210.239990234375, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 1.6068760209223802\n\n\nValidation Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.47it/s]\n\n\nAverage Validation Loss: 1.5493133068084717\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 168.63999938964844, 470.0799865722656], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"king of hearts\", \"9 of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 43.84000015258789, 515.5199584960938, 207.0399932861328], [1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.0, 83.5199966430664, 379.8399963378906, 322.8800048828125], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [87.36000061035156, 164.8000030517578, 254.39999389648438, 402.8800048828125], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.03997802734375]], \"labels\": [\"9 of spades\", \"queen of clubs\", \"jack of clubs\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [209.59999084472656, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 1.5756018082885181\n\n\nValidation Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.47it/s]\n\n\nAverage Validation Loss: 1.5355284512043\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [162.239990234375, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 168.63999938964844, 470.0799865722656], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"king of hearts\", \"9 of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 43.84000015258789, 515.5199584960938, 207.0399932861328], [1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.63999938964844, 83.5199966430664, 379.8399963378906, 322.8800048828125], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [88.63999938964844, 164.8000030517578, 254.39999389648438, 402.8800048828125], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375]], \"labels\": [\"9 of spades\", \"queen of clubs\", \"jack of clubs\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [209.59999084472656, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:39&lt;00:00,  1.37it/s]\n\n\nAverage Training Loss: 1.5696636911700754\n\n\nValidation Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.45it/s]\n\n\nAverage Validation Loss: 1.528022438287735\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.9599609375, 358.0799865722656], [173.1199951171875, 14.399999618530273, 303.03997802734375, 253.1199951171875], [53.439998626708984, 239.0399932861328, 168.63999938964844, 470.0799865722656], [162.239990234375, 330.55999755859375, 301.1199951171875, 585.2799682617188], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"jack of hearts\", \"9 of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 43.84000015258789, 515.5199584960938, 207.0399932861328], [1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.63999938964844, 83.5199966430664, 379.8399963378906, 322.8800048828125], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 490.55999755859375], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [88.63999938964844, 164.8000030517578, 254.39999389648438, 402.8800048828125], [186.55999755859375, 273.6000061035156, 397.1199951171875, 511.03997802734375]], \"labels\": [\"9 of spades\", \"queen of clubs\", \"jack of clubs\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [209.59999084472656, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 1.5620801659191357\n\n\nValidation Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.46it/s]\n\n\nAverage Validation Loss: 1.525883749127388\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.9599609375, 358.0799865722656], [173.1199951171875, 14.399999618530273, 303.03997802734375, 253.1199951171875], [53.439998626708984, 239.0399932861328, 168.63999938964844, 470.0799865722656], [162.239990234375, 330.55999755859375, 301.1199951171875, 585.2799682617188], [310.0799865722656, 360.0, 447.03997802734375, 616.6400146484375]], \"labels\": [\"queen of hearts\", \"jack of hearts\", \"9 of hearts\", \"king of hearts\", \"10 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[335.03997802734375, 43.84000015258789, 515.5199584960938, 207.0399932861328], [1.5999999046325684, 129.59999084472656, 266.55999755859375, 410.55999755859375], [200.63999938964844, 83.5199966430664, 379.8399963378906, 322.8800048828125], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.8399963378906], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875], [87.36000061035156, 164.8000030517578, 254.39999389648438, 402.8800048828125], [186.55999755859375, 273.6000061035156, 397.1199951171875, 512.3200073242188]], \"labels\": [\"9 of spades\", \"jack of sp clubs\", \"queen of spade\", \"king of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[438.0799865722656, 157.1199951171875, 555.8399658203125, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of sp clubs\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 222.39999389648438, 636.47998046875, 406.0799865722656], [328.6399841308594, 192.3199920654297, 467.5199890136719, 397.1199951171875], [209.59999084472656, 285.1199951171875, 346.55999755859375, 461.7599792480469], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"5 of hearts\", \"6 of hearts\", \"7 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 423.3599853515625, 548.7999877929688, 561.5999755859375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 427.8399963378906], [291.5199890136719, 175.0399932861328, 625.5999755859375, 399.03997802734375]], \"labels\": [\"5 of clubs\", \"6 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nCPU times: user 15min 1s, sys: 3min 29s, total: 18min 30s\nWall time: 17min 26s"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tuned-model-evaluation",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#fine-tuned-model-evaluation",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tuned model evaluation",
    "text": "Fine-tuned model evaluation\n\n# @title Check if the model can still detect objects outside of the custom dataset\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = peft_model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\nNOTE: It seems that the model can still detect classes that don‚Äôt belong to our custom dataset.\n\n# @title Collect predictions\n\nPATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)&lt;loc_\\d+&gt;'\n\ndef extract_classes(dataset: DetectionDataset):\n    class_set = set()\n    for i in range(len(dataset.dataset)):\n        image, data = dataset.dataset[i]\n        suffix = data[\"suffix\"]\n        classes = re.findall(PATTERN, suffix)\n        class_set.update(classes)\n    return sorted(class_set)\n\nCLASSES = extract_classes(train_dataset)\n\ntargets = []\npredictions = []\n\nfor i in range(len(val_dataset.dataset)):\n    image, data = val_dataset.dataset[i]\n    prefix = data['prefix']\n    suffix = data['suffix']\n\n    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    prediction = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n    prediction.confidence = np.ones(len(prediction))\n\n    target = processor.post_process_generation(suffix, task='&lt;OD&gt;', image_size=image.size)\n    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n    targets.append(target)\n    predictions.append(prediction)\n\n\n# @title Calculate mAP\n# mean_average_precision = sv.MeanAveragePrecision.from_detections(\n#     predictions=predictions,\n#     targets=targets,\n# )\nmean_average_precision = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n\nmap50_95: 0.47\nmap50: 0.52\nmap75: 0.52\n\n\n\np = sv.metrics.Precision()\np = p.update(predictions, targets).compute()\nprint(p.precision_at_50)\n\nr = sv.metrics.Recall()\nr = r.update(predictions, targets).compute()\nprint(r.recall_at_50)\n\n0.6609560067681894\n0.5786802030456852\n\n\ninvalid value encountered in divide\n\n\n\n# @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=CLASSES\n)\n\n_ = confusion_matrix.plot()"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#save-fine-tuned-model-on-hard-drive",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#save-fine-tuned-model-on-hard-drive",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Save fine-tuned model on hard drive",
    "text": "Save fine-tuned model on hard drive\n\npeft_model.save_pretrained(\"/content/florence2-lora\")\nprocessor.save_pretrained(\"/content/florence2-lora/\")\n!ls -la /content/florence2-lora/\n\n\n---------------------------------------------------------------------------\nPermissionError                           Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 peft_model.save_pretrained(\"/content/florence2-lora\")\n      2 processor.save_pretrained(\"/content/florence2-lora/\")\n      3 get_ipython().system('ls -la /content/florence2-lora/')\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/peft_model.py:320, in PeftModel.save_pretrained(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, path_initial_model_for_weight_conversion, **kwargs)\n    317     return output_state_dict\n    319 if is_main_process:\n--&gt; 320     os.makedirs(save_directory, exist_ok=True)\n    321     self.create_or_update_model_card(save_directory)\n    323 for adapter_name in selected_adapters:\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--&gt; 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:225, in makedirs(name, mode, exist_ok)\n    223         return\n    224 try:\n--&gt; 225     mkdir(name, mode)\n    226 except OSError:\n    227     # Cannot rely on checking for EEXIST, since the operating system\n    228     # could give priority to other errors like EACCES or EROFS\n    229     if not exist_ok or not path.isdir(name):\n\nPermissionError: [Errno 13] Permission denied: '/content'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#upload-model-to-roboflow-optional",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#upload-model-to-roboflow-optional",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Upload model to Roboflow (optional)",
    "text": "Upload model to Roboflow (optional)\nYou can deploy your Florence-2 object detection model on your own hardware (i.e.¬†a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\nTo deploy your model, you will need a free Roboflow account.\nTo get started, create a new Project in Roboflow if you don‚Äôt already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\nYou can read our full Deploy Florence-2 with Roboflow guide for step-by-step instructions of these steps.\nOnce you have trained your model A, you can upload it to Roboflow using the following code:\n\nimport roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace(\"workspace-id\").project(\"project-id\")\nversion = project.version(VERSION)\n\nversion.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")\n\nAbove, replace:\n\nAPI_KEY with your Roboflow API key.\nworkspace-id and project-id with your workspace and project IDs.\nVERSION with your project version.\n\nIf you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\nWhen you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\nYour model will be uploaded to Roboflow."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#deploy-to-your-hardware",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy 3.html#deploy-to-your-hardware",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Deploy to your hardware",
    "text": "Deploy to your hardware\nOnce your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\nInference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\nFor this guide, we will show how to deploy the model with the Python SDK.\nFirst, install inference:\n\n!pip install inference\n\nThen, create a new Python file and add the following code:\n\nimport os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)\n\nIn the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\nWhen you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#setup",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#setup",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Setup",
    "text": "Setup\n\nConfigure your API keys\nTo fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n\nOpen your HuggingFace Settings page. Click Access Tokens then New Token to generate new token.\nGo to your Roboflow Settings page. Click Copy. This will place your private key in the clipboard.\nIn Colab, go to the left pane and click on Secrets (üîë).\n\nStore HuggingFace Access Token under the name HF_TOKEN.\nStore Roboflow API Key under the name ROBOFLOW_API_KEY.\n\n\n\n\nSelect the runtime\nLet‚Äôs make sure that we have access to GPU. We can use nvidia-smi command to do that. In case of any problems navigate to Edit -&gt; Notebook settings -&gt; Hardware accelerator, set it to L4 GPU, and then click Save.\n\n!nvidia-smi\n\nSun Feb 16 17:52:44 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n| N/A   54C    P0            101W /  500W |   19377MiB /  81920MiB |     99%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n| N/A   42C    P0             78W /  500W |   12139MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n| N/A   58C    P0            325W /  500W |   19831MiB /  81920MiB |     94%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n| N/A   33C    P0             62W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   2629326      C   ...naconda3/envs/zeel_py310/bin/python      19366MiB |\n|    1   N/A  N/A   2616743      C   ...onda3/envs/shataxi_space/bin/python      12128MiB |\n|    2   N/A  N/A   2627487      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n\nDownload example data\nNOTE: Feel free to replace our example image with your own photo.\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!ls -lh\n\ntotal 12M\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.1\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.2\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.3\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.4\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.5\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.6\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.7\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:42 'how-to-finetune-florence-2-on-detection-dataset copy 2.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.6M Feb 16 17:52 'how-to-finetune-florence-2-on-detection-dataset copy.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.6M Feb 16 17:48  how-to-finetune-florence-2-on-detection-dataset.ipynb\ndrwxrwxr-x 6 patel_zeel patel_zeel 4.0K Feb 16 17:52  model_checkpoints\ndrwxrwxr-x 5 patel_zeel patel_zeel 4.0K Feb 16 17:29  poker-cards-4\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.5M Jan 22 10:51  scratchpad.ipynb\n\n\n\nEXAMPLE_IMAGE_PATH = \"dog.jpeg\""
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#download-and-configure-the-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#download-and-configure-the-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Download and configure the model",
    "text": "Download and configure the model\nLet‚Äôs download the model checkpoint and configure it so that you can fine-tune it later on.\n\n# !pip install -q transformers flash_attn timm einops peft\n# !pip install -q roboflow git+https://github.com/roboflow/supervision.git\n\n\n# @title Imports\n\nimport io\nimport os\nimport re\nimport json\nimport torch\nimport html\nimport base64\nimport itertools\n\nimport numpy as np\nimport supervision as sv\n\n# from google.colab import userdata\nfrom IPython.core.display import display, HTML\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom roboflow import Roboflow\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n\n\nLoad the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library. Note that you need to pass trust_remote_code as True since this model is not a standard transformers model.\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\n# REVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#run-inference-with-pre-trained-florence-2-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#run-inference-with-pre-trained-florence-2-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Run inference with pre-trained Florence-2 model",
    "text": "Run inference with pre-trained Florence-2 model\n\n# @title Example object detection inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\n\n# @title Example image captioning inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;DETAILED_CAPTION&gt;\"\ntext = \"&lt;DETAILED_CAPTION&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\nresponse\n\n{'&lt;DETAILED_CAPTION&gt;': 'In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.'}\n\n\n\n# @title Example caption to phrase grounding inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\ntext = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt; Vehicle\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tune-florence-2-on-custom-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tune-florence-2-on-custom-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom dataset",
    "text": "Fine-tune Florence-2 on custom dataset\n\nDownload dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 5 {dataset.location}/train/annotations.jsonl\n\n{\"image\":\"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;jack  of clubs&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;queen of clubs&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;king of clubs&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n{\"image\":\"IMG_20220316_171515_jpg.rf.e3b1932bb375b3b3912027647586daa8.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;7 of clubs&lt;loc_363&gt;&lt;loc_484&gt;&lt;loc_552&gt;&lt;loc_905&gt;8 of clubs&lt;loc_535&gt;&lt;loc_449&gt;&lt;loc_757&gt;&lt;loc_971&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e30257ec169a2bfdfecb693211d37250.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_596&gt;&lt;loc_535&gt;&lt;loc_859&gt;&lt;loc_982&gt;jack of diamonds&lt;loc_211&gt;&lt;loc_546&gt;&lt;loc_411&gt;&lt;loc_880&gt;queen of diamonds&lt;loc_430&gt;&lt;loc_34&gt;&lt;loc_692&gt;&lt;loc_518&gt;king of diamonds&lt;loc_223&gt;&lt;loc_96&gt;&lt;loc_451&gt;&lt;loc_523&gt;10 of diamonds&lt;loc_387&gt;&lt;loc_542&gt;&lt;loc_604&gt;&lt;loc_925&gt;\"}\n{\"image\":\"IMG_20220316_143407_jpg.rf.e1eb3be3efc6c3bbede436cfb5489e7c.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"ace of hearts&lt;loc_345&gt;&lt;loc_315&gt;&lt;loc_582&gt;&lt;loc_721&gt;2 of hearts&lt;loc_709&gt;&lt;loc_115&gt;&lt;loc_888&gt;&lt;loc_509&gt;3 of hearts&lt;loc_529&gt;&lt;loc_228&gt;&lt;loc_735&gt;&lt;loc_613&gt;4 of hearts&lt;loc_98&gt;&lt;loc_421&gt;&lt;loc_415&gt;&lt;loc_845&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e4c229a9128494d17992cbe88af575df.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_141&gt;&lt;loc_18&gt;&lt;loc_404&gt;&lt;loc_465&gt;jack of diamonds&lt;loc_589&gt;&lt;loc_120&gt;&lt;loc_789&gt;&lt;loc_454&gt;queen of diamonds&lt;loc_308&gt;&lt;loc_482&gt;&lt;loc_570&gt;&lt;loc_966&gt;king of diamonds&lt;loc_549&gt;&lt;loc_477&gt;&lt;loc_777&gt;&lt;loc_904&gt;10 of diamonds&lt;loc_396&gt;&lt;loc_75&gt;&lt;loc_613&gt;&lt;loc_458&gt;\"}\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n# # read jsonl file\n# def read_jsonl(file_path: str) -&gt; Generator[Dict[str, Any], None, None]:\n#     with open(file_path, \"r\") as f:\n#         for line in f:\n#             yield json.loads(line)\n\n# lines = []\n# split = \"test\"     \n# for line in read_jsonl(dataset.location + f\"/{split}/annotations.jsonl\"):\n#     # print(line)\n#     # edit = True\n#     # copied_line = list(line['suffix'])\n#     # for i in range(len(copied_line)):\n#     #     if copied_line[i] == \"&lt;\":\n#     #         edit = False\n#     #     elif copied_line[i] == \"&gt;\":\n#     #         edit = True\n#     #     else:\n#     #         if edit:\n#     #             copied_line[i] = chr(ord(copied_line[i]) + 1)\n#     # copied_line = \"\".join(copied_line)\n#     # line['suffix'] = copied_line\n    \n#     line['suffix'] = line['suffix'].replace(\"club\", \"dog\").replace(\"diamond\", \"cat\").replace(\"heart\", \"bird\").replace(\"spade\", \"fish\")\n#     print(line)\n#     lines.append(line)\n\n# with open(dataset.location + f\"/{split}/annotations.jsonl\", \"w\") as f:\n#     for line in lines:\n#         f.write(json.dumps(line) + \"\\n\")\n\n\n# @title Define `DetectionsDataset` class\n\nclass JSONLDataset:\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.jsonl_file_path = jsonl_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -&gt; List[Dict[str, Any]]:\n        entries = []\n        with open(self.jsonl_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -&gt; int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Image.Image, Dict[str, Any]]:\n        if idx &lt; 0 or idx &gt;= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n\n\nclass DetectionDataset(Dataset):\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, data = self.dataset[idx]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        return prefix, suffix, image\n\n\n# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n    return inputs, answers\n\ntrain_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/train/\"\n)\nval_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/valid/\"\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n\n# @title Setup LoRA Florence-2 model\n\n# config = LoraConfig(\n#     r=8,\n#     lora_alpha=8,\n#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n#     task_type=\"CAUSAL_LM\",\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     inference_mode=False,\n#     use_rslora=True,\n#     init_lora_weights=\"gaussian\",\n# )\nconfig = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n        task_type=\"CAUSAL_LM\",\n        lora_dropout=0.05,\n        bias=\"none\",\n        # inference_mode=False,\n        # use_rslora=True,\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n\ntrainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n\n\n\ntorch.cuda.empty_cache()\n\n\n# @title Run inference with pre-trained Florence-2 model on validation dataset\n\ndef render_inline(image: Image.Image, resize=(128, 128)):\n    \"\"\"Convert image into inline html.\"\"\"\n    image.resize(resize)\n    with io.BytesIO() as buffer:\n        image.save(buffer, format='jpeg')\n        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n        return f\"data:image/jpeg;base64,{image_b64}\"\n\n\ndef render_example(image: Image.Image, response):\n    try:\n        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n    except:\n        print('failed to redner model response')\n    return f\"\"\"\n&lt;div style=\"display: inline-flex; align-items: center; justify-content: center;\"&gt;\n    &lt;img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" /&gt;\n    &lt;p style=\"width:512px; margin:10px; font-size:small;\"&gt;{html.escape(json.dumps(response))}&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n\ndef render_inference_results(model, dataset: DetectionDataset, count: int):\n    html_out = \"\"\n    count = min(count, len(dataset))\n    for i in range(count):\n        image, data = dataset.dataset[i]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        answer = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n        html_out += render_example(image, answer)\n\n    display(HTML(html_out))\n\nrender_inference_results(peft_model, val_dataset, 4)\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom object detection dataset",
    "text": "Fine-tune Florence-2 on custom object detection dataset\n\n# @title Define train loop\n\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    render_inference_results(peft_model, val_loader.dataset, 6)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n            render_inference_results(peft_model, val_loader.dataset, 6)\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n\n%%time\n\nEPOCHS = 10\nLR = 5e-6\n\ntrain_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"tablecloth\"]}}\n\n\n\nTraining Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:53&lt;00:00,  1.28s/it]\n\n\nAverage Training Loss: 6.217629111864987\n\n\nValidation Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.80it/s]\n\n\nAverage Validation Loss: 5.286705791950226\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"tablecloth\"]}}\n\n\n\nSetting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\nTraining Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:31&lt;00:00,  1.11s/it]\n\n\nAverage Training Loss: 5.043076939442578\n\n\nValidation Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.75it/s]\n\n\nAverage Validation Loss: 4.181661516427994\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[198.0800018310547, 175.0399932861328, 487.3599853515625, 496.3199768066406], [0.3199999928474426, 129.59999084472656, 267.1999816894531, 410.55999755859375]], \"labels\": [\"playing card\", \"playing card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[321.6000061035156, 212.1599884033203, 344.0, 243.51998901367188]], \"labels\": [\"human face\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"dining table\"]}}\n\n\n\nTraining Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:32&lt;00:00,  1.12s/it]\n\n\nAverage Training Loss: 4.1506180728183075\n\n\nValidation Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.77it/s]\n\n\nAverage Validation Loss: 3.521006762981415\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [197.44000244140625, 173.75999450683594, 488.0, 496.9599914550781], [198.72000122070312, 82.23999786376953, 380.47998046875, 323.5199890136719], [333.1199951171875, 43.20000076293945, 516.1599731445312, 207.0399932861328]], \"labels\": [\"6 of hearts\", \"6 of diamonds\", \"7 of diamonds\", \"5 of diamonds\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 274.239990234375, 395.8399963378906, 511.67999267578125]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[56.0, 229.44000244140625, 331.1999816894531, 639.0399780273438], [296.6399841308594, 252.47999572753906, 459.8399963378906, 550.719970703125], [436.79998779296875, 157.1199951171875, 557.1199951171875, 392.0]], \"labels\": [\"queen of spades\", \"6 of spade\", \"6 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[15.039999961853027, 255.0399932861328, 213.44000244140625, 464.9599914550781], [208.95999145507812, 285.7599792480469, 345.2799987792969, 461.7599792480469]], \"labels\": [\"queen of spades\", \"queen card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[294.0799865722656, 176.3199920654297, 624.9599609375, 399.03997802734375], [11.199999809265137, 228.1599884033203, 275.5199890136719, 427.8399963378906]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\nTraining Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:31&lt;00:00,  1.12s/it]\n\n\nAverage Training Loss: 3.746520130073323\n\n\nValidation Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.81it/s]\n\n\nAverage Validation Loss: 3.1994041204452515\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [164.16000366210938, 330.55999755859375, 301.1199951171875, 585.2799682617188]], \"labels\": [\"queen of spades\", \"king of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 268.47998046875, 412.47998046875], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.67999267578125]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 273.6000061035156, 395.8399963378906, 511.67999267578125], [368.3199768066406, 235.1999969482422, 517.4400024414062, 491.1999816894531]], \"labels\": [\"queen of spades\", \"queen spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[56.0, 228.79998779296875, 331.8399963378906, 639.0399780273438], [437.44000244140625, 156.47999572753906, 557.1199951171875, 392.0]], \"labels\": [\"8 of spades\", \"6 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[13.119999885559082, 254.39999389648438, 213.44000244140625, 466.239990234375], [208.95999145507812, 285.1199951171875, 345.2799987792969, 463.67999267578125]], \"labels\": [\"queen of spades\", \"queen card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[293.44000244140625, 176.3199920654297, 624.9599609375, 399.03997802734375], [11.199999809265137, 228.1599884033203, 275.5199890136719, 428.47998046875], [96.95999908447266, 432.9599914550781, 314.55999755859375, 566.0800170898438], [309.44000244140625, 423.3599853515625, 548.7999877929688, 563.5199584960938]], \"labels\": [\"9 of spades\", \"7 of clubs\", \"9 of clubs\", \"9 of hearts\"]}}\n\n\n\nTraining Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:33&lt;00:00,  1.13s/it]\n\n\nAverage Training Loss: 3.459920299403808\n\n\nValidation Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.81it/s]\n\n\nAverage Validation Loss: 2.9735917448997498\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625], [164.16000366210938, 330.55999755859375, 301.1199951171875, 585.2799682617188], [173.1199951171875, 14.399999618530273, 303.03997802734375, 252.47999572753906], [53.439998626708984, 239.67999267578125, 165.44000244140625, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"queen spades\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 268.47998046875, 412.47998046875], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.67999267578125]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 273.6000061035156, 396.47998046875, 511.67999267578125], [368.3199768066406, 235.1999969482422, 517.4400024414062, 491.1999816894531]], \"labels\": [\"queen of spades\", \"9 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[55.36000061035156, 228.79998779296875, 333.1199951171875, 639.0399780273438], [436.79998779296875, 156.47999572753906, 557.760009765625, 392.0], [297.91998291015625, 252.47999572753906, 458.55999755859375, 550.0800170898438]], \"labels\": [\"8 of spades\", \"6 of spade\", \"7 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.7200012207031], [208.95999145507812, 285.1199951171875, 345.91998291015625, 463.67999267578125]], \"labels\": [\"queen of spades\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 565.4400024414062], [310.7200012207031, 424.6399841308594, 548.1599731445312, 562.8800048828125]], \"labels\": [\"2 of spades\", \"5 of spade\", \"6 of spoons\"]}}\n\n\n\nTraining Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:56&lt;00:00,  1.17it/s]\n\n\nAverage Training Loss: 3.2458674925215103\n\n\nValidation Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.75it/s]\n\n\nAverage Validation Loss: 2.800899028778076\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 166.72000122070312, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 268.47998046875, 412.47998046875], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [330.55999755859375, 42.55999755859375, 517.4400024414062, 207.67999267578125]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.27999877929688, 272.32000732421875, 396.47998046875, 511.67999267578125], [367.67999267578125, 234.55999755859375, 518.0800170898438, 491.1999816894531]], \"labels\": [\"queen of spades\", \"9 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [208.95999145507812, 285.1199951171875, 345.91998291015625, 461.7599792480469]], \"labels\": [\"queen of spades\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.7999877929688], [310.7200012207031, 424.0, 548.7999877929688, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"10 of sp clubs\"]}}\n\n\n\nTraining Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:32&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 3.0911339188323304\n\n\nValidation Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.77it/s]\n\n\nAverage Validation Loss: 2.675829589366913\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 166.72000122070312, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.67999267578125]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.27999877929688, 272.32000732421875, 396.47998046875, 511.67999267578125], [367.03997802734375, 234.55999755859375, 518.0800170898438, 491.1999816894531]], \"labels\": [\"queen of spades\", \"9 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [208.95999145507812, 285.7599792480469, 345.91998291015625, 462.3999938964844]], \"labels\": [\"queen of spades\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.7999877929688], [310.7200012207031, 424.6399841308594, 548.1599731445312, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"9 of sp clubs\"]}}\n\n\n\nTraining Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:33&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 2.9872096601654503\n\n\nValidation Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.73it/s]\n\n\nAverage Validation Loss: 2.5895615220069885\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 127.68000030517578, 268.47998046875, 412.47998046875], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [330.55999755859375, 42.55999755859375, 517.4400024414062, 207.67999267578125]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 273.6000061035156, 396.47998046875, 511.67999267578125], [368.3199768066406, 235.1999969482422, 517.4400024414062, 491.1999816894531], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [86.72000122070312, 164.16000366210938, 255.67999267578125, 403.5199890136719]], \"labels\": [\"queen of spades\", \"9 of spade\", \"10 of sp clubs\", \"king of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [208.95999145507812, 285.7599792480469, 345.91998291015625, 461.7599792480469]], \"labels\": [\"queen of spades\", \"7 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.0, 548.7999877929688, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"9 of sp clubs\"]}}\n\n\n\nTraining Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:33&lt;00:00,  1.46it/s]\n\n\nAverage Training Loss: 2.8879061095854817\n\n\nValidation Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.59it/s]\n\n\nAverage Validation Loss: 2.5414960384368896\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.67999267578125]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 273.6000061035156, 396.47998046875, 511.67999267578125], [368.3199768066406, 235.1999969482422, 517.4400024414062, 491.1999816894531], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [86.72000122070312, 164.16000366210938, 255.67999267578125, 403.5199890136719]], \"labels\": [\"queen of spades\", \"9 of spade\", \"10 of sp clubs\", \"king of spates\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [208.95999145507812, 285.7599792480469, 345.91998291015625, 462.3999938964844], [328.6399841308594, 192.3199920654297, 467.5199890136719, 398.3999938964844]], \"labels\": [\"queen of spades\", \"7 of hearts\", \"6 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.6399841308594, 548.7999877929688, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"9 of sp clubs\"]}}\n\n\n\nTraining Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:32&lt;00:00,  1.47it/s]\n\n\nAverage Training Loss: 2.8460742568268493\n\n\nValidation Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:02&lt;00:00,  2.74it/s]\n\n\nAverage Validation Loss: 2.526287943124771\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.67999267578125]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[185.9199981689453, 273.6000061035156, 396.47998046875, 511.67999267578125], [368.3199768066406, 235.1999969482422, 517.4400024414062, 491.1999816894531], [19.520000457763672, 289.6000061035156, 223.67999267578125, 582.0800170898438], [86.72000122070312, 164.16000366210938, 255.67999267578125, 403.5199890136719]], \"labels\": [\"queen of spades\", \"9 of spade\", \"10 of sp clubs\", \"king of spates\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"7 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [208.95999145507812, 285.7599792480469, 345.91998291015625, 462.3999938964844], [328.6399841308594, 192.3199920654297, 467.5199890136719, 398.3999938964844]], \"labels\": [\"queen of spades\", \"7 of hearts\", \"6 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 228.1599884033203, 275.5199890136719, 428.47998046875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.6399841308594, 548.7999877929688, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"9 of sp clubs\"]}}\n\n\n\nCPU times: user 19min 11s, sys: 4min 23s, total: 23min 34s\nWall time: 22min 28s"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tuned-model-evaluation",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#fine-tuned-model-evaluation",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tuned model evaluation",
    "text": "Fine-tuned model evaluation\n\n# @title Check if the model can still detect objects outside of the custom dataset\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = peft_model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\nNOTE: It seems that the model can still detect classes that don‚Äôt belong to our custom dataset.\n\n# @title Collect predictions\n\nPATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)&lt;loc_\\d+&gt;'\n\ndef extract_classes(dataset: DetectionDataset):\n    class_set = set()\n    for i in range(len(dataset.dataset)):\n        image, data = dataset.dataset[i]\n        suffix = data[\"suffix\"]\n        classes = re.findall(PATTERN, suffix)\n        class_set.update(classes)\n    return sorted(class_set)\n\nCLASSES = extract_classes(train_dataset)\n\ntargets = []\npredictions = []\n\nfor i in range(len(val_dataset.dataset)):\n    image, data = val_dataset.dataset[i]\n    prefix = data['prefix']\n    suffix = data['suffix']\n\n    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    prediction = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n    prediction.confidence = np.ones(len(prediction))\n\n    target = processor.post_process_generation(suffix, task='&lt;OD&gt;', image_size=image.size)\n    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n    targets.append(target)\n    predictions.append(prediction)\n\n\n# @title Calculate mAP\n# mean_average_precision = sv.MeanAveragePrecision.from_detections(\n#     predictions=predictions,\n#     targets=targets,\n# )\nmean_average_precision = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n\nmap50_95: 0.18\nmap50: 0.20\nmap75: 0.20\n\n\n\np = sv.metrics.Precision()\np = p.update(predictions, targets).compute()\nprint(p.precision_at_50)\n\nr = sv.metrics.Recall()\nr = r.update(predictions, targets).compute()\nprint(r.recall_at_50)\n\n0.2074450084602369\n0.14213197969543148\n\n\ninvalid value encountered in divide\n\n\n\n# @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=CLASSES\n)\n\n_ = confusion_matrix.plot()"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#save-fine-tuned-model-on-hard-drive",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#save-fine-tuned-model-on-hard-drive",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Save fine-tuned model on hard drive",
    "text": "Save fine-tuned model on hard drive\n\npeft_model.save_pretrained(\"/content/florence2-lora\")\nprocessor.save_pretrained(\"/content/florence2-lora/\")\n!ls -la /content/florence2-lora/\n\n\n---------------------------------------------------------------------------\nPermissionError                           Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 peft_model.save_pretrained(\"/content/florence2-lora\")\n      2 processor.save_pretrained(\"/content/florence2-lora/\")\n      3 get_ipython().system('ls -la /content/florence2-lora/')\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/peft_model.py:320, in PeftModel.save_pretrained(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, path_initial_model_for_weight_conversion, **kwargs)\n    317     return output_state_dict\n    319 if is_main_process:\n--&gt; 320     os.makedirs(save_directory, exist_ok=True)\n    321     self.create_or_update_model_card(save_directory)\n    323 for adapter_name in selected_adapters:\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--&gt; 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:225, in makedirs(name, mode, exist_ok)\n    223         return\n    224 try:\n--&gt; 225     mkdir(name, mode)\n    226 except OSError:\n    227     # Cannot rely on checking for EEXIST, since the operating system\n    228     # could give priority to other errors like EACCES or EROFS\n    229     if not exist_ok or not path.isdir(name):\n\nPermissionError: [Errno 13] Permission denied: '/content'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#upload-model-to-roboflow-optional",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#upload-model-to-roboflow-optional",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Upload model to Roboflow (optional)",
    "text": "Upload model to Roboflow (optional)\nYou can deploy your Florence-2 object detection model on your own hardware (i.e.¬†a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\nTo deploy your model, you will need a free Roboflow account.\nTo get started, create a new Project in Roboflow if you don‚Äôt already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\nYou can read our full Deploy Florence-2 with Roboflow guide for step-by-step instructions of these steps.\nOnce you have trained your model A, you can upload it to Roboflow using the following code:\n\nimport roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace(\"workspace-id\").project(\"project-id\")\nversion = project.version(VERSION)\n\nversion.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")\n\nAbove, replace:\n\nAPI_KEY with your Roboflow API key.\nworkspace-id and project-id with your workspace and project IDs.\nVERSION with your project version.\n\nIf you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\nWhen you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\nYour model will be uploaded to Roboflow."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#deploy-to-your-hardware",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset copy.html#deploy-to-your-hardware",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Deploy to your hardware",
    "text": "Deploy to your hardware\nOnce your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\nInference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\nFor this guide, we will show how to deploy the model with the Python SDK.\nFirst, install inference:\n\n!pip install inference\n\nThen, create a new Python file and add the following code:\n\nimport os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)\n\nIn the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\nWhen you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
  },
  {
    "objectID": "posts/2021-10-26-anonymization-tips.html",
    "href": "posts/2021-10-26-anonymization-tips.html",
    "title": "Anonymization tips for double-blind submission",
    "section": "",
    "text": "Use following command locally to search for author names, institute name and other terms you think may violate double-blind\n\ngit grep &lt;query&gt;\n\nAbove command matches the query everywhere and thus a safe way. Avoid GitHub search for this purpose, it fails to identify some terms many times and there is no regex there (yet)!\n\n\nDo not use full paths inside README file. If you move content in other repo, the links will either become unusable or may violate double-blind. So follow the example below.\n\nBad practice: [link](https://github.com/patel-zeel/reponame/blob/master/dataset)\nGood practice: [link](dataset)\n\nPoint no. 2 does not work for GitHub pages links (username.github.io/stuff). Thus, keep in mind to manually update those (if you have a better idea, let everyone know in comments below)\nDownload the repo zip locally and create an anonymized repository in your anonymized GitHub account. Open the GitHub web editor by pressing ‚Äú.‚Äù (dot) at repo homepage.\nNow, you can select and drag all folders to the left pan of the web editor to upload them at once. Finally, commit with a meaningfull message and the changes will automatically be uploaded to the mail branch of your anonymized repo.\nUpdate the link in your manuscipt and submit !!\n\n\nEdit:\nAfter acceptance, transfer the ownership to personal account and delete the ownership of anonymized account from the personal account. This will remove all the traces of repository from the anonymized account. However, repository will still show that the commits were made by anonymized account which is anyway not violation of the doule-blind explicitely."
  },
  {
    "objectID": "posts/CNPs_for_Images.html",
    "href": "posts/CNPs_for_Images.html",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# turn off preallocation by JAX\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nimport distrax as dx\n\nimport optax\n\n# load mnist dataset from tensorflow datasets\nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n# define initializers\ndef first_layer_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-1.0/num_input, maxval=1.0/num_input)\n\ndef other_layers_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-np.sqrt(6 / num_input)/30, maxval=np.sqrt(6 / num_input)/30)\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n    \n    \n    for n_features in self.features[1:]:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n\n    for n_features in self.features:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.output_dim*2)(x)\n    loc, raw_scale = x[:, :self.output_dim], x[:, self.output_dim:]\n    scale = jnp.exp(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features, self.output_dim)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=0.005+scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/CNPs_for_Images.html#load-mnist",
    "href": "posts/CNPs_for_Images.html#load-mnist",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "Load MNIST",
    "text": "Load MNIST\n\nds = tfds.load('mnist')\n\n\ndef dataset_to_arrays(dataset):\n    data = []\n    labels = []\n    stopper = 0\n    end = 100\n    for sample in dataset:\n        data.append(sample[\"image\"].numpy())\n        labels.append(sample[\"label\"].numpy())\n        stopper += 1\n        if stopper == end:\n            break\n    return np.array(data), np.array(labels)[..., None]\n\ntrain_data, train_labels = dataset_to_arrays(ds[\"train\"])\ntest_data, test_labels = dataset_to_arrays(ds[\"test\"])\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n2023-06-02 09:58:48.609001: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2023-06-02 09:58:48.681190: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n((100, 28, 28, 1), (100, 1), (100, 28, 28, 1), (100, 1))\n\n\n\ncoords = np.linspace(-1, 1, 28)\nx, y = np.meshgrid(coords, coords)\ntrain_X = jnp.stack([x, y], axis=-1).reshape(-1, 2)\n\ntrain_y = jax.vmap(lambda x: x.reshape(-1, 1))(train_data) / 255.0\ntrain_X.shape, train_y.shape, type(train_X), type(train_y)\n\n((784, 2),\n (100, 784, 1),\n jaxlib.xla_extension.ArrayImpl,\n jaxlib.xla_extension.ArrayImpl)\n\n\n\niterations = 10000\n\ndef loss_fn(params, context_X, context_y, target_X, target_y):\n  def loss_fn_per_sample(context_X, context_y, target_X, target_y):\n    loc, scale = model.apply(params, context_X, context_y, target_X)\n    # predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    # return -predictive_distribution.log_prob(target_y)\n    return jnp.square(loc.ravel() - target_y.ravel()).mean()\n  \n  return jax.vmap(loss_fn_per_sample, in_axes=(None, 0, None, 0))(context_X, context_y, target_X, target_y).mean()\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nmodel = CNP([256]*2, 128, [256]*4, 1)\nparams = model.init(jax.random.PRNGKey(0), train_X, train_y[0], train_X)\noptimizer = optax.adam(1e-5)\nstate = optimizer.init(params)\n\n# losses = []\n# for iter in tqdm(range(iterations)):\n#   tmp_index = jax.random.permutation(jax.random.PRNGKey(iter), index)\n#   context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n#   context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n#   target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n#   target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  \n#   # print(context_X.shape, context_y.shape, target_X.shape, target_y.shape)\n#   # print(loss_fn(params, context_X, context_y, target_X, target_y).shape)\n  \n#   loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n#   updates, state = optimizer.update(grads, state)\n#   params = optax.apply_updates(params, updates)\n#   losses.append(loss.item())\n\ndef one_step(params_and_state, key):\n  params, state = params_and_state\n  tmp_index = jax.random.permutation(key, train_X.shape[0])\n  context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n  context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n  target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n  target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n  updates, state = optimizer.update(grads, state)\n  params = optax.apply_updates(params, updates)\n  return (params, state), loss\n\n(params, state), loss_history = jax.lax.scan(one_step, (params, state), jax.random.split(jax.random.PRNGKey(0), iterations))\n\n\nplt.plot(loss_history[10:]);\n\n\n\n\n\n\n\n\n\ntest_key = jax.random.PRNGKey(0)\ntmp_index = jax.random.permutation(test_key, train_X.shape[0])\ncontext_X = train_X[tmp_index][:int(train_X.shape[0]*0.5)]\ncontext_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.5), :]\ntarget_X = train_X#[tmp_index][int(train_X.shape[0]*0.5):]\ntarget_y = train_y#[:, tmp_index, :][:, int(train_X.shape[0]*0.5):, :]\n\nid = 91\nplt.imshow(train_y[id].reshape(28, 28), cmap=\"gray\", interpolation=None);\n\nlocs, scales = jax.vmap(model.apply, in_axes=(None, None, 0, None))(params, context_X, context_y, target_X)\n# full_preds = jnp.concatenate([context_y, locs], axis=1)\n# full_preds = full_preds.at[:, tmp_index, :].set(full_preds).__array__()\n\nplt.figure()\nplt.imshow(locs[id].reshape(28, 28), cmap=\"gray\", interpolation=None);"
  },
  {
    "objectID": "posts/2025-02-10-object-detection-random-baseline.html",
    "href": "posts/2025-02-10-object-detection-random-baseline.html",
    "title": "Object Detection Random Baseline",
    "section": "",
    "text": "Why Random Baseline?\nGiven a standard dataset with a fixed set of models, it is easier to compare the performance of different models. But what if we are working on a new model which has performance far from the best set of models but as a first step, we simply want to check if the model is learning anything at all. In such cases, it is useful to compare the performance of the model with a random baseline.\n\n\nProposed Idea\n\nTo formalize the problem, let‚Äôs say for an arbitrary image, model predicts \\(k\\) bounding boxes with sizes \\((h_1, w_1), (h_2, w_2), \\ldots, (h_k, w_k)\\).\nA simple random baseline would be to generate \\(k\\) random bounding boxes for that image with sizes \\((h_1, w_1), (h_2, w_2), \\ldots, (h_k, w_k)\\). In other words, we can simply move the predicted bounding boxes to random locations ensuring that the bounding boxes are within the image.\n\n\n\nImports\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\n\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport supervision as sv\nfrom roboflow import Roboflow\nfrom dotenv import load_dotenv\nfrom ultralytics import YOLO\nfrom copy import deepcopy\nfrom PIL import Image\nfrom IPython.display import clear_output\n\nload_dotenv()\n\nTrue\n\n\n\n\nDataset\n\ndata_location = \"/tmp/poker-cards-fmjio\"\n\nrf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"yolov8\", location=data_location)\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n\nTrain Model\n\nmodel = YOLO(\"yolo11m\")\n\nmodel.train(data=f\"{data_location}/data.yaml\", epochs=1, project=\"/tmp/poker-cards-fmjio\", exist_ok=True)\nclear_output()\n\n\n\nEvaluate Model\n\ntest_dataset = sv.DetectionDataset.from_yolo(f\"{data_location}/test/images\", f\"{data_location}/test/labels\", f\"{data_location}/data.yaml\")\nlen(test_dataset)\n\n44\n\n\n\nannotations_list = []\ndetections_list = []\nfor _, img, annotations in tqdm(test_dataset):\n    results = model.predict(img, verbose=False)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    annotations_list.append(annotations)\n    detections_list.append(detections)\n\n\n\n\n\nmAP = sv.metrics.MeanAveragePrecision().update(detections_list, annotations_list).compute()\nmAP.map50\n\n0.1417744455736285\n\n\n\n\nRandom Baseline\nAs per our assumption, we would simply need to randomly move the existing bounding boxes keeping their sizes constant with the following constraints:\n\nThe bounding box should be within the image boundaries.\n\n\nmin_size = 0\nmax_size = model.args['imgsz']\n\nmAPs = []\nfor random_seed in tqdm(range(100)):\n    np.random.seed(random_seed)\n    random_detections_list = []\n    for detections in detections_list:\n        random_detections = deepcopy(detections)\n        shift = np.random.rand(len(detections))\n        lower_limit = - detections.xyxy.min(axis=1) + 1e-6\n        upper_limit = max_size - detections.xyxy.max(axis=1) - 1e-6\n        transformed_shift = lower_limit + shift * (upper_limit - lower_limit)\n        random_detections.xyxy = random_detections.xyxy + transformed_shift.reshape(-1, 1)\n        random_detections_list.append(random_detections)\n        \n    mAP = sv.metrics.MeanAveragePrecision().update(random_detections_list, annotations_list).compute()\n    mAPs.append(mAP.map50)\n    \nprint(f\"mAP50: {mAP.map50:.2f} +/- {np.std(mAPs):.2f}\")\n\n\n\n\nmAP50: 0.04 +/- 0.01\n\n\nWe can also modify the confidence values.\n\nmin_size = 0\nmax_size = model.args['imgsz']\n\nmAPs = []\nfor random_seed in tqdm(range(100)):\n    np.random.seed(random_seed)\n    random_detections_list = []\n    for detections in detections_list:\n        random_detections = deepcopy(detections)\n        shift = np.random.rand(len(detections))\n        lower_limit = - detections.xyxy.min(axis=1) + 1e-6\n        upper_limit = max_size - detections.xyxy.max(axis=1) - 1e-6\n        transformed_shift = lower_limit + shift * (upper_limit - lower_limit)\n        random_detections.xyxy = random_detections.xyxy + transformed_shift.reshape(-1, 1)\n        random_detections.confidence = np.random.rand(len(detections))\n        random_detections_list.append(random_detections)\n        \n    mAP = sv.metrics.MeanAveragePrecision().update(random_detections_list, annotations_list).compute()\n    mAPs.append(mAP.map50)\n    \nprint(f\"mAP50: {mAP.map50:.2f} +/- {np.std(mAPs):.2f}\")\n\n\n\n\nmAP50: 0.03 +/- 0.01\n\n\nSo, in this case, our model got better than the random baseline with a single epoch."
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport GPy\nimport jax\nimport gpytorch\nimport botorch\nimport tinygp\nimport jax.numpy as jnp\nimport optax\nfrom IPython.display import clear_output\n\nfrom sklearn.preprocessing import StandardScaler\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Data",
    "text": "Data\n\nnp.random.seed(0) # We don't want surprices in a presentation :)\nN = 10\ntrain_x = torch.linspace(0, 1, N)\ntrain_y = torch.sin(train_x * (2 * math.pi)) + torch.normal(0, 0.1, size=(N,))\n \ntest_x = torch.linspace(0, 1, N*10)\ntest_y = torch.sin(test_x * (2 * math.pi))\n\n\nplt.plot(train_x, train_y, 'ko', label='train');\nplt.plot(test_x, test_y, label='test');\nplt.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Defining kernel",
    "text": "Defining kernel\n\\[\\begin{equation}\n\\sigma_f^2 = \\text{variance}\\\\\n\\ell = \\text{lengthscale}\\\\\nk_{RBF}(x_1, x_2) = \\sigma_f^2 \\exp \\left[-\\frac{\\lVert x_1 - x_2 \\rVert^2}{2\\ell^2}\\right]\n\\end{equation}\\]\n\nGPy\n\ngpy_kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\ngpy_kernel\n\n\n\n\n\n\nrbf.\nvalue\nconstraints\npriors\n\n\nvariance\n1.0\n+ve\n\n\n\nlengthscale\n1.0\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\ngpytorch_kernel.outputscale = 1. # variance\ngpytorch_kernel.base_kernel.lengthscale = 1. # lengthscale\n\ngpytorch_kernel\n\nScaleKernel(\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n)\n\n\n\n\nTinyGP\n\ndef RBFKernel(variance, lengthscale):\n    return jnp.exp(variance) * tinygp.kernels.ExpSquared(scale=jnp.exp(lengthscale))\n    \ntinygp_kernel = RBFKernel(variance=1., lengthscale=1.)\ntinygp_kernel\n\n&lt;tinygp.kernels.Product at 0x7f544039d710&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Define model",
    "text": "Define model\n\\[\n\\sigma_n^2 = \\text{noise variance}\n\\]\n\nGPy\n\ngpy_model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], gpy_kernel)\ngpy_model.Gaussian_noise.variance = 0.1\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 16.757933772959404\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n1.0\n+ve\n\n\n\nrbf.lengthscale\n1.0\n+ve\n\n\n\nGaussian_noise.variance\n0.1\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, kernel):\n        super().__init__(train_x, train_y, likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ngpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngpytorch_model = ExactGPModel(train_x, train_y, gpytorch_likelihood, gpytorch_kernel)\n\ngpytorch_model.likelihood.noise = 0.1\ngpytorch_model\n\nExactGPModel(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\n\nTinyGP\n\ndef build_gp(theta, X):\n    mean = theta[0] \n    variance, lengthscale, noise_variance = jnp.exp(theta[1:])\n    \n    kernel = variance * tinygp.kernels.ExpSquared(lengthscale)\n    \n    return tinygp.GaussianProcess(kernel, X, diag=noise_variance, mean=mean)\n\ntinygp_model = build_gp(theta=np.array([0., 1., 1., 0.1]), X=train_x.numpy())\n\ntinygp_model\n# __repr__\n\n&lt;tinygp.gp.GaussianProcess at 0x7f5440401850&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Train the model",
    "text": "Train the model\n\nGPy\n\ngpy_model.optimize(max_iters=50)\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 3.944394423452163\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n0.9376905183253631\n+ve\n\n\n\nrbf.lengthscale\n0.2559000163858406\n+ve\n\n\n\nGaussian_noise.variance\n0.012506184441481319\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(gpytorch_likelihood, gpytorch_model)\nbotorch.fit_gpytorch_model(mll)\n\ndisplay(gpytorch_model.mean_module.constant, # Mean\n        gpytorch_model.covar_module.outputscale, # Variance\n        gpytorch_model.covar_module.base_kernel.lengthscale, # Lengthscale \n        gpytorch_model.likelihood.noise) # Noise variance\n\n /opt/conda/lib/python3.7/site-packages/botorch/fit.py:143: UserWarning:CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/c10/cuda/CUDAFunctions.cpp:112.)\n\n\nParameter containing:\ntensor([0.0923], requires_grad=True)\n\n\ntensor(0.9394, grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([[0.2560]], grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([0.0124], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nTinyGP\n\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(theta, X, y):\n    gp = build_gp(theta, X)\n    return -gp.condition(y)\n\n\nobj = jax.jit(jax.value_and_grad(neg_log_likelihood))\nresult = minimize(obj, [0., 1., 1., 0.1], jac=True, args=(train_x.numpy(), train_y.numpy()))\nresult.x[0], np.exp(result.x[1:])\n\n(0.09213499552879165, array([0.9395271 , 0.25604163, 0.01243025]))"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Inference",
    "text": "Inference\n\ndef plot_gp(pred_y, var_y):\n    std_y = var_y ** 0.5\n    plt.figure()\n    plt.scatter(train_x, train_y, label='train')\n    plt.plot(test_x, pred_y, label='predictive mean')\n    plt.fill_between(test_x.ravel(), \n                     pred_y.ravel() - 2*std_y.ravel(), \n                     pred_y.ravel() + 2*std_y.ravel(), alpha=0.2, label='95% confidence')\n    plt.legend()\n\n\nGPy\n\npred_y, var_y = gpy_model.predict(test_x.numpy()[:, None])\nplot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_model.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = gpytorch_likelihood(gpytorch_model(test_x))\n    pred_y, var_y = pred_dist.mean, pred_dist.variance\n    plot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nTinyGP\n\ntinygp_model = build_gp(result.x, train_x.numpy())\npred_y, var_y = tinygp_model.predict(train_y.numpy(), test_x.numpy(), return_var=True)\n\nplot_gp(pred_y, var_y)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Tiny GP on CO2 dataset",
    "text": "Tiny GP on CO2 dataset\n\ndata = pd.read_csv(\"data/co2.csv\")\n\n# Train test split\nX = data[\"0\"].iloc[:290].values.reshape(-1, 1)\nX_test = data[\"0\"].iloc[290:].values.reshape(-1, 1)\ny = data[\"1\"].iloc[:290].values\ny_test = data[\"1\"].iloc[290:].values\n\n# Scaling the dataset\nXscaler = StandardScaler()\nX = Xscaler.fit_transform(X)\nX_test = Xscaler.transform(X_test)\n\nyscaler = StandardScaler()\ny = yscaler.fit_transform(y.reshape(-1, 1)).ravel()\ny_test = yscaler.transform(y_test.reshape(-1, 1)).ravel()\n\n\nplt.plot(X, y, label='train');\nplt.plot(X_test, y_test, label='test');\nplt.legend();\n\n\n\n\n\n\n\n\n\nclass SpectralMixture(tinygp.kernels.Kernel):\n    def __init__(self, weight, scale, freq):\n        self.weight = jnp.atleast_1d(weight)\n        self.scale = jnp.atleast_1d(scale)\n        self.freq = jnp.atleast_1d(freq)\n\n    def evaluate(self, X1, X2):\n        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n        return jnp.sum(\n            self.weight\n            * jnp.prod(\n                jnp.exp(-2 * jnp.pi ** 2 * tau ** 2 / self.scale ** 2)\n                * jnp.cos(2 * jnp.pi * self.freq * tau),\n                axis=-1,\n            )\n        )\n    \ndef build_spectral_gp(theta):\n    kernel = SpectralMixture(\n        jnp.exp(theta[\"log_weight\"]),\n        jnp.exp(theta[\"log_scale\"]),\n        jnp.exp(theta[\"log_freq\"]),\n    )\n    return tinygp.GaussianProcess(\n        kernel, X, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n    )\n\n\nK = 4 # Number of mixtures\ndiv_factor = 0.4\nnp.random.seed(1)\nparams = {\n    \"log_weight\": np.abs(np.random.rand(K))/div_factor,\n    \"log_scale\": np.abs(np.random.rand(K))/div_factor,\n    \"log_freq\": np.abs(np.random.rand(K))/div_factor,\n    \"log_diag\": np.abs(np.random.rand(1))/div_factor,\n    \"mean\": 0.,\n}\n\n@jax.jit\n@jax.value_and_grad\ndef loss(theta):\n    return -build_spectral_gp(theta).condition(y)\n# opt = optax.sgd(learning_rate=0.001)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\nlosses = []\nfor i in range(100):\n    loss_val, grads = loss(params)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    losses.append(loss_val)\n    clear_output(wait=True)\n    print(f\"iter {i}, loss {loss_val}\")\n\nopt_gp = build_spectral_gp(params)\n\nparams\n\niter 99, loss 27.987701416015625\n\n\n{'log_diag': DeviceArray([-2.7388687], dtype=float32),\n 'log_freq': DeviceArray([-3.6072493, -3.1795945, -3.4490397, -2.373117 ], dtype=float32),\n 'log_scale': DeviceArray([3.9890492, 3.8530042, 4.0878096, 4.4860597], dtype=float32),\n 'log_weight': DeviceArray([-1.3715047, -0.6132469, -2.413771 , -1.6582283], dtype=float32),\n 'mean': DeviceArray(0.38844627, dtype=float32)}\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nmu, var = opt_gp.predict(y, X_test, return_var=True)\n\nplt.plot(X, y, c='k')\nplt.fill_between(\n    X_test.ravel(), mu + np.sqrt(var), mu - np.sqrt(var), color=\"C0\", alpha=0.5\n)\nplt.plot(X_test, mu, color=\"C0\", lw=2)\n\n# plt.xlim(t.min(), 2025)\nplt.xlabel(\"year\")\n_ = plt.ylabel(\"CO$_2$ in ppm\")"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)",
    "text": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)",
    "text": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)\n\nMinimize inverse term fully\nNow, Minimize both togather"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html",
    "href": "posts/non-gaussian-likelihood-mlps.html",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "",
    "text": "# %pip install mapie\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mapie.metrics import regression_coverage_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\n\nN = 100\nx = dist.Uniform(-1, 1).sample((N, 1)).sort(dim=0).values\nx_test = torch.linspace(-1, 1, 2 * N).view(-1, 1).sort(dim=0).values\ny = 3 * x**3 - 2 * x + 1\ny_noisy = y + dist.Gamma(0.1, 0.3).sample((N, 1))\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\n\nplt.legend()\nprint(\"x.shape:\", x.shape, \"y.shape:\", y.shape)\n\nx.shape: torch.Size([100, 1]) y.shape: torch.Size([100, 1])"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Define a Gaussian/Gamma MLP",
    "text": "Define a Gaussian/Gamma MLP\n\nclass ProbabilisticMLP(nn.Module):\n    def __init__(self, input_dim, feature_dims, type):\n        super().__init__()\n        self.input_dim = input_dim\n        self.feature_dims = feature_dims\n        self.type = type  # \"gaussian\" or \"gamma\"\n\n        self.layers = nn.ModuleList()\n        self.layers.append(nn.Linear(input_dim, feature_dims[0]))\n        for i in range(len(feature_dims) - 1):\n            self.layers.append(nn.Linear(feature_dims[i], feature_dims[i + 1]))\n        self.layers.append(nn.Linear(feature_dims[-1], 2))\n\n        # likelihood parameters\n        # if self.type == \"gaussian\":\n        #     self.register_buffer(\"likelihood_mean\", torch.zeros(1))\n        #     self.likelihood_log_std = nn.Parameter(torch.zeros(1))\n        # elif self.type == \"gamma\":\n        #     self.likelihood_log_concentration = nn.Parameter(torch.zeros(1))\n        #     self.likelihood_log_rate = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = torch.relu(layer(x))\n\n        if self.type == \"gaussian\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_mean = self.likelihood_mean.expand(y_pred.shape[0])\n            # likelihood_log_std = self.likelihood_log_std.expand(y_pred.shape[0])\n            # likelihood_std = torch.exp(likelihood_log_std)\n            # return y_pred, likelihood_mean, likelihood_std\n\n            y_out = self.layers[-1](x)\n            mean = y_out[:, 0]\n            log_std = y_out[:, 1]\n            std = torch.exp(log_std)\n            return mean.ravel(), std.ravel()\n\n        elif self.type == \"gamma\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_log_concentration = self.likelihood_log_concentration.expand(\n            #     y_pred.shape[0]\n            # )\n            # likelihood_log_rate = self.likelihood_log_rate.expand(y_pred.shape[0])\n            # likelihood_concentration = torch.exp(likelihood_log_concentration)\n            # likelihood_rate = torch.exp(likelihood_log_rate)\n            # return y_pred, likelihood_concentration, likelihood_rate\n\n            y_out = self.layers[-1](x)\n            log_concentration = y_out[:, 0]\n            log_rate = y_out[:, 1]\n            concentration = torch.exp(log_concentration)\n            rate = torch.exp(log_rate)\n            return concentration, rate\n\n    def loss_fn(self, y, param1, param2):\n        if self.type == \"gaussian\":\n            # epsilon = y - y_pred\n            # mean = param1\n            # std = param2\n            # dist = torch.distributions.Normal(mean, std + 1e-6)\n            # return -dist.log_prob(epsilon).mean()\n            mean = param1\n            std = param2\n            dist = torch.distributions.Normal(mean, std + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()\n\n        elif self.type == \"gamma\":\n            # epsilon = torch.clip(y - y_pred, min=1e-6, max=1e6)\n            # concentration = param1\n            # rate = param2\n            # dist = torch.distributions.Gamma(concentration, rate)\n            # return -dist.log_prob(epsilon).mean()\n            concentration = param1\n            rate = param2\n            dist = torch.distributions.Gamma(concentration + 1e-3, rate + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gaussian MLP",
    "text": "Fit Gaussian MLP\n\ntorch.manual_seed(0)\n\nmodel = ProbabilisticMLP(1, [32, 32], \"gaussian\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 500\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.4503: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01&lt;00:00, 291.18it/s]\n\n\n\n\n\n\n\n\n\n\n# sns.kdeplot(param2.cpu().detach().numpy(), label=\"std\")\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x_test.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n    # y_mean = y_pred.cpu().numpy().ravel() + mean.cpu().numpy().ravel()\n    # y_std = std.cpu().numpy().ravel()\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mean, label=\"y_mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    y_mean - 2 * y_std,\n    y_mean + 2 * y_std,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n\nupper = y_mean + 2 * y_std\nlower = y_mean - 2 * y_std\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.91"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gamma MLP",
    "text": "Fit Gamma MLP\n\nmodel = ProbabilisticMLP(1, [32, 32, 32], \"gamma\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 1000\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0775: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03&lt;00:00, 266.98it/s]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.special import gammaincinv, gamma\n\nwith torch.no_grad():\n    concetration, rate = model(x_test.to(device))\n    concetration = concetration.cpu().ravel().numpy()\n    rate = rate.cpu().ravel().numpy()\n\n    y_mode = (concetration - 1) / rate\n\n    quantile_fn = lambda p: gammaincinv(concetration, gamma(concetration) * p) / rate\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mode, label=\"mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    lower,\n    upper,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    param1, param2 = model(x.to(device))\n    concetration = param1.cpu().numpy().ravel()\n    rate = param2.cpu().numpy().ravel()\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.07"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html",
    "href": "posts/Multiclass_GP_classification.html",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.gaussian_process import GaussianProcessClassifier, kernels\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#common-functions",
    "href": "posts/Multiclass_GP_classification.html#common-functions",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Common functions",
    "text": "Common functions\n\ndef get_kernel(ard):\n    return GPy.kern.RBF(2, ARD=ard)\n\ndef create_and_fit_model(model_class, X, y, ard, **kwargs):\n    model = model_class(X, y, get_kernel(ard), **kwargs)\n    model.optimize()\n    return model"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "href": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\n\nX, y = make_blobs(n_samples=200, centers=9, random_state=0)\n# X, y = make_moons(n_samples=200, noise=0.1, random_state=0)\ny = y.reshape(-1, 1)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');\n\n\n\n\n\n\n\n\n\ngrid1 = np.linspace(X.min(axis=0)[0]-1, X.max(axis=0)[0]+1, 50)\ngrid2 = np.linspace(X.min(axis=0)[1]-1, X.max(axis=0)[1]+1, 50)\nGrid1, Grid2 = np.meshgrid(grid1, grid2)\nX_grid = np.vstack([Grid1.ravel(), Grid2.ravel()]).T"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#train-test-split",
    "href": "posts/Multiclass_GP_classification.html#train-test-split",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Train-test split",
    "text": "Train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\ny_train_one_hot = encoder.transform(y_train)\ny_test_one_hot = encoder.transform(y_test)"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a regression problem",
    "text": "Treat it as a regression problem\nHere we can regress over the class labels as if they are discrete realizations of a continuous variable. We will round the predictions to the nearest integer to get the class labels.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.67      0.46      0.55        13\n           1       0.33      0.57      0.42         7\n           2       0.57      0.62      0.59        13\n           3       0.71      0.45      0.56        11\n           4       0.71      0.91      0.80        11\n           5       0.67      0.80      0.73        10\n           6       0.33      0.50      0.40         6\n           7       0.85      0.65      0.73        17\n           8       1.00      0.83      0.91        12\n\n    accuracy                           0.65       100\n   macro avg       0.65      0.64      0.63       100\nweighted avg       0.69      0.65      0.66       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.60      0.46      0.52        13\n           1       0.27      0.43      0.33         7\n           2       0.57      0.62      0.59        13\n           3       0.83      0.45      0.59        11\n           4       0.73      1.00      0.85        11\n           5       0.62      0.80      0.70        10\n           6       0.38      0.50      0.43         6\n           7       0.92      0.65      0.76        17\n           8       1.00      0.92      0.96        12\n\n    accuracy                           0.66       100\n   macro avg       0.66      0.65      0.64       100\nweighted avg       0.70      0.66      0.67       100\n\n\n\nWe get the raw predictions as below:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nmappables = []\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0]\n    mappable = ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    mappables.append(mappable)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\n# put a common colorbar for both mappables\nfig.colorbar(mappables[0], ax=ax, cax=fig.add_axes([0.92, 0.1, 0.02, 0.8]));\n\n\n\n\n\n\n\n\nNow, let us see the classification boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].round().astype(int)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a multi-output regression problem",
    "text": "Treat it as a multi-output regression problem\nIn this method, we learn a shared GP model among each class in one v/s rest setting. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\nWhat will happen if we ignore the points where the predictions are below 0.5?\n\nprint(\"ARD\")\npred_probas = ard_model.predict(X_test)[0]\npred_proba = pred_probas.max(axis=1)\n\nmask = pred_proba &gt; 0.5\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\npreds_masked = preds[mask]\nground_truth_masked = y_test[mask]\n\nprint(ground_truth_masked.shape, preds_masked.shape)\nprint(classification_report(ground_truth_masked, preds_masked))\n\nARD\n(98, 1) (98,)\n              precision    recall  f1-score   support\n\n           0       0.78      0.64      0.70        11\n           1       0.67      0.86      0.75         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.92        98\n   macro avg       0.91      0.92      0.91        98\nweighted avg       0.92      0.92      0.92        98\n\n\n\nLet‚Äôs visualize the uncertain points:\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='rainbow', edgecolor='k')\nplt.scatter(X_test[~mask, 0], X_test[~mask, 1], s=150, c=y_test[~mask], cmap='rainbow', edgecolor='k', label='uncertain points');\nplt.legend();\n\n\n\n\n\n\n\n\nWe see that some points close to the decision boundary are uncertain. We can now plot the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].argmax(axis=1) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n    \n# known = set()\n# for i in range(len(y_grid)):\n#     pred = y_grid[i]\n#     if pred in known:\n#         x = np.random.uniform()\n#         if x &lt; 0.2:\n#             known.remove(pred)\n#     else:\n#         known.add(pred)\n#         ax[0].text(X_grid[i, 0], X_grid[i, 1], str(pred), fontsize=10, color='k', ha='center', va='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\npd.Series(y_grid).value_counts()\n\n7    478\n5    396\n8    378\n4    324\n6    297\n1    265\n2    201\n3     98\n0     63\ndtype: int64"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a One v/s Rest classification problem",
    "text": "Treat it as a One v/s Rest classification problem\nHere we learn a separate model for each class. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=[0.01, 0.01])\n# non_ard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=0.1)\nard_model = GaussianProcessClassifier(kernel=ard_kernel, random_state=0)\n# non_ard_model = GaussianProcessClassifier(kernel=non_ard_kernel, random_state=0)\nard_model.fit(X_train, y_train.ravel())\n# non_ard_model.fit(X_train, y_train.ravel())\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)\nprint(classification_report(y_test, preds))\n# print(\"Non-ARD\")\n# preds = non_ard_model.predict(X_test)\n# print(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        13\n           1       0.00      0.00      0.00         7\n           2       1.00      0.08      0.14        13\n           3       0.00      0.00      0.00        11\n           4       0.00      0.00      0.00        11\n           5       0.00      0.00      0.00        10\n           6       0.00      0.00      0.00         6\n           7       0.00      0.00      0.00        17\n           8       0.12      1.00      0.22        12\n\n    accuracy                           0.13       100\n   macro avg       0.12      0.12      0.04       100\nweighted avg       0.14      0.13      0.04       100\n\n\n\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\nVisualizing the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model]:\n    y_grid = model.predict_proba(X_grid).argmax(axis=1)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={bool(i)}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "href": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Try random forest model",
    "text": "Try random forest model\n\nmodel = RandomForestClassifier(n_estimators=1000, random_state=0)\nmodel.fit(X_train, y_train.ravel())\n\nprint(\"Random Forest\")\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))\n\nRandom Forest\n              precision    recall  f1-score   support\n\n           0       0.78      0.54      0.64        13\n           1       0.60      0.86      0.71         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.91      1.00      0.95        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.92      0.90       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\ny_grid = model.predict(X_grid) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\nax.contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html",
    "title": "Machine Learning Cheat Sheet",
    "section": "",
    "text": "\\[\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x} x P(X=x) \\\\\n\\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} x p(x) dx \\\\\n\\bar{x} &= \\frac{1}{n} \\sum_{i=1}^{n} x_i \\\\\n\\mathbb{E}[aX + b] &= a\\mathbb{E}[X] + b\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\\n\\text{Var}(X) &= \\sum_{x} (x - \\mathbb{E}[X])^2 P(X=x) \\\\\n\\text{Var}(X) &= \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])^2 p(x) dx\\\\\n\\text{Var}(X) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])^2 \\\\\n\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{SD}(X) &= \\sqrt{\\text{Var}(X)}\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Cov}(X, Y) &= \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] \\\\\n\\text{Cov}(X, Y) &= \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n\\text{Cov}(X, Y) &= \\sum_{x,y} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) P(X=x, Y=y) \\\\\n\\text{Cov}(X, Y) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) p(x,y) dx dy \\\\\n\\text{Cov}(X, Y) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])(y_i - \\mathbb{E}[Y]) \\\\\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Corr}(X, Y) &= \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} \\\\\n&= \\frac{\\text{Cov}(X, Y)}{\\text{SD}(X) \\text{SD}(Y)} \\\\\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\mathbb{E}[X + Y] &= \\mathbb{E}[X] + \\mathbb{E}[Y] \\\\\n\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] &= \\sum_{i=1}^{n} \\mathbb{E}[X_i]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#probability",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#probability",
    "title": "Machine Learning Cheat Sheet",
    "section": "",
    "text": "\\[\\begin{align*}\n\\mathbb{E}[X] &= \\sum_{x} x P(X=x) \\\\\n\\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} x p(x) dx \\\\\n\\bar{x} &= \\frac{1}{n} \\sum_{i=1}^{n} x_i \\\\\n\\mathbb{E}[aX + b] &= a\\mathbb{E}[X] + b\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\\n\\text{Var}(X) &= \\sum_{x} (x - \\mathbb{E}[X])^2 P(X=x) \\\\\n\\text{Var}(X) &= \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])^2 p(x) dx\\\\\n\\text{Var}(X) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])^2 \\\\\n\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{SD}(X) &= \\sqrt{\\text{Var}(X)}\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Cov}(X, Y) &= \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] \\\\\n\\text{Cov}(X, Y) &= \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n\\text{Cov}(X, Y) &= \\sum_{x,y} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) P(X=x, Y=y) \\\\\n\\text{Cov}(X, Y) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) p(x,y) dx dy \\\\\n\\text{Cov}(X, Y) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])(y_i - \\mathbb{E}[Y]) \\\\\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\text{Corr}(X, Y) &= \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} \\\\\n&= \\frac{\\text{Cov}(X, Y)}{\\text{SD}(X) \\text{SD}(Y)} \\\\\n\\end{align*}\\]\n\n\n\n\\[\\begin{align*}\n\\mathbb{E}[X + Y] &= \\mathbb{E}[X] + \\mathbb{E}[Y] \\\\\n\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] &= \\sum_{i=1}^{n} \\mathbb{E}[X_i]\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#classification",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#classification",
    "title": "Machine Learning Cheat Sheet",
    "section": "Classification",
    "text": "Classification\n\nLogistic Regression\n\n\n\nEntity\nMath\nShape\nType\n\n\n\n\nFeatures\n\\(X\\)\n(n, d)\nContinuous\n\n\nWeights\n\\(W\\)\n(d, 1)\nContinuous\n\n\nBias\n\\(b\\)\n(1, 1)\nContinuous\n\n\nOutput\n\\(\\boldsymbol{y}\\)\n(n, 1)\nBinary\n\n\n\n\\[\\begin{align*}\n\\hat{\\boldsymbol{y}} &= \\sigma(XW + b) \\\\\n&= \\sigma(\\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times 1} + \\underbrace{b}_{1}) \\\\\n&= \\sigma(\\underbrace{\\boldsymbol{z}}_{n \\times 1}) \\\\\n\\text{where } \\sigma(a) &= \\frac{1}{1 + e^{-a}}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n}\n\\begin{cases}\n\\log(\\hat{\\boldsymbol{y}}_i) & \\text{if } \\boldsymbol{y}_i = 1 \\\\\n\\log(1 - \\hat{\\boldsymbol{y}}_i) & \\text{if } \\boldsymbol{y}_i = 0\n\\end{cases}\n\\end{align*}\\]\nAnother way to write the loss function is:\n\\[\\begin{align*}\n\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\boldsymbol{y}_i \\log(\\hat{\\boldsymbol{y}}_i) + (1 - \\boldsymbol{y}_i) \\log(1 - \\hat{\\boldsymbol{y}}_i)\\right)\n\\end{align*}\\]\n\n\nMulti-Class Logistic Regression\n\n\n\nEntity\nMath\nShape\nType\n\n\n\n\nFeatures\n\\(X\\)\n(n, d)\nContinuous\n\n\nWeights\n\\(W\\)\n(d, k)\nContinuous\n\n\nBias\n\\(\\boldsymbol{b}\\)\n(1, k)\nContinuous\n\n\nOutput\n\\(Y\\)\n(n, k)\nOne-Hot\n\n\n\n\\[\\begin{align*}\n\\hat{Y} &= \\text{softmax}(XW + \\boldsymbol{b}) \\\\\n&= \\text{softmax}(\\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times k} + \\underbrace{\\boldsymbol{b}}_{1 \\times k}) \\\\\n&= \\text{softmax}(\\underbrace{Z}_{n \\times k}) \\\\\n\\text{where } \\hat{Y}_{ij} = \\text{softmax}(Z_{ij}) &= \\frac{e^{Z_{ij}}}{\\sum_{j=1}^{k} e^{Z_{ij}}}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n\\end{align*}\\]\n\n\nMetrics\n\nAccuracy\n\\[\\begin{align*}\n\\text{Accuracy} &= \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} \\\\\n&= \\frac{TP + TN}{TP + TN + FP + FN}\n\\end{align*}\\]\n\n\nTrue Positive Rate (TPR)\n\\[\\begin{align*}\n\\text{True Positive Rate} &= \\frac{\\text{True Positives}}{\\text{Actual Positives}} \\\\\n&= \\frac{TP}{TP + FN}\n\\end{align*}\\]\n\n\nFalse Positive Rate (FPR)\n\\[\\begin{align*}\n\\text{False Positive Rate} &= \\frac{\\text{False Positives}}{\\text{Actual Negatives}} \\\\\n&= \\frac{FP}{FP + TN}\n\\end{align*}\\]\n\n\nROC Curve (Receiver Operating Characteristic Curve)\n\nX-axis: FPR, Y-axis: TPR\n\n\n\nArea Under the ROC Curve (AUC)\n\nAUC = 1: Perfect model\nAUC = 0.5: Random model\nAUC &lt; 0.5: Model is worse than random\n\n\n\nPrecision\n\\[\\begin{align*}\n\\text{Precision} &= \\frac{\\text{True Positives}}{\\text{Predicted Positives}} \\\\\n&= \\frac{TP}{TP + FP}\n\\end{align*}\\]\n\n\nRecall\n\\[\\begin{align*}\n\\text{Recall} &= \\frac{\\text{True Positives}}{\\text{Actual Positives}} \\\\\n&= \\frac{TP}{TP + FN}\n\\end{align*}\\]\n\n\nF1 Score\nHarmonic mean of precision and recall.\n\\[\\begin{align*}\n\\text{F1 Score} &= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\\n&= \\frac{2TP}{2TP + FP + FN}\n\\end{align*}\\]\n\n\nMatthews Correlation Coefficient\nSimilar to Pearson correlation coefficient, but for binary classification.\n\nThe Matthews correlation coefficient (MCC) is pearson correlation coefficient between the observed and predicted binary classifications.\n\nDerivation: (Homework) Range: -1 to 1\n\n1: Total disagreement\n0: Random agreement\n1: Perfect agreement\n\nFormula:\n\\[\\begin{align*}\n\\text{MCC} &= \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n\\end{align*}\\]\n\n\nCohen‚Äôs Kappa\nDefinition: Cohen‚Äôs Kappa is a statistic that measures agreement between two rates while correcting for chance. Range: -1 to 1\n\n1: Total disagreement\n0: Random agreement\n1: Perfect agreement\n\nFormula:\n\\[\\begin{align*}\n\\text{Cohen's Kappa} &= \\frac{P_o - P_e}{1 - P_e} \\\\\nP_o &= \\text{Accuracy} \\\\\nP_e &= \\sum_{i=1}^{k} \\frac{\\text{predicted}_i}{\\text{total samples}} \\cdot \\frac{\\text{actual}_i}{\\text{total samples}}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#regression",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#regression",
    "title": "Machine Learning Cheat Sheet",
    "section": "Regression",
    "text": "Regression\n\nLinear Regression\n\n\n\nEntity\nMath\nShape\nType\n\n\n\n\nFeatures\n\\(X\\)\n(n, d)\nContinuous\n\n\nWeights\n\\(W\\)\n(d, 1)\nContinuous\n\n\nBias\n\\(b\\)\n(1, 1)\nContinuous\n\n\nOutput\n\\(\\boldsymbol{y}\\)\n(n, 1)\nContinuous\n\n\n\n\\[\\begin{align*}\n\\hat{\\boldsymbol{y}} &= XW + b \\\\\n&= \\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times 1} + \\underbrace{b}_{1} \\\\\n&= \\underbrace{\\boldsymbol{z}}_{n \\times 1}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\text{Mean Squared Loss} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2\n\\end{align*}\\]\nNormal Equation:\n\\[\\begin{align*}\n\\tilde{X} &= \\begin{bmatrix}\n1 & X\n\\end{bmatrix} \\\\\n\\tilde{W} &= \\begin{bmatrix}\nb \\\\\nW\n\\end{bmatrix} \\\\\n\\hat{\\boldsymbol{y}} &= \\tilde{X} \\tilde{W} \\\\\n\\tilde{W} &= (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T \\boldsymbol{y}\n\\end{align*}\\]\n\n\nMetrics\n\nMean Absolute Error (MAE)\n\\[\\begin{align*}\n\\text{MAE} &= \\frac{1}{n} \\sum_{i=1}^{n} |\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i|\n\\end{align*}\\]\n\n\nMean Squared Error (MSE)\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2\n\\end{align*}\\]\n\n\nRoot Mean Squared Error (RMSE)\n\\[\\begin{align*}\n\\text{RMSE} &= \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2}\n\\end{align*}\\]\n\n\nR-squared (Coefficient of Determination)\n\\[\\begin{align*}\n\\text{R-squared} &= 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} \\\\\n\\text{where } \\text{SS}_{\\text{res}} &= \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2 \\\\\n\\text{and } \\text{SS}_{\\text{tot}} &= \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\bar{\\boldsymbol{y}})^2\n\\end{align*}\\]\n\n\nMean Absolute Percentage Error (MAPE)\n\\[\\begin{align*}\n\\text{MAPE} &= \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i}{\\boldsymbol{y}_i} \\right| \\times 100\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#clustering",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#clustering",
    "title": "Machine Learning Cheat Sheet",
    "section": "Clustering",
    "text": "Clustering\n\nK-Means Clustering\n\nRandomly initialize \\(k\\) centroids\n\n\\[\\begin{align*}\n\\mu_j &= \\text{randomly initialize } j \\text{th centroid}\n\\end{align*}\\]\n\nAssign each data point to the nearest centroid\n\n\\[\\begin{align*}\n\\hat{y}_i &= \\arg\\min_{j} ||x_i - \\mu_j||^2\n\\end{align*}\\]\n\nUpdate the centroids\n\n\\[\\begin{align*}\n\\mu_j &= \\frac{1}{n_j} \\sum_{i: \\hat{y}_i = j} x_i\n\\end{align*}\\]\n\nK-Means is guaranteed to converge, but not necessarily to the global optimum.\nK-Means is sensitive to the initial placement of centroids.\nK-Means is sensitive to outliers."
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#bias-variance-tradeoff",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#bias-variance-tradeoff",
    "title": "Machine Learning Cheat Sheet",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\n\n\nHigh Bias\nHigh Variance\n\n\n\n\nSimple model\nComplex model\n\n\nUnderfitting\nOverfitting\n\n\nHigh training error\nLow training error\n\n\nHigh test error\nHigh test error\n\n\nNot sensitive to noise\nSensitive to noise\n\n\n\nDecompose the error into three components:\n\nThis decomposition is applied to a single point \\((x, y)\\).\nTrain K models on different training datasets of the same distribution.\nAll expectations are taken over predictions \\(\\hat{y}_i\\) of the K models on the same point \\((x, y)\\) where \\(i = 1, \\ldots, K\\).\n\n\\[\\begin{align*}\n\\text{Error} &= \\text{Variance} + \\text{Bias}^2 + \\sigma^2 \\\\\nE[(\\hat{y} - y)^2] &= E[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2] + (\\mathbb{E}[\\hat{y}] - y)^2 + \\sigma^2\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/2025-04-24-ml-basics-cheatsheet.html#optimization",
    "href": "posts/2025-04-24-ml-basics-cheatsheet.html#optimization",
    "title": "Machine Learning Cheat Sheet",
    "section": "Optimization",
    "text": "Optimization\n\nGradient Descent\n\\[\\begin{align*}\n\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t) \\\\\n\\text{where } \\alpha &= \\text{learning rate} \\\\\n\\text{and } J(\\theta) &= \\text{cost function}\n\\end{align*}\\]\n\n\nStochastic Gradient Descent (SGD)\n\\[\\begin{align*}\n\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t, x_i, y_i) \\\\\n\\text{where } (x_i, y_i) &= \\text{randomly selected training example}\n\\end{align*}\\]\n\n\nMini-Batch Gradient Descent\n\\[\\begin{align*}\n\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t, B) \\\\\n\\text{where } B &= \\text{mini-batch of training examples}\n\\end{align*}\\]\n\n\nSGD with Momentum\n\\[\\begin{align*}\nv_t &= \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta_t) \\\\\n\\theta_{t+1} &= \\theta_t - \\alpha v_t\n\\end{align*}\\] where \\(\\beta\\) is the momentum term (usually set to 0.9).\n\n\nRMSProp\n\\[\\begin{align*}\ns_t &= \\beta s_{t-1} + (1 - \\beta) (\\nabla J(\\theta_t))^2 \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{s_t} + \\epsilon} \\nabla J(\\theta_t)\n\\end{align*}\\] where \\(\\epsilon\\) is a small constant to prevent division by zero.\n\n\nAdam\n\\[\\begin{align*}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(\\theta_t) \\\\\ns_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(\\theta_t))^2 \\\\\n\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n\\hat{s}_t &= \\frac{s_t}{1 - \\beta_2^t} \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{s}_t} + \\epsilon} \\hat{m}_t\n\\end{align*}\\] where \\(\\beta_1\\) and \\(\\beta_2\\) are the exponential decay rates for the first and second moment estimates, respectively (usually set to 0.9 and 0.999)."
  },
  {
    "objectID": "posts/climate-modeling-with-SpecialGP.html",
    "href": "posts/climate-modeling-with-SpecialGP.html",
    "title": "Climate Modeling with GPs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pyproj\nimport numpy as np\nimport xarray as xr\n\nfrom skgpytorch.models import GPRegression\n\nimport matplotlib.pyplot as plt\n\n\n# def haversine(lon1, lat1, lon2, lat2):\n#     \"\"\"\n#     Calculate the great circle distance in kilometers between two points \n#     on the earth (specified in decimal degrees)\n#     \"\"\"\n#     # convert decimal degrees to radians \n#     lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n#     # haversine formula \n#     dlon = lon2 - lon1 \n#     dlat = lat2 - lat1 \n#     a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n#     c = 2 * np.arcsin(np.sqrt(a)) \n#     r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n#     return c * r\n\n# def new_coords(lat1, long1):\n#     new_lat1 = haversine(0, 0, 0, lat1)\n#     new_long1 = haversine(0, 0, long1, 0)\n#     return new_lat1, new_long1\n\ndef lat_long_to_cartesian(latitude, longitude):\n    # Convert latitude and longitude to radians\n    phi = np.radians(latitude)\n    lam = np.radians(longitude)\n\n    # Constants for WGS 84 ellipsoid\n    a = 6378137.0  # equatorial radius in meters\n    e = 0.0818191908426  # eccentricity\n\n    # Calculate Earth's radius at the given latitude\n    R = a / np.sqrt(1 - (e ** 2) * (np.sin(phi) ** 2))\n\n    # Convert to Cartesian coordinates\n    X = R * np.sin(lam)\n    Y = R * np.tan(phi)\n\n    return X, Y\n\ndef wgs84_coords(lat, lon):    \n    # Define coordinate systems\n    wgs84 = pyproj.CRS.from_epsg(4326)  # WGS 84 lat-long system\n    utm_zone_32n = pyproj.CRS.from_string(\"+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n\n    # Create a transformer object\n    transformer = pyproj.Transformer.from_crs(wgs84, utm_zone_32n)\n\n    # Convert lat-long coordinates to UTM coordinates\n    utm_easting, utm_northing = transformer.transform(lon, lat)\n\n    return utm_northing, utm_easting\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------\n# Position embedding utils\n# --------------------------------------------------------\n\n\n# --------------------------------------------------------\n# 2D sine-cosine position embedding\n# References:\n# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n# MoCo v3: https://github.com/facebookresearch/moco-v3\n# --------------------------------------------------------\ndef get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size_h, dtype=np.float32)\n    grid_w = np.arange(grid_size_w, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\n# --------------------------------------------------------\n# Interpolate position embeddings for high-resolution\n# References:\n# DeiT: https://github.com/facebookresearch/deit\n# --------------------------------------------------------\ndef interpolate_pos_embed(model, checkpoint_model, new_size=(64, 128)):\n    if \"net.pos_embed\" in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model[\"net.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        orig_num_patches = pos_embed_checkpoint.shape[-2]\n        patch_size = model.patch_size\n        w_h_ratio = 2\n        orig_h = int((orig_num_patches // w_h_ratio) ** 0.5)\n        orig_w = w_h_ratio * orig_h\n        orig_size = (orig_h, orig_w)\n        new_size = (new_size[0] // patch_size, new_size[1] // patch_size)\n        # print (orig_size)\n        # print (new_size)\n        if orig_size[0] != new_size[0]:\n            print(\"Interpolate PEs from %dx%d to %dx%d\" % (orig_size[0], orig_size[1], new_size[0], new_size[1]))\n            pos_tokens = pos_embed_checkpoint.reshape(-1, orig_size[0], orig_size[1], embedding_size).permute(\n                0, 3, 1, 2\n            )\n            new_pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size[0], new_size[1]), mode=\"bicubic\", align_corners=False\n            )\n            new_pos_tokens = new_pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            checkpoint_model[\"net.pos_embed\"] = new_pos_tokens\n\n\ndef interpolate_channel_embed(checkpoint_model, new_len):\n    if \"net.channel_embed\" in checkpoint_model:\n        channel_embed_checkpoint = checkpoint_model[\"net.channel_embed\"]\n        old_len = channel_embed_checkpoint.shape[1]\n        if new_len &lt;= old_len:\n            checkpoint_model[\"net.channel_embed\"] = channel_embed_checkpoint[:, :new_len]\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=initializers.RandomUniform(-1 / input_dim, 1 / input_dim), activation=tf.sin))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[i-1]) / activation_scale, np.sqrt(6 / features[i-1]) / activation_scale), activation=tf.sin))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[-1]) / activation_scale, np.sqrt(6 / features[-1]) / activation_scale), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../data/2m_temperature_2018_5.625deg_Jan.nc\").to_dataframe().reset_index()\ndata1 = xr.open_dataset(\"../data/2m_temperature_2018_1.40625deg_Jan.nc\").to_dataframe().reset_index()\n\n\ndata5.head()\n\n\n\n\n\n\n\n\nlon\nlat\ntime\nt2m\n\n\n\n\n0\n0.0\n-87.1875\n2018-01-01 00:00:00\n250.728180\n\n\n1\n0.0\n-87.1875\n2018-01-01 01:00:00\n250.468552\n\n\n2\n0.0\n-87.1875\n2018-01-01 02:00:00\n250.250931\n\n\n3\n0.0\n-87.1875\n2018-01-01 03:00:00\n250.040314\n\n\n4\n0.0\n-87.1875\n2018-01-01 04:00:00\n249.993790\n\n\n\n\n\n\n\n\ntime_stamp = \"2018-01-01 01:00:00\"\ntrain_df = data5[data5.time == time_stamp]\ntest_df = data1[data1.time == time_stamp]\n\nX = np.stack([train_df.lat.values, train_df.lon.values], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\nrff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(2048, 2), y.shape=(2048, 1)\nX_test.shape=(32768, 2), y_test.shape=(32768, 1)\nX.shape=(2048, 2), X_test.shape=(32768, 2)\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = MLP(2, 1, [256]*4, 30.0, 0.0)\n# model = ResNet()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n\nhistory = model.fit(X_scaled, y_scaled, epochs=5000, batch_size=X_scaled.shape[0], verbose=0)\n\n\nplt.plot(history.history['loss']);\n\n\n\n\n\n\n\n\n\ny_pred = model.predict(X_test_scaled) * (y_max - y_min) + y_min\nplt.imshow(y_pred.reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n1024/1024 [==============================] - 1s 1ms/step\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\n\n\n\n\n\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\nText(0.5, 1.0, 'Diff')\n\n\n\n\n\n\n\n\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\nrmse = np.sqrt(np.mean((y_pred.ravel() - y_test.ravel())**2))\nprint(f\"{rmse=}\")\n\nrmse=2.7606046\n\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")\n\nmean_bias=0.10866926"
  },
  {
    "objectID": "posts/PurpleAir.html",
    "href": "posts/PurpleAir.html",
    "title": "Download low-cost data from OpenAQ",
    "section": "",
    "text": "import requests\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom glob import glob\n\nfrom geopy.geocoders import Nominatim\n\nOpenAQ has an aws s3 bucket that contains all the data to download for free. This is a guide to download the data from the bucket. If you have enough space and bandwidth, aws s3 commands are the fastest way to download the data. If you don‚Äôt have enough space/bandwidth or you want to download specific data only then follow along.\nAcknowledgement: Much help is taken from ChatGPT for some complex Linux commands.\nWe will mostly use the following commands:\naws s3 ls\naws s3 cp\nScenario 1: We want to download PurpleAir sensors data for Delhi for entire 2022. I am taking Delhi‚Äôs example since there are far lesser sensors in Delhi than in the US. So, it will be easier for this demo.\nSome statistics that I have calculated for PurpleAir sensors in US are as follows:\n\n\n\nCountry\nNumber of sensors\nTotal size\nyears\n\n\n\n\nUSA\n22497\n90.945 GB\n2018, 2019, 2020, 2021, 2022, 2023\n\n\n\nLet‚Äôs see how to calculate these statistics for India.\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ &gt; /tmp/in.txt\n\nThe output of above command contains all sensor IDs within India. These sensor IDs are assigned by OpenAQ and they are different from PurpleAir ‚Äúsensor_index‚Äù. The output looks like the following.\n\nwith open('/tmp/in.txt', 'r') as f:\n    lines = [line.strip() for line in f.readlines()]\n  \nprint(f\"Number of locations = {len(lines)}\\n\")\nprint(*lines[:3], sep='\\n')\n\nNumber of locations = 623\n\nPRE locationid=160485/\nPRE locationid=218334/\nPRE locationid=218336/\n\n\nCounting total size of files:\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ --recursive &gt; /tmp/in_files.txt\n\n\n!awk '{ sum += $3 } END { print (sum/1024/1024/1024) \" GB\"}' /tmp/in_files.txt\n\n1.20947 GB\n\n\nNumber of years:\n\n!cat /tmp/in_files.txt | grep -oP 'year=\\K\\w+' | sort | uniq\n\n2019\n2020\n2021\n2022\n2023\n\n\nLet‚Äôs find out which sensors among these belong to Delhi.\n\n!cat /tmp/in.txt | grep -oP 'locationid=\\K\\w+' | sort | uniq &gt; /tmp/in_locations.txt\n\nNow we use OpenAQ REST API. It has limit of 300 requests per 5 minutes. After the limit exceeds, it will return an error.\n\nurl = 'https://api.openaq.org/v2/locations'\nparams = {\n    'limit': 1000,\n    'country_id': 'IN',\n    \"modelName\": \"PurpleAir Sensor\"\n}\n\n# Request headers\nheaders = {\n    'accept': 'application/json'\n}\n\nresponse = requests.get(url, params=params, headers=headers)\n\nif response.status_code == 200:\n    data = response.json()\nelse:\n    print(response.status_code, response.reason)\n\n500 Internal Server Error\n\n\n\ndf = pd.DataFrame(data['results'])\ndf.shape\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df = pd.DataFrame(data['results'])\n      2 df.shape\n\nNameError: name 'data' is not defined\n\n\n\nThe following method works but takes too much time since it is online:\n\ndef get_city_name(coords):\n    latitude = coords[\"latitude\"]\n    longitude = coords[\"longitude\"]\n    geolocator = Nominatim(user_agent=\"myGeocoder\")  # Replace \"myGeocoder\" with your desired user agent\n    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n\n    if location:\n        address = location.raw.get('address', {})\n        city = address.get('city', '')\n        return city\n\n    return None\n\ndf[\"city\"] = df[\"coordinates\"].apply(get_city_name)\ndf\n\n\n\n\n\n\n\n\nid\ncity\nname\nentity\ncountry\nsources\nisMobile\nisAnalysis\nparameters\nsensorType\ncoordinates\nlastUpdated\nfirstUpdated\nmeasurements\nbounds\nmanufacturers\n\n\n\n\n0\n318146\nGangtok\nNASA_AQCS_201_cpa\nNone\nIN\nNone\nFalse\nNone\n[{'id': 135, 'unit': 'particles/cm¬≥', 'count':...\nNone\n{'latitude': 27.31013, 'longitude': 88.59687}\n2023-07-26T02:34:05+00:00\n2022-04-23T07:42:24+00:00\n1168071\n[88.59687, 27.31013, 88.59687, 27.31013]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n1\n220706\nGangtok\nNASA_AQCS_139\nNone\nIN\nNone\nFalse\nNone\n[{'id': 100, 'unit': 'c', 'count': 28600, 'ave...\nNone\n{'latitude': 27.310116, 'longitude': 88.59682}\n2023-07-26T02:34:04+00:00\n2021-02-17T09:56:06+00:00\n2205110\n[88.59682, 27.310116, 88.59682, 27.310116]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n2\n66673\nHisar\nNASA_AQCS_160\nNone\nIN\nNone\nFalse\nNone\n[{'id': 132, 'unit': 'mb', 'count': 28651, 'av...\nNone\n{'latitude': 29.146254, 'longitude': 75.72236}\n2023-07-26T02:33:56+00:00\n2021-01-19T23:59:16+00:00\n2396934\n[75.72236, 29.146254, 75.72236, 29.146254]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n3\n72977\nBengaluru\nUT Sensor 101\nNone\nIN\nNone\nFalse\nNone\n[{'id': 126, 'unit': 'particles/cm¬≥', 'count':...\nNone\n{'latitude': 13.045313, 'longitude': 77.573395}\n2023-07-26T02:33:55+00:00\n2021-01-14T01:18:23+00:00\n2498544\n[77.573395, 13.045313, 77.573395, 13.045313]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n4\n235916\nBengaluru\nUW Sensor 311\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm¬≥', 'count':...\nNone\n{'latitude': 13.048528, 'longitude': 77.582275}\n2023-07-26T02:33:49+00:00\n2021-09-16T12:29:52+00:00\n1773924\n[77.582275, 13.048528, 77.582275, 13.048528]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n604\n73922\nNew Delhi District\nUS Embassy A\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': '¬µg/m¬≥', 'count': 347, 'ave...\nNone\n{'latitude': 28.5979, 'longitude': 77.1847}\n2021-02-05T18:25:34+00:00\n2021-01-08T12:12:29+00:00\n2082\n[77.1847, 28.5979, 77.1847, 28.5979]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n605\n73924\nNew Delhi District\nUS Embassy B\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': '¬µg/m¬≥', 'count': 347, 'ave...\nNone\n{'latitude': 28.5982, 'longitude': 77.1837}\n2021-02-05T18:23:59+00:00\n2021-01-08T12:11:29+00:00\n2082\n[77.1837, 28.5982, 77.1837, 28.5982]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n606\n219370\nBhubaneswar Municipal Corporation\nBhubaneswar India\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': '¬µg/m¬≥', 'count': 180, 'ave...\nNone\n{'latitude': 20.2853, 'longitude': 85.7685}\n2021-02-03T10:18:41+00:00\n2021-02-03T03:02:39+00:00\n1080\n[85.7685, 20.2853, 85.7685, 20.2853]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n607\n71465\nGurugram District\nNASA_AQCS_152\nNone\nIN\nNone\nFalse\nNone\n[{'id': 1, 'unit': '¬µg/m¬≥', 'count': 1, 'avera...\nNone\n{'latitude': 28.4522, 'longitude': 77.0949}\n2021-01-14T01:18:54+00:00\n2021-01-14T01:18:54+00:00\n6\n[77.0949, 28.4522, 77.0949, 28.4522]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n608\n221974\nBengaluru\nUT sensor\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm¬≥', 'count':...\nNone\n{'latitude': 13.0449, 'longitude': 77.5788}\n2019-12-17T10:32:35+00:00\n2019-12-17T10:32:35+00:00\n6\n[77.5788, 13.0449, 77.5788, 13.0449]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n\n\n609 rows √ó 16 columns\n\n\n\nNow, we use another method of shapefile to do this:\n\n!wget --no-check-certificate \"https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\" -O /tmp/delhi.zip\n!unzip -o /tmp/delhi.zip -d /tmp/delhi\n\n--2023-07-26 08:37:34--  https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\nResolving groups.google.com (groups.google.com)... 216.239.32.177, 216.239.36.177, 216.239.38.177, ...\nConnecting to groups.google.com (groups.google.com)|216.239.32.177|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0 [following]\n--2023-07-26 08:37:35--  https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0\nResolving 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)... 142.251.10.137, 2404:6800:4003:c0f::89\nConnecting to 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)|142.251.10.137|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/zip]\nSaving to: ‚Äò/tmp/delhi.zip‚Äô\n\n/tmp/delhi.zip          [ &lt;=&gt;                ]  16.42K  --.-KB/s    in 0.04s   \n\n2023-07-26 08:37:37 (429 KB/s) - ‚Äò/tmp/delhi.zip‚Äô saved [16812]\n\nArchive:  /tmp/delhi.zip\n  inflating: /tmp/delhi/Delhi.kml    \n  inflating: /tmp/delhi/Districts.dbf  \n  inflating: /tmp/delhi/Districts.prj  \n  inflating: /tmp/delhi/Districts.qpj  \n  inflating: /tmp/delhi/Districts.shp  \n  inflating: /tmp/delhi/Districts.shx  \n\n\n\ngdf = gpd.read_file('/tmp/delhi/Districts.shp')\ngdf.plot(color=\"none\", edgecolor=\"black\");\n\n\n\n\n\n\n\n\n\n# check if a point is within Delhi\ndef is_within_delhi(coords):\n    point = Point(coords[\"longitude\"], coords[\"latitude\"])\n    for i, row in gdf.iterrows():\n        if row.geometry.contains(point):\n            return True\n    return False\n\ndf[\"is_within_delhi\"] = df[\"coordinates\"].apply(is_within_delhi)\n\n\ndelhi_df = df[df[\"is_within_delhi\"]]\ndelhi_df.shape\n\n(311, 17)\n\n\n\ndelhi_df.city.value_counts()\n\n                      194\nNew Delhi District    112\nDwarka                  3\nGhaziabad               2\nName: city, dtype: int64\n\n\nSeems like many points were not detected by the online geopy encoder.\nNow, we know that out of 623, 311 sensors belong to Delhi. Let‚Äôs download the data for these sensors. For illustration, I will download data for 3 sensors for year 2022 and month of Jan.\n\n# dump delhi_df.id to a file\ndelhi_df.id.to_csv('/tmp/delhi_locations.txt', index=False, header=False)\n!head -n3 /tmp/delhi_locations.txt\n\n274208\n221227\n273205\n\n\n\n!head -n3 /tmp/delhi_locations.txt &gt; /tmp/delhi_locations_3.txt\n!while read -r sensor_id; do aws s3 --no-sign-request cp s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=$sensor_id/year=2022/month=01 /tmp/delhi_data --recursive; done &lt; /tmp/delhi_locations_3.txt\n\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220107.csv.gz to ../../../../tmp/delhi_data/location-274208-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220120.csv.gz to ../../../../tmp/delhi_data/location-274208-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220131.csv.gz to ../../../../tmp/delhi_data/location-274208-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220122.csv.gz to ../../../../tmp/delhi_data/location-274208-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220130.csv.gz to ../../../../tmp/delhi_data/location-274208-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220129.csv.gz to ../../../../tmp/delhi_data/location-274208-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220114.csv.gz to ../../../../tmp/delhi_data/location-274208-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220123.csv.gz to ../../../../tmp/delhi_data/location-274208-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220113.csv.gz to ../../../../tmp/delhi_data/location-274208-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220121.csv.gz to ../../../../tmp/delhi_data/location-274208-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220107.csv.gz to ../../../../tmp/delhi_data/location-221227-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220104.csv.gz to ../../../../tmp/delhi_data/location-221227-20220104.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220115.csv.gz to ../../../../tmp/delhi_data/location-221227-20220115.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220105.csv.gz to ../../../../tmp/delhi_data/location-221227-20220105.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220108.csv.gz to ../../../../tmp/delhi_data/location-221227-20220108.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220106.csv.gz to ../../../../tmp/delhi_data/location-221227-20220106.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220102.csv.gz to ../../../../tmp/delhi_data/location-221227-20220102.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220117.csv.gz to ../../../../tmp/delhi_data/location-221227-20220117.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220118.csv.gz to ../../../../tmp/delhi_data/location-221227-20220118.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220110.csv.gz to ../../../../tmp/delhi_data/location-221227-20220110.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220131.csv.gz to ../../../../tmp/delhi_data/location-221227-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220113.csv.gz to ../../../../tmp/delhi_data/location-221227-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220119.csv.gz to ../../../../tmp/delhi_data/location-221227-20220119.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220121.csv.gz to ../../../../tmp/delhi_data/location-221227-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220123.csv.gz to ../../../../tmp/delhi_data/location-221227-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220116.csv.gz to ../../../../tmp/delhi_data/location-221227-20220116.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220128.csv.gz to ../../../../tmp/delhi_data/location-221227-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220122.csv.gz to ../../../../tmp/delhi_data/location-221227-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220101.csv.gz to ../../../../tmp/delhi_data/location-221227-20220101.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220111.csv.gz to ../../../../tmp/delhi_data/location-221227-20220111.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220120.csv.gz to ../../../../tmp/delhi_data/location-221227-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220109.csv.gz to ../../../../tmp/delhi_data/location-221227-20220109.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220129.csv.gz to ../../../../tmp/delhi_data/location-221227-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220103.csv.gz to ../../../../tmp/delhi_data/location-221227-20220103.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220112.csv.gz to ../../../../tmp/delhi_data/location-221227-20220112.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220114.csv.gz to ../../../../tmp/delhi_data/location-221227-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220130.csv.gz to ../../../../tmp/delhi_data/location-221227-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220120.csv.gz to ../../../../tmp/delhi_data/location-273205-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220124.csv.gz to ../../../../tmp/delhi_data/location-273205-20220124.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220107.csv.gz to ../../../../tmp/delhi_data/location-273205-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220121.csv.gz to ../../../../tmp/delhi_data/location-273205-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220126.csv.gz to ../../../../tmp/delhi_data/location-273205-20220126.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220114.csv.gz to ../../../../tmp/delhi_data/location-273205-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220129.csv.gz to ../../../../tmp/delhi_data/location-273205-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220130.csv.gz to ../../../../tmp/delhi_data/location-273205-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220113.csv.gz to ../../../../tmp/delhi_data/location-273205-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220123.csv.gz to ../../../../tmp/delhi_data/location-273205-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220122.csv.gz to ../../../../tmp/delhi_data/location-273205-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220131.csv.gz to ../../../../tmp/delhi_data/location-273205-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220128.csv.gz to ../../../../tmp/delhi_data/location-273205-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220125.csv.gz to ../../../../tmp/delhi_data/location-273205-20220125.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220127.csv.gz to ../../../../tmp/delhi_data/location-273205-20220127.csv.gz\n\n\nVerify if we got all the sensors data as we needed.\n\n!ls /tmp/delhi_data/location-221227* | wc -l\n!ls /tmp/delhi_data/location-274208* | wc -l\n!ls /tmp/delhi_data/location-273205* | wc -l\n\n27\n10\n15\n\n\n\nsensor_df = pd.read_csv('/tmp/delhi_data/location-274208-20220107.csv.gz')\nsensor_df.parameter.value_counts()\n\npm10     64\npm25     64\npm1      64\num010    64\num025    64\num100    64\nName: parameter, dtype: int64"
  },
  {
    "objectID": "posts/presentation_tips.html",
    "href": "posts/presentation_tips.html",
    "title": "Conference Presentation Tips",
    "section": "",
    "text": "General\n\nFirst page goes like this:\n\nTitle\nAuthors (Underline presenting author, no need to put * in case of equal contribution)\nAffiliations\nConference name\n\nIf importing figures from paper, avoid including the captions.\nInclude lot of images and less maths\nTalk should end with summary and not the future work or thank you slide or something.\nCite the references on the same slide in bottom.\n\nRefer to ‚ÄúGiving talks‚Äù section of this blog.\n\n\nDos and Don‚Äôts\n\nNever put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram."
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html",
    "href": "posts/2023-04-29-sine-combination-netowrks.html",
    "title": "Sine Combination Networks",
    "section": "",
    "text": "We know that any continuous signal can be represented as a sum of sinusoids. The question is, how many sinusoids do we need to represent a signal? In this notebook, we will explore this question.\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "title": "Sine Combination Networks",
    "section": "Random Combination of Sinusoids",
    "text": "Random Combination of Sinusoids\n\nN = 1000\nx = jnp.linspace(-10, 10, N).reshape(-1, 1)\ny = jnp.sin(x) + jnp.sin(2*x) #+ jax.random.normal(jax.random.PRNGKey(0), (N, 1)) * 0.1\nplt.plot(x, y, \"kx\");\nprint(x.shape, y.shape)\n\n(1000, 1) (1000, 1)"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "title": "Sine Combination Networks",
    "section": "Recover the Signal",
    "text": "Recover the Signal\n\ndef get_weights(key):\n    w1 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    key = jax.random.split(key)[0]\n    w2 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    return w1, w2\n    \ndef get_sine(weights, x):\n    w1, w2 = weights\n    return jnp.sin(w1*x) + jnp.sin(w2*x)\n\ndef loss_fn(weights, x, y):\n    output = get_sine(weights, x)\n    w1, w2 = weights\n    return jnp.mean((output.ravel() - y.ravel())**2)\n\n\ndef one_step(weights_and_state, xs):\n    weights, state = weights_and_state\n    loss, grads = value_and_grad_fn(weights, x, y)\n    updates, state = optimizer.update(grads, state)\n    weights = optax.apply_updates(weights, updates)\n    return (weights, state), (loss, weights)\n\nepochs = 1000\noptimizer = optax.adam(1e-2)\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nfig, ax = plt.subplots(4, 3, figsize=(15, 12))\nfig2, ax2 = plt.subplots(4, 3, figsize=(15, 12))\nax = ax.ravel()\nax2 = ax2.ravel()\nfor seed in tqdm(range(12)):\n    key = jax.random.PRNGKey(seed)\n    init_weights = get_weights(key)\n    state = optimizer.init(init_weights)\n    (weights, _), (loss_history, _) = jax.lax.scan(one_step, (init_weights, state), None, length=epochs)\n    y_pred = get_sine(weights, x)\n    ax[seed].plot(x, y, \"kx\")\n    ax[seed].plot(x, y_pred, \"r-\")\n    ax[seed].set_title(f\"w_init=({init_weights[0]:.2f}, {init_weights[1]:.2f}), w_pred=({weights[0]:.2f}, {weights[1]:.2f}), loss={loss_fn(weights, x, y):.2f}\")\n    ax2[seed].plot(loss_history)\nfig.tight_layout()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00&lt;00:00, 15.91it/s]"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "title": "Sine Combination Networks",
    "section": "Plot loss surface",
    "text": "Plot loss surface\n\nw1 = jnp.linspace(0, 3, 100)\nw2 = jnp.linspace(0, 3, 100)\nW1, W2 = jnp.meshgrid(w1, w2)\nloss = jax.vmap(jax.vmap(lambda w1, w2: loss_fn((w1, w2), x, y)))(W1, W2)\n\n# plot the loss surface in 3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(W1, W2, loss, cmap=\"viridis\", alpha=0.9);\nax.set_xlabel(\"w1\");\nax.set_ylabel(\"w2\");\n# top view\nax.view_init(30, 45)"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html",
    "href": "posts/2022-10-27-mogp.html",
    "title": "Multi-Output Gaussian Processes",
    "section": "",
    "text": "Inspired from this GPSS video.\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\n\nimport optax\n\nimport matplotlib.pyplot as plt\nfrom tinygp import kernels"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#helper-functions",
    "href": "posts/2022-10-27-mogp.html#helper-functions",
    "title": "Multi-Output Gaussian Processes",
    "section": "Helper functions",
    "text": "Helper functions\n\ndef random_fill(key, params):\n    values, unravel_fn = ravel_pytree(params)\n    random_values = jax.random.normal(key, shape=values.shape)\n    return unravel_fn(random_values)\n\ndef get_real_params(params):\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = params[f'a{i}'].reshape(n_outputs, rank)\n    if method == 'icm':\n        params['var'] = jnp.exp(params['log_var'])\n        params['scale'] = jnp.exp(params['log_scale'])\n        params['noise'] = jnp.exp(params['log_noise'])\n    elif method == 'lmc':\n        for i in range(1, q_len+1):\n            params[f'var{i}'] = jnp.exp(params[f'log_var{i}'])\n            params[f'scale{i}'] = jnp.exp(params[f'log_scale{i}'])\n            params[f'noise{i}'] = jnp.exp(params[f'log_noise{i}'])\n    return params\n\ndef kron_cov_fn(params, x1, x2, add_noise=False):\n    params = get_real_params(params)\n    a_list = [params[f'a{i}'] for i in range(1, q_len+1)]\n\n    if method == 'icm':\n        kernel_fn = params['var'] * kernels.ExpSquared(scale=params['scale'])\n        cov = kernel_fn(x1, x2)\n        if add_noise:\n            cov = cov + jnp.eye(cov.shape[0])*params['noise']\n\n        B = jax.tree_util.tree_reduce(lambda x1, x2: x1@x1.T+x2@x2.T, a_list)\n#         print(B.shape, cov.shape)\n        return jnp.kron(B, cov)\n\n    elif method == 'lmc':\n        cov_list = []\n        for idx in range(1, q_len+1):\n            kernel_fn = params[f'var{idx}'] * kernels.ExpSquared(scale=params[f'scale{idx}'])\n            cov = kernel_fn(x1, x2)\n            if add_noise:\n                cov = cov + jnp.eye(cov.shape[0])*params[f'noise{idx}']\n\n            B = a_list[idx-1]@a_list[idx-1].T\n            cov_list.append(jnp.kron(B, cov))\n            \n        return jax.tree_util.tree_reduce(lambda x1, x2: x1+x2, cov_list)"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#configuration",
    "href": "posts/2022-10-27-mogp.html#configuration",
    "title": "Multi-Output Gaussian Processes",
    "section": "Configuration",
    "text": "Configuration\n\nq_len = 2\nrank = 2 # if 1, slfm\nn_outputs = 2\n\nmethod = 'lmc' # lmc, icm\n\nif rank = 1, lmc becomes slfm.\n\nGenerative process\n\nx_key = jax.random.PRNGKey(4)\n\nx = jax.random.uniform(x_key, shape=(40, 1)).sort(axis=0)\nx_test = jnp.linspace(0,1,100).reshape(-1, 1)\n\ne1_key, e2_key = jax.random.split(x_key)\n\ne1 = jax.random.normal(e1_key, shape=(x.shape[0],))\ne2 = jax.random.normal(e2_key, shape=(x.shape[0],))\n\nif method == 'icm':\n    noise = 0.01\n    gen_kernel = 1.2*kernels.ExpSquared(scale=0.2)\n    gen_covariance = gen_kernel(x, x) + jnp.eye(x.shape[0])*noise\n    gen_chol = jnp.linalg.cholesky(gen_covariance)\n    \n    y1 = gen_chol@e1\n    y2 = gen_chol@e2\n\n    y = jnp.concatenate([y1, y2])\n    \nelif method == 'lmc':\n    noise1 = 0.01\n    noise2 = 0.1\n    gen_kernel1 = 1.2*kernels.ExpSquared(scale=0.1)\n    gen_covariance1 = gen_kernel1(x, x) + jnp.eye(x.shape[0])*noise1\n    gen_chol1 = jnp.linalg.cholesky(gen_covariance1)\n\n    gen_kernel2 = 0.8*kernels.ExpSquared(scale=0.2)\n    gen_covariance2 = gen_kernel2(x, x) + jnp.eye(x.shape[0])*noise2\n    gen_chol2 = jnp.linalg.cholesky(gen_covariance2)\n    \n    y1 = gen_chol1@e1\n    y2 = gen_chol2@e2\n\n    y = jnp.concatenate([y1, y2])\n    \n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.legend();\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\n\n\n\n\ndef loss_fn(params):\n    mo_cov = kron_cov_fn(params, x, x, add_noise=True)\n#     print(y.shape, mo_cov.shape)\n    return -jax.scipy.stats.multivariate_normal.logpdf(y, jnp.zeros_like(y), mo_cov)\n\n\nkey = jax.random.PRNGKey(1)\nif method == 'icm':\n    params = {'log_var':0.0, 'log_scale':0.0, 'log_noise':0.0}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\nelif method == 'lmc':\n    params = {}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\n        params[f'log_var{i}'] = 0.0\n        params[f'log_scale{i}'] = 0.0\n        params[f'log_noise{i}'] = 0.0\n\nparams = random_fill(key, params)\nparams\n\n{'a1': DeviceArray([[-0.764527 ,  1.0286916],\n              [-1.0690447, -0.7921495]], dtype=float32),\n 'a2': DeviceArray([[ 0.8845895, -1.1941622],\n              [-1.7434924,  1.5159688]], dtype=float32),\n 'log_noise1': DeviceArray(-1.1254696, dtype=float32),\n 'log_noise2': DeviceArray(-0.22446911, dtype=float32),\n 'log_scale1': DeviceArray(0.39719132, dtype=float32),\n 'log_scale2': DeviceArray(-0.22453257, dtype=float32),\n 'log_var1': DeviceArray(-0.7590596, dtype=float32),\n 'log_var2': DeviceArray(-0.08601531, dtype=float32)}\n\n\n\nloss_fn(params)\n\nDeviceArray(116.04026, dtype=float32)\n\n\n\nkey = jax.random.PRNGKey(3)\nparams = random_fill(key, params)\n\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), (params, loss)\n\n(tuned_params, state), (params_history, loss_history) = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(loss_history);\n\n\n\n\n\n\n\n\n\ndef predict_fn(params, x_test):\n    cov = kron_cov_fn(params, x, x, add_noise=True)\n    test_cov = kron_cov_fn(params, x_test, x_test, add_noise=True)\n    cross_cov = kron_cov_fn(params, x_test, x, add_noise=False)\n    \n    chol = jnp.linalg.cholesky(cov)\n    k_inv_y = jax.scipy.linalg.cho_solve((chol, True), y)\n    k_inv_cross_cov = jax.scipy.linalg.cho_solve((chol, True), cross_cov.T)\n\n    pred_mean = cross_cov@k_inv_y\n    pred_cov = test_cov - cross_cov@k_inv_cross_cov\n    return pred_mean, pred_cov\n\n\npred_mean, pred_cov = predict_fn(tuned_params, x_test)\npred_conf = 2 * jnp.diag(pred_cov)**0.5\n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.plot(x_test, pred_mean[:x_test.shape[0]], label='pred_y1')\nplt.plot(x_test, pred_mean[x_test.shape[0]:], label='pred_y2')\nplt.fill_between(x_test.ravel(), pred_mean[:x_test.shape[0]] - pred_conf[:x_test.shape[0]], pred_mean[:x_test.shape[0]] + pred_conf[:x_test.shape[0]], label='pred_conf_y1', alpha=0.3)\nplt.fill_between(x_test.ravel(), pred_mean[x_test.shape[0]:] - pred_conf[x_test.shape[0]:], pred_mean[x_test.shape[0]:] + pred_conf[x_test.shape[0]:], label='pred_conf_y2', alpha=0.3)\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\nfor name, value in get_real_params(tuned_params).items():\n    if not name.startswith('log_'):\n        print(name, value)\n\na1 [[0.03664799 0.00039898]\n [0.3191718  0.00344488]]\na2 [[ 0.1351072   0.00248941]\n [-0.05392759 -0.04239884]]\nnoise1 0.6797133\nnoise2 0.4154678\nscale1 5.048228\nscale2 0.10743636\nvar1 0.016275918\nvar2 41.034225"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html",
    "href": "posts/bayesian-gaussian-basis-regression.html",
    "title": "Bayesian Basis Regression",
    "section": "",
    "text": "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport pandas as pd\nimport regdata as rd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\"\nrd.set_backend(\"torch\")"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "href": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "title": "Bayesian Basis Regression",
    "section": "Generate data",
    "text": "Generate data\n\n# x = torch.linspace(-1, 1, 100)\n# y = (torch.sin(x * 2 * torch.pi) + torch.randn(x.size()) * 0.1).unsqueeze(1)\nx, y, _ = rd.MotorcycleHelmet().get_data()\nx = x.ravel().to(torch.float32)\nidx = np.argsort(x)\nx = x[idx]\ny = y.to(torch.float32)\ny = y[idx]\n\nx = torch.vstack([torch.ones_like(x), x]).T\nprint(x.shape, y.shape)\nx = x.to(device)\ny = y.to(device)\nprint(x.dtype, y.dtype)\n\nplt.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n\ntorch.Size([94, 2]) torch.Size([94])\ntorch.float32 torch.float32\n\n\n\n\n\n\n\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, neurons, transform=None):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.transform = transform\n        if transform is None:\n            self.transform = lambda x: x\n            self.layers.append(nn.Linear(in_dim, neurons[0]))\n        else:\n            self.layers.append(nn.Linear(self.transform.n_grid + 1, neurons[0]))\n        for i in range(1, len(neurons)):\n            self.layers.append(nn.Linear(neurons[i - 1], neurons[i]))\n        self.layers.append(nn.Linear(neurons[-1], out_dim))\n\n    def forward(self, x):\n        x = self.transform(x)\n        # print(x.shape)\n        for layer in self.layers[:-1]:\n            x = F.gelu(layer(x))\n        return self.layers[-1](x)\n\n\nclass RBF(nn.Module):\n    def __init__(self, log_gauss_var, n_grid):\n        super().__init__()\n        self.log_gauss_var = nn.Parameter(torch.tensor(log_gauss_var))\n        self.n_grid = n_grid\n        self.grid = nn.Parameter(torch.linspace(-1, 1, n_grid))\n        self.register_buffer(\"bias\", torch.zeros(1))\n\n    def forward(self, x):\n        self.dist = dist.Normal(self.grid, torch.exp(self.log_gauss_var))\n        features = torch.exp(self.dist.log_prob(x[:, 1:2]))\n        # print(features.shape)\n        features = torch.cat(\n            [\n                torch.ones_like(self.bias.repeat(features.shape[0])).reshape(-1, 1),\n                features,\n            ],\n            dim=1,\n        )\n        return features\n\n\nRBF(0.0, 10).to(device)(x).shape\n\ntorch.Size([94, 11])\n\n\n\n# def transform_fn(x):\n#     all_x = []\n#     for i in range(2, 11):\n#         all_x.append(x[:, 1:2] ** i)\n#     return torch.hstack([x] + all_x)\n\n\ndef get_mn_sn(x, s0):\n    x = transform_fn(x)\n    sn_inv = (x.T @ x) / torch.exp(log_var_noise)\n    diag = sn_inv.diagonal()\n    diag += 1 / s0\n    sn = torch.inverse(sn_inv)\n    mn = sn @ ((x.T @ y) / torch.exp(log_var_noise))\n    return mn, sn\n\n\ndef neg_log_likelihood(x, y, m0, s0):\n    x = transform_fn(x)\n    cov = (x @ x.T) / s0\n    diag = cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    return (\n        -dist.MultivariateNormal(m0.repeat(y.shape[0]), cov).log_prob(y.ravel()).sum()\n    )\n\n\ndef get_pred_post(sn, mn, x):\n    x = transform_fn(x)\n    pred_cov = x @ sn @ x.T\n    diag = pred_cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    pred_mean = x @ mn\n    return pred_mean, pred_cov\n\n\ndef plot_preds_and_95(ax, x, pred_mean, pred_cov):\n    with torch.no_grad():\n        x = x[:, 1].cpu().numpy()\n        pred_mean = pred_mean.ravel().cpu().numpy()\n        pred_var = pred_cov.diagonal().cpu().numpy()\n        ax.plot(x, pred_mean, color=\"red\", label=\"mean\")\n        ax.fill_between(\n            x,\n            (pred_mean - 2 * np.sqrt(pred_var)),\n            (pred_mean + 2 * np.sqrt(pred_var)),\n            color=\"red\",\n            alpha=0.2,\n            label=\"95% CI\",\n        )\n        return ax\n\n\nmlp = MLP(2, 1, [256, 256, 256]).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = True\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 30.6285: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02&lt;00:00, 209.49it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    # ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\ntorch.exp(log_var_noise), s0, m0\n\n(tensor(0.1191, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;),\n tensor(1.3897, device='cuda:0', requires_grad=True),\n tensor([-0.0693], device='cuda:0', requires_grad=True))"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "href": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "title": "Bayesian Basis Regression",
    "section": "Add Gaussian transform",
    "text": "Add Gaussian transform\n\nmlp = MLP(2, 1, [256, 256, 256], transform=RBF(0.1, 10)).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: -29.9227: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:03&lt;00:00, 156.90it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "href": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "title": "Bayesian Basis Regression",
    "section": "Just Gaussian basis",
    "text": "Just Gaussian basis\n\n# mlp = MLP(2, 1, [32, 32, 32], transform=RBF(0.1, 10)).to(device)\nmlp = RBF(1.0, 5).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.001)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 207.0843: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02&lt;00:00, 195.61it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "href": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "title": "Bayesian Basis Regression",
    "section": "Appendix",
    "text": "Appendix\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\ndata = pd.read_csv(\"~/datasets/uci/bike/hour.csv\", header=None).iloc[:, 1:]\ndata.shape\n\n(17379, 18)\n\n\n\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nX_train = x_scaler.fit_transform(X_train)\ny_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\nX_test = x_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test.reshape(-1, 1))\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((10427, 17), (6952, 17), (10427, 1), (6952, 1))\n\n\n\n[X_train, X_test, y_train, y_test] = map(\n    lambda x: torch.tensor(x, dtype=torch.float32).to(device),\n    [X_train, X_test, y_train, y_test],\n)\n\n\nmlp = MLP(17, 1, [10, 10]).to(device)\n\noptimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = F.mse_loss(mlp(X_train), y_train)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0040: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01&lt;00:00, 482.25it/s]\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_pred = mlp(X_test).cpu().numpy()\n    if isinstance(y_test, torch.Tensor):\n        y_test = y_test.cpu().numpy()\n    print(y_pred.shape, y_test.shape)\n    print(\"RMSE\", mean_squared_error(y_test, y_pred, squared=False))\n\n(6952, 1) (6952, 1)\nRMSE 0.08354535"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html",
    "href": "posts/pruning_vs_uncertainty.html",
    "title": "Pruning vs Uncertainty",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\n# import pruning library\nimport torch.nn.utils.prune as prune\n\n# import torchvision\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n\ntry:\n    from laplace import Laplace\nexcept ModuleNotFoundError:\n    %pip install laplace-torch\n    from laplace import Laplace\n\n&lt;frozen importlib._bootstrap&gt;:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "href": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "title": "Pruning vs Uncertainty",
    "section": "Train a model on MNIST",
    "text": "Train a model on MNIST\n\n# Define data transformations\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n        # convert dtype to float32\n        # transforms.Lambda(lambda x: x.to(torch.float32)),\n    ]\n)\n\n\n# Load MNIST dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, transform=transform, download=True\n)\nprint(\"Train size\", len(train_dataset))\n\ntrain_dataset = TensorDataset(\n    train_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    train_dataset.targets.to(device),\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\", train=False, transform=transform, download=True\n)\nprint(\"Test size\", len(test_dataset))\ntest_dataset = TensorDataset(\n    test_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    test_dataset.targets.to(device),\n)\n\nUsing cuda device\nTrain size 60000\nTest size 10000\n\n\n\ntrain_dataset[0][0].dtype, train_dataset[0][1].dtype\n\n(torch.float32, torch.int64)\n\n\n\n# Define data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# Load pre-trained ResNet model\nresnet = torchvision.models.resnet18(pretrained=True)\nprint(\"Loaded pre-trained ResNet18 model\")\nprint(resnet.fc.in_features)\n\n# Modify the last fully connected layer to match MNIST's number of classes (10)\nnum_classes = 10\nresnet.fc = nn.Sequential(\n    nn.Linear(resnet.fc.in_features, resnet.fc.in_features),\n    nn.GELU(),\n    nn.Linear(resnet.fc.in_features, num_classes),\n)\n\n# Freeze all layers except the last fully connected layer\nfor name, param in resnet.named_parameters():\n    param.requires_grad = False\nresnet.fc.requires_grad_(True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n\n# Training loop\nnum_epochs = 50\nprint(f\"Training on device {device}\")\nresnet.to(device)\n\nprint(\"Training ResNet18 model\")\nfor epoch in range(num_epochs):\n    resnet.train()\n    epoch_loss = 0.0\n    for images, labels in tqdm(train_loader):\n        optimizer.zero_grad()\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n\n    # Evaluation\n    resnet.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        predicted_list = []\n        for images, labels in test_loader:\n            outputs = resnet(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nLoaded pre-trained ResNet18 model\n512\nTraining on device cuda\nTraining ResNet18 model\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 242.75it/s]\n\n\nEpoch [1/50] Loss: 1.0877\nAccuracy on the test set: 75.42%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 262.53it/s]\n\n\nEpoch [2/50] Loss: 0.8051\nAccuracy on the test set: 76.74%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 270.43it/s]\n\n\nEpoch [3/50] Loss: 0.7578\nAccuracy on the test set: 78.27%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 265.38it/s]\n\n\nEpoch [4/50] Loss: 0.7290\nAccuracy on the test set: 78.71%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 265.51it/s]\n\n\nEpoch [5/50] Loss: 0.7083\nAccuracy on the test set: 79.62%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 266.62it/s]\n\n\nEpoch [6/50] Loss: 0.6761\nAccuracy on the test set: 79.82%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 268.49it/s]\n\n\nEpoch [7/50] Loss: 0.6627\nAccuracy on the test set: 80.47%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 266.33it/s]\n\n\nEpoch [8/50] Loss: 0.6423\nAccuracy on the test set: 80.24%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 268.52it/s]\n\n\nEpoch [9/50] Loss: 0.6257\nAccuracy on the test set: 81.11%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 269.38it/s]\n\n\nEpoch [10/50] Loss: 0.6131\nAccuracy on the test set: 81.42%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 264.77it/s]\n\n\nEpoch [11/50] Loss: 0.5911\nAccuracy on the test set: 82.02%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 266.07it/s]\n\n\nEpoch [12/50] Loss: 0.5765\nAccuracy on the test set: 82.32%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 262.19it/s]\n\n\nEpoch [13/50] Loss: 0.5611\nAccuracy on the test set: 82.30%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 214.62it/s]\n\n\nEpoch [14/50] Loss: 0.5466\nAccuracy on the test set: 82.49%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 219.31it/s]\n\n\nEpoch [15/50] Loss: 0.5358\nAccuracy on the test set: 82.81%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 226.53it/s]\n\n\nEpoch [16/50] Loss: 0.5266\nAccuracy on the test set: 83.30%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:05&lt;00:00, 171.25it/s]\n\n\nEpoch [17/50] Loss: 0.5137\nAccuracy on the test set: 83.37%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 278.59it/s]\n\n\nEpoch [18/50] Loss: 0.5051\nAccuracy on the test set: 83.17%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 248.82it/s]\n\n\nEpoch [19/50] Loss: 0.4969\nAccuracy on the test set: 83.46%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:05&lt;00:00, 175.56it/s]\n\n\nEpoch [20/50] Loss: 0.4811\nAccuracy on the test set: 83.76%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 277.18it/s]\n\n\nEpoch [21/50] Loss: 0.4714\nAccuracy on the test set: 83.57%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 273.71it/s]\n\n\nEpoch [22/50] Loss: 0.4624\nAccuracy on the test set: 84.25%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 242.18it/s]\n\n\nEpoch [23/50] Loss: 0.4553\nAccuracy on the test set: 84.27%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 279.42it/s]\n\n\nEpoch [24/50] Loss: 0.4506\nAccuracy on the test set: 84.62%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 269.21it/s]\n\n\nEpoch [25/50] Loss: 0.4394\nAccuracy on the test set: 83.97%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 227.36it/s]\n\n\nEpoch [26/50] Loss: 0.4346\nAccuracy on the test set: 84.16%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 222.91it/s]\n\n\nEpoch [27/50] Loss: 0.4271\nAccuracy on the test set: 84.38%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 223.68it/s]\n\n\nEpoch [28/50] Loss: 0.4193\nAccuracy on the test set: 84.84%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 261.50it/s]\n\n\nEpoch [29/50] Loss: 0.4148\nAccuracy on the test set: 85.05%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 246.52it/s]\n\n\nEpoch [30/50] Loss: 0.4040\nAccuracy on the test set: 84.49%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 281.60it/s]\n\n\nEpoch [31/50] Loss: 0.3990\nAccuracy on the test set: 84.59%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 278.41it/s]\n\n\nEpoch [32/50] Loss: 0.4016\nAccuracy on the test set: 84.92%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 275.60it/s]\n\n\nEpoch [33/50] Loss: 0.3979\nAccuracy on the test set: 85.01%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 250.04it/s]\n\n\nEpoch [34/50] Loss: 0.3844\nAccuracy on the test set: 84.82%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 280.53it/s]\n\n\nEpoch [35/50] Loss: 0.3789\nAccuracy on the test set: 85.49%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 279.26it/s]\n\n\nEpoch [36/50] Loss: 0.3760\nAccuracy on the test set: 85.26%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 207.71it/s]\n\n\nEpoch [37/50] Loss: 0.3733\nAccuracy on the test set: 85.36%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 265.92it/s]\n\n\nEpoch [38/50] Loss: 0.3655\nAccuracy on the test set: 84.98%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 279.79it/s]\n\n\nEpoch [39/50] Loss: 0.3627\nAccuracy on the test set: 85.19%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 276.73it/s]\n\n\nEpoch [40/50] Loss: 0.3517\nAccuracy on the test set: 84.78%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 278.32it/s]\n\n\nEpoch [41/50] Loss: 0.3526\nAccuracy on the test set: 85.43%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 243.70it/s]\n\n\nEpoch [42/50] Loss: 0.3523\nAccuracy on the test set: 85.55%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 240.48it/s]\n\n\nEpoch [43/50] Loss: 0.3457\nAccuracy on the test set: 85.02%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 274.70it/s]\n\n\nEpoch [44/50] Loss: 0.3447\nAccuracy on the test set: 85.20%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 276.08it/s]\n\n\nEpoch [45/50] Loss: 0.3411\nAccuracy on the test set: 85.47%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:04&lt;00:00, 215.18it/s]\n\n\nEpoch [46/50] Loss: 0.3312\nAccuracy on the test set: 85.55%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 244.20it/s]\n\n\nEpoch [47/50] Loss: 0.3290\nAccuracy on the test set: 85.52%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 267.56it/s]\n\n\nEpoch [48/50] Loss: 0.3277\nAccuracy on the test set: 85.35%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 267.91it/s]\n\n\nEpoch [49/50] Loss: 0.3241\nAccuracy on the test set: 85.80%\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [00:03&lt;00:00, 266.04it/s]\n\n\nEpoch [50/50] Loss: 0.3217\nAccuracy on the test set: 84.93%\n\n\n\n# Evaluation\nresnet.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    predicted_list = []\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        softmax_outputs = nn.Softmax(dim=1)(outputs)\n        predicted_list.append(softmax_outputs.data.cpu().numpy())\n\nall_predicted = np.concatenate(predicted_list, axis=0)\nprint(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\nAccuracy on the test set: 84.93%"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#check-calibration",
    "href": "posts/pruning_vs_uncertainty.html#check-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Check calibration",
    "text": "Check calibration\n\ntest_dataset.tensors[1].cpu().numpy().shape, all_predicted.shape\n\n((10000,), (10000, 10))\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = all_predicted[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    # ece_score = compute_ece(predicted_probabilities, true_labels, num_bins=10)\n    # print(f\"Expected Calibration Error (ECE): {ece_score:.4f}\")\n\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\n# Compute expected calibration error (ECE)\nece = compute_ece(all_predicted, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.021885250088572478\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(\n        test_dataset.tensors[1].cpu().numpy(), all_predicted.argmax(axis=1)\n    )\n)\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90       980\n           1       0.91      0.97      0.94      1135\n           2       0.75      0.72      0.73      1032\n           3       0.77      0.74      0.75      1010\n           4       0.82      0.88      0.85       982\n           5       0.68      0.70      0.69       892\n           6       0.85      0.84      0.85       958\n           7       0.80      0.79      0.79      1028\n           8       0.76      0.75      0.75       974\n           9       0.81      0.74      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000\n\n\n\n\ndef compute_ece(predicted_probs, true_labels, num_bins=10):\n    # Ensure predicted_probs is a NumPy array\n    predicted_probs = np.array(predicted_probs)\n    true_labels = np.array(true_labels)\n\n    # Calculate predicted class labels\n    predicted_labels = np.argmax(predicted_probs, axis=1)\n\n    # Calculate confidence scores (maximum predicted probability)\n    confidence_scores = np.max(predicted_probs, axis=1)\n\n    # Create bins for confidence scores\n    bin_edges = np.linspace(0, 1, num_bins + 1)\n\n    ece = 0.0\n    total_samples = len(true_labels)\n\n    for bin_idx in range(num_bins):\n        # Find examples whose confidence scores fall into the current bin\n        bin_mask = (confidence_scores &gt;= bin_edges[bin_idx]) & (\n            confidence_scores &lt; bin_edges[bin_idx + 1]\n        )\n\n        if np.any(bin_mask):\n            # Calculate the accuracy of predictions in this bin\n            bin_accuracy = np.mean(predicted_labels[bin_mask] == true_labels[bin_mask])\n\n            # Calculate the fraction of examples in this bin\n            bin_fraction = np.sum(bin_mask) / total_samples\n\n            # Calculate the calibration error in this bin\n            bin_error = np.abs(bin_accuracy - np.mean(confidence_scores[bin_mask]))\n\n            # Weighted contribution to ECE\n            ece += bin_fraction * bin_error\n\n    return ece"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "href": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Does MC-dropout help with calibration?",
    "text": "Does MC-dropout help with calibration?"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "href": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "title": "Pruning vs Uncertainty",
    "section": "Last layer only",
    "text": "Last layer only\n\nclass MCDropout(nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n        self.dropout = nn.Dropout(p=self.p)\n\n    def forward(self, x):\n        self.train()\n        return self.dropout(x)\n\n\nresnet_with_dropout = torchvision.models.resnet18(pretrained=True)\nresnet_with_dropout.fc = nn.Sequential(\n    nn.Linear(\n        resnet_with_dropout.fc.in_features, resnet_with_dropout.fc.in_features // 2\n    ),\n    nn.GELU(),\n    MCDropout(p=0.33),\n    nn.Linear(resnet_with_dropout.fc.in_features // 2, num_classes),\n)\n\nresnet_with_dropout.load_state_dict(resnet.state_dict())\n\nresnet_with_dropout.to(device)\n\nmc_samples = 1000\n\noutputs = []\nfor _ in tqdm(range(mc_samples)):\n    output = resnet_with_dropout(test_dataset.tensors[0])\n    softmax_output = nn.Softmax(dim=1)(output)\n    outputs.append(softmax_output.data.cpu().numpy())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:18&lt;00:00, 55.50it/s]\n\n\n\nmc_mean = np.mean(outputs, axis=0)\nmc_std = np.std(outputs, axis=0)\nmc_mean.shape\n\n(10000, 10)\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = mc_mean[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\nece = compute_ece(mc_mean, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.04250686831623317\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(test_dataset.tensors[1].cpu().numpy(), mc_mean.argmax(axis=1))\n)\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.92      0.90       980\n           1       0.91      0.98      0.94      1135\n           2       0.73      0.71      0.72      1032\n           3       0.78      0.72      0.75      1010\n           4       0.83      0.87      0.85       982\n           5       0.68      0.71      0.69       892\n           6       0.82      0.88      0.85       958\n           7       0.80      0.78      0.79      1028\n           8       0.77      0.74      0.75       974\n           9       0.82      0.72      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000"
  },
  {
    "objectID": "posts/torch-tips.html",
    "href": "posts/torch-tips.html",
    "title": "PyTorch Tips",
    "section": "",
    "text": "Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code.\n\nAll the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy.\n.cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable.\nDo not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead.\nNeed to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f\"name_{zero}\"). They can be accessed with model.name_0.\nHave something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer.\nLet .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as:\n\nmodule.to(deivce) sends all parameters and buffers of model/submodules to the device.\n\nmodule.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively.\nLet .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions.\ntorch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code.\nLink the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are:\n\nsetting module.train() or module.eval() puts all submodules in train mode or eval mode respectively.\nAll submodules parameters can be accesses directly from the parent module with module.parameters().\n\nCreating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html",
    "href": "posts/2022-10-31-stochastic-variational-gp.html",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "",
    "text": "I recently read a compact and clean explanation of SVGP in the following blog post by Dr.¬†Martin Ingram:\nNow, I am attempting to implement a practical code from scratch for the same (What is practical about it? Sometimes math does not simply translate to code without careful modifications). I am assuming that you have read the blog post cited above before moving further. Let‚Äôs go for coding!"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Imports",
    "text": "Imports\n\n# JAX\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\n# Partially initialize functions\nfrom functools import partial\n\n# TFP\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\n# GP Kernels\nfrom tinygp import kernels\n\n# sklearn\nfrom sklearn.datasets import make_moons, make_blobs, make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Optimization\nimport optax\n\n# Plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['scatter.edgecolors'] = \"k\"\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Jitter\nJITTER = 1e-6\n\n# Enable JAX 64bit\njax.config.update(\"jax_enable_x64\", True)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Dataset",
    "text": "Dataset\nFor this blog post, we will stick to the classification problem and pick a reasonable classification dataset.\n\nn_samples = 100\nnoise = 0.1\nrandom_state = 0\nshuffle = True\n\nX, y = make_moons(\n    n_samples=n_samples, random_state=random_state, noise=noise, shuffle=shuffle\n)\nX = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n\nX, y = map(jnp.array, (X, y))\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Methodology",
    "text": "Methodology\nTo define a GP, we need a kernel function. Let us use the RBF or Exponentiated Quadratic or Squared Exponential kernel.\n\nlengthscale = 1.0\nvariance = 1.0\n\nkernel_fn = variance * kernels.ExpSquared(scale=lengthscale)\n\nkernel_fn(X, X).shape\n\n(100, 100)\n\n\nAs explained in the blog post, we want to minimize the following loss function:\n\\[\nKL[q(u|\\eta) || p(u|y, \\theta)] = KL[q(u|\\eta) || p(u | \\theta)] - \\mathbb{E}_{u \\sim q(u|\\eta)} \\log p(y | u, \\theta) + const\n\\]\nLet us break down the loss and discuss each componant.\n\nKL divergence\nIn the first term, we want to compute the KL divergence between prior and variational distribution of GP at inducing points. First, we need to define the inducing points.\n\nkey = jax.random.PRNGKey(0)\nn_inducing = 10\nn_dim = X.shape[1]\n\nX_inducing = jax.random.normal(key, shape=(n_inducing, n_dim))\nX_inducing.shape\n\n(10, 2)\n\n\nNow, defining the prior and variational distributions.\n\ngp_mean = 0.43  # a scalar parameter to train\n\nprior_mean = gp_mean * jnp.zeros(n_inducing)\nprior_cov = kernel_fn(X_inducing, X_inducing)\n\nprior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n\nvariational_mean = jax.random.uniform(key, shape=(n_inducing,)) # a vector parameter to train\n\nA covariance matrix can not be learned directly due to positive definite constraint. We can decompose a covariance matrix in a following way:\n\\[\n\\begin{aligned}\nK &= diag(\\boldsymbol{\\sigma})\\Sigma diag(\\boldsymbol{\\sigma})\\\\\n  &= diag(\\boldsymbol{\\sigma})LL^T diag(\\boldsymbol{\\sigma})\n\\end{aligned}\n\\]\nWhere, \\(\\Sigma\\) is a correlation matrix, \\(L\\) is a lower triangular cholesky decomposition of \\(\\Sigma\\) and \\(\\boldsymbol{\\sigma}\\) is the variance vector. We can use tfb.CorrelationCholesky to generate \\(L\\) from an unconstrained vector:\n\nrandom_vector = jax.random.normal(key, shape=(3,))\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\ncorrelation = corr_chol@corr_chol.T\ncorrelation\n\nDeviceArray([[ 1.        ,  0.54464529, -0.7835968 ],\n             [ 0.54464529,  1.        , -0.33059078],\n             [-0.7835968 , -0.33059078,  1.        ]], dtype=float64)\n\n\nTo constrain \\(\\boldsymbol{\\sigma}\\), any positivity constraint would suffice. So, combining these tricks, we can model the covariance as following:\n\nrandom_vector = jax.random.normal(\n    key, shape=(n_inducing * (n_inducing - 1) // 2,)\n)  # a trainable parameter\nlog_sigma = jax.random.normal(key, shape=(n_inducing, 1))  # a trainable parameter\n\n\nsigma = jnp.exp(log_sigma)\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\nvariational_cov = sigma * sigma.T * (corr_chol @ corr_chol.T)\nprint(variational_cov.shape)\n\nvariational_distribution = tfd.MultivariateNormalFullCovariance(variational_mean, variational_cov\n)\n\n(10, 10)\n\n\nNow, we can compute the KL divergence:\n\nvariational_distribution.kl_divergence(prior_distribution)\n\nDeviceArray(416.89357355, dtype=float64)\n\n\n\n\nExpectation over the likelihood\nWe want to compute the following expectation:\n\\[\n-\\sum_{i=1}^N \\mathbb{E}_{f_i \\sim q(f_i | \\eta, \\theta)} \\log p(y_i| f_i, \\theta)\n\\]\nNote that, \\(p(y_i| f_i, \\theta)\\) can be any likelihood depending upon the problem, but for classification, we may use a Bernoulli likelihood.\n\nf = jax.random.normal(key, shape=y.shape)\nlikelihood_distribution = tfd.Bernoulli(logits=f)\n\nlog_likelihood = likelihood_distribution.log_prob(y).sum()\nlog_likelihood\n\nDeviceArray(-72.04665624, dtype=float64)\n\n\nWe need to sample \\(f_i\\) from \\(q(f_i | \\eta, \\theta)\\) which has the following form:\n\\[\n\\begin{aligned}\nq(u) &\\sim \\mathcal{N}(\\boldsymbol{m}, S)\\\\\nq(f_i | \\eta, \\theta) &\\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\\\\n\\mu_i &= A\\boldsymbol{m}\\\\\n\\sigma_i^2 &= K_{ii} + A(S - K_{mm})A^T\\\\\nA &= K_{im}K_{mm}^{-1}\n\\end{aligned}\n\\]\nNote that matrix inversion is often unstable with jnp.linalg.inv and thus we will use cholesky tricks to compute \\(A\\).\n\ndef q_f(x_i):\n    x_i = x_i.reshape(1, -1) # ensure correct shape\n    K_im = kernel_fn(x_i, X_inducing)\n    K_mm = kernel_fn(X_inducing, X_inducing)\n    chol_mm = jnp.linalg.cholesky(K_mm + jnp.eye(K_mm.shape[0])*JITTER)\n    A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n    \n    mu_i = A@variational_mean\n    sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_cov - prior_cov)@A.T\n    \n    return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n\nHere is a function to compute log likelihood for a single data-point:\n\ndef log_likelihood(x_i, y_i, seed):\n    sample = q_f(x_i).sample(seed=seed)\n    log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n    return log_likelihood.squeeze()\n\n\nlog_likelihood(X[0], y[0], seed=key)\n\nDeviceArray(-0.17831203, dtype=float64)\n\n\nWe can use jax.vmap to compute log_likelihood over a batch. With that, we can leverage the stochastic variational inference following section 10.3.1 (Eq. 10.108) from pml book2. Basically, in each iteration, we need to multiply the batch log likelihood with \\(\\frac{N}{B}\\) to get an unbiased minibatch approximation where \\(N\\) is size of the full dataset and \\(B\\) is the batch size.\n\nbatch_size = 10\n\nseeds = jax.random.split(key, num=batch_size)\n\nll = len(y)/batch_size * jax.vmap(log_likelihood)(X[:batch_size], y[:batch_size], seeds).sum()\nll\n\nDeviceArray(-215.46520331, dtype=float64)\n\n\nNote that, once the parameters are optimized, we can use the derivations of \\(q(f_i | \\eta, \\theta)\\) to compute the posterior distribution. We have figured out all the pieces by now so it is the time to put it togather in a single class. Some pointers to note are the following:\n\nWe define a single function get_constrained_params to transform all unconstrained parameters.\njax.lax.scan gives a huge boost to a training loop.\nThere is some repeatation of code due to lack of super code optimization. You can do it at your end if needed."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "All in one",
    "text": "All in one\n\nclass SVGP:\n    def __init__(self, X_inducing, data_size):\n        self.X_inducing = X_inducing\n        self.n_inducing = len(X_inducing)\n        self.data_size = data_size\n        \n    def init_params(self, seed):\n        variational_corr_chol_param = tfb.CorrelationCholesky().inverse(jnp.eye(self.n_inducing))\n        \n        dummy_params = {\"log_variance\": jnp.zeros(()),\n               \"log_scale\": jnp.zeros(()), \n               \"mean\": jnp.zeros(()),\n               \"X_inducing\": self.X_inducing,\n               \"variational_mean\": jnp.zeros(self.n_inducing),\n               \"variational_corr_chol_param\": variational_corr_chol_param,\n               \"log_variational_sigma\": jnp.zeros((self.n_inducing, 1)),\n               }\n        \n        flat_params, unravel_fn = ravel_pytree(dummy_params)\n        random_params = jax.random.normal(key, shape=(len(flat_params), ))\n        params = unravel_fn(random_params)\n        return params\n    \n    @staticmethod\n    def get_constrained_params(params):\n        return {\"mean\": params[\"mean\"],\n                \"variance\": jnp.exp(params['log_variance']), \n                \"scale\": jnp.exp(params['log_scale']), \n                \"X_inducing\": params[\"X_inducing\"],\n                \"variational_mean\": params[\"variational_mean\"],\n                \"variational_corr_chol_param\": params[\"variational_corr_chol_param\"],\n                \"variational_sigma\": jnp.exp(params[\"log_variational_sigma\"])}\n    \n    @staticmethod\n    def get_q_f(params, x_i, prior_distribution, variational_distribution):\n        x_i = x_i.reshape(1, -1) # ensure correct shape\n        \n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        K_im = kernel_fn(x_i, params[\"X_inducing\"])\n        K_mm = prior_distribution.covariance()\n        chol_mm = jnp.linalg.cholesky(K_mm)\n        A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n\n        mu_i = A@params[\"variational_mean\"]\n        sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_distribution.covariance() - K_mm)@A.T\n\n        return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n    \n    def get_distributions(self, params):\n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        prior_mean = params[\"mean\"]\n        prior_cov = kernel_fn(params[\"X_inducing\"], params[\"X_inducing\"]) + jnp.eye(self.n_inducing)*JITTER\n        prior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n        corr_chol = tfb.CorrelationCholesky()(params[\"variational_corr_chol_param\"])\n        sigma = jnp.diag(params[\"variational_sigma\"])\n        variational_cov = sigma*sigma.T*(corr_chol@corr_chol.T) + jnp.eye(self.n_inducing)*JITTER\n        variational_distribution = tfd.MultivariateNormalFullCovariance(params[\"variational_mean\"], variational_cov)\n        \n        return prior_distribution, variational_distribution\n    \n    def loss_fn(self, params, X_batch, y_batch, seed):\n        params = self.get_constrained_params(params)\n        \n        # Get distributions\n        prior_distribution, variational_distribution = self.get_distributions(params)\n        \n        # Compute kl\n        kl = variational_distribution.kl_divergence(prior_distribution)\n\n        # Compute log likelihood\n        def log_likelihood_fn(x_i, y_i, seed):\n            q_f = self.get_q_f(params, x_i, prior_distribution, variational_distribution)\n            sample = q_f.sample(seed=seed)\n            log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n            return log_likelihood.squeeze()\n        \n        seeds = jax.random.split(seed, num=len(y_batch))\n        log_likelihood = jax.vmap(log_likelihood_fn)(X_batch, y_batch, seeds).sum() * self.data_size/len(y_batch)\n\n        return kl - log_likelihood\n    \n    def fit_fn(self, X, y, init_params, optimizer, n_iters, batch_size, seed):\n        state = optimizer.init(init_params)\n        value_and_grad_fn = jax.value_and_grad(self.loss_fn)\n        \n        def one_step(params_and_state, seed):\n            params, state = params_and_state\n            idx = jax.random.choice(seed, self.data_size, (batch_size,), replace=False)\n            X_batch, y_batch = X[idx], y[idx]\n            \n            seed2 = jax.random.split(seed, 1)[0]\n            loss, grads = value_and_grad_fn(params, X_batch, y_batch, seed2)\n            updates, state = optimizer.update(grads, state)\n            params = optax.apply_updates(params, updates)\n            return (params, state), (loss, params)\n        \n        seeds = jax.random.split(seed, num=n_iters)\n        (best_params, _), (loss_history, params_history) = jax.lax.scan(one_step, (init_params, state), xs=seeds)\n        return best_params, loss_history, params_history\n\n    def predict_fn(self, params, X_new):\n        constrained_params = self.get_constrained_params(params)\n        prior_distribution, variational_distribution = self.get_distributions(constrained_params)\n        \n        def _predict_fn(x_i):    \n            # Get posterior\n            q_f = self.get_q_f(constrained_params, x_i, prior_distribution, variational_distribution)\n            return q_f.mean().squeeze(), q_f.variance().squeeze()\n        \n        mean, var = jax.vmap(_predict_fn)(X_new)\n        return mean.squeeze(), var.squeeze()"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Train and predict",
    "text": "Train and predict\n\nn_inducing = 20\nn_epochs = 100\nbatch_size = 10\ndata_size = len(y)\nn_iters = n_epochs*(data_size/batch_size)\nn_iters\n\n1000.0\n\n\n\nkey = jax.random.PRNGKey(0)\nkey2, subkey = jax.random.split(key)\noptimizer = optax.adam(learning_rate=0.01)\n\nX_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\nmodel = SVGP(X_inducing, data_size)\n\ninit_params = model.init_params(key2)\n\nmodel.loss_fn(init_params, X, y, key)\nbest_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\nplt.figure()\nplt.plot(loss_history);\nplt.title(\"Loss\");\n\n\n\n\n\n\n\n\n\nx = jnp.linspace(-3.5, 3.5, 100)\nseed = jax.random.PRNGKey(123)\n\nX1, X2 = jnp.meshgrid(x, x)\nf = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\npred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\nlogits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\nproba = jax.nn.sigmoid(logits)\n\nproba_mean = proba.mean(axis=0)\nproba_std2 = proba.std(axis=0)*2\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12,4))\ncplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot1, ax=ax[0])\n\ncplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot2, ax=ax[1])\n\nax[0].scatter(X[:, 0], X[:, 1], c=y);\nax[1].scatter(X[:, 0], X[:, 1], c=y);\n\nax[0].set_title(\"Posterior $\\mu$\");\nax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Some more datasets",
    "text": "Some more datasets\n\ndef fit_and_plot(X, y):\n    X = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n    X, y = map(jnp.array, (X, y))\n\n    X_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\n    model = SVGP(X_inducing, data_size)\n\n    init_params = model.init_params(key2)\n\n    model.loss_fn(init_params, X, y, key)\n    best_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\n    plt.figure()\n    plt.plot(loss_history);\n    plt.title(\"Loss\");\n    \n    f = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\n    pred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\n    logits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\n    proba = jax.nn.sigmoid(logits)\n\n    proba_mean = proba.mean(axis=0)\n    proba_std2 = proba.std(axis=0)*2\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    cplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot1, ax=ax[0])\n\n    cplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot2, ax=ax[1])\n\n    ax[0].scatter(X[:, 0], X[:, 1], c=y);\n    ax[1].scatter(X[:, 0], X[:, 1], c=y);\n\n    ax[0].set_title(\"Posterior $\\mu$\");\n    ax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");\n\n\nmake_blobs\n\nX, y = make_blobs(n_samples=n_samples, random_state=random_state, centers=2)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmake_circles\n\nX, y = make_circles(n_samples=n_samples, random_state=random_state, noise=noise, factor=0.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)"
  },
  {
    "objectID": "posts/2022-10-18-kfac-laplace.html",
    "href": "posts/2022-10-18-kfac-laplace.html",
    "title": "Train NN with KFAC-Laplace in JAX",
    "section": "",
    "text": "from math import prod\nfrom functools import partial\nfrom time import time\n\nimport blackjax\nimport flax.linen as nn\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.tree_util as jtu\nimport jax.numpy as jnp\n# jnp.set_printoptions(linewidth=2000)\n\nimport optax\nfrom tqdm import trange\n\nimport arviz as az\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\njax.config.update(\"jax_enable_x64\", False)\n\n%reload_ext watermark\n\nSome helper functions:\n\njitter = 1e-6\n\ndef get_shapes(params):\n    return jtu.tree_map(lambda x:x.shape, params)\n\ndef svd_inverse(matrix):\n    U, S, V = jnp.linalg.svd(matrix+jnp.eye(matrix.shape[0])*jitter)\n    \n    return V.T/S@U.T\n\n\nDataset\nWe take XOR dataset to begin with:\n\nX = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = jnp.array([0, 1, 1, 0])\n\nX.shape, y.shape\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n((4, 2), (4,))\n\n\n\n\nNN model\n\nclass MLP(nn.Module):\n    features: []\n\n    @nn.compact\n    def __call__(self, x):\n        for n_features in self.features[:-1]:\n            x = nn.Dense(n_features, kernel_init=jax.nn.initializers.glorot_normal(), bias_init=jax.nn.initializers.normal())(x)\n            x = nn.relu(x)\n        \n        x = nn.Dense(features[-1])(x)\n        return x.ravel()\n\nLet us initialize the weights of NN and inspect shapes of the parameters:\n\nfeatures = [2, 1]\nkey = jax.random.PRNGKey(0)\n\nmodel = MLP(features)\nparams = model.init(key, X).unfreeze()\n\nget_shapes(params)\n\n{'params': {'Dense_0': {'bias': (2,), 'kernel': (2, 2)},\n  'Dense_1': {'bias': (1,), 'kernel': (2, 1)}}}\n\n\n\nmodel.apply(params, X)\n\nDeviceArray([ 0.00687164, -0.01380461,  0.        ,  0.        ], dtype=float32)\n\n\n\n\nNegative Log Joint\n\nnoise_var = 0.1\n\ndef neg_log_joint(params):\n    y_pred = model.apply(params, X)\n    flat_params = ravel_pytree(params)[0]\n    log_prior = jax.scipy.stats.norm.logpdf(flat_params).sum()\n    log_likelihood = jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=noise_var).sum()\n    \n    return -(log_prior + log_likelihood)\n\nTesting if it works:\n\nneg_log_joint(params)\n\nDeviceArray(105.03511, dtype=float32)\n\n\n\n\nFind MAP\n\nkey = jax.random.PRNGKey(0)\nparams = model.init(key, X).unfreeze()\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(neg_log_joint))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n    \n(params, state), losses = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\ny_map = model.apply(params, X)\ny_map\n\nDeviceArray([0.01383345, 0.98666817, 0.98563665, 0.01507111], dtype=float32)\n\n\n\nx = jnp.linspace(-0.1,1.1,100)\nX1, X2 = jnp.meshgrid(x, x)\n\ndef predict_fn(x1, x2):\n    return model.apply(params, jnp.array([x1,x2]).reshape(1,2))\n\npredict_fn_vec = jax.jit(jax.vmap(jax.vmap(predict_fn)))\n\nZ = predict_fn_vec(X1, X2).squeeze()\n\nplt.contourf(X1, X2, Z)\nplt.colorbar();\n\n\n\n\n\n\n\n\n\n\nFull Hessian Laplace\n\nflat_params, unravel_fn = ravel_pytree(params)\n\ndef neg_log_joint_flat(flat_params):\n    return neg_log_joint(unravel_fn(flat_params))\n\nH = jax.hessian(neg_log_joint_flat)(flat_params)\n\nsns.heatmap(H);\n\n\n\n\n\n\n\n\n\nposterior_cov = svd_inverse(H)\n\nsns.heatmap(posterior_cov);\n\n\n\n\n\n\n\n\nNote that we can sample parameters from the posterior and revert them to correct structure with the unravel_fn. Here is a class to do it all:\n\nclass FullHessianLaplace:\n    def __init__(self, map_params, model):\n        flat_params, self.unravel_fn = ravel_pytree(map_params)\n\n        def neg_log_joint_flat(flat_params):\n            params = unravel_fn(flat_params)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(flat_params)\n        \n        self.mean = flat_params\n        self.cov = svd_inverse(self.H)\n        self.model = model\n\n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample(self, seed):\n        sample = jax.random.multivariate_normal(seed, mean=self.mean, cov=self.cov)\n        return self.unravel_fn(sample)\n    \n    def sample(self, seed, shape):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nposterior = FullHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 100000\ny_pred_full = posterior.predict(X, seed=seed, shape=(n_samples,))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i]);\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_pred_mean={y_pred_full[:, i].mean():.3f}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nKFAC-Laplace\nWe need to invert partial Hessians to do KFAC-Laplace. We can use tree_flatten with ravel_pytree to ease the workflow. We need to: 1. pick up partial Hessians in pure matrix form to be able to invert them. 2. Create layer-wise distributions and sample them. These samples will be 1d arrays. 3. We need to convert those 1d arrays to params dictionary form so that we can plug it into the flax model and get posterior predictions.\nFirst we need to segregate the parameters layer-wise. We will use is_leaf condition to stop traversing the parameter PyTree at a perticular depth. See how it is different from vanilla tree_flatten:\n\nflat_params, tree_def = jtu.tree_flatten(params)\ndisplay(flat_params, tree_def)\n\n[DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n DeviceArray([[ 0.8275324 , -0.8314813 ],\n              [-0.8276633 ,  0.83254045]], dtype=float32),\n DeviceArray([0.01351773], dtype=float32),\n DeviceArray([[1.1750739],\n              [1.1685134]], dtype=float32)]\n\n\nPyTreeDef({'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}})\n\n\n\nis_leaf = lambda param: 'bias' in param\nlayers, tree_def = jtu.tree_flatten(params, is_leaf=is_leaf)\ndisplay(layers, tree_def)\n\n[{'bias': DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n  'kernel': DeviceArray([[ 0.8275324 , -0.8314813 ],\n               [-0.8276633 ,  0.83254045]], dtype=float32)},\n {'bias': DeviceArray([0.01351773], dtype=float32),\n  'kernel': DeviceArray([[1.1750739],\n               [1.1685134]], dtype=float32)}]\n\n\nPyTreeDef({'params': {'Dense_0': *, 'Dense_1': *}})\n\n\nThe difference is clearly evident. Now, we need to flatten the inner dictionaries to get 1d arrays.\n\nflat_params = list(map(lambda x: ravel_pytree(x)[0], layers))\nunravel_fn_list = list(map(lambda x: ravel_pytree(x)[1], layers))\ndisplay(flat_params, unravel_fn_list)\n\n[DeviceArray([-2.4912864e-04,  2.7019347e-04,  8.2753241e-01,\n              -8.3148128e-01, -8.2766330e-01,  8.3254045e-01],            dtype=float32),\n DeviceArray([0.01351773, 1.1750739 , 1.1685134 ], dtype=float32)]\n\n\n[&lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;,\n &lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;]\n\n\n\ndef modified_neg_log_joint_fn(flat_params):\n    layers = jtu.tree_map(lambda unravel_fn, flat_param: unravel_fn(flat_param), unravel_fn_list, flat_params)\n    params = tree_def.unflatten(layers)\n    return neg_log_joint(params)\n\nfull_hessian = jax.hessian(modified_neg_log_joint_fn)(flat_params)\n\n# Pick diagonal entries from the Hessian\nuseful_hessians = [full_hessian[i][i] for i in range(len(full_hessian))]\nuseful_hessians\n\n[DeviceArray([[139.07985,   0.     , 138.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 410.62708,   0.     , 136.54236,   0.     ,\n               273.08472],\n              [138.07985,   0.     , 139.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 136.54236,   0.     , 137.54236,   0.     ,\n               136.54236],\n              [  0.     ,   0.     ,   0.     ,   0.     ,   1.     ,\n                 0.     ],\n              [  0.     , 273.08472,   0.     , 136.54236,   0.     ,\n               274.08472]], dtype=float32),\n DeviceArray([[400.99997,  82.72832,  83.44101],\n              [ 82.72832,  69.43975,   0.     ],\n              [ 83.44101,   0.     ,  70.35754]], dtype=float32)]\n\n\nEach entry in above list corresponds to layer-wise hessian matrices. Now, we need to create layer-wise distributions, sample from them and reconstruct params using the similar tricks we used above:\n\nclass KFACHessianLaplace:\n    def __init__(self, map_params, model):\n        self.model = model\n        layers, self.tree_def = jtu.tree_flatten(map_params, is_leaf=lambda x: 'bias' in x)\n        flat_layers = [ravel_pytree(layer) for layer in layers]\n        self.means = list(map(lambda x: x[0], flat_layers))\n        self.unravel_fn_list = list(map(lambda x: x[1], flat_layers))\n\n        def neg_log_joint_flat(flat_params):\n            flat_layers = [self.unravel_fn_list[i](flat_params[i]) for i in range(len(flat_params))]\n            params = self.tree_def.unflatten(flat_layers)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(self.means)\n        self.useful_H = [self.H[i][i] for i in range(len(self.H))]\n        \n        self.covs = [svd_inverse(matrix) for matrix in self.useful_H]\n        \n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample_partial(self, seed, unravel_fn, mean, cov):\n        sample = jax.random.multivariate_normal(seed, mean=mean, cov=cov)\n        return unravel_fn(sample)\n    \n    def _sample(self, seed):\n        seeds = [seed for seed in jax.random.split(seed, num=len(self.means))]\n        flat_sample = jtu.tree_map(self._sample_partial, seeds, self.unravel_fn_list, self.means, self.covs)\n        sample = self.tree_def.unflatten(flat_sample)\n        return sample\n    \n    def sample(self, seed, n_samples=1):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nkfac_posterior = KFACHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 1000000\ny_pred_kfac = kfac_posterior.predict(X, seed=seed, shape=(n_samples, ))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that KFAC is approximating the trend of Full Hessian Laplace. We can visualize the Covariance matrices as below.\n\nfig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\n\n\n\n\n\n\n\n\n\n\nComparison with MCMC\nInspired from a blackjax docs example.\n\nkey = jax.random.PRNGKey(0)\nwarmup_key, inference_key = jax.random.split(key, 2)\nnum_warmup = 5000\nnum_samples = n_samples\n\ninitial_position = model.init(key, X)\ndef logprob(params): \n    return -neg_log_joint(params)\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, _ = kernel(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n\ninit = time()\nadapt = blackjax.window_adaptation(blackjax.nuts, logprob, num_warmup)\nfinal_state, kernel, _ = adapt.run(warmup_key, initial_position)\nstates = inference_loop(inference_key, kernel, final_state, num_samples)\nsamples = states.position.unfreeze()\nprint(f\"Sampled {n_samples} samples in {time()-init:.2f} seconds\")\n\nSampled 1000000 samples in 27.85 seconds\n\n\n\ny_pred_mcmc = jax.vmap(model.apply, in_axes=(0, None))(samples, X)\n\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    az.plot_dist(y_pred_mcmc[:, i], ax=ax[i], label='mcmc', color='k')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,3,figsize=(18,5))\nfig.subplots_adjust(wspace=0.1)\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\nmcmc_cov = jnp.cov(jax.vmap(lambda x: ravel_pytree(x)[0])(samples).T)\n\nsns.heatmap(mcmc_cov, ax=ax[2], annot=True, fmt = '.2f')\nax[2].set_title('MCMC');\n\n\n\n\n\n\n\n\n\n\nLibrary versions\n\n%watermark --iversions\n\nflax      : 0.6.1\nblackjax  : 0.8.2\noptax     : 0.1.3\nmatplotlib: 3.5.1\njax       : 0.3.23\narviz     : 0.12.1\nseaborn   : 0.11.2\njson      : 2.0.9"
  },
  {
    "objectID": "posts/gcloud.html",
    "href": "posts/gcloud.html",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don‚Äôt know if this is required or not. This command should trigger installation of ‚Äúgcloud Beta Commands‚Äù automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#initial-setup",
    "href": "posts/gcloud.html#initial-setup",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don‚Äôt know if this is required or not. This command should trigger installation of ‚Äúgcloud Beta Commands‚Äù automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms",
    "href": "posts/gcloud.html#working-with-tpu-vms",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs",
    "text": "Working with TPU VMs\nThere are two different terms here: ‚ÄúTPU VMs‚Äù and ‚ÄúTPU nodes‚Äù. TPU nodes can be connected externally via another VM. TPU VMs are stand-alone systems with TPUs, RAM and CPU (96 core Intel 2 GHz processor and 335 GB RAM). We may be charged via GCP for the VM (CPUs and RAM). (I will update this info once I know for sure):\n\n\nTo create a TPU VM in preferred zone via CLI (be careful about the --zone to avoid charges, check the first email received from TRC team to see what kind of TPUs are free in different zones. if --zone is not passed, VM will be created in the default zone that we set initially. This command triggered installation of ‚Äúgcloud Alpha Commands‚Äù):\n\ngcloud alpha compute tpus tpu-vm create vm-1 --accelerator-type v2-8 --version tpu-vm-tf-2.8.0 --zone us-central1-f\n\nTo get the list of TPU nodes/VMs:\n\ngcloud compute tpus list\n\nTo delete a TPU node/VM:\n\ngcloud compute tpus delete vm-1\n\nTo connect with a vm via ssh (this automatically creates ssh key pair and places in default ssh config location):\n\ngcloud alpha compute tpus tpu-vm ssh vm-1\n\nFollow this guide to create and attach a persistent disk with the TPU VM"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "href": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs via VS-code",
    "text": "Working with TPU VMs via VS-code\n\nInstall the following extension on VS-code: \nUse the following button to connect to a remote machine (use ‚ÄúConnect to Host‚Ä¶‚Äù button): \nManually update the default ssh config file (in my case, ‚ÄúC:\\.ssh‚Äù) to add a VM in VS-code (you can use VS-code command palette to figure out the config file for you and edit it. Please see the screeshot below).\n\n\n\nNote that ssh public-private key pair with name google_compute_engine is automatically generated when you connect with the VM for the first time with gcloud alpha compute tpus tpu-vm ssh command. The VM config for me looks like this:\n\nHost Cloud-TPU-Node-2\n  HostName &lt;External-IP-of-your-TPU-VM&gt;\n  User zeelp\n  Port 22\n  IdentityFile C:\\Users\\zeelp\\.ssh\\google_compute_engine"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html",
    "href": "posts/2022-06-10-jaxoptimizers.html",
    "title": "JAX Optimizers",
    "section": "",
    "text": "%%capture\n%pip install -U jax\nimport jax\nimport jax.numpy as jnp\ntry:\n  import jaxopt\nexcept ModuleNotFoundError:\n  %pip install -qq jaxopt\n  import jaxopt\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install -qq optax\n  import optax\n\nimport tensorflow_probability.substrates.jax as tfp"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "href": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "title": "JAX Optimizers",
    "section": "Loss function",
    "text": "Loss function\n\ndef loss_fun(x, a):\n  return (((x['param1'] - a) + (x['param2'] - (a+1)))**2).sum()"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "href": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "title": "JAX Optimizers",
    "section": "Initial parameters",
    "text": "Initial parameters\n\nN = 3\ninit_params = lambda: {'param1': jnp.zeros(N), 'param2': jnp.ones(N)}\na = 2.0"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "href": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "title": "JAX Optimizers",
    "section": "Optimizers",
    "text": "Optimizers\n\nJaxOpt ScipyMinimize\n\n%%time\nsolver = jaxopt.ScipyMinimize('L-BFGS-B', fun=loss_fun)\nans = solver.run(init_params(), a)\nprint(ans)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nOptStep(params={'param1': DeviceArray([1.9999999, 1.9999999, 1.9999999], dtype=float32), 'param2': DeviceArray([3., 3., 3.], dtype=float32)}, state=ScipyMinimizeInfo(fun_val=DeviceArray(4.2632564e-14, dtype=float32), success=True, status=0, iter_num=2))\nCPU times: user 78.3 ms, sys: 18.5 ms, total: 96.8 ms\nWall time: 95.8 ms\n\n\n\nPros\n\nTwo lines of code will do it all.\n\n\n\nCons\n\nIt only returns the final parameters and final loss. No option to retrive in-between loss values.\n\n\n\n\nOptax\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nvalue_and_grad_fun = jax.jit(jax.value_and_grad(loss_fun, argnums=0))\nparams = init_params()\nstate = optimizer.init(params)\n\nfor _ in range(100):\n  loss_value, gradients = value_and_grad_fun(params, a)\n  updates, state = optimizer.update(gradients, state)\n  params = optax.apply_updates(params, updates)\n\nprint(params)\n\n{'param1': DeviceArray([2.0084236, 2.0084236, 2.0084236], dtype=float32), 'param2': DeviceArray([3.0084238, 3.0084238, 3.0084238], dtype=float32)}\nCPU times: user 3.09 s, sys: 63.4 ms, total: 3.16 s\nWall time: 4.2 s\n\n\n\nPros:\n\nFull control in user‚Äôs hand. We can save intermediate loss values.\n\n\n\nCons:\n\nIts code is verbose, similar to PyTorch optimizers.\n\n\n\n\nJaxopt OptaxSolver\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nsolver = jaxopt.OptaxSolver(loss_fun, optimizer, maxiter=100)\nans = solver.run(init_params(), a)\nprint(ans)\n\nOptStep(params={'param1': DeviceArray([2.008423, 2.008423, 2.008423], dtype=float32), 'param2': DeviceArray([3.008423, 3.008423, 3.008423], dtype=float32)}, state=OptaxState(iter_num=DeviceArray(100, dtype=int32, weak_type=True), value=DeviceArray(0.00113989, dtype=float32), error=DeviceArray(0.09549397, dtype=float32), internal_state=(ScaleByAdamState(count=DeviceArray(100, dtype=int32), mu={'param1': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32), 'param2': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32)}, nu={'param1': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32), 'param2': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32)}), EmptyState()), aux=None))\nCPU times: user 719 ms, sys: 13.4 ms, total: 732 ms\nWall time: 1.09 s\n\n\n\nPros:\n\nLess lines of code.\nApplies lax.scan internally to make it fast [reference].\n\n\n\nCons:\n\nNot able to get in-between state/loss values\n\n\n\n\ntfp math minimize\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nparams, losses = tfp.math.minimize_stateless(loss_fun, (init_params(), a), num_steps=1000, optimizer=optimizer)\nprint(params)\nprint(losses[:5])\n\n({'param1': DeviceArray([1.0000008, 1.0000008, 1.0000008], dtype=float32), 'param2': DeviceArray([1.9999989, 1.9999989, 1.9999989], dtype=float32)}, DeviceArray(0.9999999, dtype=float32))\n[48.       38.88006  30.751791 23.626852 17.507807]\nCPU times: user 880 ms, sys: 15.2 ms, total: 895 ms\nWall time: 1.53 s\n\n\n\nPros:\n\nOne line of code to optimize the function and return in-between losses.\n\n\n\nCons:\n\nBy default, it optimizes all arguments passed to the loss function. In above example, we can not control if a should be optimized or not. I have raised an issue here for this problem."
  },
  {
    "objectID": "posts/GNNs_and_GPs.html",
    "href": "posts/GNNs_and_GPs.html",
    "title": "GNNs and GPs",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport regdata as rd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\nx_train, y_train, x_test = rd.Step().get_data()\ny_train = y_train.reshape(-1, 1)\nx_test = x_test * 1.5\nprint(x_train.shape, y_train.shape, x_test.shape)\n\nplt.scatter(x_train, y_train, label='train');\n\n(50, 1) (50, 1) (100, 1)\n\n\n\n\n\n\n\n\n\n\nkernel = GPy.kern.RBF(1, variance=1, lengthscale=1)\nmodel = GPy.models.GPRegression(x_train, y_train.reshape(-1, 1), kernel)\nmodel.Gaussian_noise.variance = 0.1\n\ny_pred_gp, y_var = model.predict(x_test)\n\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_test, y_pred_gp, label='pred');\n\n\n\n\n\n\n\n\n\nclass GCN_Forward(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = self.fc(x)\n        x = torch.matmul(A, x)\n        return x\n    \nclass GCN_Reverse(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = torch.matmul(A, x)\n        x = self.fc(x)\n        return x\n\nclass NN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.features = features\n        \n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', nn.Linear(in_features, out_features))\n            \n        self.last_layer = nn.Linear(features[-1], 1)\n        \n    def forward(self, x, A):\n        for i in range(len(self.features) - 1):\n            if isinstance(getattr(self, f'layer_{i}'), GCN_Forward):\n                x = getattr(self, f'layer_{i}')(x, A)\n            else:\n                x = getattr(self, f'layer_{i}')(x)\n            x = nn.functional.gelu(x)\n            \n        x = self.last_layer(x)\n        return x\n\nclass GCN(NN):\n    def __init__(self, features):\n        super().__init__(features)\n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', GCN_Forward(in_features, out_features))\n\n\nA = torch.tensor(kernel.K(x_train, x_train)).float()\n# A.fill_diagonal_(0)\nA = A / A.sum(dim=0, keepdim=True)\n# A.fill_diagonal_(1)\n\nnum_epochs = 500\nfeatures = [1, 1024]\n\ngcn_model = GCN(features=features)\nnn_model = NN(features=features)\n\ngcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\nnn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.01)\n\ncriterion = nn.MSELoss()\n\nx_train_torch = torch.from_numpy(x_train).float()\ny_train_torch = torch.from_numpy(y_train).float()\n\ngcn_losses = []\nnn_losses = []\nfor epoch in range(num_epochs):\n    gcn_optimizer.zero_grad()\n    nn_optimizer.zero_grad()\n    \n    y_out_gcn = gcn_model(x_train_torch, A)\n    y_out_nn = nn_model(x_train_torch, A)\n    gcn_loss = criterion(y_out_gcn, y_train_torch)\n    nn_loss = criterion(y_out_nn, y_train_torch)\n    \n    gcn_loss.backward()\n    nn_loss.backward()\n    \n    gcn_losses.append(gcn_loss.item())\n    nn_losses.append(nn_loss.item())\n    \n    gcn_optimizer.step()\n    nn_optimizer.step()\n        \nplt.plot(gcn_losses, label='gcn');\nplt.plot(nn_losses, label='nn');\nplt.legend();\n\n\n\n\n\n\n\n\n\nA_test = torch.tensor(kernel.K(x_test, x_test)).float()\n# A_test.fill_diagonal_(0)\nA_test = A_test / A_test.sum(dim=0, keepdim=True)\n# A_test.fill_diagonal_(1)\n\ny_pred_nn = nn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\ny_pred_gcn = gcn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_train, y_out_gcn.detach().numpy(), label='pred GCN train');\nplt.plot(x_train, y_out_nn.detach().numpy(), label='pred NN train');\nplt.plot(x_test, y_pred_gp, label='pred GP', linestyle='--');\nplt.plot(x_test, y_pred_nn, label='pred NN');\nplt.plot(x_test, y_pred_gcn, label='pred GCN');\nplt.ylim(-3, 3);\nplt.legend();"
  },
  {
    "objectID": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "href": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "title": "Active Learning with Bayesian Linear Regression",
    "section": "",
    "text": "A quick wrap-up for Bayesian Linear Regression (BLR)\nWe have a feature matrix \\(X\\) and a target vector \\(Y\\). We want to obtain \\(\\theta\\) vector in such a way that the error \\(\\epsilon\\) for the following equation is minimum.\n\\[\nY = X^T\\theta + \\epsilon\n\\] Prior PDF for \\(\\theta\\) is,\n\\[\np(\\theta) \\sim \\mathcal{N}(M_0, S_0)\n\\]\nWhere \\(S_0\\) is prior covariance matrix, and \\(M_0\\) is prior mean.\nPosterier PDF can be given as,\n\\[\n\\begin{aligned}\np(\\theta|X,Y) &\\sim \\mathcal{N}(\\theta | M_n, S_n) \\\\\nS_n &= (S_0^{-1} + \\sigma_{mle}^{-2}X^TX) \\\\\nM_n &= S_n(S_0^{-1}M_0+\\sigma_{mle}^{-2}X^TY)\n\\end{aligned}\n\\]\nMaximum likelihood estimation of \\(\\sigma\\) can be calculated as,\n\\[\n\\begin{aligned}\n\\theta_{mle} &= (X^TX)^{-1}X^TY \\\\\n\\sigma_{mle} &= ||Y - X^T\\theta_{mle}||\n\\end{aligned}\n\\]\nFinally, predicted mean \\(\\hat{Y}_{mean}\\) and predicted covariance matrix \\(\\hat{Y}_{cov}\\) can be given as,\n\\[\n\\begin{aligned}\n\\hat{Y} &\\sim \\mathcal{N}(\\hat{Y}_{mean}, \\hat{Y}_{cov}) \\\\\n\\hat{Y}_{mean} &= XM_n \\\\\n\\hat{Y}_{cov} &= X^TS_nX\n\\end{aligned}\n\\]\nNow, let‚Äôs put everything together and write a class for Bayesian Linear Regression.\n\n\nCreating scikit-learn like class with fit predict methods for BLR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 0 # random seed for train_test_split\n\n\nclass BLR():\n  def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix\n    self.S0 = S0\n    self.M0 = M0\n\n  def fit(self,x,y, return_self = False):\n    self.x = x\n    self.y = y\n\n    # Maximum likelihood estimation for sigma parameter\n    theta_mle = np.linalg.pinv(x.T@x)@(x.T@y)\n    sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2\n    sigma_mle = np.sqrt(sigma_2_mle)\n\n    # Calculating predicted mean and covariance matrix for theta\n    self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x)\n    self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze())\n\n    # Calculating predicted mean and covariance matrix for data\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    if return_self:\n      return (self.y_hat_map, self.pred_var)\n    \n  def predict(self, x):\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    return (self.y_hat_map, self.pred_var)\n\n  def plot(self, s=1): # s -&gt; size of dots for scatter plot\n    individual_var = self.pred_var.diagonal()\n    plt.figure()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.plot(self.x[:,1], self.y_hat_map, color='black', label='model')\n    plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color='black', label='uncertainty')\n    plt.scatter(self.x[:,1], self.y, label='actual data',s=s)\n    plt.title('MAE is '+str(np.mean(np.abs(self.y - self.y_hat_map))))\n    plt.legend()\n\n\n\nCreating & visualizing dataset\nTo start with, let‚Äôs create a random dataset with degree 3 polynomial function with some added noise.\n\\[\nY = (5X^3 - 4X^2 + 3X - 2) + \\mathcal{N}(0,1)\n\\]\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, )\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\nWe‚Äôll try to fit a degree 5 polynomial function to our data.\n\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nN_features = X.shape[1]\n\n\nplt.scatter(X[:,1], Y, s=0.5, label = 'data points')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLearning a BLR model on the entire data\nWe‚Äôll take \\(M_0\\) (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about \\(M_0\\). We‚Äôre taking \\(S_0\\) (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other.\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\n\n\n\nVisualising the fit\n\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nThis doesn‚Äôt look like a good fit, right? Let‚Äôs set the prior closer to the real values and visualize the fit again.\n\n\nVisualising the fit after changing the prior\n\nnp.random.seed(seed)\nS0 = np.eye(N_features)\nM0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, )\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nHmm, better. Now let‚Äôs see how it fits after reducing the noise and setting the prior mean to zero vector again.\n\n\nVisualising the fit after reducing the noise\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nWhen the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit.\n\n\nIntuition to Active Learning (Uncertainty Sampling) with an example\nLet‚Äôs take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How?\nWe train our model with existing data and test it on all the suspected patients‚Äô data. Let‚Äôs say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing.\nThis method is called Uncertainty Sampling in Active Learning. Now let‚Äôs formally define Active Learning. From Wikipedia,\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.\nNow, we‚Äôll go through the active learning procedure step by step.\n\n\nTrain set, test set, and pool. What is what?\nThe train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty.\nSo, the algorithm can be represented as the following,\n\nTrain the model with the train set.\nTest the performance on the test set (This is expected to improve).\nTest the model with the pool.\nQuery for the most uncertain datapoint from the pool.\nAdd that datapoint into the train set.\nRepeat step 1 to step 5 for \\(K\\) iterations (\\(K\\) ranges from \\(0\\) to the pool size).\n\n\n\nCreating initial train set, test set, and pool\nLet‚Äôs take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let‚Äôs start with 2 data points as the train set.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed)\n\nVisualizing train, test and pool.\n\nplt.scatter(test_X[:,1], test_Y, label='test set',color='r', s=2)\nplt.scatter(train_X[:,1], train_Y, label='train set',marker='s',color='k', s=50)\nplt.scatter(pool_X[:,1], pool_Y, label='pool',color='b', s=2)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs initialize a few dictionaries to keep track of each iteration.\n\ntrain_X_iter = {} # to store train points at each iteration\ntrain_Y_iter = {} # to store corresponding labels to the train set at each iteration\nmodels = {} # to store the models at each iteration\nestimations = {} # to store the estimations on the test set at each iteration\ntest_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n\n\n\nTraining & testing initial learner on train set (Iteration 0)\nNow we will train the model for the initial train set, which is iteration 0.\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\nCreating a plot method to visualize train, test and pool with estimations and uncertainty.\n\ndef plot(ax, model, init_title=''):\n  # Plotting the pool\n  ax.scatter(pool_X[:,1], pool_Y, label='pool',s=1,color='r',alpha=0.4)\n  \n  # Plotting the test data\n  ax.scatter(test_X[:,1], test_Y, label='test data',s=1, color='b', alpha=0.4)\n  \n  # Combining the test & the pool\n  test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y)\n  \n  # Sorting test_pool for plotting\n  sorted_inds = np.argsort(test_pool_X[:,1])\n  test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n  \n  # Plotting test_pool with uncertainty\n  model.predict(test_pool_X)\n  individual_var = model.pred_var.diagonal()\n  ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n  ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                  , alpha=0.2, color='black', label='uncertainty')\n  \n  # Plotting the train data\n  ax.scatter(model.x[:,1], model.y,s=40, color='k', marker='s', label='train data')\n  ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n  \n  # Plotting MAE on the test set\n  model.predict(test_X)\n  ax.set_title(init_title+' MAE is '+str(np.mean(np.abs(test_Y - model.y_hat_map))))\n  ax.set_xlabel('x')\n  ax.set_ylabel('y')\n  ax.legend()\n\nPlotting the estimations and uncertainty.\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nLet‚Äôs check the maximum uncertainty about any point for the model.\n\nmodels[0].pred_var.diagonal().max()\n\n4.8261426545316604e-29\n\n\nOops!! There is almost no uncertainty in the model. Why? let‚Äôs try again with more train points.\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed)\n\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nNow uncertainty is visible, and currently, it‚Äôs high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model.\nLet‚Äôs evaluate the performance on the test set.\n\nestimations[0], _ = models[0].predict(test_X)\ntest_mae_error[0] = np.mean(np.abs(test_Y - estimations[0]))\n\nMean Absolute Error (MAE) on the test set is\n\ntest_mae_error[0]\n\n0.5783654195019617\n\n\n\n\nMoving the most uncertain point from the pool to the train set\nIn the previous plot, we saw that the model was least certain about the left-most point. We‚Äôll move that point from the pool to the train set and see the effect.\n\nesimations_pool, _ = models[0].predict(pool_X)\n\nFinding out a point having the most uncertainty.\n\nin_var = models[0].pred_var.diagonal().argmax()\nto_add_x = pool_X[in_var,:]\nto_add_y = pool_Y[in_var]\n\nAdding the point from the pool to the train set.\n\ntrain_X_iter[1] = np.vstack([train_X_iter[0], to_add_x])\ntrain_Y_iter[1] = np.append(train_Y_iter[0], to_add_y)\n\nDeleting the point from the pool.\n\npool_X = np.delete(pool_X, in_var, axis=0)\npool_Y = np.delete(pool_Y, in_var)\n\n\n\nTraining again and visualising the results (Iteration 1)\nThis time, we will pass previously learnt prior to the next iteration.\n\nS0 = np.eye(N_features)\nmodels[1] = BLR(S0, models[0].MN)\n\n\nmodels[1].fit(train_X_iter[1], train_Y_iter[1])\n\n\nestimations[1], _ = models[1].predict(test_X)\ntest_mae_error[1] = np.mean(np.abs(test_Y - estimations[1]))\n\nMAE on the test set is\n\ntest_mae_error[1]\n\n0.5779411133071186\n\n\nVisualizing the results.\n\nfig, ax = plt.subplots()\nplot(ax, models[1])\n\n\n\n\n\n\n\n\nBefore & after adding most uncertain point\n\nfig, ax = plt.subplots(1,2, figsize=(13.5,4.5))\nplot(ax[0], models[0],'Before')\nplot(ax[1], models[1],'After')\n\n\n\n\n\n\n\n\nWe can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data.\nNow let‚Äôs do this for few more iterations in a loop and visualise the results.\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x= np.zeros((num_iterations+1, N_features))\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(2, num_iterations+1):\n    # Making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool, var = models[iteration-1].predict(pool_X)\n    \n    # Finding the point from the pool with highest uncertainty\n    in_var = var.diagonal().argmax()\n    to_add_x = pool_X[in_var,:]\n    to_add_y = pool_Y[in_var]\n    points_added_x[iteration-1,:] = to_add_x\n    points_added_y.append(to_add_y)\n    \n    # Adding the point to the train set from the pool\n    train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x])\n    train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var, axis=0)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set\n    models[iteration] = BLR(S0, models[iteration-1].MN)\n    models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration])\n    \n    estimations[iteration], _ = models[iteration].predict(test_X)\n    test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n2 0.49023173501654815\n3 0.4923391714942153\n4 0.49040074812746753\n5 0.49610198614600165\n6 0.5015282102751122\n7 0.5051264429971314\n8 0.5099913097301352\n9 0.504455016053513\n10 0.5029219102020734\n11 0.5009762782262487\n12 0.5004883097883343\n13 0.5005169638980388\n14 0.5002731089932334\n15 0.49927485683909884\n16 0.49698416490822594\n17 0.49355398855432897\n18 0.49191185613804617\n19 0.491164833699368\n20 0.4908067530719673\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let‚Äôs visualise fits for all the iterations. We‚Äôll discuss this behaviour after that.\n\n\nVisualizing active learning procedure\n\nprint('Initial model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[0].MN[::-1]))\nprint('\\nFinal model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[num_iterations].MN[::-1]))\n\nInitial model\nY = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63\n\nFinal model\nY = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58\n\n\n\ndef update(iteration):\n    ax.cla()\n    plot(ax, models[iteration])\n    fig.tight_layout()\n\n\nfig, ax = plt.subplots()\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250)\nplt.close()\nrc('animation', html='jshtml')\n\n\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually.\nNow, let‚Äôs put everything together and create a class for active learning procedure\n\n\nCreating a class for active learning procedure\n\nclass ActiveL():\n  def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1):\n    self.X_init = X\n    self.y = y\n    self.S0 = S0\n    self.M0 = M0\n    self.train_X_iter = {} # to store train points at each iteration\n    self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration\n    self.models = {} # to store the models at each iteration\n    self.estimations = {} # to store the estimations on the test set at each iteration\n    self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n    self.test_size = test_size\n    self.degree = degree\n    self.iterations = iterations\n    self.seed = seed\n    self.train_size = degree + 2\n\n  def data_preperation(self):\n    # Adding polynomial features\n    self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init)\n    N_features = self.X.shape[1]\n    \n    # Splitting into train, test and pool\n    train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, \n                                                                            test_size=self.test_size,\n                                                                            random_state=self.seed)\n    self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, \n                                                                            train_size=self.train_size, \n                                                                            random_state=self.seed)\n    \n    # Setting BLR prior incase of not given\n    if self.M0 == None:\n      self.M0 = np.zeros((N_features, ))\n    if self.S0 == None:\n      self.S0 = np.eye(N_features)\n    \n  def main(self):\n    # Training for iteration 0\n    self.train_X_iter[0] = self.train_X\n    self.train_Y_iter[0] = self.train_Y\n    self.models[0] = BLR(self.S0, self.M0)\n    self.models[0].fit(self.train_X, self.train_Y)\n\n    # Running loop for all iterations\n    for iteration in range(1, self.iterations+1):\n      # Making predictions on the pool set based on model learnt in the respective train set \n      estimations_pool, var = self.models[iteration-1].predict(self.pool_X)\n      \n      # Finding the point from the pool with highest uncertainty\n      in_var = var.diagonal().argmax()\n      to_add_x = self.pool_X[in_var,:]\n      to_add_y = self.pool_Y[in_var]\n      \n      # Adding the point to the train set from the pool\n      self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x])\n      self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y)\n      \n      # Deleting the point from the pool\n      self.pool_X = np.delete(self.pool_X, in_var, axis=0)\n      self.pool_Y = np.delete(self.pool_Y, in_var)\n      \n      # Training on the new set\n      self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN)\n      self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration])\n      \n      self.estimations[iteration], _ = self.models[iteration].predict(self.test_X)\n      self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean()\n\n  def _plot_iter_MAE(self, ax, iteration):\n    ax.plot(list(self.test_mae_error.values())[:iteration+1], 'ko-')\n    ax.set_title('MAE on test set over iterations')\n    ax.set_xlim((-0.5, self.iterations+0.5))\n    ax.set_ylabel(\"MAE on test set\")\n    ax.set_xlabel(\"# Points Queried\")\n  \n  def _plot(self, ax, model):\n    # Plotting the pool\n    ax.scatter(self.pool_X[:,1], self.pool_Y, label='pool',s=1,color='r',alpha=0.4)\n    \n    # Plotting the test data\n    ax.scatter(self.test_X[:,1], self.test_Y, label='test data',s=1, color='b', alpha=0.4)\n    \n    # Combining test_pool\n    test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y)\n    \n    # Sorting test_pool\n    sorted_inds = np.argsort(test_pool_X[:,1])\n    test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n    \n    # Plotting test_pool with uncertainty\n    preds, var = model.predict(test_pool_X)\n    individual_var = var.diagonal()\n    ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n    ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                    , alpha=0.2, color='black', label='uncertainty')\n    \n    # plotting the train data\n    ax.scatter(model.x[:,1], model.y,s=10, color='k', marker='s', label='train data')\n    ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n    \n    # plotting MAE\n    preds, var = model.predict(self.test_X)\n    ax.set_title('MAE is '+str(np.mean(np.abs(self.test_Y - preds))))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    \n  def visualise_AL(self):\n    fig, ax = plt.subplots(1,2,figsize=(13,5))\n    def update(iteration):\n      ax[0].cla()\n      ax[1].cla()\n      self._plot(ax[0], self.models[iteration])\n      self._plot_iter_MAE(ax[1], iteration)\n      fig.tight_layout()\n\n    print('Initial model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[0].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n    print('\\nFinal model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[self.iterations].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250)\n    plt.close()\n\n    rc('animation', html='jshtml')\n    return anim\n\n\n\nVisualizing a different polynomial fit on the same dataset\nLet‚Äôs try to fit a degree 7 polynomial to the same data now.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nmodel = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7\n\nFinal model\nY = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations.\n\n\nActive learning for diabetes dataset from the Scikit-learn module\nLet‚Äôs run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We‚Äôll choose only ‚Äòweight‚Äô feature, which seems to have more correlation with the target.\nWe‚Äôll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let‚Äôs check the performance of Scikit-learn linear regression model.\n\nX, Y = datasets.load_diabetes(return_X_y=True)\nX = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression\n\n# Normalizing\nX = (X - X.min())/(X.max() - X.min())\nY = (Y - Y.min())/(Y.max() - Y.min())\n\nVisualizing the dataset.\n\nplt.scatter(X, Y)\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs fit the Scikit-learn linear regression model with 50% train-test split.\n\nfrom sklearn.linear_model import LinearRegression\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed)\n\n\nclf = LinearRegression()\n\n\nclf.fit(train_X, train_Y)\npred_Y = clf.predict(test_X)\n\nVisualizing the fit & MAE.\n\nplt.scatter(X, Y, label='data', s=5)\nplt.plot(test_X, pred_Y, label='model', color='r')\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.title('MAE is '+str(np.mean(np.abs(pred_Y - test_Y))))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow we‚Äôll fit the same data with our BLR model\n\nmodel = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = 0.41 + 0.16 X^1\n\nFinal model\nY = 0.13 + 0.86 X^1\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nInitially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It‚Äôs interesting to see that our initial train points tend to make a vertical fit, but the model doesn‚Äôt get carried away by that and stabilizes the self with prior.\n\nprint('MAE for Scikit-learn Linear Regression is',np.mean(np.abs(pred_Y - test_Y)))\nprint('MAE for Bayesian Linear Regression is', model.test_mae_error[20])\n\nMAE for Scikit-learn Linear Regression is 0.15424985705353944\nMAE for Bayesian Linear Regression is 0.15738001811804758\n\n\nAt the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we‚Äôve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit."
  },
  {
    "objectID": "posts/2021-10-23-warped-gp.html",
    "href": "posts/2021-10-23-warped-gp.html",
    "title": "Input Warped GPs - A failed idea",
    "section": "",
    "text": "Comments\n\nWe are warping inputs \\(\\mathbf{x}\\) into \\(\\mathbf{w}\\cdot\\mathbf{x}\\)\nLearning second level GP over \\(\\mathbf{w}\\).\nAppling penalty over \\(\\mathbf{w}\\) if varies too much unnecessary.\nSee problems at the end of the notebook.\nWe need to check mathematical concerns related to this transformation.\n\n\nimport math\nimport numpy as np\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport regdata as rd\nfrom sklearn.cluster import KMeans\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nclass ExactNSGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_latent):\n        super(ExactNSGPModel, self).__init__(train_x, train_y, likelihood)\n#         inds = np.random.choice(train_x.shape[0], size=num_latent, replace=False)\n#         self.x_bar = train_x[inds]\n        self.x_bar = torch.tensor(KMeans(n_clusters=num_latent).fit(train_x).cluster_centers_).to(train_x)\n        self.w_bar = torch.nn.Parameter(torch.ones(num_latent,).to(self.x_bar))\n        self.bias = torch.nn.Parameter(torch.zeros(1,).to(self.x_bar))\n        self.latent_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n#       We can fix noise to be minimum but it is not ideal. Ideally, noise should automatically reduce to reasonable value.\n#         self.latent_likelihood.raw_noise.requires_grad = False\n#         self.latent_likelihood.raw_noise = torch.tensor(-10.)\n        self.latent_model = ExactGPModel(self.x_bar, self.w_bar, self.latent_likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        self.latent_model.eval()\n        with gpytorch.settings.detach_test_caches(False):  # needed to back propagate thru predictive posterior\n            self.latent_model.set_train_data(self.x_bar, self.w_bar, strict=False)\n            self.w = self.latent_likelihood(self.latent_model(x))  # predictive posterior\n        x_warped = x*self.w.mean[:, None] + self.bias\n        mean_x = self.mean_module(x_warped)\n        covar_x = self.covar_module(x_warped)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\ndef training(model, likelihood):\n    training_iter = 100\n\n    # Find optimal model hyperparameters\n    model.train()\n    likelihood.train()\n\n    # Use the adam optimizer\n    optimizer = torch.optim.Adam([\n        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n    ], lr=0.1)\n\n    # \"Loss\" for GPs - the marginal log likelihood\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    for i in range(training_iter):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        output = model(train_x)\n        # Calc loss and backprop gradients\n        try:\n            loss = -mll(output, train_y) + torch.square(model.w.mean-1).mean()\n#             print(model.latent_likelihood.noise)\n        except AttributeError:\n            loss = -mll(output, train_y)\n        loss.backward()\n#         print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n#             i + 1, training_iter, loss.item(),\n#             model.covar_module.base_kernel.lengthscale.item(),\n#             model.likelihood.noise.item()\n#         ))\n        optimizer.step()\n    \ndef predict_plot(model, likelihood, title):\n    # Get into evaluation (predictive posterior) mode\n    model.eval()\n    likelihood.eval()\n\n    # Test points are regularly spaced along [0,1]\n    # Make predictions by feeding model through likelihood\n    with torch.no_grad():\n        observed_pred = likelihood(model(test_x))\n\n    with torch.no_grad():\n        # Initialize plot\n        f, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n        # Get upper and lower confidence bounds\n        lower, upper = observed_pred.confidence_region()\n        # Plot training data as black stars\n        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n        # Plot predictive means as blue line\n        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n        # Shade between the lower and upper confidence bounds\n        ax.fill_between(test_x.numpy().ravel(), lower.numpy(), upper.numpy(), alpha=0.5)\n        ax.legend(['Observed Data', 'Mean', 'Confidence'])\n        ax.set_title(title)\n    return observed_pred\n\n\ndef GP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactGPModel(train_x, train_y, likelihood)\n    \n    training(model, likelihood)\n    predict_plot(model, likelihood, 'GP')\n\ndef NSGP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactNSGPModel(train_x, train_y, likelihood, num_latent)\n    \n    training(model, likelihood)\n    observed_pred = predict_plot(model, likelihood, 'NSGP')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x*model.w.mean[:, None], observed_pred.mean.numpy())\n        plt.title('Warped test inputs v/s test outputs')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x, model.w.mean, label='interpolated')\n        plt.scatter(model.x_bar, model.w_bar, label='learned')\n        plt.ylim(0,2)\n        plt.title('Test input v/s weights')\n        plt.legend()\n\n\n\nTesting over various datasets\n\ntrain_x, train_y, test_x = rd.DellaGattaGene(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Heinonen4(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Jump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.MotorcycleHelmet(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Olympic(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineJump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineNoisy(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblems\n\nTransformation from x to x_warped is not monotonic."
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "title": "Get a list of contributors from a repo",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "title": "Get a list of contributors from a repo",
    "section": "Config",
    "text": "Config\n\nowner = \"probml\"\nrepo = \"pyprobml\""
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Get all contributors to a repo",
    "text": "Get all contributors to a repo\n\ncontributors = pd.read_json(f\"https://api.github.com/repos/{owner}/{repo}/contributors?per_page=100\")\ncontributors = contributors.set_index(\"login\")\nprint(f\"Number of contributors: {len(contributors.index.unique())}\")\ncontributors.head(2)\n\nNumber of contributors: 47\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nnode_id\navatar_url\ngravatar_id\nurl\nhtml_url\nfollowers_url\nfollowing_url\ngists_url\nstarred_url\nsubscriptions_url\norganizations_url\nrepos_url\nevents_url\nreceived_events_url\ntype\nsite_admin\ncontributions\n\n\nlogin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmurphyk\n4632336\nMDQ6VXNlcjQ2MzIzMzY=\nhttps://avatars.githubusercontent.com/u/463233...\n\nhttps://api.github.com/users/murphyk\nhttps://github.com/murphyk\nhttps://api.github.com/users/murphyk/followers\nhttps://api.github.com/users/murphyk/following...\nhttps://api.github.com/users/murphyk/gists{/gi...\nhttps://api.github.com/users/murphyk/starred{/...\nhttps://api.github.com/users/murphyk/subscript...\nhttps://api.github.com/users/murphyk/orgs\nhttps://api.github.com/users/murphyk/repos\nhttps://api.github.com/users/murphyk/events{/p...\nhttps://api.github.com/users/murphyk/received_...\nUser\nFalse\n1777\n\n\nNeoanarika\n5188337\nMDQ6VXNlcjUxODgzMzc=\nhttps://avatars.githubusercontent.com/u/518833...\n\nhttps://api.github.com/users/Neoanarika\nhttps://github.com/Neoanarika\nhttps://api.github.com/users/Neoanarika/followers\nhttps://api.github.com/users/Neoanarika/follow...\nhttps://api.github.com/users/Neoanarika/gists{...\nhttps://api.github.com/users/Neoanarika/starre...\nhttps://api.github.com/users/Neoanarika/subscr...\nhttps://api.github.com/users/Neoanarika/orgs\nhttps://api.github.com/users/Neoanarika/repos\nhttps://api.github.com/users/Neoanarika/events...\nhttps://api.github.com/users/Neoanarika/receiv...\nUser\nFalse\n184"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Fetch all PRs from a repo",
    "text": "Fetch all PRs from a repo\n\npage_range = range(1, 6)\nget_pr_df = lambda page: pd.read_json(f\"https://api.github.com/repos/probml/pyprobml/pulls?state=all&per_page=100&page={page}\")\npull_requests = pd.concat(map(get_pr_df, page_range))\nprint(f\"Number of PRs: {len(pull_requests)}\")\npull_requests.head(2)\n\nNumber of PRs: 497\n\n\n\n  \n    \n      \n\n\n\n\n\n\nurl\nid\nnode_id\nhtml_url\ndiff_url\npatch_url\nissue_url\nnumber\nstate\nlocked\n...\nreview_comments_url\nreview_comment_url\ncomments_url\nstatuses_url\nhead\nbase\n_links\nauthor_association\nauto_merge\nactive_lock_reason\n\n\n\n\n0\nhttps://api.github.com/repos/probml/pyprobml/p...\n938329819\nPR_kwDOA-3vB8437cbb\nhttps://github.com/probml/pyprobml/pull/841\nhttps://github.com/probml/pyprobml/pull/841.diff\nhttps://github.com/probml/pyprobml/pull/841.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n841\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:posrprocessing', 'ref': ...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n1\nhttps://api.github.com/repos/probml/pyprobml/p...\n938317389\nPR_kwDOA-3vB8437ZZN\nhttps://github.com/probml/pyprobml/pull/840\nhttps://github.com/probml/pyprobml/pull/840.diff\nhttps://github.com/probml/pyprobml/pull/840.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n840\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:master', 'ref': 'master'...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n\n\n2 rows √ó 36 columns"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "title": "Get a list of contributors from a repo",
    "section": "Get a list of contributors sorted by count of PRs",
    "text": "Get a list of contributors sorted by count of PRs\n\npull_requests['login'] = pull_requests['user'].apply(lambda x: x[\"login\"])\nsorted_by_pr_count = pull_requests.groupby(\"login\").agg({'url': len}).sort_values(by='url', ascending=False)\nsorted_by_pr_count.rename(columns={'url': 'Number of PRs'}, inplace=True)\nsorted_by_pr_count.head(5)\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of PRs\n\n\nlogin\n\n\n\n\n\nDrishttii\n79\n\n\ngerdm\n55\n\n\nkaralleyna\n43\n\n\nalways-newbie161\n29\n\n\nkarm-patel\n29"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "title": "Get a list of contributors from a repo",
    "section": "Create a dashboard",
    "text": "Create a dashboard\n\ndef get_href_user(user):\n  username, profile_link = user.split(\"|\")\n  return f\"[{username}]({profile_link})\"\n\ndashboard = pd.DataFrame(index=sorted_by_pr_count.index)\ndashboard[\"Avatar\"] = contributors.avatar_url.apply(lambda url: f'&lt;img width=\"25\" alt=\"image\" src=\"{url}\"&gt;')\ndashboard[\"Contributor\"] = (contributors.index +\"|\"+ contributors['html_url']).apply(get_href_user)\ndashboard[\"Number of PRs\"] = sorted_by_pr_count[\"Number of PRs\"]\nprint(dashboard.dropna().T.to_markdown())\n\n|               | Drishttii                                                                               | gerdm                                                                                  | karalleyna                                                                              | always-newbie161                                                                        | karm-patel                                                                              | Duane321                                                                                | Nirzu97                                                                                 | patel-zeel                                                                              | animesh-007                                                                             | ashishpapanai                                                                           | shivaditya-meduri                                                                       | Neoanarika                                                                             | andrewnc                                                                               | nappaillav                                                                              | Abdelrahman350                                                                          | mjsML                                                                                  | jdf22                                                                                  | kzymgch                                                                                 | nalzok                                                                                  | nitish1295                                                                              | Garvit9000c                                                                             | AnkitaKumariJain14                                                                      | rohit-khoiwal-30                                                                        | shobro                                                                                  | raymondyeh07                                                                           | khanshehjad                                                                             | alenm10                                                                                 | firatoncel                                                                             | AnandShegde                                                                             | Aadesh-1404                                                                             | nealmcb                                                                               | nipunbatra                                                                           | petercerno                                                                             | posgnu                                                                                  | mvervuurt                                                                              | hieuza                                                                                 | Prahitha                                                                                | TripleTop                                                                               | UmarJ                                                                                   | Vishal987595                                                                            | a-fakhri                                                                                | adamnemecek                                                                           | galv                                                                                   | jlh2018                                                                                 | krasserm                                                                              | yuanx749                                                                                |\n|:--------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|\n| Avatar        | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/35187749?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4108759?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/36455180?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/66471669?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59387624?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/19956442?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/28842790?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59758528?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/53366877?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/52123364?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/77324692?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5188337?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7716402?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/43855961?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47902062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7131192?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1637094?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/10054419?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/13443062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/21181046?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68856476?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/62535006?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/87682045?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/54628243?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5696982?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/31896767?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/42214173?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/9141211?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/79975787?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68186100?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/119472?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/60985?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1649209?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/30136201?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/6399881?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1021144?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/44160152?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/48208522?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/34779641?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/97757583?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/65111198?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/182415?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4767568?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/40842099?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/202907?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47032563?v=4\"&gt; |\n| Contributor   | [Drishttii](https://github.com/Drishttii)                                               | [gerdm](https://github.com/gerdm)                                                      | [karalleyna](https://github.com/karalleyna)                                             | [always-newbie161](https://github.com/always-newbie161)                                 | [karm-patel](https://github.com/karm-patel)                                             | [Duane321](https://github.com/Duane321)                                                 | [Nirzu97](https://github.com/Nirzu97)                                                   | [patel-zeel](https://github.com/patel-zeel)                                             | [animesh-007](https://github.com/animesh-007)                                           | [ashishpapanai](https://github.com/ashishpapanai)                                       | [shivaditya-meduri](https://github.com/shivaditya-meduri)                               | [Neoanarika](https://github.com/Neoanarika)                                            | [andrewnc](https://github.com/andrewnc)                                                | [nappaillav](https://github.com/nappaillav)                                             | [Abdelrahman350](https://github.com/Abdelrahman350)                                     | [mjsML](https://github.com/mjsML)                                                      | [jdf22](https://github.com/jdf22)                                                      | [kzymgch](https://github.com/kzymgch)                                                   | [nalzok](https://github.com/nalzok)                                                     | [nitish1295](https://github.com/nitish1295)                                             | [Garvit9000c](https://github.com/Garvit9000c)                                           | [AnkitaKumariJain14](https://github.com/AnkitaKumariJain14)                             | [rohit-khoiwal-30](https://github.com/rohit-khoiwal-30)                                 | [shobro](https://github.com/shobro)                                                     | [raymondyeh07](https://github.com/raymondyeh07)                                        | [khanshehjad](https://github.com/khanshehjad)                                           | [alenm10](https://github.com/alenm10)                                                   | [firatoncel](https://github.com/firatoncel)                                            | [AnandShegde](https://github.com/AnandShegde)                                           | [Aadesh-1404](https://github.com/Aadesh-1404)                                           | [nealmcb](https://github.com/nealmcb)                                                 | [nipunbatra](https://github.com/nipunbatra)                                          | [petercerno](https://github.com/petercerno)                                            | [posgnu](https://github.com/posgnu)                                                     | [mvervuurt](https://github.com/mvervuurt)                                              | [hieuza](https://github.com/hieuza)                                                    | [Prahitha](https://github.com/Prahitha)                                                 | [TripleTop](https://github.com/TripleTop)                                               | [UmarJ](https://github.com/UmarJ)                                                       | [Vishal987595](https://github.com/Vishal987595)                                         | [a-fakhri](https://github.com/a-fakhri)                                                 | [adamnemecek](https://github.com/adamnemecek)                                         | [galv](https://github.com/galv)                                                        | [jlh2018](https://github.com/jlh2018)                                                   | [krasserm](https://github.com/krasserm)                                               | [yuanx749](https://github.com/yuanx749)                                                 |\n| Number of PRs | 79                                                                                      | 55                                                                                     | 43                                                                                      | 29                                                                                      | 29                                                                                      | 29                                                                                      | 25                                                                                      | 23                                                                                      | 18                                                                                      | 17                                                                                      | 16                                                                                      | 10                                                                                     | 10                                                                                     | 10                                                                                      | 8                                                                                       | 7                                                                                      | 7                                                                                      | 6                                                                                       | 6                                                                                       | 5                                                                                       | 4                                                                                       | 4                                                                                       | 3                                                                                       | 3                                                                                       | 2                                                                                      | 2                                                                                       | 2                                                                                       | 2                                                                                      | 2                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                    | 1                                                                                      | 1                                                                                       | 1                                                                                      | 1                                                                                      | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                      | 1                                                                                       | 1                                                                                     | 1                                                                                       |"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html",
    "href": "posts/2022-10-21-gaussian-processes.html",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\n\nfrom tinygp.kernels import ExpSquared\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#regression",
    "href": "posts/2022-10-21-gaussian-processes.html#regression",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Regression",
    "text": "Regression\nIn this post, we will consider the regression problem of finding a reasonable map \\(X \\to \\boldsymbol{y}\\) along with uncertainty. We can do this in a simplest setting with Bayesian linear regression assuming a MultiVariate Normal (MVN) prior \\(\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\theta, \\Sigma_\\theta)\\) (why MVN? because \\(\\theta \\in (-\\infty, \\infty)\\)) and Normal likelihood \\(y \\sim \\mathcal{N}(\\boldsymbol{x}^T\\theta, \\sigma_n^2)\\) with i.i.d. assumption.\nTo start with Gaussian process regression, let us first focus on \\(\\boldsymbol{y}\\) (and ignore \\(X\\)). We assume \\(\\boldsymbol{f}\\) as a random variable and \\(\\boldsymbol{y}\\) as a realization of \\(\\boldsymbol{f}\\) with some noise. It would be a natural probabilistic assumption to assume \\(\\boldsymbol{f}\\) to be MVN distributed since its range is \\((-\\infty, \\infty)\\).\n\\[\np(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\n\\tag{prior}\n\\]\nNow, we need to bring in \\(X\\) in a reasonable way to this formulation. A core assumption connecting \\(X\\) with \\(\\boldsymbol{y}\\) is the following: &gt; if two inputs \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{x}'\\) are close to each other (how to define the closeness? kernels!), corresponding \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{y}'\\) are likely to be similar.\nWe use something known as covariance function or kernel (later is more prevalent) to define this closeness. For example, RBF or squared exponential is a well-known kernel:\n\\[\nk_{RBF}(\\boldsymbol{x}, \\boldsymbol{x}') = \\sigma^2 \\exp \\left(-{\\frac {\\|\\boldsymbol{x} -\\boldsymbol{x}' \\|^{2}}{2\\ell ^{2}}}\\right)\n\\tag{kernel}\n\\]\n\nx = jnp.array(0.0).reshape(1, 1)\nx_prime = jnp.linspace(-5,5,100).reshape(-1, 1)\n\nplt.plot(x_prime, ExpSquared()(x_prime, x));\nplt.xlabel(\"$x'$\")\nplt.title(f\"$k(x,x')$ where $x={x[0][0]}$ and $x' \\in ${plt.xlim()}\");\n\n\n\n\n\n\n\n\nThe plot above shows that value of \\(k(\\boldsymbol{x}, \\boldsymbol{x}')\\) increases as \\(\\boldsymbol{x}'\\) approaches \\(\\boldsymbol{x}\\) and reduces as it moves far from \\(\\boldsymbol{x}\\). Now, we will connect \\(X\\) with \\(\\boldsymbol{f}\\) (and thus with \\(\\boldsymbol{y}\\)) through kernel \\(k\\) with two following assumptions:\n\nDiagonal entries of \\(K_{ff}\\) represent variance of \\(f_i\\), which can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_i)\\).\nNon-diagonal entries of \\(K_{ff}\\) represent covariance between \\(f_i\\) and \\(f_j\\) and can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\).\n\nAt this point, we have made everything clear about prior \\(p(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\\). Now, we will look at the likelihood. As mentioned earlier, \\(\\boldsymbol{y}\\) is noisy realization of \\(f\\) so the following likelihood would be a simple and natural choice.\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{f}, \\sigma_n^2I)\n\\tag{likelihood}\n\\]\nTill now, we followed bottom-up approach and defined prior and likelihood for this problem. Now we will explore the top-down approach.\nOur ultimate goal is derive \\(p(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X)\\) at new inputs \\(X^*\\). This can be written as:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) = \\int p(\\boldsymbol{y}^*|\\boldsymbol{f}^*)p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)d\\boldsymbol{f}^*\n\\tag{pred post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) is the posterior distribution at inputs \\(X^*\\). Once we derive posterior \\(p(\\boldsymbol{f}|\\boldsymbol{y},X)\\), We can find \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) like following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) = \\int p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)p(\\boldsymbol{f}|\\boldsymbol{y}, X)d\\boldsymbol{f}\n\\tag{post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)\\) is a conditional Gaussian distribution with the following closed form:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{f}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*})\n\\tag{cond}\n\\]\nPosterior \\(p(\\boldsymbol{f}|\\boldsymbol{y}, X)\\) can be derived following ‚ÄúBayes‚Äô rule for Gaussians‚Äù (section 2.2.6.2 in pml book2):\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff})\n\\tag{post}\n\\]\nWe can now substitute Eq. (post) and Eq. (cond) in Eq. (post new). The integral can be solved with using Eq. 2.90 in section 2.2.6.2 in pml book2 and also mentioned in Eq. (int gaussians) in Appendix.\n\\[\n\\begin{aligned}\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{\\mu}^*, \\Sigma^*)\\\\\n\\boldsymbol{\\mu}^* &= \\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\left[\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\right]-\\boldsymbol{m}_{f})\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f))\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\\\\n\\\\\n\\Sigma^* &= K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}\\left[K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[I - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[K_{ff}^{-1} - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}\\right]K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}K_{ff^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*})\n\\end{aligned}\n\\]\nNow, we are almost there. Plugging in the above formula in Eq. (pred post) and using known result in Eq. (int gaussians), we get the predictive posterior as following:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*} + \\sigma_n^2I)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe did not exploit the special structure of likelihood variance \\(\\sigma_n^2I\\) anywhere, so, these derivations hold true for full rank likelihood covariance matrices also.\n\n\n\nOptimization\nWe perform type-II likelihood estimation (in other words, minimize log marginal likelihood or evidence term). Our goal is to find optimal model \\(\\mathcal{M}\\) represented by prior (or kernel) hyperparameters and likelihood hyperparameters. We can get the log marginal likelihood using Eq. (int gaussians):\n\\[\n\\begin{aligned}\np(\\boldsymbol{y}|X, \\mathcal{M}) &= \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\\\\n&\\sim \\int \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{f}, \\sigma_n^2I) \\mathcal{N}(\\boldsymbol{f}|\\boldsymbol{m}_f, K_{ff})\\\\\n&\\sim \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{m}_f, K_{ff}+\\sigma_n^2I)\n\\end{aligned}\n\\]\nFor case of RBF kernel, \\(\\mathcal{M}\\) parameters will be \\(\\{\\sigma, \\ell, \\sigma_n\\}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "href": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Classification (with Laplace approximation)",
    "text": "Classification (with Laplace approximation)\nWe will derive a GP predictive posterior for binary case only because for multi-class, it gets a bit complex. Our assumption for prior over the \\(\\boldsymbol{f}\\) can still be the same but likelihood needs to be changed because \\(\\boldsymbol{y}\\) is no more a real number but rather a binary value e.g.¬†0 or 1. From Bayesian point-of-view, Bernoulli likelihood would be the most appropriate as a likelihood here:\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) = \\prod_{i=1}^{N} \\sigma(f_i)^{y_i=1}(1-\\sigma(f_i))^{y_i=0}\n\\tag{class likelihood}\n\\]\nSince, MVN prior and Bernoulli likelihood are not conjugate, we need to use an approximate method of inference here. We use Laplace approximation to get the MAP estimate \\(\\boldsymbol{\\hat{f}}\\) and by computing the Hessian \\(H\\) of negative log joint (log prior + log likelihood) with respect to \\(\\boldsymbol{\\hat{f}}\\), we can get the posterior distribution as the following:\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{\\hat{f}}, H^{-1})\n\\tag{class post}\n\\]\nEq. (cond) will be the same in this case, and thus, we can solve Eq. (post new) as we did for regression case, like the following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{\\hat{f}}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}H^{-1}K_{ff}^{-1}K_{ff^*})\n\\]\n\nOptimization\nTo perform Type-II likelihood estimation for binary classification, we first need to derive the log marginal likelihood which can be approximated with Laplace approximation. First, we define the following quantity:\n\\[\n\\boldsymbol{\\psi}(\\boldsymbol{f}) \\triangleq \\log p(\\boldsymbol{y}|\\boldsymbol{f}) + \\log p(\\boldsymbol{f})\n\\]\nNow, computing the log marginal likelihood as suggested in section 3.4.4 of GPML book:\n\\[\n\\begin{aligned}\n\\log p(\\boldsymbol{y}|X, \\mathcal{M}) &\\sim \\log \\left[ \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\right]\\\\\n&= \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{f})\\right)d\\boldsymbol{f} \\right]\\\\\n&\\thickapprox \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) -\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f} \\right]\\\\\n&= \\log \\left[ \\exp \\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) \\int exp\\left(-\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f}\\right]\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) + \\log p(\\boldsymbol{\\hat{f}}) - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}|\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) -\\frac{1}{2}\\boldsymbol{\\hat{f}}^TK_{ff}^{-1}\\boldsymbol{\\hat{f}} - \\frac{1}{2}\\log|K_{ff}| - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}| - \\frac{N}{2}\\log(2\\pi)\n\\end{aligned}\n\\]\nOur final optimization algorithm would be as following: 1. For N iterations do 2. to 4. 2. Optimize for \\(\\boldsymbol{\\hat{f}}\\) with M iterations using standard MAP estimation (maybe use non-centered parametrization). 3. Compute gradient of parameters of \\(\\mathcal{M}\\) w.r.t. log marginal likelihood 4. Update parameters of \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#appendix",
    "href": "posts/2022-10-21-gaussian-processes.html#appendix",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Appendix",
    "text": "Appendix\n\\[\n\\int \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{x}+\\boldsymbol{b}, \\Sigma) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}, K) = \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{\\mu}+b, WKW^T+\\Sigma)\n\\tag{int gaussians}\n\\]"
  },
  {
    "objectID": "posts/climate-modeling-with-siren.html",
    "href": "posts/climate-modeling-with-siren.html",
    "title": "Climate Modeling with SIRENs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\nimport numpy as np\nimport xarray as xr\nfrom tqdm.keras import TqdmCallback\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, initializers, activations\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\n\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-07-18 05:13:52.439735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-18 05:13:53.232689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    first_init = lambda input_dim: initializers.RandomUniform(-1 / input_dim, 1 / input_dim)\n    other_init = lambda input_dim: initializers.RandomUniform(-np.sqrt(6 / input_dim) / activation_scale, np.sqrt(6 / input_dim) / activation_scale)\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=first_init(input_dim), activation=lambda x: tf.sin(activation_scale*x)))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=other_init(features[i-1]), activation=lambda x: tf.sin(activation_scale*x)))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=other_init(features[-1]), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../../super_res/data/era5_low_res/2m_temperature/2m_temperature_2018_5.625deg.nc\")\ndata1 = xr.open_dataset(\"../../super_res/data/era5_high_res/2m_temperature/2m_temperature_2018_1.40625deg.nc\")\n\n\ndata5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lon: 64, lat: 32, time: 8760)\nCoordinates:\n  * lon      (lon) float64 0.0 5.625 11.25 16.88 ... 337.5 343.1 348.8 354.4\n  * lat      (lat) float64 -87.19 -81.56 -75.94 -70.31 ... 75.94 81.56 87.19\n  * time     (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00\nData variables:\n    t2m      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  CF-1.6\n    history:      2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...xarray.DatasetDimensions:lon: 64lat: 32time: 8760Coordinates: (3)lon(lon)float640.0 5.625 11.25 ... 348.8 354.4array([  0.   ,   5.625,  11.25 ,  16.875,  22.5  ,  28.125,  33.75 ,  39.375,\n        45.   ,  50.625,  56.25 ,  61.875,  67.5  ,  73.125,  78.75 ,  84.375,\n        90.   ,  95.625, 101.25 , 106.875, 112.5  , 118.125, 123.75 , 129.375,\n       135.   , 140.625, 146.25 , 151.875, 157.5  , 163.125, 168.75 , 174.375,\n       180.   , 185.625, 191.25 , 196.875, 202.5  , 208.125, 213.75 , 219.375,\n       225.   , 230.625, 236.25 , 241.875, 247.5  , 253.125, 258.75 , 264.375,\n       270.   , 275.625, 281.25 , 286.875, 292.5  , 298.125, 303.75 , 309.375,\n       315.   , 320.625, 326.25 , 331.875, 337.5  , 343.125, 348.75 , 354.375])lat(lat)float64-87.19 -81.56 ... 81.56 87.19array([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875])time(time)datetime64[ns]2018-01-01 ... 2018-12-31T23:00:00long_name :timearray(['2018-01-01T00:00:00.000000000', '2018-01-01T01:00:00.000000000',\n       '2018-01-01T02:00:00.000000000', ..., '2018-12-31T21:00:00.000000000',\n       '2018-12-31T22:00:00.000000000', '2018-12-31T23:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)t2m(time, lat, lon)float32...units :Klong_name :2 metre temperature[17940480 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([    0.0,   5.625,   11.25,  16.875,    22.5,  28.125,   33.75,  39.375,\n          45.0,  50.625,   56.25,  61.875,    67.5,  73.125,   78.75,  84.375,\n          90.0,  95.625,  101.25, 106.875,   112.5, 118.125,  123.75, 129.375,\n         135.0, 140.625,  146.25, 151.875,   157.5, 163.125,  168.75, 174.375,\n         180.0, 185.625,  191.25, 196.875,   202.5, 208.125,  213.75, 219.375,\n         225.0, 230.625,  236.25, 241.875,   247.5, 253.125,  258.75, 264.375,\n         270.0, 275.625,  281.25, 286.875,   292.5, 298.125,  303.75, 309.375,\n         315.0, 320.625,  326.25, 331.875,   337.5, 343.125,  348.75, 354.375],\n      dtype='float64', name='lon'))latPandasIndexPandasIndex(Index([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875],\n      dtype='float64', name='lat'))timePandasIndexPandasIndex(DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',\n               '2018-01-01 02:00:00', '2018-01-01 03:00:00',\n               '2018-01-01 04:00:00', '2018-01-01 05:00:00',\n               '2018-01-01 06:00:00', '2018-01-01 07:00:00',\n               '2018-01-01 08:00:00', '2018-01-01 09:00:00',\n               ...\n               '2018-12-31 14:00:00', '2018-12-31 15:00:00',\n               '2018-12-31 16:00:00', '2018-12-31 17:00:00',\n               '2018-12-31 18:00:00', '2018-12-31 19:00:00',\n               '2018-12-31 20:00:00', '2018-12-31 21:00:00',\n               '2018-12-31 22:00:00', '2018-12-31 23:00:00'],\n              dtype='datetime64[ns]', name='time', length=8760, freq=None))Attributes: (2)Conventions :CF-1.6history :2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmwf/eccodes/bin/grib_to_netcdf -o /cache/data2/adaptor.mars.internal-1573035683.1772008-2550-1-601d5659-dae2-45e1-902b-45825d30e8d0.nc /cache/tmp/601d5659-dae2-45e1-902b-45825d30e8d0-adaptor.mars.internal-1573035683.1790879-2550-1-tmp.grib\n\n\n\ntime_stamp = slice(\"2018-01\", \"2018-03\")\ntrain_df = data5.sel(time=time_stamp).to_dataframe().reset_index()\ntest_df = data1.sel(time=time_stamp).to_dataframe().reset_index()\n\nX = np.stack([train_df.lat.values, train_df.lon.values, train_df.time.astype(np.int64) / 10**9], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values, test_df.time.astype(np.int64) / 10**9], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\n# rff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(4423680, 3), y.shape=(4423680, 1)\nX_test.shape=(70778880, 3), y_test.shape=(70778880, 1)\nX.shape=(4423680, 3), X_test.shape=(70778880, 3)\n\n\n\n32*64*24*(31+28+31)\n\n4423680\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\n# Scaling time\nif X.shape[1] == 3:\n    X_scaled[:, 2] = X_scaled[:, 2] * 10 - 5\n    X_test_scaled[:, 2] = X_test_scaled[:, 2] * 10 - 5\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = SIREN(3, 1, [256]*4, 30.0, 0.0)\n# model = MLP(3, 1, [256]*4, 0.0)\n# model = ResNet()S\n# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-3,\n#     maximal_learning_rate=1e-2,\n#     scale_fn=lambda x: 1/(2.**(x-1)),\n#     step_size=2\n# )\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n2023-07-18 05:14:34.531498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n2023-07-18 05:14:34.531583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78884 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n\n\n\n0.00148\n\n0.00148\n\n\n\ncallbacks = [TqdmCallback(verbose=1)]\nhistory = model.fit(X_scaled, y_scaled, epochs=1000, batch_size=X_scaled.shape[0], verbose=0, callbacks=callbacks)\n\n  0%|          | 0/1000 [00:00&lt;?, ?epoch/s]2023-07-18 05:14:38.677299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n2023-07-18 05:14:39.357828: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fb81b946130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2023-07-18 05:14:39.357901: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2023-07-18 05:14:39.363158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-07-18 05:14:40.399794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n2023-07-18 05:14:40.557542: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [04:46&lt;00:00,  3.49epoch/s, loss=0.000295]\n\n\n\nplt.plot(history.history['loss'][200:], label='loss');\n# plt.plot(history.history['val_loss'][200:], label='val_loss');\nplt.legend();\n\n\n\n\n\n\n\n\n\n128*256*24\n\n786432\n\n\n\nimg_index = 0\ny_pred = model.predict(X_test_scaled, batch_size=20480) * (y_max - y_min) + y_min\nprint(y_pred.shape)\nplt.imshow(y_pred[img_index*(256*128):(img_index+1)*(256*128)].reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n3456/3456 [==============================] - 5s 1ms/step\n(70778880, 1)\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\ndef get_lat_weights(lat):\n    lat_weights = np.cos(np.deg2rad(lat))\n    lat_weights = lat_weights / lat_weights.mean()\n    return lat_weights\n\nlat_weights = get_lat_weights(X_test[:, 0])\nprint(f\"{lat_weights.shape=}\")\n\nlat_squared_error = lat_weights * (y_pred.ravel() - y_test.ravel())**2\nlat_rmse = np.sqrt(lat_squared_error.mean())\nprint(f\"{lat_rmse=}\")\n# y_pred.shape, lat_weights.shape\n\nlat_weights.shape=(70778880,)\nlat_rmse=2.6118446730600438\n\n\n\n# lat_rmse=3.4826956884024356\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")"
  },
  {
    "objectID": "posts/py_over_ipynb.html",
    "href": "posts/py_over_ipynb.html",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "",
    "text": "I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files:"
  },
  {
    "objectID": "posts/py_over_ipynb.html#fewer-errors",
    "href": "posts/py_over_ipynb.html#fewer-errors",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Fewer Errors",
    "text": "Fewer Errors\n\n.py files are easier to debug with a VS code like IDE, making it easier to find the errors.\nExecution of .py starts fresh, unlike some left out variables silently getting carried over from the last execution/deleted cells in .ipynb files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "href": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Better Usage of a Shared Server",
    "text": "Better Usage of a Shared Server\n\n.py files release the resources (e.g., GPU memory) once executed. It could be inconvenient to repeatedly remind or be reminded by someone to release the resources manually from a Jupyter notebook."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-productivity",
    "href": "posts/py_over_ipynb.html#increased-productivity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Productivity",
    "text": "Increased Productivity\n\nYou can make use of fantastic auto-complete, syntax-highlighting extensions in VS code to save a lot of time while working with .py files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#boost-collaboration",
    "href": "posts/py_over_ipynb.html#boost-collaboration",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Boost Collaboration",
    "text": "Boost Collaboration\n\n.py do not take time to render on GitHub because they are just plain text files, unlike .ipynb files.\nIt is a lot easier to see the changes made by others in a .py file than a .ipynb file."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-modularity",
    "href": "posts/py_over_ipynb.html#increased-modularity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Modularity",
    "text": "Increased Modularity\n\nFunction and Class calls from other files are seamless with .py files.\n\nFeel free to comment your views/suggestions/additions in the comment box."
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html",
    "href": "posts/2021-10-12-sparsegps.html",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#imports",
    "href": "posts/2021-10-12-sparsegps.html#imports",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#data-preperation",
    "href": "posts/2021-10-12-sparsegps.html#data-preperation",
    "title": "SparseGPs in Stheno",
    "section": "Data preperation",
    "text": "Data preperation\n\n# Define points to predict at.\nx = B.linspace(0, 10, 100)\nx_obs = B.linspace(0, 7, 50_000)\nx_ind = B.linspace(0, 10, 20)\n\n# Construct a prior.\nf = GP(EQ().periodic(2 * B.pi))\n\n# Sample a true, underlying function and observations.\nf_true = B.sin(x)\ny_obs = B.sin(x_obs) + B.sqrt(0.5) * B.randn(*x_obs.shape)"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#plotting-function",
    "href": "posts/2021-10-12-sparsegps.html#plotting-function",
    "title": "SparseGPs in Stheno",
    "section": "Plotting function",
    "text": "Plotting function\n\ndef plot(method):\n    if method == 'VFE':\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            obs.mu(f.measure)[:, 0],\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()\n    else:\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            B.dense(f_post(x_ind).mean),\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "title": "SparseGPs in Stheno",
    "section": "Sparse regression with Variational Free Energy (VFE) method",
    "text": "Sparse regression with Variational Free Energy (VFE) method\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsVFE(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('VFE')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "title": "SparseGPs in Stheno",
    "section": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod",
    "text": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsFITC(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('FITC')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "href": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "title": "SparseGPs in Stheno",
    "section": "Hyperparameter tuning (Noisy Sine data)",
    "text": "Hyperparameter tuning (Noisy Sine data)\n\ndef model(vs):\n    \"\"\"Constuct a model with learnable parameters.\"\"\"\n    return vs['variance']*GP(EQ().stretch(vs['length_scale']))\n\n\ntorch.manual_seed(123)\n\ndataObj = rd.SineNoisy(scale_X=False, scale_y=False, return_test=True, backend='torch')\nx_obs, y_obs, x = dataObj.get_data()\n\n\nplt.scatter(x_obs, y_obs, s=2);\n\n\n\n\n\n\n\n\n\nVFE\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsVFE(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nFITC\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsFITC(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html",
    "href": "posts/2024-12-29-object-detection-how-to.html",
    "title": "Object Detection - A how-to guide",
    "section": "",
    "text": "# Config\nimport os\n\n# Basic\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport matplotlib.pyplot as plt\n\n# Monitoring\nfrom tqdm.notebook import tqdm\n\n# IO\nfrom os.path import join, exists, basename, dirname, splitext, expanduser\nfrom glob import glob\n\n# Parallel processing\nfrom joblib import Parallel, delayed\n\nimport yaml\nfrom PIL import Image\nimport supervision as sv\nimport cv2\nfrom supervision.utils.file import list_files_with_extensions, read_txt_file\nfrom supervision.detection.utils import polygon_to_xyxy\nfrom ultralytics import YOLO\nfrom ultralytics.utils.downloads import download\nfrom pathlib import Path\nfrom inference.models.utils import get_roboflow_model\nfrom roboflow import Roboflow\nfrom typing import List, Tuple\nfrom dotenv import load_dotenv\nload_dotenv()\n\n%reload_ext memory_profiler\n\nsv.__version__\n\n'0.25.1'"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#dataset",
    "href": "posts/2024-12-29-object-detection-how-to.html#dataset",
    "title": "Object Detection - A how-to guide",
    "section": "Dataset",
    "text": "Dataset\n\nDownload\n\nrf = Roboflow(api_key=os.getenv(\"ROBOFLOW_API_KEY\"))\nws = rf.workspace(\"plan-zkend\")\nproject = ws.project(\"animals-ksxhf-plgrl\")\nversion = project.version(2)\nrf_dataset = version.download(\"yolov8\", location=\"/tmp/tmp\", overwrite=True)\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\nDownloading Dataset Version Zip in /tmp/tmp to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3047/3047 [00:02&lt;00:00, 1202.83it/s]\n\n\n\n\n\n\nExtracting Dataset Version Zip to /tmp/tmp in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [00:00&lt;00:00, 5963.05it/s]\n\n\n\nrf_dataset.location\n\n'/tmp/tmp'\n\n\n\n!ls -lh {rf_dataset.location}\n\ntotal 24K\n-rw-rw-r-- 1 patel_zeel patel_zeel  423 Feb  3 11:40 data.yaml\n-rw-rw-r-- 1 patel_zeel patel_zeel  141 Feb  3 11:40 README.dataset.txt\n-rw-rw-r-- 1 patel_zeel patel_zeel  896 Feb  3 11:40 README.roboflow.txt\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 18:47 test\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 18:47 train\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Jan 17 22:52 valid\n\n\n\n!ls -lh {rf_dataset.location}/test\n\ntotal 8.0K\ndrwxrwxr-x 2 patel_zeel patel_zeel 4.0K Jan 17 18:47 images\ndrwxrwxr-x 2 patel_zeel patel_zeel 4.0K Jan 17 18:47 labels\n\n\n\n!ls -l {rf_dataset.location}/test/images/*.jpg | wc -l\n\n7\n\n\n\n!ls -l {rf_dataset.location}/test/labels/*.txt | wc -l\n\n7\n\n\n\nCheck a sample manually\n\nimage_paths = glob(f\"{rf_dataset.location}/test/images/*.jpg\")\nsample_image_path = image_paths[0]\nsample_image = Image.open(sample_image_path)\nsample_image\n\n\n\n\n\n\n\n\n\nlabel_paths = glob(f\"{rf_dataset.location}/test/labels/*.txt\")\nsample_label_path = label_paths[0]\nsample_label = np.loadtxt(sample_label_path, ndmin=2)\nsample_label.shape\n\n(1, 5)\n\n\n\nsample_label\n\narray([[         18,     0.67217,     0.47797,     0.53625,     0.51148]])\n\n\n\n\n\nLoad with supervision\n\n%%time\n\ndataset = sv.DetectionDataset.from_yolo(images_directory_path=f\"{rf_dataset.location}/test/images\", annotations_directory_path=f\"{rf_dataset.location}/test/labels\", data_yaml_path=f\"{rf_dataset.location}/data.yaml\")\nlen(dataset)\n\nCPU times: user 11.6 ms, sys: 0 ns, total: 11.6 ms\nWall time: 10.6 ms\n\n\n7\n\n\n\n\nVisualize\nIdeally, LabelAnnotator should show the class names on top of the bounding boxes but currently it shows class IDs. This issue is tracked here.\n\nimage_path, image, detection = dataset[0]\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_frame = box_annotator.annotate(image.copy(), detection)\nannotated_frame = label_annotator.annotate(annotated_frame.copy(), detection)\n\nImage.fromarray(annotated_frame)\n\n\n\n\n\n\n\n\nA quick fix for now.\n\nimage_path, image, detection = dataset[0]\nnp_classes = np.array(dataset.classes)\ndetection.data['class_name'] = np_classes[detection.class_id]\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_frame = box_annotator.annotate(image.copy(), detection)\nannotated_frame = label_annotator.annotate(annotated_frame.copy(), detection)\n\nImage.fromarray(annotated_frame)"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#inference",
    "href": "posts/2024-12-29-object-detection-how-to.html#inference",
    "title": "Object Detection - A how-to guide",
    "section": "Inference",
    "text": "Inference\n\nWith roboflow models\n\nrf_model = get_roboflow_model(\"yolov8s-640\")\nprediction = rf_model.infer(image)[0]\ndetection = sv.Detections.from_inference(prediction)\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\nImage.fromarray(annotated_image)\n\nSpecified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\nSpecified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'TensorrtExecutionProvider, CUDAExecutionProvider, CPUExecutionProvider'\n2025-02-03 11:42:55.119414030 [E:onnxruntime:Default, provider_bridge_ort.cc:1862 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1539 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn_adv.so.9: cannot open shared object file: No such file or directory\n\n2025-02-03 11:42:55.119453180 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:993 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Require cuDNN 9.* and CUDA 12.*. Please install all dependencies as mentioned in the GPU requirements page (https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements), make sure they're in the PATH, and that your GPU is supported.\n\n\n\n\n\n\n\n\n\n\n\nWith ultralytics models\n\nmodel = YOLO(\"yolov8s\")\nprediction = model(image)[0]\ndetection = sv.Detections.from_ultralytics(prediction)\nannotated_image = box_annotator.annotate(image.copy(), detection)\nannotated_image = label_annotator.annotate(annotated_image, detection)\nImage.fromarray(annotated_image)\n\n\n0: 640x640 1 dog, 2 horses, 1 sheep, 1 cow, 6.3ms\nSpeed: 10.2ms preprocess, 6.3ms inference, 274.3ms postprocess per image at shape (1, 3, 640, 640)"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#metrics",
    "href": "posts/2024-12-29-object-detection-how-to.html#metrics",
    "title": "Object Detection - A how-to guide",
    "section": "Metrics",
    "text": "Metrics\n\nmodel = YOLO(\"yolov8x\")\ntargets = []\npredictions = []\nnp_classes = np.array(dataset.classes)\nfor image_path, image, target in tqdm(dataset):\n    # add class names to detection\n    # target.data['class_name'] = np_classes[target.class_id]\n\n    # remove classes not in model\n    # target = target[np.isin(target['class_name'], list(model.names.values()))]\n    # if len(target) == 0:\n    #     print(f\"Skipping {image_path} as it has no classes in model\")\n    #     continue\n    \n    prediction = model(image, verbose=False)[0]\n    detection = sv.Detections.from_ultralytics(prediction)\n    \n    # remove classes not in dataset\n    detection = detection[np.isin(detection['class_name'], dataset.classes)]\n    \n    # remap class ids\n    detection.class_id = np.array([dataset.classes.index(class_name) for class_name in detection['class_name']])\n    \n    targets.append(target)\n    predictions.append(detection)\n\n\n\n\n\nmAP = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\nmAP50 = mAP.mAP_scores[0]\nmAP5095 = mAP.mAP_scores.mean()\nprint(f\"mAP50: {mAP50:.2f}, mAP50-95: {mAP5095:.2f}\")\n\nmAP50: 0.29, mAP50-95: 0.19"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#dataset-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#dataset-1",
    "title": "Object Detection - A how-to guide",
    "section": "Dataset",
    "text": "Dataset\n\nDownload\n\nif not exists('/tmp/DOTAv1.zip'):\n    # Downloaded in 4m 5s with 6.09 MB/s\n    !wget https://github.com/ultralytics/assets/releases/download/v0.0.0/DOTAv1.zip -O /tmp/DOTAv1.zip\nelse:\n    print('DOTAv1.zip already downloaded')\n    \nif not exists('/tmp/DOTAv1'):\n    !unzip /tmp/DOTAv1.zip -d /tmp\nelse:\n    print('DOTAv1 already unzipped')\n\nDOTAv1.zip already downloaded\nDOTAv1 already unzipped\n\n\n\n!ls /tmp/DOTAv1\n\n21.84s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nimages  labels\n\n\n\n!ls /tmp/DOTAv1/images\n\n27.24s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\ntest  train  val\n\n\n\nprint(f\"Number of train samples: {len(glob('/tmp/DOTAv1/images/train/*.jpg'))}\")\nprint(f\"Number of val samples: {len(glob('/tmp/DOTAv1/images/val/*.jpg'))}\")\n\nNumber of train samples: 1411\nNumber of val samples: 458\n\n\nKeep 20 samples in each and delete the rest\n\npaths = {'images': {}, 'labels': {}}\n\nfor split in ['train', 'val']:\n    paths['images'][split] = glob(f\"/tmp/DOTAv1/images/{split}/*.jpg\")[:100]\n    paths['labels'][split] = [p.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\") for p in paths['images'][split]]\n    !mkdir -p /tmp/DOTAv1_small/images/{split}\n    !mkdir -p /tmp/DOTAv1_small/labels/{split}\n\n    for img, label in tqdm(zip(paths['images'][split], paths['labels'][split])):\n        os.system(f\"cp {img} /tmp/DOTAv1_small/images/{split}/\")\n        os.system(f\"cp {label} /tmp/DOTAv1_small/labels/{split}/\")\n\n32.81s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n38.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\n\n\n\n44.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n49.55s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\n\n\n\n\nprint(f\"Number of train samples: {len(glob('/tmp/DOTAv1_small/images/train/*.jpg'))}\")\nprint(f\"Number of val samples: {len(glob('/tmp/DOTAv1_small/images/val/*.jpg'))}\")\n\nNumber of train samples: 100\nNumber of val samples: 100\n\n\n\n\nCheck a sample\n\ntrain_images = glob(\"/tmp/DOTAv1_small/images/train/*.jpg\")\nsample_image_path = train_images[1]\nsample_image_path\n\n'/tmp/DOTAv1_small/images/train/P2732.jpg'\n\n\n\nsample_image = Image.open(sample_image_path)\nsample_image.size\n\n(3087, 2632)\n\n\n\nsample_image.reduce(10)\n\n\n\n\n\n\n\n\n\nsample_label_path = sample_image_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\nassert exists(sample_label_path), f\"Error: {sample_label_path} does not exist\"\n\n\nsample_label = np.loadtxt(sample_label_path, ndmin=2)\nsample_label.shape\n\n(51, 9)\n\n\n\nsample_label[0]\n\narray([          7,     0.72076,     0.45023,     0.72238,     0.44871,     0.73178,     0.46087,     0.73016,     0.46201])\n\n\nThe above is YOLO-Oriented Bounding Box (OBB) format: class_id, x1, y1, x2, y2, x3, y3, x4, y4\n\n\nLoad with supervision\nsupervision does not support DOTA dataset yet, but ultralytics has already converted it to YOLO format. Let‚Äôs create data.yml file for DOTA dataset.\n\n%%writefile /tmp/DOTAv1_small/data.yml\ntrain: /tmp/DOTAv1_small/images/train\nval: /tmp/DOTAv1_small/images/val\ntest: /tmp/DOTAv1_small/images/test\nnc: 15\nnames: ['plane', 'ship', 'storage tank', 'baseball diamond', 'tennis court', 'basketball court', 'ground track field', 'harbor', 'bridge', 'large vehicle', 'small vehicle', 'helicopter', 'roundabout', 'soccer ball field', 'swimming pool']\n\nOverwriting /tmp/DOTAv1_small/data.yml\n\n\n\n# %%memit\n\ndataset = sv.DetectionDataset.from_yolo(\n    \"/tmp/DOTAv1_small/images/train\",\n    \"/tmp/DOTAv1_small/labels/train\",\n    data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n    is_obb=True,\n)\n\n\n\nVisualize\n\nsample = dataset[0]\nimg_array = sample[1]\nimg_detections = sample[2]\n\nannotator = sv.OrientedBoxAnnotator()\nannotated_img = annotator.annotate(img_array, img_detections)\n\nplt.imshow(annotated_img)\nplt.ylim(0, annotated_img.shape[1] // 2)\nplt.axis(\"off\")"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#inference-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#inference-1",
    "title": "Object Detection - A how-to guide",
    "section": "Inference",
    "text": "Inference\n\niou = Non-max suppression IOU threshold\nconf = Object confidence threshold\n\n\nInline method\n\nmodel = YOLO(\"yolo11x-obb\")\n\ndetections = []\npredictions = []\nfor img_path, img, detection in tqdm(dataset):\n    prediction = model(img, imgsz=1024, iou=0.33, max_det=300, conf=0.001, verbose=False)[0]\n    predictions.append(sv.Detections.from_ultralytics(prediction))\n    detections.append(detection)\n\n\n\n\n\n\nCLI method\n\n!cd /tmp && yolo obb predict model=yolo11x-obb source=/tmp/DOTAv1_small/images/val exist_ok=True save=False save_txt=True imgsz=1024 iou=0.33 max_det=300 conf=0.001 verbose=False\n\nUltralytics 8.3.55 üöÄ Python-3.10.15 torch-2.5.0+cu124 CUDA:0 (NVIDIA A100-SXM4-80GB, 81156MiB)\nYOLO11x-obb summary (fused): 483 layers, 58,752,928 parameters, 0 gradients, 202.8 GFLOPs\nResults saved to runs/obb/predict\n20 labels saved to runs/obb/predict/labels\nüí° Learn more at https://docs.ultralytics.com/modes/predict"
  },
  {
    "objectID": "posts/2024-12-29-object-detection-how-to.html#metrics-1",
    "href": "posts/2024-12-29-object-detection-how-to.html#metrics-1",
    "title": "Object Detection - A how-to guide",
    "section": "Metrics",
    "text": "Metrics\n\nInline method\n\nConfusion matrix\n\nAt the time of writing this post, supervision‚Äôs ConfusionMatrix does not support OBB. Follow this issue for updates.\n\n\nconf_threshold ‚Äì minimum confidence threshold for a detection to be considered. Instances with confidence below this threshold are ignored as if they were not predicted.\niou_threshold ‚Äì minimum intersection over union (IoU) threshold for a detection to be considered a true positive. Predictions with IoU below this threshold are considered false positives.\n\n\ncm = sv.ConfusionMatrix.from_detections(\n    predictions, detections, classes=dataset.classes, conf_threshold=0.25, iou_threshold=0.33\n)\n_ = cm.plot()\n\n\n\n\n\n\n\n\n\n\nPrecision, Recall & F1 Score\nYou know the formulas of Precision and Recall.\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nWe can also write them as the following:\nPrecision = TP / PP\nRecall = TP / AP\nwhere PP is the number of predicted positives and AP is the number of actual positives.\nTo calculate TP, we can sum the values along the diagonal of the confusion matrix.\n\nTP = cm.matrix.diagonal().sum()\nTP\n\nnp.float64(847.0)\n\n\nTo calculate PP, we should remove all cells which represent ‚Äúnot predicted‚Äù instances. That is nothing but FN. Thus, we will remove the last column representing FN.\n\nPP = cm.matrix[:, :-1].sum()\nPP\n\nnp.float64(931.0)\n\n\nTo calculate AP, we should remove all cells which represent ‚Äúpredicted but wrong‚Äù instances. That is nothing but FP. Thus, we will remove the last row representing FP.\n\nAP = cm.matrix[:-1, :].sum()\nAP\n\nnp.float64(1059.0)\n\n\n\nP = TP / PP\nR = TP / AP\nF1 = 2 * P * R / (P + R)\nprint(f\"P: {P:.2f}, R: {R:.2f}, F1: {F1:.2f}\")\n\nP: 0.91, R: 0.80, F1: 0.85\n\n\nNotice that to compute P, R and F1, we need to fix a confidence threshold and an IOU threshold. Now, we will see some metrics which integrate over confidence thresholds and use only IoU threshold.\nThere are specific methods in supervision to compute Precision, Recall and F1 Score, but they are significantly slow. If they become faster in future, one can use them with the following code.\n\n# f1_score = sv.metrics.MeanAveragePrecision(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# f1_score.update(predictions, detections).compute()\n\n\n# precision = sv.metrics.Precision(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# precision.update(predictions, detections).compute()\n\n\n# recall = sv.metrics.Recall(sv.metrics.MetricTarget.ORIENTED_BOUNDING_BOXES)\n# recall.update(predictions, detections).compute()\n\n\n\n\nCLI method\nWhen we use CLI method of inference in Ultralytics, results are saved on the disk. Now, we need to load them back to calculate metrics.\n\nfrom ultralytics.engine.results import Results\n\nThe following method is extremely space consuming unless the following issue is resolved: https://github.com/roboflow/supervision/issues/1762. Not adding further steps until the issue is resolved but the steps should be similar to the axis-aligned bounding box case.\n\npredicted_dataset = sv.DetectionDataset.from_yolo(\n    \"/tmp/DOTAv1_small/images/val\",\n    \"/tmp/runs/obb/predict/labels\",\n    data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n    is_obb=True,\n)\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[78], line 1\n----&gt; 1 predicted_dataset = sv.DetectionDataset.from_yolo(\n      2     \"/tmp/DOTAv1_small/images/val\",\n      3     \"/tmp/runs/obb/predict/labels\",\n      4     data_yaml_path=\"/tmp/DOTAv1_small/data.yml\",\n      5     is_obb=True,\n      6 )\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/core.py:497, in DetectionDataset.from_yolo(cls, images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\n    445 @classmethod\n    446 def from_yolo(\n    447     cls,\n   (...)\n    452     is_obb: bool = False,\n    453 ) -&gt; DetectionDataset:\n    454     \"\"\"\n    455     Creates a Dataset instance from YOLO formatted data.\n    456 \n   (...)\n    495         ```\n    496     \"\"\"\n--&gt; 497     classes, image_paths, annotations = load_yolo_annotations(\n    498         images_directory_path=images_directory_path,\n    499         annotations_directory_path=annotations_directory_path,\n    500         data_yaml_path=data_yaml_path,\n    501         force_masks=force_masks,\n    502         is_obb=is_obb,\n    503     )\n    504     return DetectionDataset(\n    505         classes=classes, images=image_paths, annotations=annotations\n    506     )\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:182, in load_yolo_annotations(images_directory_path, annotations_directory_path, data_yaml_path, force_masks, is_obb)\n    180     with_masks = _with_mask(lines=lines)\n    181     with_masks = force_masks if force_masks else with_masks\n--&gt; 182     annotation = yolo_annotations_to_detections(\n    183         lines=lines,\n    184         resolution_wh=resolution_wh,\n    185         with_masks=with_masks,\n    186         is_obb=is_obb,\n    187     )\n    188     annotations[image_path] = annotation\n    189 return classes, image_paths, annotations\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:120, in yolo_annotations_to_detections(lines, resolution_wh, with_masks, is_obb)\n    115     return Detections(class_id=class_id, xyxy=xyxy, data=data)\n    117 polygons = [\n    118     (polygon * np.array(resolution_wh)).astype(int) for polygon in relative_polygon\n    119 ]\n--&gt; 120 mask = _polygons_to_masks(polygons=polygons, resolution_wh=resolution_wh)\n    121 return Detections(class_id=class_id, xyxy=xyxy, data=data, mask=mask)\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/supervision/dataset/formats/yolo.py:50, in _polygons_to_masks(polygons, resolution_wh)\n     47 def _polygons_to_masks(\n     48     polygons: List[np.ndarray], resolution_wh: Tuple[int, int]\n     49 ) -&gt; np.ndarray:\n---&gt; 50     return np.array(\n     51         [\n     52             polygon_to_mask(polygon=polygon, resolution_wh=resolution_wh)\n     53             for polygon in polygons\n     54         ],\n     55         dtype=bool,\n     56     )\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/numpy-algebra- copy.html",
    "href": "posts/numpy-algebra- copy.html",
    "title": "How numpy handles day-to-day algebra?",
    "section": "",
    "text": "import numpy as np\n\nnp.__version__\n\n118 False\n140 False\numath 8 False\numath 10 False\numath 12 True\ncore 73 False\nYes!\nnumeric 29 False\nnumeric 41 False\nnumeric 1128 False\nnumeric 2515 False\nnumeric 2517 True\ncore 75 False\ncore 77 True\ncore 83 True\ncore 94 True\n142 False\n144 True\n\n\n'1.25.2'"
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#motivation",
    "href": "posts/numpy-algebra- copy.html#motivation",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Motivation",
    "text": "Motivation\nIn this blog post, we will try to figure out how numpy handles seemingly simple math operations. The motivation behind this exploration is to figure out if there are a few foundational operations behind most of the frequently used functions in numpy. For the sake of right level of abstraction, we will not look into addition, subtraction, multiplication, and division."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#pi",
    "href": "posts/numpy-algebra- copy.html#pi",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Pi",
    "text": "Pi\n\nBackground\npi is an irrational number and computing its value to a certain precision is a challenging task. This video talks in detail how people used to compute pi in the past. At the time of writing this blog post, Google holds the record for computing pi to the highest precision to 100 trilian digits. They used y-cruncher program (it‚Äôs free. try it!) with Chudnovsky algorithm to compute pi. Here are the first 100 digits of pi:\n\\(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679\\)\n\n\nSource code\nThis is how pi is defined in numpy source code upto 36 digits.\n#define NPY_PI        3.141592653589793238462643383279502884  /* pi */\nLet‚Äôs verify it.\n\nprint(f\"{np.pi:.64f}\")\n\n3.1415926535897931159979634685441851615905761718750000000000000000\n\n\nHmm, that looks off. From 16th digit onwards, the values are different. Let‚Äôs try to figure out why.\n\npi = 3.141592653589793238462643383279502884\npi = np.array(pi, dtype=np.float64)\npi = f\"{pi:.64f}\"\nnp_pi = f\"{np.pi:.64f}\"\nassert np_pi == pi\n\nOkay, so it seems like converting 36 digits of pi to 64 bit precision went wrong from 16th digit onwards. What a waste of last 20 digits of pi due to floating point errors! Anyways, let‚Äôs move on."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#power",
    "href": "posts/numpy-algebra- copy.html#power",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Power",
    "text": "Power\nLet‚Äôs find out what happens when you execute the following code in numpy.\n\nnumber = np.float64(1.1)\nnumber**1.2\n\n1.1211693641406024"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html",
    "href": "posts/2022-03-08-torch-essentials.html",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "href": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "href": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "title": "Torch essentials",
    "section": "Gradient is all you need",
    "text": "Gradient is all you need\n\nimport matplotlib.pyplot as plt\n\n\nx = torch.rand(5,1)\ny = 3 * x + 2 + torch.randn_like(x)*0.1\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nx_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\nx_plus_ones.shape\n\ntorch.Size([5, 2])\n\n\n\ntheta = torch.zeros(2,1, requires_grad=True)\ntheta\n\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\ntheta.grad\n\n\ntheta.grad_fn\n\n\nlr = 0.1\n\ny_pred = x_plus_ones@theta\nloss = ((y_pred - y)**2).mean()\nloss.backward()\n# y_pred = torch.matmul(x_plus_ones, theta)\n# y_pred = torch.mm(x_plus_ones, theta)\n\n\ntheta.grad # dloss/dtheta\n\ntensor([[-6.3681],\n        [-2.8128]])\n\n\n\ntheta.grad_fn\n\n\ntheta.data -= lr * theta.grad.data\n\n\ntheta\n\ntensor([[0.6368],\n        [0.2813]], requires_grad=True)\n\n\n\ntheta.grad_fn\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)\n\n\n\n\n\n\n\n\n\nfor i in range(10):\n    theta.grad.data.zero_()\n    y_pred = x_plus_ones@theta\n    loss = ((y_pred - y)**2).mean()\n    loss.backward()\n    theta.data -= lr * theta.grad\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#advanced",
    "href": "posts/2022-03-08-torch-essentials.html#advanced",
    "title": "Torch essentials",
    "section": "Advanced",
    "text": "Advanced\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = torch.nn.Parameter(torch.zeros(2,1))\n#         self.register_parameter(theta, torch.zeros(2,1))\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = x_plus_ones@self.theta\n        return y_pred\n\n\nmodel = LinearRegression()\nmodel\n\nLinearRegression()\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value)\n\ntheta Parameter containing:\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    \n    optimizer.step()\n\n\nmodel.state_dict()\n\nOrderedDict([('theta',\n              tensor([[0.9799],\n                      [0.9808]]))])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "href": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "title": "Torch essentials",
    "section": "Wanna run on GPU?",
    "text": "Wanna run on GPU?\n\nx_gpu = x.to(device)\ny_gpu = y.to(device)\n\n\nprint(model.theta)\nmodel.to(device)\nprint(model.theta)\n\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], requires_grad=True)\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], device='cuda:0', requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x_gpu)\n    loss = loss_fn(y_pred, y_gpu)\n    loss.backward()\n    \n    optimizer.step()"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "href": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "title": "Torch essentials",
    "section": "State dictionary",
    "text": "State dictionary\n\n# torch.save(model.state_dict(), path)\n# model.load_state_dict(torch.load(path))"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#nn-way",
    "href": "posts/2022-03-08-torch-essentials.html#nn-way",
    "title": "Torch essentials",
    "section": "NN way",
    "text": "NN way\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(2, 1) # torch.nn.Linear(128, 64)\n        # What else? \n#         self.activation = torch.nn.ReLU()\n#         torch.nn.LSTM()\n#         torch.nn.Conv2d()\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = self.layer(x_plus_ones)\n        return y_pred"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom time import time\n\n# Enable high precision\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n# To enable animation inside notebook\nplt.rc(\"animation\", html=\"jshtml\")"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Create dataset",
    "text": "Create dataset\n\nfeatures, labels = make_blobs(100, n_features=2, centers=2, random_state=0)\nplt.scatter(features[:, 0], features[:, 1], c=labels);\n\n\n\n\n\n\n\n\n\nprint(features.shape, features.dtype, labels.shape, labels.dtype)\n\n(100, 2) float64 (100,) int64"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing Newton‚Äôs method (naive way)",
    "text": "Implementing Newton‚Äôs method (naive way)\nWe will first try to implement Eq. 10.31 directly from PML book1:\n\\[\n\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t} \\mathbf{H}_{t}^{-1} \\boldsymbol{g}_{t}\n\\]\n\ndef get_logits(params, feature):  # for a single data-point\n  logits = jnp.sum(feature * params[\"w\"]) + params[\"b\"]\n  return logits\n\ndef naive_loss(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n\n  # Check if label is 1 or 0\n  is_one = (label == 1)\n  loss_if_one = lambda: -jnp.log(prob)  # loss if label is 1\n  loss_if_zero = lambda: -jnp.log(1 - prob)  # loss if labels is 0\n\n  # Use lax.cond to convert if..else.. in jittable format\n  loss = jax.lax.cond(is_one, loss_if_one, loss_if_zero)\n\n  return loss\n\ndef naive_loss_batch(params, features, labels):  # for a batch of data-points\n   losses = jax.vmap(naive_loss, in_axes=(None, 0, 0))(params, features, labels)\n   return jnp.mean(losses)\n\nWriting the train function\n\ndef naive_train_step(params, features, labels, learning_rate):\n  # Find gradient\n  loss_value, grads = jax.value_and_grad(naive_loss_batch)(params, features, labels)\n\n  # Find Hessian\n  hess = jax.hessian(naive_loss_batch)(params, features, labels)\n\n  # Adjust Hessian matrix nicely\n  hess_matrix = jnp.block([[hess[\"b\"][\"b\"], hess[\"b\"][\"w\"]],\n                           [hess[\"w\"][\"b\"], hess[\"w\"][\"w\"]]])\n  \n  # Adjust gradient vector nicely\n  grad_vector = jnp.r_[grads[\"b\"], grads[\"w\"]]\n\n  # Find H^-1g\n  h_inv_g = jnp.dot(jnp.linalg.inv(hess_matrix), grad_vector)\n\n  # Get back the structure\n  h_inv_g = {\"b\": h_inv_g[0], \"w\": h_inv_g[1:]}\n\n  # Apply the update\n  params = jax.tree_map(lambda p, g: p - learning_rate*g, params, h_inv_g)\n\n  return params, loss_value\n\n# First order method\n# vg = jax.value_and_grad(naive_loss_batch)\n# def train_step(params, features, labels, learning_rate):\n#   # Find gradient\n#   loss_value, grads = vg(params, features, labels)\n\n#   # Apply the update\n#   params = jax.tree_map(lambda p, g: p - learning_rate*g, params, grads)\n\n#   return params, loss_value\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3, ))\n# \"b\" should have shape (1,) for hessian trick with jnp.block to work\nparams = {\"w\": random_params[:2], \"b\": random_params[2].reshape(1,)}\nlearning_rate = 1.0\nepochs = 20\n\ntrain_step_jitted = jax.jit(naive_train_step)\n\nhistory = {\"loss\": [], \"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels, learning_rate)\n\ninit = time()\nfor _ in range(epochs):\n  history[\"params\"].append(params)\n  params, loss_value = train_step_jitted(params, features, labels, learning_rate)\n  history[\"loss\"].append(loss_value)\nprint(time() - init, \"seconds\")\nprint(params)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n0.0015490055084228516 seconds\n{'b': DeviceArray([13.22076694], dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}\n\n\nA helper function to animate the learning.\n\ndef animate(history):\n  fig, ax = plt.subplots(1, 2, figsize=(10,4))\n  def update(idx):\n    # Clear previous frame\n    ax[0].cla()\n    ax[1].cla()\n\n    # Plot data\n    params = history[\"params\"][idx]\n    losses = history[\"loss\"][:idx]\n    ax[0].scatter(features[:, 0], features[:, 1], c=labels)\n    \n    # Calculate and plot decision boundary\n    x0_min, x0_max = features[:, 0].min(), features[:, 0].max()\n    x1_min = -(params[\"b\"] + params[\"w\"][0] * x0_min)/params[\"w\"][1]\n    x1_max = -(params[\"b\"] + params[\"w\"][0] * x0_max)/params[\"w\"][1]\n\n    ax[0].plot([x0_min, x0_max], [x1_min, x1_max], label='decision boundary')\n\n    # Plot losses\n    ax[1].plot(losses, label=\"loss\")\n    ax[1].set_xlabel(\"Iterations\")\n\n    ax[0].legend()\n    ax[1].legend()\n\n  anim = FuncAnimation(fig, update, range(epochs))\n  plt.close()\n  return anim\n\n\nanimate(history)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing IRLS algorithm",
    "text": "Implementing IRLS algorithm\n\ndef get_s_and_z(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n  s = prob * (1 - prob)\n  z = logits + (label - prob)/s\n  return s, z\n\ndef irls_train_step(params, features, labels):\n  s, z = jax.vmap(get_s_and_z, in_axes=(None, 0, 0))(params, features, labels)\n  S = jnp.diag(s.flatten())  # convert into a diagonal matrix\n\n  # Add column with ones\n  X = jnp.c_[jnp.ones(len(z)), features]\n\n  # Get weights\n  weights = jnp.linalg.inv(X.T@S@X)@X.T@S@z.flatten()\n\n  # get correct format\n  params = {\"b\": weights[0], \"w\": weights[1:]}\n\n  return params\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3,))\nparams = {\"w\": random_params[:2], \"b\": random_params[2]}\nepochs = 20\n\ntrain_step_jitted = jax.jit(irls_train_step)\n\nirls_history = {\"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels)\n\ninit = time()\nfor _ in range(epochs):\n  irls_history[\"params\"].append(params)\n  params = train_step_jitted(params, features, labels)\nprint(time() - init, \"seconds\")\nprint(params)\n\n0.0016303062438964844 seconds\n{'b': DeviceArray(13.22076694, dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Comparison",
    "text": "Comparison\n\nnaive_params_b = list(map(lambda x: x[\"b\"], history[\"params\"]))\nirls_params_b = list(map(lambda x: x[\"b\"], irls_history[\"params\"]))\n\nnaive_params_w = list(map(lambda x: x[\"w\"], history[\"params\"]))\nirls_params_w = list(map(lambda x: x[\"w\"], irls_history[\"params\"]))\n\n\nplt.plot(naive_params_b, \"o-\", label=\"Naive\")\nplt.plot(irls_params_b, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Bias\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(naive_params_w, \"o-\", label=\"Naive\")\nplt.plot(irls_params_w, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Weights\")\nplt.legend();"
  },
  {
    "objectID": "posts/docker_cheatsheet.html",
    "href": "posts/docker_cheatsheet.html",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#images",
    "href": "posts/docker_cheatsheet.html#images",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#containers",
    "href": "posts/docker_cheatsheet.html#containers",
    "title": "Docker Cheatsheet",
    "section": "Containers",
    "text": "Containers\nCreate a new container from an image with following flags 1. -v: for telling docker to use a shared directory between host and container 2. -p \\&lt;host-port\\&gt;\\&lt;container-port\\&gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. 3. --name generates a name for the container for easy reference in other commands 4. --gpus all tells docker to use all GPUs available on host 5. --memory-swap Restricts RAM+Swap usage 6. -it makes sure container awaits after starting instead of instantly shutting down if no startup scripts are configured.\nTo create new container with default params:\ndocker create tensorflow/tensorflow:2.4.0-gpu-jupyter \nTo create new container with manual params:\ndocker create -it \\\n-v \\path_in_host:\\path_in_container \\\n-p9000:8888 \\\n--name aaai \\\n--cpus 2 \\\n--gpus all \\ # To use specific gpus: --gpu '\"device=0,2\"'\n--memory 90g \\ # Uses 90g memory\n--memory-swap 100g \\ # --memory-swap is a modifier flag that only has meaning if --memory is also set. In this case 10g of swap will be used.\ntensorflow/tensorflow:2.4.0-gpu-jupyter\nUpdate some of the above configurations after container creation:\n# change RAM limit of a container named \"aaai\"\ndocker update --memory-swap 50g aaai\n\nNote: In general, changes made to container persist when container is stopped.\n\nCheck containers:\ndocker ps # shows running containers\ndocker ps -a # shows all containers\nStart a container (default script will be executed with this if any):\n# docker start &lt;container-name&gt;\ndocker start aaai\nStop a container:\n# docker stop &lt;container-name&gt; \ndocker stop aaai\nDelete a container:\n# docker rm &lt;container-name&gt;\ndocker rm aaai\nGo to a running container‚Äôs shell:\n#docker exec -it &lt;container-name&gt; bash\ndocker exec -it aaai bash # -it stands for interactive\nExecute any command on a running container without opening a shell in container:\n# docker exec -it &lt;container-name&gt; &lt;command&gt;\ndocker exec -it aaai jupyter notebook list\nCheck container logs (including shell commands output):\n# docker logs &lt;container-name&gt;\ndocker logs aaai"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#system",
    "href": "posts/docker_cheatsheet.html#system",
    "title": "Docker Cheatsheet",
    "section": "System",
    "text": "System\nCheck all images, all containers and space occupied by them:\ndocker system df -v"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "href": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "title": "Docker Cheatsheet",
    "section": "Set up rootless docker",
    "text": "Set up rootless docker\nRefer to this guide: https://docs.docker.com/engine/security/rootless/\nMain steps:\n\nRun dockerd-rootless-setuptool.sh install.\nSetup PATH and DOCKER_HOME as suggested by command output.\nsystemctl --user restart docker.\nTry docker images to check if things worked.\nTry docker run --rm hello-world to check if things really worked."
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "title": "Uncertainty in Deep Learning",
    "section": "",
    "text": "import torch"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "title": "Uncertainty in Deep Learning",
    "section": "1 - Introduction",
    "text": "1 - Introduction\n\nAn online deep learning book from Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n1.1 - Deep Learning\nWe define a single layer network as the following:\n\nclass SingleLayerNetwork(torch.nn.Module):\n    def __init__(self, Q, D, K):\n        \"\"\"\n        Q: number of features\n        D: number of outputs\n        K: number of hidden features\n        \"\"\"\n        super().__init__()\n        self.input = torch.nn.Linear(Q, K) # Transforms Q features into K hidden features\n        self.output = torch.nn.Linear(K, D) # Transforms K hidden features to D output features\n        self.non_lin_transform = torch.nn.ReLU() # A non-linear transformation\n        \n    def forward(self, X):\n        \"\"\"\n        X: input (N x Q)\n        \"\"\"\n        self.linear_transformed_X = self.input(X)  # (N, Q) -&gt; (N, K)\n        self.non_lin_transformed_X = self.non_lin_transform(linear_transformed_X)  # (N, K) -&gt; (N, K)\n        output = self.output(self.non_lin_transformed_X)  # (N, K) -&gt; (N, D)\n        return output\n\n\nQ = 10 # Number of features\nN = 100 # Number of samples\nD = 15 # Number of outputs\nK = 32 # Number of hidden features\n\nX = torch.rand(N, Q) # Input\nY = torch.rand(N, D) # Output\n\n\nmodel = SingleLayerNetwork(Q=Q, D=D, K=K)\nmodel\n\nSingleLayerNetwork(\n  (input): Linear(in_features=10, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=15, bias=True)\n  (non_lin_transform): ReLU()\n)\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value.shape)\n\ninput.weight torch.Size([32, 10])\ninput.bias torch.Size([32])\noutput.weight torch.Size([15, 32])\noutput.bias torch.Size([15])\n\n\nReLU is does not contain any parameters here so it is merely a function.\n\n\n1.2 Model Uncertainty\nIn which cases we want our model to be uncertain?\n\nWhen it encounters a out-of-the-distribution data\nWhen training data is noisy (irreducible/aleatoric uncertainty)\nWhen we have multiple predictors (model/epistemic uncertainty)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html",
    "href": "posts/2024-12-10-cpcb-download.html",
    "title": "Download CPCB live data",
    "section": "",
    "text": "import os\nimport re\nfrom glob import glob\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom time import sleep\n\nHOME_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\"\nDOWNLOAD_OLD_DATA_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/caaqm-data-repository\"\nDOWNLOAD_PAGE_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/data\"\ndef click_it(driver, element):\n    driver.execute_script(\"arguments[0].click();\", element)\n    \ndef find_it(element, option):\n    return element.find_element(By.XPATH, f\"//li[contains(text(), '{option}')]\")\n\ndef select_dropdown_option(driver, element, option):\n    element.click()\n    option = find_it(element, option)\n    click_it(driver, option)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "href": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "title": "Download CPCB live data",
    "section": "Dry run to get metadata",
    "text": "Dry run to get metadata\n\n# headless chrome\noptions = Options()\noptions.add_argument(\"--headless\")\n\n# open the browser\ndriver = webdriver.Chrome(options=options)\n\n# open the website\ndriver.get(DOWNLOAD_OLD_DATA_URL)\n\n# wait for the page to load and the dropdowns to appear\ndropdowns = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".select-box\")))\nlen(dropdowns)\n\n5\n\n\n\ndrop_data_type, drop_frequency, drop_states, drop_cities, drop_stations = dropdowns\n\n\n# Select data type\nselect_dropdown_option(driver, drop_data_type, \"Raw data\")\n\n# Select frequency\nselect_dropdown_option(driver, drop_frequency, \"1 day\")\n\n# Get the states\ndrop_states.click() # Open the dropdown\nstates = drop_states.text.replace(\"‚ñ≤\\n\", \"\").split(\"\\n\")\nprint(\"Number of states:\", len(states))\ndrop_states.click() # Close the dropdown\n\nNumber of states: 31\n\n\n\nmetadata_df = pd.DataFrame(columns=[\"State\", \"City\", \"Station\", \"site_id\"])\n\n# This loop took less than a minute to run\nprogress_bar = tqdm(total=600) # as of 2024, 560 stations. update this number if it changes\nfor state in states:\n    select_dropdown_option(driver, drop_states, state)\n    \n    # Get all cities\n    drop_cities.click() # Open the dropdown\n    cities = drop_cities.text.replace(\"‚ñ≤\\n\", \"\").split(\"\\n\")\n    drop_cities.click() # Close the dropdown\n    \n    for city in cities:\n        select_dropdown_option(driver, drop_cities, city)\n        \n        # Get all stations\n        drop_stations.click() # Open the dropdown\n        stations = drop_stations.text.replace(\"‚ñ≤\\n\", \"\").split(\"\\n\")\n        drop_stations.click() # Close the dropdown\n        \n        for station in stations:\n            # corner cases\n            if station == \"Municipal Corporation Office, Dharuhera - HSPCB\":\n                site_id = \"site_5044\"\n            elif station == \"Civil Lines, Ajmer - RSPCB\":\n                site_id = \"site_1392\"\n            else:\n                try:\n                    select_dropdown_option(driver, drop_stations, station)\n                except:\n                    print(\"Unable to select station\")\n                    print(station)\n                    print(drop_stations.text)\n                    continue\n                site_id = drop_stations.get_attribute(\"ng-reflect-model\")\n            metadata_df.loc[len(metadata_df)] = [state, city, station, site_id]\n            progress_bar.update(1)\n\n\n\n\n\nlen(metadata_df)\n\n560\n\n\n\nmetadata_df.head()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n2\nAndhra Pradesh\nChittoor\nGangineni Cheruvu, Chittoor - APPCB\nsite_5665\n\n\n3\nAndhra Pradesh\nKadapa\nYerramukkapalli, Kadapa - APPCB\nsite_5693\n\n\n4\nAndhra Pradesh\nRajamahendravaram\nAnand Kala Kshetram, Rajamahendravaram - APPCB\nsite_1399\n\n\n\n\n\n\n\n\nmetadata_df.tail()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n555\nWest Bengal\nKolkata\nRabindra Bharati University, Kolkata - WBPCB\nsite_296\n\n\n556\nWest Bengal\nKolkata\nFort William, Kolkata - WBPCB\nsite_5110\n\n\n557\nWest Bengal\nKolkata\nVictoria, Kolkata - WBPCB\nsite_309\n\n\n558\nWest Bengal\nKolkata\nBidhannagar, Kolkata - WBPCB\nsite_5129\n\n\n559\nWest Bengal\nSiliguri\nWard-32 Bapupara, Siliguri - WBPCB\nsite_1419\n\n\n\n\n\n\n\n\nfor site_id, more_than_1 in (metadata_df.site_id.value_counts() &gt; 1).items():\n    if more_than_1:\n        print(metadata_df[metadata_df.site_id == site_id])\n\n           State        City                               Station    site_id\n25         Bihar  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n254  Maharashtra  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n           State        City                              Station   site_id\n26         Bihar  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n255  Maharashtra  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n             State           City                                    Station  \\\n499  Uttar Pradesh  Greater Noida  Knowledge Park - V, Greater Noida - UPPCB   \n526  Uttar Pradesh          Noida  Knowledge Park - V, Greater Noida - UPPCB   \n\n       site_id  \n499  site_5121  \n526  site_5121  \n             State           City  \\\n498  Uttar Pradesh  Greater Noida   \n525  Uttar Pradesh          Noida   \n\n                                         Station    site_id  \n498  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n525  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n           State        City                              Station    site_id\n28         Bihar  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n257  Maharashtra  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n           State        City                           Station    site_id\n27         Bihar  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n256  Maharashtra  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n\n\n\n# clean up\ndrop_items = [metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - III, Greater Noida - UPPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"More Chowk Waluj, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Maharashtra\") & (metadata_df.Station == \"Gurdeo Nagar, Aurangabad - BSPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"Rachnakar Colony, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - V, Greater Noida - UPPCB\")].index.item()]\n\nmetadata_df.drop(drop_items, inplace=True)\nlen(metadata_df)\n\n554\n\n\n\nassert set(metadata_df.site_id.value_counts()) == {1}\n\n\nmetadata_df.to_csv(\"metadata.csv\", index=False)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "href": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "title": "Download CPCB live data",
    "section": "Downloading data",
    "text": "Downloading data\n\n# URL is specific to PM2.5 and PM10 so update it as per your needs\ndef get_url(state, city, site_id):\n    return f\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-view-data-report/%2522%257B%255C%2522parameter_list%255C%2522%253A%255B%257B%255C%2522id%255C%2522%253A0%252C%255C%2522itemName%255C%2522%253A%255C%2522PM2.5%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_193%255C%2522%257D%252C%257B%255C%2522id%255C%2522%253A1%252C%255C%2522itemName%255C%2522%253A%255C%2522PM10%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_215%255C%2522%257D%255D%252C%255C%2522criteria%255C%2522%253A%255C%252224%2520Hours%255C%2522%252C%255C%2522reportFormat%255C%2522%253A%255C%2522Tabular%255C%2522%252C%255C%2522fromDate%255C%2522%253A%255C%252201-01-2024%2520T00%253A00%253A00Z%255C%2522%252C%255C%2522toDate%255C%2522%253A%255C%252211-12-2024%2520T16%253A45%253A59Z%255C%2522%252C%255C%2522state%255C%2522%253A%255C%2522{state.replace(' ', '%2520')}%255C%2522%252C%255C%2522city%255C%2522%253A%255C%2522{city.replace(' ', '%2520')}%255C%2522%252C%255C%2522station%255C%2522%253A%255C%2522{site_id}%255C%2522%252C%255C%2522parameter%255C%2522%253A%255B%255C%2522parameter_193%255C%2522%252C%255C%2522parameter_215%255C%2522%255D%252C%255C%2522parameterNames%255C%2522%253A%255B%255C%2522PM2.5%255C%2522%252C%255C%2522PM10%255C%2522%255D%257D%2522\"\n\n\n# add download directory\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\"prefs\", {\n    \"download.default_directory\": \"/Users/project561/cpcb_downloads\"\n})\n\ndriver = webdriver.Chrome(options=options)\ndriver.get(HOME_URL)\n\nEnter Captcha manually before moving ahead\n\nmetadata_df = pd.read_csv(\"metadata.csv\")\nmetadata_df.head(2)\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n\n\n\n\n\n\nfiles = glob(\"/Users/project561/cpcb_downloads/*.xlsx\")\nprint(\"Number of files in the download directory:\", len(files))\nsite_ids = [re.search(r\"site_\\d+?2024\", file).group()[:-4] for file in files]\n# assert len(set(site_ids)) == len(site_ids), pd.Series(site_ids).value_counts()\nsite_ids = set(site_ids)\n\nfor i in range(len(metadata_df)):\n    state, city, station, site_id = metadata_df.iloc[i]\n    if site_id in site_ids:\n        # print(\"Already downloaded\", i, state, city, station, site_id)\n        continue\n    print(\"Downloading\", i, state, city, station, site_id)\n    url = get_url(state, city, site_id)\n    \n    # open new tab\n    driver.execute_script(\"window.open('');\")\n    driver.switch_to.window(driver.window_handles[-1])\n    driver.get(url)\n    excel_button = WebDriverWait(driver, 20).until(\n    EC.element_to_be_clickable((By.CLASS_NAME, \"fa-file-excel-o\")))\n    click_it(driver, excel_button)\n    sleep(1)\n    \n    if len(driver.window_handles) &gt; 10:\n        # close first 9 windows\n        for _ in range(9):\n            driver.switch_to.window(driver.window_handles[0])\n            driver.close()\n            \n        driver.switch_to.window(driver.window_handles[-1])\n        sleep(1)\n\nNumber of files in the download directory: 302\nDownloading 301 Maharashtra Nagpur Ram Nagar, Nagpur - MPCB site_5793\nDownloading 302 Maharashtra Nagpur Mahal, Nagpur - MPCB site_5796\nDownloading 303 Maharashtra Nagpur Opp GPO Civil Lines, Nagpur - MPCB site_303\nDownloading 304 Maharashtra Nagpur Ambazari, Nagpur - MPCB site_5792\nDownloading 305 Maharashtra Nanded Sneh Nagar, Nanded - MPCB site_5795\nDownloading 306 Maharashtra Nashik Pandav Nagari, Nashik - MPCB site_5779\nDownloading 307 Maharashtra Nashik MIDC Ambad, Nashik - MPCB site_5781\nDownloading 308 Maharashtra Nashik Gangapur Road, Nashik - MPCB site_304\nDownloading 309 Maharashtra Nashik Hirawadi, Nashik - MPCB site_5782\nDownloading 310 Maharashtra Navi Mumbai Tondare-Taloja, Navi Mumbai - MPCB site_5803\nDownloading 311 Maharashtra Navi Mumbai Sanpada, Navi Mumbai - MPCB site_5815\nDownloading 312 Maharashtra Navi Mumbai Airoli, Navi Mumbai - MPCB site_261\nDownloading 313 Maharashtra Navi Mumbai Mahape, Navi Mumbai - MPCB site_5114\nDownloading 314 Maharashtra Navi Mumbai Kopripada-Vashi, Navi Mumbai - MPCB site_5805\nDownloading 315 Maharashtra Navi Mumbai Sector-19A Nerul, Navi Mumbai - IITM site_5401\nDownloading 316 Maharashtra Navi Mumbai Nerul, Navi Mumbai - MPCB site_5103\nDownloading 317 Maharashtra Navi Mumbai Sector-2E Kalamboli, Navi Mumbai - MPCB site_5799\nDownloading 318 Maharashtra Parbhani Masoom Colony, Parbhani - MPCB site_5794\nDownloading 319 Maharashtra Pimpri-Chinchwad Park Street Wakad, Pimpri Chinchwad - MPCB site_5764\nDownloading 320 Maharashtra Pimpri-Chinchwad Savta Mali Nagar, Pimpri-Chinchwad - IITM site_5998\nDownloading 321 Maharashtra Pimpri-Chinchwad Thergaon, Pimpri Chinchwad - MPCB site_5765\nDownloading 322 Maharashtra Pimpri-Chinchwad Gavalinagar, Pimpri Chinchwad - MPCB site_5763\nDownloading 323 Maharashtra Pune Revenue Colony-Shivajinagar, Pune - IITM site_5409\nDownloading 324 Maharashtra Pune Mhada Colony, Pune - IITM site_5404\nDownloading 325 Maharashtra Pune Savitribai Phule Pune University, Pune - MPCB site_5767\nDownloading 326 Maharashtra Pune Bhumkar Nagar, Pune - IITM site_5988\nDownloading 327 Maharashtra Pune Hadapsar, Pune - IITM site_5407\nDownloading 328 Maharashtra Pune Karve Road, Pune - MPCB site_292\nDownloading 329 Maharashtra Pune Alandi, Pune - IITM site_5405"
  },
  {
    "objectID": "posts/GNN_for_regression.html",
    "href": "posts/GNN_for_regression.html",
    "title": "Graph Neural Networks for Regression",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport GPy\n\nimport torch\nimport torch.nn as nn\n\nfrom tqdm import trange\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ndevice = \"cuda\""
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "href": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a synthetic dataset",
    "text": "Create a synthetic dataset\n\nnp.random.seed(0)\ntorch.random.manual_seed(4)\n\nN = 50\nx = np.linspace(-1, 1, N).reshape(-1, 1)\nkernel = GPy.kern.RBF(input_dim=1, variance=1, lengthscale=0.1)\ny = np.random.multivariate_normal(np.zeros(N), kernel.K(x)).reshape(-1, 1)\ny_noisy = y + np.random.normal(0, 0.1, N).reshape(-1, 1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y_noisy, test_size=0.4, random_state=0)\n\nplt.plot(x, y, label=\"True\");\nplt.plot(train_x, train_y, 'o', label='train')\nplt.plot(test_x, test_y, 'o', label='test')\nplt.legend();\n\nx, y, y_noisy = map(lambda x: torch.tensor(x).float().to(device), (x, y, y_noisy))\ntrain_x, test_x, train_y, test_y = map(lambda x: torch.tensor(x).float().to(device), (train_x, test_x, train_y, test_y))\nprint(x.shape, y.shape, y_noisy.shape)\n\ntorch.Size([50, 1]) torch.Size([50, 1]) torch.Size([50, 1])"
  },
  {
    "objectID": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "href": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "title": "Graph Neural Networks for Regression",
    "section": "Fit with a simple MLP",
    "text": "Fit with a simple MLP\n\ndef fit(model, x, y, A=None, lr=0.01, epochs=100):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n    \n    if A is None:\n        inputs = (x,)\n    else:\n        inputs = (x, A)\n    \n    losses = []\n    pbar = trange(epochs)\n    for epoch in pbar:\n        optimizer.zero_grad()\n        y_hat = model(*inputs)\n        loss = loss_fn(y_hat, y)\n        losses.append(loss.item())\n        pbar.set_description(f\"Epoch {epoch} Loss: {loss.item()}\")\n        loss.backward()\n        optimizer.step()\n            \n    return losses\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [nn.Linear(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\ntorch.manual_seed(0)\nmodel = SimpleMLP([10, 10, 10]).to(device)\nfit(model, train_x, train_y, lr=0.01, epochs=1000);\n\npred_y = model(x)\n\n(x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\nplt.plot(x_, y_, label=\"True\");\nplt.plot(train_x_, train_y_, 'o', label='train')\nplt.plot(test_x_, test_y_, 'o', label='test')\nplt.plot(x_, pred_y_, label='pred')\nplt.legend();\n\nEpoch 999 Loss: 0.07143261283636093: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 410.79it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "href": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a GCN layer",
    "text": "Create a GCN layer\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x, A):    \n        return self.linear(A @ x)\n    \n    \nclass GCN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [GCNLayer(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(GCNLayer(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x, A):\n        for layer in self.layers:\n            if isinstance(layer, GCNLayer):\n                x = layer(x, A)\n            else:\n                x = layer(x)\n        return x\n    \ndef get_eucledean_A(x, exponent):\n    d = ((x - x.T)**2)**0.5\n    d = torch.where(d==0, torch.min(d[d!=0])/2, d)  # self distance is 0, so replace it with half of the min distance\n    A = 1/(d**exponent)\n    return A/A.sum(dim=1, keepdim=True)\n\ndef get_KNN_A(x, k):\n    d = torch.abs(x - x.T)\n    A = torch.zeros_like(d)\n    _, indices = torch.topk(d, k, dim=1, largest=False)\n    for i, index in enumerate(indices):\n        A[i, index] = 1\n    return A/A.sum(dim=1, keepdim=True)\n\ndef fit_and_plot(title):\n    model = GCN([10, 10, 10]).to(device)\n    losses = fit(model, train_x, train_y, A=A_train, lr=0.001, epochs=3000);\n\n    pred_y = model(x, A_all)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    axes = ax[0]\n    axes.plot(losses)\n    axes.set_title(\"Losses\")\n\n    (x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\n    axes = ax[1]\n    axes.plot(x_, y_, label=\"True\");\n    axes.plot(train_x_, train_y_, 'o', label='train')\n    axes.plot(test_x_, test_y_, 'o', label='test')\n    axes.plot(x_, pred_y_, label='pred')\n    axes.set_title(title)\n    axes.legend();"
  },
  {
    "objectID": "posts/GNN_for_regression.html#idw-setting",
    "href": "posts/GNN_for_regression.html#idw-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "IDW setting",
    "text": "IDW setting\n\nexponent = 1\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.05447980388998985: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 390.93it/s] \n\n\n\n\n\n\n\n\n\n\nexponent = 2\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.06475391983985901: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 413.49it/s]\n\n\n\n\n\n\n\n\n\n\nexponent = 3\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.043554823845624924: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:08&lt;00:00, 367.28it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#knn-setting",
    "href": "posts/GNN_for_regression.html#knn-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "KNN Setting",
    "text": "KNN Setting\n\nK = 1\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.04107221961021423: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 383.88it/s] \n\n\n\n\n\n\n\n\n\n\nK = 3\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.14372628927230835: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 404.74it/s]\n\n\n\n\n\n\n\n\n\n\nK = 7\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.13950258493423462: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 381.66it/s]\n\n\n\n\n\n\n\n\n\n\nK = 15\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.33879855275154114: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:07&lt;00:00, 376.56it/s]"
  },
  {
    "objectID": "posts/2024-12-27-download_caaqm_locations copy.html",
    "href": "posts/2024-12-27-download_caaqm_locations copy.html",
    "title": "Download CPCB CAAQM locations",
    "section": "",
    "text": "try:\n    import selenium\nexcept ModuleNotFoundError:\n    %pip install selenium\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm, trange\nfrom time import sleep, time\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n!rm log.txt\n\ndef print_it(*args, **kwargs):\n    print(*args, **kwargs)\n    with open('log.txt', 'a') as f:\n        print(*args, **kwargs, file=f)\n\nglobal_init = time()\n\nrm: log.txt: No such file or directory\n\n\n\n# Set up WebDriver\nop = webdriver.ChromeOptions()\n\ndriver = webdriver.Chrome(options=op)\n\n# Navigate to the website and manually solve the CAPTCHA\ndriver.get(\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\")\n\n\nManually solve captcha before moving on to the next cell..\n\n\n# leaflet-marker-icon custom-div-icon map_markers station_status_live leaflet-zoom-animated leaflet-interactive\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\nall_stations_len = len(all_station_markers)\nprint(\"Total stations: \", all_stations_len)\n\nTotal stations:  558\n\n\n\ndef get_after(string, phrase):\n    return string[string.index(phrase) + len(phrase):]\n\ndata = {}\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\nmarker_id = 0\nprogress_bar = tqdm(total=all_stations_len, desc=\"Progress\")\nwhile marker_id &lt; all_stations_len:\n    try:\n        marker = all_station_markers[marker_id]\n        driver.execute_script(\"arguments[0].click();\", marker)\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n        children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n        assert \"Station Name\" in children[3].text\n        \n        # parse it\n    \n        station, address, location = children[3].text.split('\\n')\n        station = get_after(station, \"Station Name: \")\n        address = get_after(address, \"Address: \")\n        latitude, longitude = location.split(\",\")\n        latitude = get_after(latitude, \"Latitude: \")\n        longitude = get_after(longitude, \"Longitude: \")\n        \n        data[station] = {\"address\": address, \"latitude\": float(latitude), \"longitude\": float(longitude)}\n        close = driver.find_element(By.CLASS_NAME, \"close\")\n        close.click()\n        sleep(0.5)\n        marker_id += 1\n        progress_bar.update(1)\n    except Exception as e:\n        driver.refresh()\n        input(\"Please manually solve the Captcha\")\n        all_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 12\n     10 marker = all_station_markers[marker_id]\n     11 driver.execute_script(\"arguments[0].click();\", marker)\n---&gt; 12 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n     13 children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n     14 assert \"Station Name\" in children[3].text\n\nFile /opt/miniconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:102, in WebDriverWait.until(self, method, message)\n    100     screen = getattr(exc, \"screen\", None)\n    101     stacktrace = getattr(exc, \"stacktrace\", None)\n--&gt; 102 time.sleep(self._poll)\n    103 if time.monotonic() &gt; end_time:\n    104     break\n\nKeyboardInterrupt: \n\n\n\n\ndf = pd.DataFrame(data).T\ndf.index.name = \"station\"\ndf.head(2)\n\n\n\n\n\n\n\n\naddress\nlatitude\nlongitude\n\n\nstation\n\n\n\n\n\n\n\nSIDCO Kurichi, Coimbatore - TNPCB\nSIDCO Kurichi, Coimbatore, Tamil Nadu.\n10.942451\n76.978996\n\n\nMuradpur, Patna - BSPCB\nS K Memorial Hall Premises, Near Gandhi Maidan...\n25.619651\n85.147382\n\n\n\n\n\n\n\n\ndf.to_csv(\"station_data.csv\")"
  },
  {
    "objectID": "posts/Rank1_GPs.html",
    "href": "posts/Rank1_GPs.html",
    "title": "Can Rank 1 GPs represent all GPs?",
    "section": "",
    "text": "from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gpytorch.kernels import RBFKernel, Kernel\n\n\nclass Rank1Kernel(nn.Module):\n    def __init__(self, input_dim, output_dim, n_neurons_per_layer, activation):\n        super().__init__()\n        self.init = nn.Linear(input_dim, n_neurons_per_layer[0])\n        self.n_neurons_per_layer = n_neurons_per_layer\n        self.activation = activation\n        \n        for i in range(1, len(n_neurons_per_layer)):\n            setattr(self, f'fc{i}', nn.Linear(n_neurons_per_layer[i-1], n_neurons_per_layer[i]))\n        \n        self.out = nn.Linear(n_neurons_per_layer[-1], output_dim)\n        \n    def forward(self, x1, x2):\n        def _forward(x):\n            x = self.init(x)\n            for i in range(1, len(self.n_neurons_per_layer)):\n                x = getattr(self, f'fc{i}')(x)\n                x = self.activation(x)\n            return self.out(x)\n        \n        x1 = _forward(x1)\n        x2 = _forward(x2)\n        \n        # print(x1.shape, x2.shape)\n        covar = x1 @ x2.T\n        # print(covar.shape, gt_covar.shape, x1.shape, x2.shape)\n        return covar\n\n\nfixed_kernel = RBFKernel()\nfixed_kernel.lengthscale = 0.3\n\nX1 = torch.linspace(-1, 1, 100).view(-1, 1)\n\n\nepochs = 1000\nn_neurons_per_layer = [64]*4\noutput_dim = 10\nkernel = Rank1Kernel(1, output_dim, n_neurons_per_layer, torch.sin)\noptimizer = torch.optim.Adam(kernel.parameters(), lr=0.001)\n\nlosses = []\nwith torch.no_grad():\n    gt_covar = fixed_kernel(X1, X1).evaluate_kernel().tensor\n    \nbar = tqdm(range(epochs))\nfor epoch in bar:\n    optimizer.zero_grad()\n    pred_covar = kernel(X1, X1)\n    loss = torch.mean((gt_covar - pred_covar)**2)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    bar.set_description(f\"Loss: {loss.item():.4f}\")\n\nLoss: 0.0001: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:06&lt;00:00, 150.34it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,2,figsize=(8, 3))\n\nsns.heatmap(gt_covar, ax=ax[0], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[0].set_title('Ground Truth Covariance')\n\nX_new = torch.linspace(-1.5, 1.5, 100).view(-1, 1)\nwith torch.no_grad():\n    est_covar = kernel(X_new, X_new)\nsns.heatmap(est_covar, ax=ax[1], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[1].set_title('Estimated Covariance');\n\n\n\n\n\n\n\n\n\n# plt.plot()\n\nX2 = torch.zeros(1, 1) + 1\nwith torch.no_grad():\n    variance = gt_covar[-1, :]\n    plt.plot(X1, variance.numpy(), label=\"fixed kernel\");\n    \n    variance = kernel(X1, X2)\n    plt.plot(X1, variance.numpy(), label=f\"rank-{output_dim} kernel\");\n    \n    plt.legend()\n\n\n\n\n\n\n\n\n\nprint(gt_covar.shape)\ntorch.random.manual_seed(2)\nnorm = dist.MultivariateNormal(torch.zeros(100), gt_covar + 1e-5 * torch.eye(100))\ny = norm.sample()\nplt.plot(X1, y);\n\ntorch.Size([100, 100])\n\n\n\n\n\n\n\n\n\n\nn = 6\n\nfig, ax = plt.subplots(1, n, figsize=(15, 2))\nd_x = X1\nd_y = y\nfor i in range(n):\n    print(f\"{i}: {torch.var(d_y)}\")\n    ax[i].plot(d_x, d_y)\n    d_x = d_x[1:] - d_x[:-1]\n    d_x = torch.cumsum(d_x, dim=0)\n    d_y = d_y[1:] - d_y[:-1]\n    \nf = lambda x: torch.zeros_like(x)\nax[-1].plot(d_x, f(d_x), c=\"r\", label=\"f(x)\")\n\n0: 0.5698477029800415\n1: 0.006691396702080965\n2: 0.0001796285796444863\n3: 0.00022799619182478637\n4: 0.0008216467685997486\n5: 0.00304242386482656"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html",
    "href": "posts/2022-01-24-query_by_committee.html",
    "title": "Query by Committee",
    "section": "",
    "text": "# Common imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\n\nplt.style.use('fivethirtyeight')\nrc('animation', html='jshtml')\n\n# Copy the models\nfrom copy import deepcopy\n\n# Sklearn imports\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Entropy function\nfrom scipy.stats import entropy\n\n# Progress helper\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "title": "Query by Committee",
    "section": "QBC by posterior sampling",
    "text": "QBC by posterior sampling\n\nInteresting fact: For probabilistic models, QBC is similar to uncertainty sampling. How?\n\nDraw \\(k\\) parameter sets from the posterior distribution representing \\(k\\) different models.\nQuery a point which shows maximum disagreement among the points."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "href": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "title": "Query by Committee",
    "section": "An example: Bayesian linear regression",
    "text": "An example: Bayesian linear regression\n\nnp.random.seed(0)\nN = 10\nX = np.linspace(-1,1,N).reshape(-1,1)\n\nt0 = 3\nt1 = 2\n\ny = X * t1 + t0 + np.random.rand(N,1)\n\nplt.scatter(X, y);\n\n\n\n\n\n\n\n\n\nAssume a posterior\n\nn_samples = 50\n\nt0_dist_samples = np.random.normal(t0, 0.1, size=n_samples)\nt1_dist_samples = np.random.normal(t1, 1, size=n_samples)\n\n\n\nPlot the models\n\nplt.scatter(X, y)\n\nfor i in range(len(t0_dist_samples)):\n    sample_t0 = t0_dist_samples[i]\n    sample_t1 = t1_dist_samples[i]\n    \n    plt.plot(X, X * sample_t1 + sample_t0,alpha=0.1)"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "title": "Query by Committee",
    "section": "QBC by bootstrapping",
    "text": "QBC by bootstrapping\n\n2 class dataset\n\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=3, shuffle=True)\n\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=y);\n\n\n\n\n\n\n\n\n\n\nFull data fit with RF\n\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X, y);\n\nRandomForestClassifier(random_state=0)\n\n\n\n\nVisualize decision boundary\n\ngrid_X1, grid_X2 = np.meshgrid(np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100), \n                    np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100))\n\ngrid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())]\n\ngrid_pred = model.predict(grid_X)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], c=y);\nplt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2);\n\n\n\n\n\n\n\n\n\n\nTrain, pool, test split\n\nX_train_pool, X_test, y_train_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nX_train, X_pool, y_train, y_pool = train_test_split(X_train_pool, y_train_pool, train_size=20, random_state=0)\n\nX_list = [X_train, X_pool, X_test]\ny_list = [y_train, y_pool, y_test]\nt_list = ['Train', 'Pool', 'Test']\n\nfig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True)\nfor i in range(3):\n    ax[i].scatter(X_list[i][:,0], X_list[i][:,1], c=y_list[i])\n    ax[i].set_title(t_list[i])\n    \n\n\n\n\n\n\n\n\n\n\nFitting a model on initial train data\n\nAL_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\nAL_model.fit(X_train, y_train);\n\nRandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\n\nGet the votes from trees on pool dataset\n\nvotes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n\nfor learner_idx, learner in enumerate(AL_model.estimators_):\n    votes[:, learner_idx] = learner.predict(X_pool)\n\n\nvotes.shape\n\n(780, 100)\n\n\n\nvotes\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 1., 1., ..., 0., 1., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nConvert to probabilities\n\np_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n\nfor vote_idx, vote in enumerate(votes):\n    vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()}\n\n    for class_idx, class_label in enumerate(range(X.shape[1])):\n        p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n\n\np_vote\n\narray([[1.  , 0.  ],\n       [0.89, 0.11],\n       [0.06, 0.94],\n       ...,\n       [0.93, 0.07],\n       [1.  , 0.  ],\n       [1.  , 0.  ]])\n\n\n\n\nCalculate dissimilarity (entropy)\n\nexample_id = 2\n\n\nans = 0\nfor category in range(X_pool.shape[1]):\n    ans += (-p_vote[example_id][category] * np.log(p_vote[example_id][category]))\n\nans\n\n0.22696752250060448\n\n\n\nentr = entropy(p_vote, axis=1)\n\n\nentr[example_id]\n\n0.22696752250060448\n\n\n\n\nActive Learning Flow\n\ndef get_query_idx():\n    # Gather the votes\n    votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n    for learner_idx, learner in enumerate(AL_model.estimators_):\n        votes[:, learner_idx] = learner.predict(X_pool)\n    \n    # Calcuate probability of votes\n    p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n    for vote_idx, vote in enumerate(votes):\n        vote_counter = {0 : (1-vote).sum(), \n                    1 : vote.sum()}\n\n        for class_idx, class_label in enumerate(range(X.shape[1])):\n            p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n    \n    # Calculate entropy for each example\n    entr = entropy(p_vote, axis=1)\n    \n    # Choose example with highest entropy (disagreement)\n    return entr.argmax()\n\n\n\nPrepare data for random sampling\n\nX_train_rand = X_train.copy()\ny_train_rand = y_train.copy()\nX_pool_rand = X_pool.copy()\ny_pool_rand = y_pool.copy()\n\nrandom_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\nRun active learning\n\nAL_iters = 100\nnp.random.seed(0)\n\nAL_inds = []\nAL_models = []\nrandom_inds = []\nrandom_models = []\n\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    ######## Active Learning ############\n    # Fit the model\n    AL_model.fit(X_train, y_train)\n    AL_models.append(deepcopy(AL_model))\n    \n    # Query a point\n    query_idx = get_query_idx()\n    AL_inds.append(query_idx)\n    \n    # Add it to the train data\n    X_train = np.concatenate([X_train, X_pool[query_idx:query_idx+1, :]], axis=0)\n    y_train = np.concatenate([y_train, y_pool[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool = np.delete(X_pool, query_idx, axis=0)\n    y_pool = np.delete(y_pool, query_idx, axis=0)\n    \n    ######## Random Sampling ############\n     # Fit the model\n    random_model.fit(X_train_rand, y_train_rand)\n    random_models.append(deepcopy(random_model))\n    \n    # Query a point\n    query_idx = np.random.choice(len(X_pool))\n    random_inds.append(query_idx)\n    # Add it to the train data\n    X_train_rand = np.concatenate([X_train_rand, X_pool_rand[query_idx:query_idx+1, :]], axis=0)\n    y_train_rand = np.concatenate([y_train_rand, y_pool_rand[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool_rand = np.delete(X_pool_rand, query_idx, axis=0)\n    y_pool_rand = np.delete(y_pool_rand, query_idx, axis=0)\n\niteration 99\n\n\n\n\nPlot accuracy\n\nrandom_scores = []\nAL_scores = []\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test)))\n    random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test)))\n    \nplt.plot(AL_scores, label='Active Learning');\nplt.plot(random_scores, label='Random Sampling');\nplt.legend();\nplt.xlabel('Iterations');\nplt.ylabel('Accuracy\\n(Higher is better)');\n\niteration 99\n\n\n\n\n\n\n\n\n\n\n\nPlot decision boundary\n\ndef update(i):\n    for each in ax:\n        each.cla()\n        \n    AL_grid_preds = AL_models[i].predict(grid_X)\n    random_grid_preds = random_models[i].predict(grid_X)\n    \n    # Active learning\n    ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label='initial_train', alpha=0.2)\n    ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], \n                  c=y_train[n_train:n_train+i], label='new_points')\n    ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[0].set_title('New points')\n    \n    ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[1].set_title('Test points');\n    ax[0].text(locs[0],locs[1],'Active Learning')\n    \n    # Random sampling\n    ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label='initial_train', alpha=0.2)\n    ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], \n                  c=y_train_rand[n_train:n_train+i], label='new_points')\n    ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[2].set_title('New points')\n    \n    ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[3].set_title('Test points');\n    ax[2].text(locs[0],locs[1],'Random Sampling');\n\n\nlocs = (2.7, 4)\nfig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True)\nax = ax.ravel()\nn_train = X_train.shape[0]-AL_iters\n\nanim = FuncAnimation(fig, func=update, frames=range(100))\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html",
    "title": "Programatically download OpenAQ data",
    "section": "",
    "text": "# uncomment to install these libraries\n# !pip install boto3 botocore\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport boto3\nimport botocore\nimport os\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "title": "Programatically download OpenAQ data",
    "section": "Setup",
    "text": "Setup\n\ns3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\nbucket_name = 'openaq-fetches'\nprefix = 'realtime-gzipped/'\n\npath = '/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/'\n\nstart_date = '2020/01/01' # start date (inclusive)\nend_date = '2020/12/31' # end date (inclusive)\n\n\nDownload\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  clear_output(wait=True)\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  print('Downloading:', date)\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    f_name = file_obj['Key']\n    tmp_path = '/'.join((path+f_name).split('/')[:-1])\n    \n    if not os.path.exists(tmp_path):\n      os.makedirs(tmp_path)\n    \n    s3.download_file(bucket_name, f_name, path+f_name)\n\nDownloading: 2020-05-04\n\n\n\n\nValidate\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    assert os.path.exists(path+file_obj['Key']), file_obj['Key']\n\n\nprint('Validated')"
  },
  {
    "objectID": "posts/Basis_functions.html",
    "href": "posts/Basis_functions.html",
    "title": "Basis functions",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"../../beat_stgnp/dataset/bjair/NP/processed_raw.csv\")\ndata[\"time\"] = pd.to_datetime(data[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\ndata[\"time\"] = data[\"time\"].apply(lambda x: x.timestamp())\n\nx = [\"latitude\", \"longitude\", \"time\"]\ny = [\"PM25_Concentration\"]\n\nx_train, x_test, y_train, y_test = train_test_split(data[x], data[y], test_size=0.2, random_state=42)\nx_train, x_test, y_train, y_test = map(lambda x: x.values, [x_train, x_test, y_train, y_test])\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nx_train = x_scaler.fit_transform(x_train)\ny_train = y_scaler.fit_transform(y_train)\nx_test = x_scaler.transform(x_test)\n\nmodel = RandomForestRegressor(n_estimators=1000, random_state=42)\nmodel.fit(x_train, y_train.ravel())\ny_pred = model.predict(x_test)\nprint(\"RMSE\", np.sqrt(np.mean((y_scaler.inverse_transform(y_pred).ravel() - y_test.ravel())**2)))\n\n /tmp/ipykernel_922642/3470971270.py:18: DataConversionWarning:A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel()."
  },
  {
    "objectID": "posts/ssh-macos.html",
    "href": "posts/ssh-macos.html",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you‚Äôd like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThat‚Äôs it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/ssh-macos.html#terminology",
    "href": "posts/ssh-macos.html#terminology",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you‚Äôd like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThat‚Äôs it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/fundamentals_across_domains.html",
    "href": "posts/fundamentals_across_domains.html",
    "title": "Fundamentals across ML domains",
    "section": "",
    "text": "Similarities among ML domains\n\n\n\nNN\nTransformer\nCNN\n\n\n\n\n-\nMulti-head\nMulti-channel\n\n\n-\nSkip-connection\nResNet\n\n\n\n\n\nProgress of Natural Language Processing\n\n\n\n\n\n\n\n\n\nModel\nMain Disadvantage\nSolved by\nHow?\n\n\n\n\nNN\nCan‚Äôt handle dynamic length input\nRNN\nRNN can handle dynamic length input\n\n\nRNN\nVanishing Gradient Problem\nLSTM\nLSTM can handle vanishing gradient problem\n\n\nLSTM\nNon parallelizable\nTransformer\nTransformer can parallelize the computation\n\n\nTrasformer\nlosses sequentiality\nTransformer\nPositional Encoding"
  },
  {
    "objectID": "posts/2023-03-28-nngp.html",
    "href": "posts/2023-03-28-nngp.html",
    "title": "Neural Network Gaussian Process",
    "section": "",
    "text": "# %%capture\n# %pip install -U --force-reinstall jaxutils\n# %pip install -U jax jaxlib optax\n\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nfrom jaxutils import Dataset\n\ntry:\n    from neural_tangents import stax\nexcept ModuleNotFoundError:\n    %pip install neural-tangents\n    from neural_tangents import stax\n\ntry:\n    import optax as ox\nexcept ModuleNotFoundError:\n    %pip install optax\n    import optax as ox\n\ntry:\n    import gpjax as gpx\nexcept ModuleNotFoundError:\n    %pip install gpjax\n    import gpjax as gpx\n\ntry:\n    import regdata as rd\nexcept ModuleNotFoundError:\n    %pip install regdata\n    import regdata as rd\n\nimport matplotlib.pyplot as plt\n\n\nclass NTK(gpx.kernels.AbstractKernel):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def __call__(self, params, x, y):\n        params = jax.tree_util.tree_map(jax.nn.softplus, params)\n        init_fn, apply_fn, kernel_fn = stax.serial(\n            stax.Dense(512, W_std=params[\"w1\"], b_std=params[\"b1\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w2\"], b_std=params[\"b2\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w3\"], b_std=params[\"b3\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w4\"], b_std=params[\"b4\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w5\"], b_std=params[\"b5\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w6\"], b_std=params[\"b6\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w7\"], b_std=params[\"b7\"]), stax.Relu(),\n            stax.Dense(1, W_std=params[\"w8\"], b_std=params[\"b8\"])\n        )\n        return kernel_fn(x.reshape(1, 1), y.reshape(1, 1)).nngp.squeeze()\n\n    def init_params(self, key):\n        # return init_fn(key, input_shape=(2,1))\n        return {\"w1\": 0.1, \"w2\": 0.2, \"w3\": 0.3, \"w4\": 0.4, \"w5\": 0.5,  \"w6\": 0.6, \"w7\": 0.7, \"w8\": 0.8,\n                \"b1\": 0.1, \"b2\": 0.2, \"b3\": 0.3, \"b4\": 0.4, \"b5\": 0.5,  \"b6\": 0.6, \"b7\": 0.7, \"b8\": 0.8\n                }\n\n    # This is depreciated. Can be removed once JaxKern is updated.\n    def _initialise_params(self, key):\n        return self.init_params(key)\n\n\nn = 100\nnoise = 0.3\nkey = jr.PRNGKey(123)\n# x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).sort().reshape(-1, 1)\n# f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n# signal = f(x)\n# y = signal + jr.normal(key, shape=signal.shape) * noise\nx, y, xtest = rd.MotorcycleHelmet().get_data()\ny = y.reshape(-1, 1)\n\nD = Dataset(X=x, y=y)\n\n# xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n# ytest = f(xtest)\n\nprint(x.shape, y.shape)\n\n(94, 1) (94, 1)\n\n\n\nkernel = NTK()\nprior = gpx.Prior(kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n\n\nkey = jr.PRNGKey(1234)\nparameter_state = gpx.initialise(posterior, key)\nparams, trainable, bijectors = parameter_state.unpack()\nparams[\"likelihood\"][\"obs_noise\"] = jnp.array(0.1)\nparameter_state = gpx.parameters.ParameterState(params, trainable, bijectors)\nprint(params)\n\n{'kernel': {'w1': 0.1, 'w2': 0.2, 'w3': 0.3, 'w4': 0.4, 'w5': 0.5, 'w6': 0.6, 'w7': 0.7, 'w8': 0.8, 'b1': 0.1, 'b2': 0.2, 'b3': 0.3, 'b4': 0.4, 'b5': 0.5, 'b6': 0.6, 'b7': 0.7, 'b8': 0.8}, 'mean_function': {}, 'likelihood': {'obs_noise': Array(0.1, dtype=float32, weak_type=True)}}\n\n\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n\n\n\nnegative_mll = jax.jit(posterior.marginal_log_likelihood(D, negative=True))\nnegative_mll(params)\n\nArray(415.1062, dtype=float32)\n\n\n\noptimiser = ox.adam(learning_rate=0.01)\n\ninference_state = gpx.fit(\n    objective=negative_mll,\n    parameter_state=parameter_state,\n    optax_optim=optimiser,\n    num_iters=500,\n)\n\nlearned_params, training_history = inference_state.unpack()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02&lt;00:00, 172.53it/s, Objective=76.34]\n\n\n\nplt.plot(training_history);\n\n\n\n\n\n\n\n\n\nlearned_params\n\n{'kernel': {'b1': Array(0.03292831, dtype=float32),\n  'b2': Array(-0.9647168, dtype=float32),\n  'b3': Array(-1.2660046, dtype=float32),\n  'b4': Array(-1.3792713, dtype=float32),\n  'b5': Array(-1.4311961, dtype=float32),\n  'b6': Array(-1.4504426, dtype=float32),\n  'b7': Array(-1.4371448, dtype=float32),\n  'b8': Array(-1.3471106, dtype=float32),\n  'w1': Array(1.0706716, dtype=float32),\n  'w2': Array(1.1768614, dtype=float32),\n  'w3': Array(1.2740505, dtype=float32),\n  'w4': Array(1.3689499, dtype=float32),\n  'w5': Array(1.462641, dtype=float32),\n  'w6': Array(1.5562503, dtype=float32),\n  'w7': Array(1.6506695, dtype=float32),\n  'w8': Array(1.7462935, dtype=float32)},\n 'likelihood': {'obs_noise': Array(0.184795, dtype=float32)},\n 'mean_function': {}}\n\n\n\nlatent_dist = posterior(learned_params, D)(xtest)\npredictive_dist = likelihood(learned_params, latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=\"tab:blue\",\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\n\n# ax.plot(\n#     xtest, ytest, label=\"Latent function\", color=\"black\", linestyle=\"--\", linewidth=1\n# )\n\nax.legend();"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html",
    "href": "posts/2021-09-27-constraints.html",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('font', **{'size':18})"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpy",
    "href": "posts/2021-09-27-constraints.html#gpy",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPy",
    "text": "GPy\n\nfrom paramz.transformations import Logexp\n\n\ngpy_trans = Logexp()\n\n\nx = torch.arange(-1000,10000).to(torch.float)\nplt.plot(x, gpy_trans.f(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpytorch",
    "href": "posts/2021-09-27-constraints.html#gpytorch",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPyTorch",
    "text": "GPyTorch\n\nfrom gpytorch.constraints import Positive\n\n\ngpytorch_trans = Positive()\n\n\nplt.plot(x, gpytorch_trans.transform(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpflow",
    "href": "posts/2021-09-27-constraints.html#gpflow",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPFlow",
    "text": "GPFlow\n\nfrom gpflow.utilities.bijectors import positive\n\n\ngpflow_trans = positive()\n\n\nplt.plot(x, gpflow_trans(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');\n\n\n\n\n\n\n\n\n\nnp.allclose(gpy_trans.f(x), gpytorch_trans.transform(x))\n\nTrue\n\n\n\nnp.allclose(gpy_trans.f(x), gpflow_trans(x))\n\nTrue"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html",
    "href": "posts/2022-08-01-conditional_neural_processes.html",
    "title": "Conditional Neural Processes in JAX",
    "section": "",
    "text": "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n# https://github.com/tensorflow/probability/issues/1523\nimport logging\n\nlogger = logging.getLogger()\n\n\nclass CheckTypesFilter(logging.Filter):\n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n  import flax.linen as nn\nexcept ModuleNotFoundError:\n  %pip install flax\n  import flax.linen as nn\n\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install optax\n  import optax\n\ntry:\n  import tensorflow_probability.substrates.jax as tfp\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#model",
    "href": "posts/2022-08-01-conditional_neural_processes.html#model",
    "title": "Conditional Neural Processes in JAX",
    "section": "Model",
    "text": "Model\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(2)(x)\n    loc, raw_scale = x[:, 0], x[:, 1]\n    scale = jax.nn.softplus(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#data",
    "href": "posts/2022-08-01-conditional_neural_processes.html#data",
    "title": "Conditional Neural Processes in JAX",
    "section": "Data",
    "text": "Data\n\nN = 100\nseed = jax.random.PRNGKey(0)\nx = jnp.linspace(-1, 1, N).reshape(-1, 1)\nf = lambda x: (jnp.sin(10*x) + x).flatten()\nnoise = jax.random.normal(seed, shape=(N,)) * 0.2\ny = f(x) + noise\n\nx_test = jnp.linspace(-2, 2, N*2+10).reshape(-1, 1)\ny_test = f(x_test) \n\nplt.scatter(x, y, label='train', zorder=5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.legend();"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#training",
    "href": "posts/2022-08-01-conditional_neural_processes.html#training",
    "title": "Conditional Neural Processes in JAX",
    "section": "Training",
    "text": "Training\n\ndef train_fn(model, optimizer, seed, n_iterations, n_context):\n  params = model.init(seed, x, y, x)\n  value_and_grad_fn = jax.value_and_grad(model.loss_fn)\n  state = optimizer.init(params)\n  indices = jnp.arange(N)\n  \n  def one_step(params_and_state, seed):\n    params, state = params_and_state\n    shuffled_indices = jax.random.permutation(seed, indices)\n    context_indices = shuffled_indices[:n_context]\n    target_indices = shuffled_indices[n_context:]\n    x_context, y_context = x[context_indices], y[context_indices]\n    x_target, y_target = x[target_indices], y[target_indices]\n    loss, grads = value_and_grad_fn(params, x_context, y_context, x_target, y_target)\n    updates, state = optimizer.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n\n  seeds = jax.random.split(seed, num=n_iterations)\n  (params, state), losses = jax.lax.scan(one_step, (params, state), seeds)\n  return params, losses\n\n\nencoder_features = [64, 16, 8]\nencoding_dims = 1\ndecoder_features = [16, 8]\nmodel = CNP(encoder_features, encoding_dims, decoder_features)\noptimizer = optax.adam(learning_rate=0.001)\n\nseed = jax.random.PRNGKey(2)\nn_context = int(0.7 * N)\nn_iterations = 20000\n\nparams, losses = train_fn(model, optimizer, seed, n_iterations=n_iterations, n_context=n_context)\n\n\nplt.plot(losses);"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "href": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "title": "Conditional Neural Processes in JAX",
    "section": "Predict",
    "text": "Predict\n\nloc, scale = model.apply(params, x, y, x_test)\nlower, upper = loc - 2*scale, loc + 2*scale\n\nplt.scatter(x, y, label='train', alpha=0.5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.plot(x_test, loc);\nplt.fill_between(x_test.flatten(), lower, upper, alpha=0.4);\nplt.ylim(-5, 5);"
  },
  {
    "objectID": "posts/kl-divergence.html",
    "href": "posts/kl-divergence.html",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#ground",
    "href": "posts/kl-divergence.html#ground",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence",
    "href": "posts/kl-divergence.html#kl-divergence",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence",
    "text": "KL divergence\nWe can use KL divergence to check how good is our model. The formula is:\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} p_G(y_i)\\log\\frac{p_G(y_i)}{p_P(y_i)}\n\\]\nFor our example,\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\log\\frac{1}{0.8}\n\\]\nIt is evident that if \\(p_P(y = L2)\\) decreses from \\(0.8\\), \\(D_{KL}(p_G\\;\\rVert\\;p_P)\\) will increase and vice versa. Note that KL divergence is not symmetric which means \\(D_{KL}(p_G\\;\\rVert\\;p_P) \\ne D_{KL}(p_P\\;\\rVert\\;p_G)\\)."
  },
  {
    "objectID": "posts/kl-divergence.html#cross-entory",
    "href": "posts/kl-divergence.html#cross-entory",
    "title": "KL divergence v/s cross-entropy",
    "section": "Cross-entory",
    "text": "Cross-entory\nCross-entropy is another measure for distribution similarity. The formula is:\n\\[\nH(p_G, p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} - p_G(y_i)\\log p_P(y_i)\n\\]\nFor our example:\n\\[\nH(p_G, p_P) = -\\log 0.8 = \\log \\frac{1}{0.8}\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "href": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence v/s cross-entropy",
    "text": "KL divergence v/s cross-entropy\nThis shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence?\nThat‚Äôs because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization."
  },
  {
    "objectID": "posts/2025-02-15-vlm-object-detection-label-sensitivity copy.html",
    "href": "posts/2025-02-15-vlm-object-detection-label-sensitivity copy.html",
    "title": "Effect of Class Names on VLM Object Detection",
    "section": "",
    "text": "try:\n    import maestro\nexcept ModuleNotFoundError:\n    %pip install \"maestro[florence_2]\""
  },
  {
    "objectID": "posts/2025-02-15-vlm-object-detection-label-sensitivity copy.html#download-dataset",
    "href": "posts/2025-02-15-vlm-object-detection-label-sensitivity copy.html#download-dataset",
    "title": "Effect of Class Names on VLM Object Detection",
    "section": "Download dataset",
    "text": "Download dataset\n\nROBOFLOW_API_KEY = os.getenv('ROBOFLOW_API_KEY')\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\", \"/tmp/poker-cards-fmjio\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 1 {dataset.location}/train/annotations.jsonl\n\n{\"image\":\"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;jack  of clubs&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;queen of clubs&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;king of clubs&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n\n\n\n\n\n\n\n\n\n\nCommand\nType\nDescription\n\n\n\n\n‚Äìdataset\nTEXT\nPath to the dataset used for training [default: None] [required]\n\n\n‚Äìmodel_id\nTEXT\nIdentifier for the Florence-2 model [default: microsoft/Florence-2-base-ft]\n\n\n‚Äìrevision\nTEXT\nModel revision to use [default: refs/pr/20]\n\n\n‚Äìdevice\nTEXT\nDevice to use for training [default: auto]\n\n\n‚Äìoptimization_strategy\nTEXT\nOptimization strategy: lora, freeze, or none [default: lora]\n\n\n‚Äìcache_dir\nTEXT\nDirectory to cache the model weights locally [default: None]\n\n\n‚Äìepochs\nINTEGER\nNumber of training epochs [default: 10]\n\n\n‚Äìlr\nFLOAT\nLearning rate for training [default: 1e-05]\n\n\n‚Äìbatch_size\nINTEGER\nTraining batch size [default: 4]\n\n\n‚Äìaccumulate_grad_batches\nINTEGER\nNumber of batches to accumulate for gradient updates [default: 8]\n\n\n‚Äìval_batch_size\nINTEGER\nValidation batch size [default: None]\n\n\n‚Äìnum_workers\nINTEGER\nNumber of workers for data loading [default:\n\n\n‚Äìval_num_workers\nINTEGER\nNumber of workers for validation data loading [default: None]\n\n\n‚Äìoutput_dir\nTEXT\nDirectory to store training outputs [default: ./training/florence_2]\n\n\n‚Äìmetrics\nTEXT\nList of metrics to track during training\n\n\n‚Äìmax_new_tokens\nINTEGER\nMaximum number of new tokens generated during inference [default: 1024]\n\n\n‚Äìrandom_seed\nINTEGER\nRandom seed for ensuring reproducibility. If None, no seed is set [default: None]"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html",
    "href": "posts/2022-04-06-github_faqs.html",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "href": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "href": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q2: What to do if the main (or master) gets updated before I open a PR?",
    "text": "Q2: What to do if the main (or master) gets updated before I open a PR?\nPull the changes directly to your branch with:\ngit pull https://github.com/probml/pyprobml"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "href": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q3: What to do with the fork‚Äôs main when the original main is updated?",
    "text": "Q3: What to do with the fork‚Äôs main when the original main is updated?\nFetch upstream with GitHub GUI or use the same solution given in Q2."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "href": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q4: Why and when keeping the fork‚Äôs main up to date with the original main is important?",
    "text": "Q4: Why and when keeping the fork‚Äôs main up to date with the original main is important?\nWhenever we need to create new branches (usually from the fork‚Äôs main)."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "href": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q5: How to update a change in a PR that is open?",
    "text": "Q5: How to update a change in a PR that is open?\nPush the change to the corresponding branch and PR will get updated automatically."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html",
    "href": "posts/2021-03-22-gp_kernels.html",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "",
    "text": "!pip install -qq GPy\nimport autograd.numpy as np\nimport pandas as pd\nimport GPy\nimport matplotlib.pyplot as plt\nfrom autograd import grad\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "href": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "RBF (Radial basis function) Kernel, Stationarity and Isotropy",
    "text": "RBF (Radial basis function) Kernel, Stationarity and Isotropy\nRBF is one of the most commonly used kernels in GPs due to it‚Äôs infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions \\(X \\to Y\\).\nRBF kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2exp\\left(-\\frac{(x-x')^2}{2l^2}\\right)\n\\end{aligned}\n\\] Where, \\(\\sigma^2\\) is variance and \\(l\\) is known as lengthscale. #### Stationarity RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, \\(\\mathcal{K}(x,x')\\) depends only on \\(x-x'\\).\n\nIsotropy\nRBF is also isotropic kernel, which means that \\(\\mathcal{K}(x,x')\\) depends only on \\(|x-x'|\\). Thus, we have \\(\\mathcal{K}(x,x') = \\mathcal{K}(x',x)\\).\nLet‚Äôs visualize few functions drawn from the RBF kernel\n\ndef K_rbf(X1, X2, sigma=1., l=1.):\n  return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2))\n\n\n\nHelper functions\n\ndef plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)):\n  mean = np.zeros(X.shape[0])\n  cov = kernel_func(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  fig = plt.figure(figsize=(14,8), constrained_layout=True)\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1])\n  ax0.set_ylim(*ax0_ylim)\n  ax1 = fig.add_subplot(gs[1, 0:2])\n  ax1.set_ylim(*ax1_ylim)\n  ax2 = fig.add_subplot(gs[1, 2:4])\n  for func in functions:\n    ax0.plot(X, func,'o-');\n  ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel');\n  ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n  sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\ndef animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), \n                      k_name='',p_name='',symbol=''):\n  fig = plt.figure(figsize=(14,8))\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]);\n  def update(p):\n    ax0.cla();ax1.cla();ax2.cla();\n    ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim)\n    if p_name == 'Lengthscale':\n      cov = kernel_func(X, X, l=p)\n    elif p_name == 'Variance':\n      cov = kernel_func(X, X, sigma=np.sqrt(p))\n    elif p_name == 'Offset':\n      cov = kernel_func(X, X, c=p)\n    elif p_name == 'Period':\n      cov = kernel_func(X, X, p=p)\n    functions = np.random.multivariate_normal(mean, cov, size=5)\n    for func in functions:\n      ax0.plot(X, func,'o-');\n    ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel\\n'+p_name+' ('+symbol+') = '+str(p));\n    ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n    sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False);\n    ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\n  anim = FuncAnimation(fig, update, frames=val_list, blit=False)\n  plt.close()\n  rc('animation', html='jshtml')\n  return anim\n\nVerifying if our kernel is consistent with GPy kernels.\n\nX = np.linspace(101,1001,200).reshape(-1,1)\nsigma, l = 7, 11\nassert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) \n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\nk_name = 'RBF'\nplot_functions(K_rbf, ax0_ylim=(-3.5,3))\n\n\n\n\n\n\n\n\nLet‚Äôs see the effect of varying parameters \\(\\sigma\\) and \\(l\\) of the RBF kernel function.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_rbf, val_list, k_name='RBF', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nl = 1.\nval_list = [1,4,9,16,25]\nanimate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26),\n                  k_name='RBF', p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWith increase in value of \\(l\\), functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in \\(l\\).\nIncreasing \\(\\sigma^2\\) increase the overall uncertainty (width of the space where 95% of the functions live) across all the points."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Matern Kernel",
    "text": "Matern Kernel\nMatern kernels are given by a general formula as following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1, x_2) =  \\sigma^2\\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\n\\Bigg)^\\nu K_\\nu\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\\Bigg)\n\\end{aligned}\n\\] Where, \\(\\Gamma\\) is gamma function and \\(K_\\nu\\) is modified Bessel function of second order.\nThe general formula is not very intuitive about the functionality of this kernel. In practice, Matern with \\(\\nu=\\{0.5,1.5,2.5\\}\\) are used, where GP with each kernel is \\((\\lceil\\nu\\rceil-1)\\) times differentiable.\nMatern functions corresponding to each \\(\\nu\\) values are defined as the following, \\[\n\\begin{aligned}\nMatern12 \\to \\mathcal{K_{\\nu=0.5}}(x_1, x_2) &=  \\sigma^2exp\\left(-\\frac{|x_1-x_2|}{l}\\right)\\\\\nMatern32 \\to \\mathcal{K_{\\nu=1.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)exp\\left(-\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)\\\\\nMatern52 \\to \\mathcal{K_{\\nu=2.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{5}|x_1-x_2|}{l}+\\frac{5(x_1-x_2)^2)}{3l^2}\\right)exp\\left(-\\frac{\\sqrt{5}|x_1-x_2|}{l}\\right)\n\\end{aligned}\n\\] Matern kernels are stationary as well as isotropic. With \\(\\nu \\to \\infty\\) they converge to \\(RBF\\) kernel. \\(Matern12\\) is also known as \\(Exponential\\) kernel in toolkits such as GPy.\nNow, let‚Äôs draw few functions from each of these versions and try to get intuition behind each of them.\n\ndef K_m12(X1, X2, sigma=1., l=1.): # v = 0.5\n  return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l))\ndef K_m32(X1, X2, sigma=1., l=1.): # v = 1.5\n  return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l))\ndef K_m52(X1, X2, sigma=1., l=1.): # v = 2.5\n  return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))*\\\n                    (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l))\n\nVerifying if our kernels are consistent with GPy kernels.\n\nX = np.linspace(101,1001,50).reshape(-1,1)\nassert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X))\nassert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X))\n\n\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\n\nfig, ax = plt.subplots(3,2,figsize=(14,10))\nnames = ['Matern12', 'Matern32', 'Matern52']\nfor k_i, kernel in enumerate([K_m12, K_m32, K_m52]):\n  mean = np.zeros(X.shape[0])\n  cov = kernel(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  for func in functions:\n    ax[k_i,0].plot(X, func);\n  ax[k_i,0].set_xlabel('X');ax[k_i,0].set_ylabel('Y');ax[k_i,0].set_title('Functions drawn from '+names[k_i]+' kernel');\n  sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax[k_i,1].set_xlabel('X');ax[k_i,1].set_ylabel('X');ax[k_i,1].set_title('Covariance matrix');\nplt.tight_layout();\n\n\n\n\n\n\n\n\nFrom the above plot, we can say that smoothness is increasing in functions as we increase \\(\\nu\\). Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52.\nLet us see effect of varying \\(\\sigma\\) and \\(l\\) on Matern32 which is more popular among the three.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_m32, val_list, k_name='Matern32', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that Matern32 kernel behaves similar to RBF with varying \\(l\\). Though, Matern32 is less smoother than RBF. A quick comparison would clarify this.\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_rbf(X,X, l=3.)[:,50], label='RBF')\nplt.plot(X, K_m32(X,X, l=3.)[:,50], label='Matern32')\nplt.legend();plt.xlabel('X');plt.ylabel('Covariance (K(0,X))');\nplt.title('K(0,X)');"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Periodic Kernel",
    "text": "Periodic Kernel\nPeriodic Kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2\\exp\\left(-\\frac{\\sin^2(\\pi|x_1 - x_2|/p)}{2l^2}\\right)\n\\end{aligned}\n\\] Where \\(p\\) is period. Let‚Äôs visualize few functions drawn from this kernel.\n\ndef K_periodic(X1, X2, sigma=1., l=1., p=3.):\n  return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2)\n\nX = np.linspace(10,1001,50).reshape(-1,1)\nassert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), \n                   GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X))\n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1\nl = 1.\np = 3.\nk_name = 'Periodic'\nplot_functions(K_periodic)\n\n\n\n\n\n\n\n\nWe will investigate the effect of varying period \\(p\\) now.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1),\n                  k_name='Periodic',p_name='Period')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFrom the above animation we can see that, all points that are \\(p\\) distance apart from each other have exactly same values because they have correlation of exactly 1 (\\(\\sigma=1 \\to covariance=correlation\\)).\nNow, we will investigate effect of lenging lengthscale \\(l\\) while other parameters are constant.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1),\n                  k_name='Periodic',p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that correlation between a pair of locations \\(\\{x_1,x_2|x_1-x_2&lt;p\\}\\) increases as the lengthscale is increased."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Linear Kernel",
    "text": "Linear Kernel\nLinear kernel (a.k.a. dot-product kernel) is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+\\sigma^2\n\\end{aligned}\n\\] Let‚Äôs visualize few functions drawn from the linear kernel\n\ndef K_lin(X1, X2, sigma=1., c=1.):\n  return (X1-c)@(X2.T-c) + sigma**2\n\n\nnp.random.seed(0)\nsigma = 1.\nc = 1.\n\nplot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7))\n\n\n\n\n\n\n\n\nLet‚Äôs see the effect of varying parameters \\(\\sigma\\) and \\(c\\) of the linear kernel function.\n\nval_list = [-3,-2,-1,0,1,2,3]\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), \n                  p_name='Offset', symbol='c')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nnp.random.seed(1)\nval_list = np.square(np.array([1,2,3,4,5,8]))\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), \n                  p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nVarying \\(c\\) parameter changes position of shallow region in covariance matrix. In other words, as \\(x \\to c\\), points close to \\(x\\) have variance \\(\\to \\sigma^2\\). Distant points have monotonically increasing variance.\nIncreasing \\(\\sigma^2\\) adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points.\n\nNon-stationary behaviour of Linear kernel\nUnlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim.\n\nfig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True)\nkerns = [K_rbf, K_m32, K_periodic, K_lin]\nk_names = ['RBF', 'Matern32', 'Periodic', 'Linear']\nX = np.linspace(-10,10,21).reshape(-1,1)\ndef update(x):\n  count = 0\n  for i in range(2):\n    for j in range(2):\n      ax.ravel()[count].cla()\n      tmp_kern = kerns[count]\n      mean = np.zeros(X.shape[0])\n      cov = tmp_kern(X,X)\n      ax.ravel()[count].plot(X, cov[:,x]);\n      ax.ravel()[count].set_xlim(X[x-3],X[x+3])\n      ax.ravel()[count].set_xlabel('X');\n      ax.ravel()[count].set_ylabel('K('+str(X[x].round(2))+',X)');\n      ax.ravel()[count].set_title('Covariance K('+str(X[x].round(2))+',X) for '+k_names[count]+' kernel');\n      count += 1\n  ax.ravel()[3].set_ylim(-5,80)\n  plt.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False)\nplt.close()\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "href": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Multiplications of kernels",
    "text": "Multiplications of kernels\nIf a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels.\n\nPeriodic * Linear\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label='Periodic')\nplt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label='Linear')\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label='Periodic*Linear')\nplt.legend(bbox_to_anchor=(1,1));plt.xlabel('X');plt.ylabel('Covariance')\nplt.title('K(0,*)');\n\n\n\n\n\n\n\n\n\n\nLinear * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nplt.plot(X, K_lin(X,X,c=-1)[:,50], label='Linear1')\nplt.plot(X, K_lin(X,X,c=1)[:,50], label='Linear2')\nplt.plot(X, K_lin(X,X,c=0.5)[:,50], label='Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label='Linear1*Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label='Linear1*Linear2*Linear3')\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\n\nMatern * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nk1 = K_lin(X,X,c=1)[:,50]\nk2 = K_m32(X,X)[:,50]\nplt.plot(X, k1, label='Linear')\nplt.plot(X, k2, label='Matern32')\nplt.plot(X, k1*k2, label='Matern32*Linear')\nplt.legend(bbox_to_anchor=(1,1));"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "href": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Appendix (Extra material)",
    "text": "Appendix (Extra material)\nAt this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes.\n\n\nPredicting at a single point after observing value at a single point\nLet‚Äôs see how would be a GP prediction after observing value at a single point.\nOur kernel function is given by, * \\(K(x,x')=(x-c) \\cdot (x'-c)+\\sigma^2\\)\nNow, we observe value \\(y\\) at a location \\(x\\) and we want to predict value \\(y^*\\) at location \\(x^*\\). \\[\n\\begin{aligned}\n(y^*|x_1,y_1,x^*) &= K(x^*,x) \\cdot K^{-1}(x,x)\\cdot y \\\\\n&= \\left(\\frac{(x-c)(x^*-c)+\\sigma^2}{(x-c)(x-c)+\\sigma^2}\\right)\\cdot y\n\\end{aligned}\n\\] \\(c\\) and \\(\\sigma^2\\) do not vary in numerator and denominator so, the value of \\(y^* \\propto x^*\\).\n\n\n\nPredicting at a single point after observing values at two points\nNow, we‚Äôll take a case where two values \\({y_1, y_2}\\) are observed at \\({x_1, x_2}\\). Let us try to predict value \\(y^*\\) at \\(x^*\\).\n$$ y^* =\n\\[\\begin{bmatrix}\nK(x_1, x^*) & K(x_2,x^*)\n\\end{bmatrix}\\begin{bmatrix}\nK(x_1, x_1) & K(x_1,x_2) \\\\\nK(x_2, x_1) & K(x_2,x_2)\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_1-c)^2+\\sigma^2 & (x_1-c) (x_2-c)+\\sigma^2 \\\\\n(x_2-c) (x_1-c)+\\sigma^2 & (x_2-c)^2 +\\sigma^2\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix} \\frac{1}{\\sigma^2(x_1-x_2)^2}\n\\begin{bmatrix}\n(x_2-c)^2+\\sigma^2 & -[(x_1-c)(x_2-c)+\\sigma^2] \\\\\n-[(x_2-c) (x_1-c)+\\sigma^2] & (x_1-c)^2 +\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix} \\tag{1}\\]\nFrom Eq. (1) second term, we can say that if \\(\\sigma^2=0\\), matrix is not-invertible because determinant is zero. It means that, if \\(\\sigma^2=0\\), observing a single point is enough, we can infer values at infinite points after observing that single point.\nEvaluating Eq. (1) further, it converges to the following equation, \\[\n\\begin{aligned}\ny^* = \\frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)}\n\\end{aligned}\n\\] Interestingly, we can see that output does not depend on \\(c\\) or \\(\\sigma^2\\) anymore. Let us verify experimentally if this is true for observing more than 2 data points.\n\n\nPrepering useful functions\n\nfrom scipy.optimize import minimize\n\n\ndef cov_func(x, x_prime, sigma, c):\n  return (x-c)@(x_prime-c) + sigma**2\n\ndef neg_log_likelihood(params):\n  n = X.shape[0]\n  sigma, c, noise_std = params\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) \n  return nll_ar[0,0]\n\ndef predict(params):\n  sigma, c, noise_std = params\n  k = cov_func(X, X.T, sigma, c)\n  np.fill_diagonal(k, k.diagonal()+noise_std**2)\n  k_inv = np.linalg.pinv(k)\n  k_star = cov_func(X_test, X.T, sigma, c)\n\n  mean = k_star@k_inv@Y\n  cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T\n  return mean, cov\n\n\n\nObserving more than two points and changing hyperparameters manually\n\nX = np.array([3,4,5,6,7,8]).reshape(-1,1)\nY = np.array([6,9,8,11,10,13]).reshape(-1,1)\nX_test = np.linspace(1,8,20).reshape(-1,1)\nparams_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], \n                [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std\n\nX_extra = np.hstack([np.ones((X.shape[0], 1)), X])\nTheta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y\nX_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\nY_test_ideal = X_test_extra@Theta\n\nfig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True)\nmeans = []\nfor p_i, params in enumerate(params_grid):\n  Y_test_mean, Y_test_cov = predict(params)\n  means.append(Y_test_mean)\n  ax[p_i].scatter(X, Y, label='train')\n  ax[p_i].scatter(X_test, Y_test_mean, label='test')\n  ax[p_i].legend();ax[p_i].set_xlabel('X');ax[p_i].set_ylabel('Y');\n  ax[p_i].set_title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\n\n\n\n\n\n\n\n\n\nnp.allclose(Y_test_ideal, means[0]),\\\nnp.allclose(Y_test_ideal, means[1]),\\\nnp.allclose(Y_test_ideal, means[2]),\\\nnp.allclose(Y_test_ideal, means[3])\n\n(True, True, True, False)\n\n\n\nmodel = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1))\n# model['Gaussian_noise'].fix(10**-10)\n# model.kern.variances.fix(10**-10)\nmodel.optimize()\nmodel.plot()\nplt.plot(X_test, Y_test_ideal, label='Normal Eq. fit')\nplt.plot(X_test,model.predict(X_test)[0], label='Prediction')\nplt.legend()\nmodel\n\n\n\n\nModel: GP regression\nObjective: 13.51314321804978\nNumber of Parameters: 2\nNumber of Optimization Parameters: 2\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nlinear.variances\n2.806515343539501\n+ve\n\n\n\nGaussian_noise.variance\n2.0834221617534134\n+ve\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there is no change in fit with change in \\(c\\) and \\(\\sigma\\). 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood.\n\nparams = [1., 1., 1.]\nresult = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)])\nparams = result.x\nprint(params, result.fun)\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(Y_test_ideal, Y_test_mean)\n\n[9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\ndef neg_log_likelihood(sigma, c, noise_std):\n  n = X.shape[0]\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov))\n  return nll_ar[0,0]\n\n\ngrad_func = grad(neg_log_likelihood, argnum=[0,1,2])\nalpha = 0.01\nloss = []\nsigma, c, noise_std = 1., 1., 1.\nfor _ in range(5000):\n  grads = grad_func(sigma, c, noise_std)\n  # print(grads)\n  sigma = sigma - alpha*grads[0]\n  c = c - alpha*grads[1]\n  noise_std = noise_std - alpha*grads[2]\n  loss.append(neg_log_likelihood(sigma, c, noise_std))\nprint(sigma, c, noise_std)\nplt.plot(loss);\nloss[-1]\n\n7.588989986845149 -2.830840439162303 32.2487569348891\n\n\n31.05187173290998\n\n\n\n\n\n\n\n\n\n\nparams = sigma, c, noise_std\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1)\n\nFalse"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes‚Äô theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes‚Äô theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "lab/scratchpad.html",
    "href": "lab/scratchpad.html",
    "title": "blog",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\ndef cdiv(a,b): return (a + b - 1) // b\n\n@triton.jit\ndef matmul(x_ptr, y_ptr, out_ptr, dx0, dx1, dy0, dy1, bs0: tl.constexpr, bs1: tl.constexpr):\n    pid_0 = tl.program_id(0)\n    pid_1 = tl.program_id(1)\n    \n    offs_0 = pid_0 * bs0 + tl.arange(0,bs0)  # 1d vector\n    offs_1 = pid_1 * bs1 + tl.arange(0,bs1)  # 1d vector\n\n    offs_x = dx1 * offs_0[:,None] + offs_1[None, :]  # 2d matrix! - we multiply first offset by width, see image above\n    mask_x_0 = offs_0 &lt; dx0\n    mask_x_1 = offs_1 &lt; dx1\n    mask_x = mask_x_0[:,None] & mask_x_1[None, :]\n    x = tl.load(x_ptr + offs_x, mask=mask_x)\n    \n    offs_y = dy1 * offs_1[:,None] + offs_0[None, :]\n    mask_y_0 = offs_0 &lt; dy0\n    mask_y_1 = offs_1 &lt; dy1\n    mask_y = mask_y_0[:,None] & mask_y_1[None, :]\n    y = tl.load(y_ptr + offs_y, mask=mask_y)\n    \n    out = x * y\n    \n    tl.store(out_ptr + offs_x, out, mask=mask_x)\n\nd0 = 5\nd1 = 7\nbs0 = 2\nbs1 = 2\nx = torch.arange(1, d0*d1+1).reshape(d0, d1).to(\"cuda\")\ny = torch.arange(d0*d1+1, 2*d0*d1+1).reshape(d1, d0).to(\"cuda\")\nprint(x)\nprint(y)\n\ngrid = lambda meta: (cdiv(d0, meta['bs0']), cdiv(d1,  meta['bs1']))\nout = torch.zeros(d0, d0, device=\"cuda\")\nmatmul[grid](x, y, out, d0, d1, d1, d0, bs0=bs0, bs1=bs1)\n\nprint(out)\n\ntensor([[ 1,  2,  3,  4,  5,  6,  7],\n        [ 8,  9, 10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19, 20, 21],\n        [22, 23, 24, 25, 26, 27, 28],\n        [29, 30, 31, 32, 33, 34, 35]], device='cuda:0')\ntensor([[36, 37, 38, 39, 40],\n        [41, 42, 43, 44, 45],\n        [46, 47, 48, 49, 50],\n        [51, 52, 53, 54, 55],\n        [56, 57, 58, 59, 60],\n        [61, 62, 63, 64, 65],\n        [66, 67, 68, 69, 70]], device='cuda:0')\ntensor([[  36.,   74.,  138.,  188.,  280.],\n        [   0.,    0.,  328.,  378.,  510.],\n        [ 572.,  732.,    0.,    0.,  570.],\n        [ 624.,  816.,  882., 1102.,    0.],\n        [   0.,  946., 1012., 1272., 1350.]], device='cuda:0')"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "",
    "text": "import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#setup",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#setup",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Setup",
    "text": "Setup\n\nConfigure your API keys\nTo fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n\nOpen your HuggingFace Settings page. Click Access Tokens then New Token to generate new token.\nGo to your Roboflow Settings page. Click Copy. This will place your private key in the clipboard.\nIn Colab, go to the left pane and click on Secrets (üîë).\n\nStore HuggingFace Access Token under the name HF_TOKEN.\nStore Roboflow API Key under the name ROBOFLOW_API_KEY.\n\n\n\n\nSelect the runtime\nLet‚Äôs make sure that we have access to GPU. We can use nvidia-smi command to do that. In case of any problems navigate to Edit -&gt; Notebook settings -&gt; Hardware accelerator, set it to L4 GPU, and then click Save.\n\n!nvidia-smi\n\nSun Feb 16 17:48:15 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:01:00.0 Off |                    0 |\n| N/A   35C    P0             64W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |\n| N/A   40C    P0             78W /  500W |   12139MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |\n| N/A   49C    P0            336W /  500W |   19831MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |\n| N/A   34C    P0             62W /  500W |       5MiB /  81920MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    1   N/A  N/A   2616743      C   ...onda3/envs/shataxi_space/bin/python      12128MiB |\n|    2   N/A  N/A   2627487      C   ...naconda3/envs/zeel_py310/bin/python      19820MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n\nDownload example data\nNOTE: Feel free to replace our example image with your own photo.\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!ls -lh\n\ntotal 12M\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.1\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.2\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.3\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.4\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.5\n-rw-rw-r-- 1 patel_zeel patel_zeel 104K Jun  2  2023  dog.jpeg.6\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:42 'how-to-finetune-florence-2-on-detection-dataset copy 2.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 3.0M Feb 16 17:42 'how-to-finetune-florence-2-on-detection-dataset copy.ipynb'\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.6M Feb 16 17:48  how-to-finetune-florence-2-on-detection-dataset.ipynb\ndrwxrwxr-x 4 patel_zeel patel_zeel 4.0K Feb 16 17:34  model_checkpoints\ndrwxrwxr-x 5 patel_zeel patel_zeel 4.0K Feb 16 17:29  poker-cards-4\n-rw-rw-r-- 1 patel_zeel patel_zeel 2.5M Jan 22 10:51  scratchpad.ipynb\n\n\n\nEXAMPLE_IMAGE_PATH = \"dog.jpeg\""
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#download-and-configure-the-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#download-and-configure-the-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Download and configure the model",
    "text": "Download and configure the model\nLet‚Äôs download the model checkpoint and configure it so that you can fine-tune it later on.\n\n# !pip install -q transformers flash_attn timm einops peft\n# !pip install -q roboflow git+https://github.com/roboflow/supervision.git\n\n\n# @title Imports\n\nimport io\nimport os\nimport re\nimport json\nimport torch\nimport html\nimport base64\nimport itertools\n\nimport numpy as np\nimport supervision as sv\n\n# from google.colab import userdata\nfrom IPython.core.display import display, HTML\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom roboflow import Roboflow\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n\n\nLoad the model using AutoModelForCausalLM and the processor using AutoProcessor classes from the transformers library. Note that you need to pass trust_remote_code as True since this model is not a standard transformers model.\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\n# REVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n\nImporting from timm.models.layers is deprecated, please import via timm.layers\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#run-inference-with-pre-trained-florence-2-model",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#run-inference-with-pre-trained-florence-2-model",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Run inference with pre-trained Florence-2 model",
    "text": "Run inference with pre-trained Florence-2 model\n\n# @title Example object detection inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\n\n# @title Example image captioning inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;DETAILED_CAPTION&gt;\"\ntext = \"&lt;DETAILED_CAPTION&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\nresponse\n\n{'&lt;DETAILED_CAPTION&gt;': 'In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.'}\n\n\n\n# @title Example caption to phrase grounding inference\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;\"\ntext = \"&lt;CAPTION_TO_PHRASE_GROUNDING&gt; Vehicle\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom dataset",
    "text": "Fine-tune Florence-2 on custom dataset\n\nDownload dataset from Roboflow Universe\n\nROBOFLOW_API_KEY = os.getenv(\"ROBOFLOW_API_KEY\")\nrf = Roboflow(api_key=ROBOFLOW_API_KEY)\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"poker-cards-fmjio\")\nversion = project.version(4)\ndataset = version.download(\"florence2-od\")\n\nloading Roboflow workspace...\nloading Roboflow project...\n\n\n\n!head -n 5 {dataset.location}/train/annotations.jsonl\n\n{\"image\":\"IMG_20220316_172418_jpg.rf.e3cb4a86dc0247e71e3697aa3e9db923.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of clubs&lt;loc_138&gt;&lt;loc_100&gt;&lt;loc_470&gt;&lt;loc_448&gt;10 of clubs&lt;loc_388&gt;&lt;loc_145&gt;&lt;loc_670&gt;&lt;loc_453&gt;jack  of clubs&lt;loc_566&gt;&lt;loc_166&gt;&lt;loc_823&gt;&lt;loc_432&gt;queen of clubs&lt;loc_365&gt;&lt;loc_465&gt;&lt;loc_765&gt;&lt;loc_999&gt;king of clubs&lt;loc_601&gt;&lt;loc_440&gt;&lt;loc_949&gt;&lt;loc_873&gt;\"}\n{\"image\":\"IMG_20220316_171515_jpg.rf.e3b1932bb375b3b3912027647586daa8.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"5 of clubs&lt;loc_554&gt;&lt;loc_2&gt;&lt;loc_763&gt;&lt;loc_467&gt;6 of clubs&lt;loc_399&gt;&lt;loc_79&gt;&lt;loc_555&gt;&lt;loc_466&gt;7 of clubs&lt;loc_363&gt;&lt;loc_484&gt;&lt;loc_552&gt;&lt;loc_905&gt;8 of clubs&lt;loc_535&gt;&lt;loc_449&gt;&lt;loc_757&gt;&lt;loc_971&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e30257ec169a2bfdfecb693211d37250.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_596&gt;&lt;loc_535&gt;&lt;loc_859&gt;&lt;loc_982&gt;jack of diamonds&lt;loc_211&gt;&lt;loc_546&gt;&lt;loc_411&gt;&lt;loc_880&gt;queen of diamonds&lt;loc_430&gt;&lt;loc_34&gt;&lt;loc_692&gt;&lt;loc_518&gt;king of diamonds&lt;loc_223&gt;&lt;loc_96&gt;&lt;loc_451&gt;&lt;loc_523&gt;10 of diamonds&lt;loc_387&gt;&lt;loc_542&gt;&lt;loc_604&gt;&lt;loc_925&gt;\"}\n{\"image\":\"IMG_20220316_143407_jpg.rf.e1eb3be3efc6c3bbede436cfb5489e7c.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"ace of hearts&lt;loc_345&gt;&lt;loc_315&gt;&lt;loc_582&gt;&lt;loc_721&gt;2 of hearts&lt;loc_709&gt;&lt;loc_115&gt;&lt;loc_888&gt;&lt;loc_509&gt;3 of hearts&lt;loc_529&gt;&lt;loc_228&gt;&lt;loc_735&gt;&lt;loc_613&gt;4 of hearts&lt;loc_98&gt;&lt;loc_421&gt;&lt;loc_415&gt;&lt;loc_845&gt;\"}\n{\"image\":\"IMG_20220316_165139_jpg.rf.e4c229a9128494d17992cbe88af575df.jpg\",\"prefix\":\"&lt;OD&gt;\",\"suffix\":\"9 of diamonds&lt;loc_141&gt;&lt;loc_18&gt;&lt;loc_404&gt;&lt;loc_465&gt;jack of diamonds&lt;loc_589&gt;&lt;loc_120&gt;&lt;loc_789&gt;&lt;loc_454&gt;queen of diamonds&lt;loc_308&gt;&lt;loc_482&gt;&lt;loc_570&gt;&lt;loc_966&gt;king of diamonds&lt;loc_549&gt;&lt;loc_477&gt;&lt;loc_777&gt;&lt;loc_904&gt;10 of diamonds&lt;loc_396&gt;&lt;loc_75&gt;&lt;loc_613&gt;&lt;loc_458&gt;\"}\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n# # read jsonl file\n# def read_jsonl(file_path: str) -&gt; Generator[Dict[str, Any], None, None]:\n#     with open(file_path, \"r\") as f:\n#         for line in f:\n#             yield json.loads(line)\n\n# lines = []\n# split = \"test\"     \n# for line in read_jsonl(dataset.location + f\"/{split}/annotations.jsonl\"):\n#     # print(line)\n#     # edit = True\n#     # copied_line = list(line['suffix'])\n#     # for i in range(len(copied_line)):\n#     #     if copied_line[i] == \"&lt;\":\n#     #         edit = False\n#     #     elif copied_line[i] == \"&gt;\":\n#     #         edit = True\n#     #     else:\n#     #         if edit:\n#     #             copied_line[i] = chr(ord(copied_line[i]) + 1)\n#     # copied_line = \"\".join(copied_line)\n#     # line['suffix'] = copied_line\n    \n#     line['suffix'] = line['suffix'].replace(\"club\", \"dog\").replace(\"diamond\", \"cat\").replace(\"heart\", \"bird\").replace(\"spade\", \"fish\")\n#     print(line)\n#     lines.append(line)\n\n# with open(dataset.location + f\"/{split}/annotations.jsonl\", \"w\") as f:\n#     for line in lines:\n#         f.write(json.dumps(line) + \"\\n\")\n\n\n# @title Define `DetectionsDataset` class\n\nclass JSONLDataset:\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.jsonl_file_path = jsonl_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -&gt; List[Dict[str, Any]]:\n        entries = []\n        with open(self.jsonl_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -&gt; int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[Image.Image, Dict[str, Any]]:\n        if idx &lt; 0 or idx &gt;= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n\n\nclass DetectionDataset(Dataset):\n    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, data = self.dataset[idx]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        return prefix, suffix, image\n\n\n# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n    return inputs, answers\n\ntrain_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/train/\"\n)\nval_dataset = DetectionDataset(\n    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n    image_directory_path = f\"{dataset.location}/valid/\"\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n\n# @title Setup LoRA Florence-2 model\n\n# config = LoraConfig(\n#     r=8,\n#     lora_alpha=8,\n#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n#     task_type=\"CAUSAL_LM\",\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     inference_mode=False,\n#     use_rslora=True,\n#     init_lora_weights=\"gaussian\",\n# )\nconfig = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n        task_type=\"CAUSAL_LM\",\n        lora_dropout=0.05,\n        bias=\"none\",\n        init_lora_weights=\"gaussian\",\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()\n\ntrainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n\n\n\ntorch.cuda.empty_cache()\n\n\n# @title Run inference with pre-trained Florence-2 model on validation dataset\n\ndef render_inline(image: Image.Image, resize=(128, 128)):\n    \"\"\"Convert image into inline html.\"\"\"\n    image.resize(resize)\n    with io.BytesIO() as buffer:\n        image.save(buffer, format='jpeg')\n        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n        return f\"data:image/jpeg;base64,{image_b64}\"\n\n\ndef render_example(image: Image.Image, response):\n    try:\n        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n    except:\n        print('failed to redner model response')\n    return f\"\"\"\n&lt;div style=\"display: inline-flex; align-items: center; justify-content: center;\"&gt;\n    &lt;img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" /&gt;\n    &lt;p style=\"width:512px; margin:10px; font-size:small;\"&gt;{html.escape(json.dumps(response))}&lt;/p&gt;\n&lt;/div&gt;\n\"\"\"\n\n\ndef render_inference_results(model, dataset: DetectionDataset, count: int):\n    html_out = \"\"\n    count = min(count, len(dataset))\n    for i in range(count):\n        image, data = dataset.dataset[i]\n        prefix = data['prefix']\n        suffix = data['suffix']\n        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n        answer = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n        html_out += render_example(image, answer)\n\n    display(HTML(html_out))\n\nrender_inference_results(peft_model, val_dataset, 4)\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tune-florence-2-on-custom-object-detection-dataset",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tune Florence-2 on custom object detection dataset",
    "text": "Fine-tune Florence-2 on custom object detection dataset\n\n# @title Define train loop\n\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    render_inference_results(peft_model, val_loader.dataset, 6)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward(), optimizer.step(), lr_scheduler.step(), optimizer.zero_grad()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n            render_inference_results(peft_model, val_loader.dataset, 6)\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n\n%%time\n\nEPOCHS = 10\nLR = 5e-6\n\ntrain_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)\n\nThis implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"table\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"chair\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"tablecloth\"]}}\n\n\n\nTraining Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:40&lt;00:00,  1.36it/s]\n\n\nAverage Training Loss: 5.220192882944556\n\n\nValidation Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.41it/s]\n\n\nAverage Validation Loss: 3.9150948226451874\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"bed\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[198.0800018310547, 175.0399932861328, 487.3599853515625, 496.3199768066406]], \"labels\": [\"playing card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[160.95999145507812, 210.87998962402344, 182.0800018310547, 248.0], [322.239990234375, 212.1599884033203, 344.0, 243.51998901367188]], \"labels\": [\"human face\", \"human face\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"furniture\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"blanket\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 0.3199999928474426, 639.0399780273438, 639.0399780273438]], \"labels\": [\"dining table\"]}}\n\n\n\nSetting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\nTraining Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:40&lt;00:00,  1.35it/s]\n\n\nAverage Training Loss: 3.782239447621738\n\n\nValidation Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.42it/s]\n\n\nAverage Validation Loss: 3.1201466619968414\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.95999908447266, 512.3200073242188, 357.44000244140625], [164.8000030517578, 330.55999755859375, 301.7599792480469, 585.2799682617188], [173.1199951171875, 14.399999618530273, 303.67999267578125, 253.1199951171875], [372.79998779296875, 112.95999908447266, 512.9599609375, 357.44000244140625], [309.44000244140625, 360.0, 447.03997802734375, 616.6400146484375], [52.79999923706055, 239.0399932861328, 166.0800018310547, 470.0799865722656]], \"labels\": [\"queen of spades\", \"king of spade\", \"queen spades\", \"queen card\", \"queen's spades\", \"queen clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.8399963378906, 411.8399963378906], [198.72000122070312, 82.23999786376953, 381.1199951171875, 324.1600036621094], [333.1199951171875, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375]], \"labels\": [\"queen of spades\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[56.0, 228.79998779296875, 331.1999816894531, 639.0399780273438], [436.79998779296875, 157.1199951171875, 557.1199951171875, 392.0], [297.91998291015625, 252.47999572753906, 459.1999816894531, 550.0800170898438], [330.55999755859375, 150.0800018310547, 479.67999267578125, 447.03997802734375]], \"labels\": [\"8 of spades\", \"6 of spade\", \"7 of clubs\", \"6 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[15.039999961853027, 254.39999389648438, 213.44000244140625, 464.9599914550781], [208.95999145507812, 285.7599792480469, 345.2799987792969, 461.7599792480469], [327.3599853515625, 191.67999267578125, 466.8799743652344, 397.7599792480469]], \"labels\": [\"queen of spades\", \"7 of hearts\", \"6 of spade\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[294.0799865722656, 176.3199920654297, 624.3200073242188, 399.03997802734375], [11.199999809265137, 228.1599884033203, 274.8800048828125, 427.8399963378906], [96.95999908447266, 432.9599914550781, 314.55999755859375, 564.7999877929688], [309.44000244140625, 423.3599853515625, 548.1599731445312, 562.239990234375]], \"labels\": [\"9 of clubs\", \"9 of hearts\", \"9 of diamonds\", \"9 of spades\"]}}\n\n\n\nTraining Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:39&lt;00:00,  1.36it/s]\n\n\nAverage Training Loss: 3.214196660939385\n\n\nValidation Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.45it/s]\n\n\nAverage Validation Loss: 2.605478435754776\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [164.16000366210938, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 166.0800018310547, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.8399963378906, 411.8399963378906], [200.63999938964844, 82.23999786376953, 381.1199951171875, 323.5199890136719], [330.55999755859375, 41.91999816894531, 517.4400024414062, 207.0399932861328]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 490.55999755859375], [186.55999755859375, 273.6000061035156, 396.47998046875, 511.67999267578125], [86.08000183105469, 163.51998901367188, 255.67999267578125, 402.8800048828125]], \"labels\": [\"queen of spades\", \"queen spades\", \"queen card\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 253.75999450683594, 457.91998291015625, 549.4400024414062], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 466.8799743652344, 397.7599792480469], [208.95999145507812, 285.7599792480469, 345.2799987792969, 461.7599792480469], [463.67999267578125, 221.1199951171875, 635.8399658203125, 406.0799865722656]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"9 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 227.51998901367188, 275.5199890136719, 428.47998046875], [98.23999786376953, 432.9599914550781, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.0, 548.1599731445312, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"6 of spoons\"]}}\n\n\n\nTraining Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:07&lt;00:00,  1.07it/s]\n\n\nAverage Training Loss: 2.6999165170332966\n\n\nValidation Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.52it/s]\n\n\nAverage Validation Loss: 2.0440672636032104\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 357.44000244140625], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 166.0800018310547, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 82.23999786376953, 381.1199951171875, 323.5199890136719], [330.55999755859375, 41.91999816894531, 516.7999877929688, 207.0399932861328]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 491.1999816894531], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 403.5199890136719]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.75999450683594, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [333.1199951171875, 152.0, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 466.8799743652344, 397.7599792480469], [208.95999145507812, 285.1199951171875, 345.2799987792969, 461.7599792480469], [463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [14.399999618530273, 254.39999389648438, 214.0800018310547, 465.5999755859375]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"9 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.7200012207031, 424.0, 548.1599731445312, 562.239990234375]], \"labels\": [\"2 of spades\", \"5 of spade\", \"9 of spoons\"]}}\n\n\n\nTraining Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 2.1655465303098453\n\n\nValidation Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.47it/s]\n\n\nAverage Validation Loss: 1.7417135536670685\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [330.55999755859375, 42.55999755859375, 516.7999877929688, 207.0399932861328]], \"labels\": [\"9 of clubs\", \"7 of clubs\", \"5 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[368.3199768066406, 234.55999755859375, 518.0800170898438, 491.8399963378906], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 403.5199890136719]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 391.3599853515625], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375]], \"labels\": [\"6 of spades\", \"7 of spade\", \"5 of spoons\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656], [14.399999618530273, 254.39999389648438, 214.0800018310547, 466.239990234375]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"5 of hearts\", \"8 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[98.23999786376953, 433.5999755859375, 314.55999755859375, 563.5199584960938], [310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"9 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:39&lt;00:00,  1.37it/s]\n\n\nAverage Training Loss: 1.8535377207924337\n\n\nValidation Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:04&lt;00:00,  1.90it/s]\n\n\nAverage Validation Loss: 1.6721670180559158\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625], [310.0799865722656, 360.0, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of sp hearts\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [330.55999755859375, 42.55999755859375, 516.7999877929688, 207.0399932861328], [197.44000244140625, 173.1199951171875, 488.6399841308594, 498.239990234375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.1999816894531], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 402.8800048828125], [258.239990234375, 168.63999938964844, 388.79998779296875, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"jack of spands\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 391.3599853515625], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [14.399999618530273, 254.39999389648438, 214.0800018310547, 466.239990234375], [462.3999938964844, 221.1199951171875, 636.47998046875, 407.3599853515625]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[97.5999984741211, 433.5999755859375, 314.55999755859375, 564.1599731445312], [310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875]], \"labels\": [\"5 of clubs\", \"9 of clubs\", \"7 of clubs\"]}}\n\n\n\nTraining Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:39&lt;00:00,  1.17s/it]\n\n\nAverage Training Loss: 1.741150463328642\n\n\nValidation Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:07&lt;00:00,  1.09it/s]\n\n\nAverage Validation Loss: 1.6433503776788712\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [162.87998962402344, 330.55999755859375, 301.1199951171875, 585.2799682617188], [53.439998626708984, 239.0399932861328, 167.36000061035156, 469.44000244140625], [310.0799865722656, 360.0, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of sp hearts\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 43.20000076293945, 516.7999877929688, 207.0399932861328], [198.72000122070312, 175.0399932861328, 488.6399841308594, 497.5999755859375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.8399963378906], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 402.8800048828125], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"jack of spands\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 391.3599853515625], [333.1199951171875, 151.36000061035156, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 550.0800170898438]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375], [463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 401.6000061035156]], \"labels\": [\"9 of clubs\", \"5 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:18&lt;00:00,  1.02s/it]\n\n\nAverage Training Loss: 1.707651296959204\n\n\nValidation Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.47it/s]\n\n\nAverage Validation Loss: 1.6269832402467728\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 113.5999984741211, 512.3200073242188, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 167.36000061035156, 470.0799865722656], [310.0799865722656, 360.0, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of sp hearts\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 42.55999755859375, 516.7999877929688, 207.0399932861328], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.8399963378906], [18.8799991607666, 289.6000061035156, 224.95999145507812, 582.0800170898438], [186.55999755859375, 273.6000061035156, 397.1199951171875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 403.5199890136719], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"jack of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [14.399999618530273, 254.39999389648438, 214.0800018310547, 466.239990234375], [462.3999938964844, 221.1199951171875, 636.47998046875, 407.3599853515625]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [98.23999786376953, 433.5999755859375, 314.55999755859375, 564.1599731445312], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 401.6000061035156]], \"labels\": [\"9 of clubs\", \"5 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:38&lt;00:00,  1.38it/s]\n\n\nAverage Training Loss: 1.6793172578601276\n\n\nValidation Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.42it/s]\n\n\nAverage Validation Loss: 1.618808850646019\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 167.36000061035156, 470.0799865722656], [310.0799865722656, 360.0, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of sp hearts\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 42.55999755859375, 516.7999877929688, 207.0399932861328], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.8399963378906], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 403.5199890136719], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"jack of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375], [463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [98.23999786376953, 433.5999755859375, 314.55999755859375, 563.5199584960938], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 401.6000061035156]], \"labels\": [\"9 of clubs\", \"5 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nTraining Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [01:40&lt;00:00,  1.35it/s]\n\n\nAverage Training Loss: 1.6717844158411026\n\n\nValidation Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03&lt;00:00,  2.47it/s]\n\n\nAverage Validation Loss: 1.615568920969963\n\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[372.79998779296875, 112.31999969482422, 512.3200073242188, 358.0799865722656], [161.59999084472656, 330.55999755859375, 301.1199951171875, 585.2799682617188], [52.79999923706055, 239.0399932861328, 167.36000061035156, 470.0799865722656], [310.0799865722656, 360.0, 446.3999938964844, 616.6400146484375]], \"labels\": [\"queen of spades\", \"king of spade\", \"9 of sp hearts\", \"10 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[0.3199999928474426, 128.3199920654297, 267.1999816894531, 411.8399963378906], [200.63999938964844, 83.5199966430664, 381.1199951171875, 323.5199890136719], [333.1199951171875, 42.55999755859375, 516.7999877929688, 207.0399932861328], [198.0800018310547, 173.1199951171875, 488.6399841308594, 497.5999755859375]], \"labels\": [\"6 of clubs\", \"7 of clubs\", \"5 of clubs\", \"8 of clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[369.6000061035156, 234.55999755859375, 518.0800170898438, 491.8399963378906], [18.8799991607666, 289.6000061035156, 223.67999267578125, 582.0800170898438], [186.55999755859375, 273.6000061035156, 396.47998046875, 512.3200073242188], [87.36000061035156, 164.16000366210938, 255.0399932861328, 403.5199890136719], [257.6000061035156, 168.63999938964844, 388.79998779296875, 317.1199951171875]], \"labels\": [\"9 of spades\", \"10 of spade\", \"king of sp clubs\", \"queen of spoons\", \"jack of sp hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[437.44000244140625, 157.1199951171875, 556.47998046875, 390.0799865722656], [333.1199951171875, 150.72000122070312, 479.03997802734375, 447.03997802734375], [298.55999755859375, 254.39999389648438, 457.2799987792969, 549.4400024414062]], \"labels\": [\"6 of spades\", \"5 of spade\", \"7 of sp clubs\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[328.6399841308594, 192.3199920654297, 467.5199890136719, 399.03997802734375], [208.95999145507812, 285.1199951171875, 346.55999755859375, 463.67999267578125], [15.039999961853027, 254.39999389648438, 214.0800018310547, 465.5999755859375], [463.67999267578125, 221.1199951171875, 636.47998046875, 406.0799865722656]], \"labels\": [\"6 of hearts\", \"7 of hearts\", \"8 of hearts\", \"5 of hearts\"]}}\n\n\n\n    \n    {\"&lt;OD&gt;\": {\"bboxes\": [[310.0799865722656, 424.0, 548.7999877929688, 562.239990234375], [98.23999786376953, 433.5999755859375, 314.55999755859375, 563.5199584960938], [10.559999465942383, 227.51998901367188, 275.5199890136719, 429.1199951171875], [291.5199890136719, 175.0399932861328, 625.5999755859375, 401.6000061035156]], \"labels\": [\"9 of clubs\", \"5 of clubs\", \"7 of clubs\", \"8 of clubs\"]}}\n\n\n\nCPU times: user 14min 58s, sys: 5min 54s, total: 20min 53s\nWall time: 19min 49s"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tuned-model-evaluation",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#fine-tuned-model-evaluation",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Fine-tuned model evaluation",
    "text": "Fine-tuned model evaluation\n\n# @title Check if the model can still detect objects outside of the custom dataset\n\nimage = Image.open(EXAMPLE_IMAGE_PATH)\ntask = \"&lt;OD&gt;\"\ntext = \"&lt;OD&gt;\"\n\ninputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\ngenerated_ids = peft_model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    num_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nresponse = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\ndetections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\nlabel_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n\nimage = bounding_box_annotator.annotate(image, detections)\nimage = label_annotator.annotate(image, detections)\nimage.thumbnail((600, 600))\nimage\n\nBoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n\n\n\n\n\n\n\n\n\nNOTE: It seems that the model can still detect classes that don‚Äôt belong to our custom dataset.\n\n# @title Collect predictions\n\nPATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)&lt;loc_\\d+&gt;'\n\ndef extract_classes(dataset: DetectionDataset):\n    class_set = set()\n    for i in range(len(dataset.dataset)):\n        image, data = dataset.dataset[i]\n        suffix = data[\"suffix\"]\n        classes = re.findall(PATTERN, suffix)\n        class_set.update(classes)\n    return sorted(class_set)\n\nCLASSES = extract_classes(train_dataset)\n\ntargets = []\npredictions = []\n\nfor i in range(len(val_dataset.dataset)):\n    image, data = val_dataset.dataset[i]\n    prefix = data['prefix']\n    suffix = data['suffix']\n\n    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    prediction = processor.post_process_generation(generated_text, task='&lt;OD&gt;', image_size=image.size)\n    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n    prediction.confidence = np.ones(len(prediction))\n\n    target = processor.post_process_generation(suffix, task='&lt;OD&gt;', image_size=image.size)\n    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n    target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n    targets.append(target)\n    predictions.append(prediction)\n\n\n# @title Calculate mAP\n# mean_average_precision = sv.MeanAveragePrecision.from_detections(\n#     predictions=predictions,\n#     targets=targets,\n# )\nmean_average_precision = sv.metrics.MeanAveragePrecision().update(predictions, targets).compute()\n\nprint(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\nprint(f\"map50: {mean_average_precision.map50:.2f}\")\nprint(f\"map75: {mean_average_precision.map75:.2f}\")\n\nmap50_95: 0.30\nmap50: 0.32\nmap75: 0.32\n\n\n\np = sv.metrics.Precision()\np = p.update(predictions, targets).compute()\nprint(p.precision_at_50)\n\nr = sv.metrics.Recall()\nr = r.update(predictions, targets).compute()\nprint(r.recall_at_50)\n\n0.5440355329949238\n0.3553299492385787\n\n\ninvalid value encountered in divide\n\n\n\n# @title Calculate Confusion Matrix\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=targets,\n    classes=CLASSES\n)\n\n_ = confusion_matrix.plot()"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#save-fine-tuned-model-on-hard-drive",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#save-fine-tuned-model-on-hard-drive",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Save fine-tuned model on hard drive",
    "text": "Save fine-tuned model on hard drive\n\npeft_model.save_pretrained(\"/content/florence2-lora\")\nprocessor.save_pretrained(\"/content/florence2-lora/\")\n!ls -la /content/florence2-lora/\n\n\n---------------------------------------------------------------------------\nPermissionError                           Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 peft_model.save_pretrained(\"/content/florence2-lora\")\n      2 processor.save_pretrained(\"/content/florence2-lora/\")\n      3 get_ipython().system('ls -la /content/florence2-lora/')\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/site-packages/peft/peft_model.py:320, in PeftModel.save_pretrained(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, path_initial_model_for_weight_conversion, **kwargs)\n    317     return output_state_dict\n    319 if is_main_process:\n--&gt; 320     os.makedirs(save_directory, exist_ok=True)\n    321     self.create_or_update_model_card(save_directory)\n    323 for adapter_name in selected_adapters:\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:215, in makedirs(name, mode, exist_ok)\n    213 if head and tail and not path.exists(head):\n    214     try:\n--&gt; 215         makedirs(head, exist_ok=exist_ok)\n    216     except FileExistsError:\n    217         # Defeats race condition when another thread created the path\n    218         pass\n\nFile /opt/anaconda3/envs/zeel_py310/lib/python3.10/os.py:225, in makedirs(name, mode, exist_ok)\n    223         return\n    224 try:\n--&gt; 225     mkdir(name, mode)\n    226 except OSError:\n    227     # Cannot rely on checking for EEXIST, since the operating system\n    228     # could give priority to other errors like EACCES or EROFS\n    229     if not exist_ok or not path.isdir(name):\n\nPermissionError: [Errno 13] Permission denied: '/content'"
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#upload-model-to-roboflow-optional",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#upload-model-to-roboflow-optional",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Upload model to Roboflow (optional)",
    "text": "Upload model to Roboflow (optional)\nYou can deploy your Florence-2 object detection model on your own hardware (i.e.¬†a cloud GPu server or an NVIDIA Jetson) with Roboflow Inference, an open source computer vision inference server.\nTo deploy your model, you will need a free Roboflow account.\nTo get started, create a new Project in Roboflow if you don‚Äôt already have one. Then, upload the dataset you used to train your model. Then, create a dataset Version, which is a snapshot of your dataset with which your model will be associated in Roboflow.\nYou can read our full Deploy Florence-2 with Roboflow guide for step-by-step instructions of these steps.\nOnce you have trained your model A, you can upload it to Roboflow using the following code:\n\nimport roboflow\n\nrf = Roboflow(api_key=\"API_KEY\")\nproject = rf.workspace(\"workspace-id\").project(\"project-id\")\nversion = project.version(VERSION)\n\nversion.deploy(model_type=\"florence-2\", model_path=\"/content/florence2-lora\")\n\nAbove, replace:\n\nAPI_KEY with your Roboflow API key.\nworkspace-id and project-id with your workspace and project IDs.\nVERSION with your project version.\n\nIf you are not using our notebook, replace /content/florence2-lora with the directory where you saved your model weights.\nWhen you run the code above, the model will be uploaded to Roboflow. It will take a few minutes for the model to be processed before it is ready for use.\nYour model will be uploaded to Roboflow."
  },
  {
    "objectID": "lab/how-to-finetune-florence-2-on-detection-dataset.html#deploy-to-your-hardware",
    "href": "lab/how-to-finetune-florence-2-on-detection-dataset.html#deploy-to-your-hardware",
    "title": "Fine-tuning Florence-2 on Object Detection Dataset",
    "section": "Deploy to your hardware",
    "text": "Deploy to your hardware\nOnce your model has been processed, you can download it to any device on which you want to deploy your model. Deployment is supported through Roboflow Inference, our open source computer vision inference server.\nInference can be run as a microservice with Docker, ideal for large deployments where you may need a centralized server on which to run inference, or when you want to run Inference in an isolated container. You can also directly integrate Inference into your project through the Inference Python SDK.\nFor this guide, we will show how to deploy the model with the Python SDK.\nFirst, install inference:\n\n!pip install inference\n\nThen, create a new Python file and add the following code:\n\nimport os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)\n\nIn the code avove, we load our model, run it on an image, then plot the predictions with the supervision Python package.\nWhen you first run the code, your model weights will be downloaded and cached to your device for subsequent runs. This process may take a few minutes depending on the strength of your internet connection."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Zeel. This is my blog, where I add coding + other resources related to my research. Head over to this page for my personal website."
  }
]