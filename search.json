[
  {
    "objectID": "posts/pruning_vs_uncertainty.html",
    "href": "posts/pruning_vs_uncertainty.html",
    "title": "Pruning vs Uncertainty",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\n# import pruning library\nimport torch.nn.utils.prune as prune\n\n# import torchvision\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n\ntry:\n    from laplace import Laplace\nexcept ModuleNotFoundError:\n    %pip install laplace-torch\n    from laplace import Laplace\n\n&lt;frozen importlib._bootstrap&gt;:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "href": "posts/pruning_vs_uncertainty.html#train-a-model-on-mnist",
    "title": "Pruning vs Uncertainty",
    "section": "Train a model on MNIST",
    "text": "Train a model on MNIST\n\n# Define data transformations\ntransform = transforms.Compose(\n    [\n        transforms.Resize((224, 224)),\n        transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n        # convert dtype to float32\n        # transforms.Lambda(lambda x: x.to(torch.float32)),\n    ]\n)\n\n\n# Load MNIST dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, transform=transform, download=True\n)\nprint(\"Train size\", len(train_dataset))\n\ntrain_dataset = TensorDataset(\n    train_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    train_dataset.targets.to(device),\n)\ntest_dataset = datasets.MNIST(\n    root=\"./data\", train=False, transform=transform, download=True\n)\nprint(\"Test size\", len(test_dataset))\ntest_dataset = TensorDataset(\n    test_dataset.data[..., None]\n    .repeat(1, 1, 1, 3)\n    .swapaxes(1, 3)\n    .swapaxes(2, 3)\n    .to(torch.float32)\n    .to(device),\n    test_dataset.targets.to(device),\n)\n\nUsing cuda device\nTrain size 60000\nTest size 10000\n\n\n\ntrain_dataset[0][0].dtype, train_dataset[0][1].dtype\n\n(torch.float32, torch.int64)\n\n\n\n# Define data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# Load pre-trained ResNet model\nresnet = torchvision.models.resnet18(pretrained=True)\nprint(\"Loaded pre-trained ResNet18 model\")\nprint(resnet.fc.in_features)\n\n# Modify the last fully connected layer to match MNIST's number of classes (10)\nnum_classes = 10\nresnet.fc = nn.Sequential(\n    nn.Linear(resnet.fc.in_features, resnet.fc.in_features),\n    nn.GELU(),\n    nn.Linear(resnet.fc.in_features, num_classes),\n)\n\n# Freeze all layers except the last fully connected layer\nfor name, param in resnet.named_parameters():\n    param.requires_grad = False\nresnet.fc.requires_grad_(True)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n\n# Training loop\nnum_epochs = 50\nprint(f\"Training on device {device}\")\nresnet.to(device)\n\nprint(\"Training ResNet18 model\")\nfor epoch in range(num_epochs):\n    resnet.train()\n    epoch_loss = 0.0\n    for images, labels in tqdm(train_loader):\n        optimizer.zero_grad()\n        outputs = resnet(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n\n    # Evaluation\n    resnet.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        predicted_list = []\n        for images, labels in test_loader:\n            outputs = resnet(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/patel_zeel/miniconda3/envs/torch_dt/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\nLoaded pre-trained ResNet18 model\n512\nTraining on device cuda\nTraining ResNet18 model\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 242.75it/s]\n\n\nEpoch [1/50] Loss: 1.0877\nAccuracy on the test set: 75.42%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 262.53it/s]\n\n\nEpoch [2/50] Loss: 0.8051\nAccuracy on the test set: 76.74%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 270.43it/s]\n\n\nEpoch [3/50] Loss: 0.7578\nAccuracy on the test set: 78.27%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 265.38it/s]\n\n\nEpoch [4/50] Loss: 0.7290\nAccuracy on the test set: 78.71%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 265.51it/s]\n\n\nEpoch [5/50] Loss: 0.7083\nAccuracy on the test set: 79.62%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 266.62it/s]\n\n\nEpoch [6/50] Loss: 0.6761\nAccuracy on the test set: 79.82%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 268.49it/s]\n\n\nEpoch [7/50] Loss: 0.6627\nAccuracy on the test set: 80.47%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 266.33it/s]\n\n\nEpoch [8/50] Loss: 0.6423\nAccuracy on the test set: 80.24%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 268.52it/s]\n\n\nEpoch [9/50] Loss: 0.6257\nAccuracy on the test set: 81.11%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 269.38it/s]\n\n\nEpoch [10/50] Loss: 0.6131\nAccuracy on the test set: 81.42%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 264.77it/s]\n\n\nEpoch [11/50] Loss: 0.5911\nAccuracy on the test set: 82.02%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 266.07it/s]\n\n\nEpoch [12/50] Loss: 0.5765\nAccuracy on the test set: 82.32%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 262.19it/s]\n\n\nEpoch [13/50] Loss: 0.5611\nAccuracy on the test set: 82.30%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 214.62it/s]\n\n\nEpoch [14/50] Loss: 0.5466\nAccuracy on the test set: 82.49%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 219.31it/s]\n\n\nEpoch [15/50] Loss: 0.5358\nAccuracy on the test set: 82.81%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 226.53it/s]\n\n\nEpoch [16/50] Loss: 0.5266\nAccuracy on the test set: 83.30%\n\n\n100%|██████████| 938/938 [00:05&lt;00:00, 171.25it/s]\n\n\nEpoch [17/50] Loss: 0.5137\nAccuracy on the test set: 83.37%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 278.59it/s]\n\n\nEpoch [18/50] Loss: 0.5051\nAccuracy on the test set: 83.17%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 248.82it/s]\n\n\nEpoch [19/50] Loss: 0.4969\nAccuracy on the test set: 83.46%\n\n\n100%|██████████| 938/938 [00:05&lt;00:00, 175.56it/s]\n\n\nEpoch [20/50] Loss: 0.4811\nAccuracy on the test set: 83.76%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 277.18it/s]\n\n\nEpoch [21/50] Loss: 0.4714\nAccuracy on the test set: 83.57%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 273.71it/s]\n\n\nEpoch [22/50] Loss: 0.4624\nAccuracy on the test set: 84.25%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 242.18it/s]\n\n\nEpoch [23/50] Loss: 0.4553\nAccuracy on the test set: 84.27%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 279.42it/s]\n\n\nEpoch [24/50] Loss: 0.4506\nAccuracy on the test set: 84.62%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 269.21it/s]\n\n\nEpoch [25/50] Loss: 0.4394\nAccuracy on the test set: 83.97%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 227.36it/s]\n\n\nEpoch [26/50] Loss: 0.4346\nAccuracy on the test set: 84.16%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 222.91it/s]\n\n\nEpoch [27/50] Loss: 0.4271\nAccuracy on the test set: 84.38%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 223.68it/s]\n\n\nEpoch [28/50] Loss: 0.4193\nAccuracy on the test set: 84.84%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 261.50it/s]\n\n\nEpoch [29/50] Loss: 0.4148\nAccuracy on the test set: 85.05%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 246.52it/s]\n\n\nEpoch [30/50] Loss: 0.4040\nAccuracy on the test set: 84.49%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 281.60it/s]\n\n\nEpoch [31/50] Loss: 0.3990\nAccuracy on the test set: 84.59%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 278.41it/s]\n\n\nEpoch [32/50] Loss: 0.4016\nAccuracy on the test set: 84.92%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 275.60it/s]\n\n\nEpoch [33/50] Loss: 0.3979\nAccuracy on the test set: 85.01%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 250.04it/s]\n\n\nEpoch [34/50] Loss: 0.3844\nAccuracy on the test set: 84.82%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 280.53it/s]\n\n\nEpoch [35/50] Loss: 0.3789\nAccuracy on the test set: 85.49%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 279.26it/s]\n\n\nEpoch [36/50] Loss: 0.3760\nAccuracy on the test set: 85.26%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 207.71it/s]\n\n\nEpoch [37/50] Loss: 0.3733\nAccuracy on the test set: 85.36%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 265.92it/s]\n\n\nEpoch [38/50] Loss: 0.3655\nAccuracy on the test set: 84.98%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 279.79it/s]\n\n\nEpoch [39/50] Loss: 0.3627\nAccuracy on the test set: 85.19%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 276.73it/s]\n\n\nEpoch [40/50] Loss: 0.3517\nAccuracy on the test set: 84.78%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 278.32it/s]\n\n\nEpoch [41/50] Loss: 0.3526\nAccuracy on the test set: 85.43%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 243.70it/s]\n\n\nEpoch [42/50] Loss: 0.3523\nAccuracy on the test set: 85.55%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 240.48it/s]\n\n\nEpoch [43/50] Loss: 0.3457\nAccuracy on the test set: 85.02%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 274.70it/s]\n\n\nEpoch [44/50] Loss: 0.3447\nAccuracy on the test set: 85.20%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 276.08it/s]\n\n\nEpoch [45/50] Loss: 0.3411\nAccuracy on the test set: 85.47%\n\n\n100%|██████████| 938/938 [00:04&lt;00:00, 215.18it/s]\n\n\nEpoch [46/50] Loss: 0.3312\nAccuracy on the test set: 85.55%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 244.20it/s]\n\n\nEpoch [47/50] Loss: 0.3290\nAccuracy on the test set: 85.52%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 267.56it/s]\n\n\nEpoch [48/50] Loss: 0.3277\nAccuracy on the test set: 85.35%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 267.91it/s]\n\n\nEpoch [49/50] Loss: 0.3241\nAccuracy on the test set: 85.80%\n\n\n100%|██████████| 938/938 [00:03&lt;00:00, 266.04it/s]\n\n\nEpoch [50/50] Loss: 0.3217\nAccuracy on the test set: 84.93%\n\n\n\n# Evaluation\nresnet.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    predicted_list = []\n    for images, labels in test_loader:\n        outputs = resnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        softmax_outputs = nn.Softmax(dim=1)(outputs)\n        predicted_list.append(softmax_outputs.data.cpu().numpy())\n\nall_predicted = np.concatenate(predicted_list, axis=0)\nprint(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")\n\nAccuracy on the test set: 84.93%"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#check-calibration",
    "href": "posts/pruning_vs_uncertainty.html#check-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Check calibration",
    "text": "Check calibration\n\ntest_dataset.tensors[1].cpu().numpy().shape, all_predicted.shape\n\n((10000,), (10000, 10))\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = all_predicted[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    # ece_score = compute_ece(predicted_probabilities, true_labels, num_bins=10)\n    # print(f\"Expected Calibration Error (ECE): {ece_score:.4f}\")\n\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\n# Compute expected calibration error (ECE)\nece = compute_ece(all_predicted, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.021885250088572478\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(\n        test_dataset.tensors[1].cpu().numpy(), all_predicted.argmax(axis=1)\n    )\n)\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90       980\n           1       0.91      0.97      0.94      1135\n           2       0.75      0.72      0.73      1032\n           3       0.77      0.74      0.75      1010\n           4       0.82      0.88      0.85       982\n           5       0.68      0.70      0.69       892\n           6       0.85      0.84      0.85       958\n           7       0.80      0.79      0.79      1028\n           8       0.76      0.75      0.75       974\n           9       0.81      0.74      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000\n\n\n\n\ndef compute_ece(predicted_probs, true_labels, num_bins=10):\n    # Ensure predicted_probs is a NumPy array\n    predicted_probs = np.array(predicted_probs)\n    true_labels = np.array(true_labels)\n\n    # Calculate predicted class labels\n    predicted_labels = np.argmax(predicted_probs, axis=1)\n\n    # Calculate confidence scores (maximum predicted probability)\n    confidence_scores = np.max(predicted_probs, axis=1)\n\n    # Create bins for confidence scores\n    bin_edges = np.linspace(0, 1, num_bins + 1)\n\n    ece = 0.0\n    total_samples = len(true_labels)\n\n    for bin_idx in range(num_bins):\n        # Find examples whose confidence scores fall into the current bin\n        bin_mask = (confidence_scores &gt;= bin_edges[bin_idx]) & (\n            confidence_scores &lt; bin_edges[bin_idx + 1]\n        )\n\n        if np.any(bin_mask):\n            # Calculate the accuracy of predictions in this bin\n            bin_accuracy = np.mean(predicted_labels[bin_mask] == true_labels[bin_mask])\n\n            # Calculate the fraction of examples in this bin\n            bin_fraction = np.sum(bin_mask) / total_samples\n\n            # Calculate the calibration error in this bin\n            bin_error = np.abs(bin_accuracy - np.mean(confidence_scores[bin_mask]))\n\n            # Weighted contribution to ECE\n            ece += bin_fraction * bin_error\n\n    return ece"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "href": "posts/pruning_vs_uncertainty.html#does-mc-dropout-help-with-calibration",
    "title": "Pruning vs Uncertainty",
    "section": "Does MC-dropout help with calibration?",
    "text": "Does MC-dropout help with calibration?"
  },
  {
    "objectID": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "href": "posts/pruning_vs_uncertainty.html#last-layer-only",
    "title": "Pruning vs Uncertainty",
    "section": "Last layer only",
    "text": "Last layer only\n\nclass MCDropout(nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n        self.dropout = nn.Dropout(p=self.p)\n\n    def forward(self, x):\n        self.train()\n        return self.dropout(x)\n\n\nresnet_with_dropout = torchvision.models.resnet18(pretrained=True)\nresnet_with_dropout.fc = nn.Sequential(\n    nn.Linear(\n        resnet_with_dropout.fc.in_features, resnet_with_dropout.fc.in_features // 2\n    ),\n    nn.GELU(),\n    MCDropout(p=0.33),\n    nn.Linear(resnet_with_dropout.fc.in_features // 2, num_classes),\n)\n\nresnet_with_dropout.load_state_dict(resnet.state_dict())\n\nresnet_with_dropout.to(device)\n\nmc_samples = 1000\n\noutputs = []\nfor _ in tqdm(range(mc_samples)):\n    output = resnet_with_dropout(test_dataset.tensors[0])\n    softmax_output = nn.Softmax(dim=1)(output)\n    outputs.append(softmax_output.data.cpu().numpy())\n\n100%|██████████| 1000/1000 [00:18&lt;00:00, 55.50it/s]\n\n\n\nmc_mean = np.mean(outputs, axis=0)\nmc_std = np.std(outputs, axis=0)\nmc_mean.shape\n\n(10000, 10)\n\n\n\n# Compute calibration curve\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 5))\naxes = axes.flatten()\n\nfor target_class in range(10):\n    true_labels = test_dataset.tensors[1].cpu().numpy() == target_class\n    predicted_probabilities = mc_mean[:, target_class]\n\n    prob_true, prob_pred = calibration_curve(\n        true_labels, predicted_probabilities, n_bins=10\n    )\n\n    # Plot calibration curve\n    axes[target_class].plot(prob_pred, prob_true, marker=\"o\", label=\"Calibration Curve\")\n    axes[target_class].plot(\n        [0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly Calibrated\"\n    )\n    axes[target_class].set_xlabel(\"Mean Predicted Probability\")\n    axes[target_class].set_ylabel(\"Observed Accuracy\")\n    axes[target_class].set_title(f\"Class {target_class}\")\nplt.tight_layout()\n\nece = compute_ece(mc_mean, test_dataset.tensors[1].cpu().numpy(), 10)\nece\n\n0.04250686831623317\n\n\n\n\n\n\n\n\n\n\nprint(\n    classification_report(test_dataset.tensors[1].cpu().numpy(), mc_mean.argmax(axis=1))\n)\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.92      0.90       980\n           1       0.91      0.98      0.94      1135\n           2       0.73      0.71      0.72      1032\n           3       0.78      0.72      0.75      1010\n           4       0.83      0.87      0.85       982\n           5       0.68      0.71      0.69       892\n           6       0.82      0.88      0.85       958\n           7       0.80      0.78      0.79      1028\n           8       0.77      0.74      0.75       974\n           9       0.82      0.72      0.77      1009\n\n    accuracy                           0.81     10000\n   macro avg       0.80      0.80      0.80     10000\nweighted avg       0.80      0.81      0.80     10000"
  },
  {
    "objectID": "posts/fundamentals_across_domains.html",
    "href": "posts/fundamentals_across_domains.html",
    "title": "Fundamentals across ML domains",
    "section": "",
    "text": "NN\nTransformer\nCNN\n\n\n\n\n-\nMulti-head\nMulti-channel\n\n\n-\nSkip-connection\nResNet"
  },
  {
    "objectID": "posts/AL_with_MNIST.html",
    "href": "posts/AL_with_MNIST.html",
    "title": "Active Learning with MNIST",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport psutil"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#imports",
    "href": "posts/AL_with_MNIST.html#imports",
    "title": "Active Learning with MNIST",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport psutil"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#load-data",
    "href": "posts/AL_with_MNIST.html#load-data",
    "title": "Active Learning with MNIST",
    "section": "Load data",
    "text": "Load data\n\nX, y = fetch_openml('mnist_784', version=1, data_home='data', return_X_y=True, as_frame=False)\n\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn("
  },
  {
    "objectID": "posts/AL_with_MNIST.html#section",
    "href": "posts/AL_with_MNIST.html#section",
    "title": "Active Learning with MNIST",
    "section": "",
    "text": "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n(60000, 784) (10000, 784) (60000,) (10000,)"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#check-if-things-are-working-as-expected",
    "href": "posts/AL_with_MNIST.html#check-if-things-are-working-as-expected",
    "title": "Active Learning with MNIST",
    "section": "Check if things are working as expected",
    "text": "Check if things are working as expected\n\n%%time\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0, n_jobs=psutil.cpu_count()//2)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\n\nprint(classification_report(y_test, preds))\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.99      0.97       980\n           1       0.98      0.99      0.98      1135\n           2       0.94      0.94      0.94      1032\n           3       0.94      0.94      0.94      1010\n           4       0.95      0.93      0.94       982\n           5       0.96      0.93      0.94       892\n           6       0.96      0.97      0.96       958\n           7       0.95      0.92      0.94      1028\n           8       0.94      0.93      0.93       974\n           9       0.88      0.94      0.91      1009\n\n    accuracy                           0.95     10000\n   macro avg       0.95      0.95      0.95     10000\nweighted avg       0.95      0.95      0.95     10000\n\nCPU times: user 1min 33s, sys: 652 ms, total: 1min 34s\nWall time: 3.73 s"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#convert-to-one-vs-rest-problem",
    "href": "posts/AL_with_MNIST.html#convert-to-one-vs-rest-problem",
    "title": "Active Learning with MNIST",
    "section": "Convert to one v/s rest problem",
    "text": "Convert to one v/s rest problem\n\ny_c = (y == '2').astype(np.int8)\n\ny_c_train, y_c_test = y_c[:60000], y_c[60000:]"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#check-if-things-are-working-as-expected-1",
    "href": "posts/AL_with_MNIST.html#check-if-things-are-working-as-expected-1",
    "title": "Active Learning with MNIST",
    "section": "Check if things are working as expected",
    "text": "Check if things are working as expected\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0, n_jobs=psutil.cpu_count()//2)\nclf.fit(X_train, y_c_train)\npreds = clf.predict(X_test)\n\n\nprint(\"Precision\", precision_score(y_c_test, preds))\nprint(\"Recall\", recall_score(y_c_test, preds))\n\nPrecision 0.9808988764044944\nRecall 0.8459302325581395"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#divide-data-into-train-and-pool",
    "href": "posts/AL_with_MNIST.html#divide-data-into-train-and-pool",
    "title": "Active Learning with MNIST",
    "section": "Divide data into train and pool",
    "text": "Divide data into train and pool\n\ntrain_size = 200\nX_train, X_pool, y_c_train, y_c_pool = train_test_split(X, y_c, train_size=train_size, random_state=42)\nprint(X_train.shape, X_pool.shape, y_c_train.shape, y_c_pool.shape)\n\n# plot a bar chart of the number of samples in each class for the training and test set\nunique, counts = np.unique(y_c_train, return_counts=True)\nprint(\"Number of samples in each class for training set\", dict(zip(unique, counts)))\nprint(\"One v/s rest ratio\", counts[0]/counts[1], \"for training set\")\n\n(200, 784) (69800, 784) (200,) (69800,)\nNumber of samples in each class for training set {0: 179, 1: 21}\nOne v/s rest ratio 8.523809523809524 for training set"
  },
  {
    "objectID": "posts/AL_with_MNIST.html#prof.-ermons-method",
    "href": "posts/AL_with_MNIST.html#prof.-ermons-method",
    "title": "Active Learning with MNIST",
    "section": "Prof. Ermon’s method",
    "text": "Prof. Ermon’s method\n\nX_train, X_pool, y_c_train, y_c_pool = train_test_split(X, y_c, train_size=train_size, random_state=42)\nprint(X_train.shape, X_pool.shape, y_c_train.shape, y_c_pool.shape)\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0, n_jobs=psutil.cpu_count()//2)\nclf.fit(X_train, y_c_train)\npreds = clf.predict(X_test)\n\ntest_recall = [recall_score(y_c_test, preds)]\ntest_precision = [precision_score(y_c_test, preds)]\npositives = [np.sum(y_c_train)]\nnegatives = [len(y_c_train) - positives[-1]]\nlabeling_cost = [0]\ntp = np.where((preds == 1) & (y_c_test == 1))[0]\nfp = np.where((preds == 1) & (y_c_test == 0))[0]\nprint(\"Test: Number of false positives\", len(fp), \"Number of true positives\", len(tp))\nprint(\"Iteration\", 0, \"Precision\", test_precision[-1], \"Recall\", test_recall[-1], \"Cost\", labeling_cost[-1])\n\nal_iters = 10\n\nfor iter in range(al_iters):\n    print()\n    preds = clf.predict(X_pool)\n    # pred_proba = clf.predict_proba(X_pool)\n    # print(pred_proba.shape)\n    # identify instances predicted as positive but are actually negative (false positives)\n    # we only pick points with more than 90% probability of being positive\n    # fp = np.where((pred_proba[:, 1] &gt; 0.8) & (y_pool == 0))[0]\n    fp = np.where((preds == 1) & (y_c_pool == 0))[0]\n    tp = np.where((preds == 1) & (y_c_pool == 1))[0]\n    fn = np.where((preds == 0) & (y_c_pool == 1))[0]\n    print(\"Pool: Number of false positives\", len(fp), \"Number of true positives\", len(tp), \"Number of false negatives\", len(fn))\n    tp_fp = np.concatenate((tp, fp))\n    # add them to the training set\n    X_train = np.concatenate((X_train, X_pool[tp_fp]))\n    y_c_train = np.concatenate((y_c_train, y_c_pool[tp_fp]))\n    positives.append(np.sum(y_c_train))\n    negatives.append(len(y_c_train) - positives[-1])\n    # remove from the pool set\n    X_pool = np.delete(X_pool, tp_fp, axis=0)\n    y_c_pool = np.delete(y_c_pool, tp_fp)\n    # add the cost of labeling to the list\n    labeling_cost.append(len(tp_fp))\n    # train the classifier again\n    clf.fit(X_train, y_c_train)\n    # predict on the test set\n    preds = clf.predict(X_test)\n    tp = np.where((preds == 1) & (y_c_test == 1))[0]\n    fp = np.where((preds == 1) & (y_c_test == 0))[0]\n    fn = np.where((preds == 0) & (y_c_test == 1))[0]\n    print(\"Test: Number of false positives\", len(fp), \"Number of true positives\", len(tp), \"Number of false negatives\", len(fn))\n    # calculate precision and recall\n    test_recall.append(recall_score(y_c_test, preds))\n    test_precision.append(precision_score(y_c_test, preds))\n    # print information\n    print(\"Iteration\", iter+1, \"Precision\", test_precision[-1], \"Recall\", test_recall[-1], \"Cost\", labeling_cost[-1])\n\nlabeling_cost = np.cumsum(labeling_cost)\n\n(200, 784) (69800, 784) (200,) (69800,)\nTest: Number of false positives 8 Number of true positives 283\nIteration 0 Precision 0.9725085910652921 Recall 0.2742248062015504 Cost 0\n\nPool: Number of false positives 73 Number of true positives 1884 Number of false negatives 5085\nTest: Number of false positives 209 Number of true positives 932 Number of false negatives 100\nIteration 1 Precision 0.8168273444347064 Recall 0.9031007751937985 Cost 1957\n\nPool: Number of false positives 1389 Number of true positives 4386 Number of false negatives 699\nTest: Number of false positives 489 Number of true positives 1016 Number of false negatives 16\nIteration 2 Precision 0.6750830564784053 Recall 0.9844961240310077 Cost 5775\n\nPool: Number of false positives 4088 Number of true positives 598 Number of false negatives 101\nTest: Number of false positives 12 Number of true positives 1006 Number of false negatives 26\nIteration 3 Precision 0.9882121807465619 Recall 0.9748062015503876 Cost 4686\n\nPool: Number of false positives 18 Number of true positives 15 Number of false negatives 86\nTest: Number of false positives 10 Number of true positives 1010 Number of false negatives 22\nIteration 4 Precision 0.9901960784313726 Recall 0.9786821705426356 Cost 33\n\nPool: Number of false positives 13 Number of true positives 5 Number of false negatives 81\nTest: Number of false positives 11 Number of true positives 1010 Number of false negatives 22\nIteration 5 Precision 0.9892262487757101 Recall 0.9786821705426356 Cost 18\n\nPool: Number of false positives 5 Number of true positives 2 Number of false negatives 79\nTest: Number of false positives 10 Number of true positives 1011 Number of false negatives 21\nIteration 6 Precision 0.990205680705191 Recall 0.9796511627906976 Cost 7\n\nPool: Number of false positives 5 Number of true positives 1 Number of false negatives 78\nTest: Number of false positives 9 Number of true positives 1011 Number of false negatives 21\nIteration 7 Precision 0.9911764705882353 Recall 0.9796511627906976 Cost 6\n\nPool: Number of false positives 5 Number of true positives 2 Number of false negatives 76\nTest: Number of false positives 11 Number of true positives 1010 Number of false negatives 22\nIteration 8 Precision 0.9892262487757101 Recall 0.9786821705426356 Cost 7\n\nPool: Number of false positives 5 Number of true positives 2 Number of false negatives 74\nTest: Number of false positives 9 Number of true positives 1010 Number of false negatives 22\nIteration 9 Precision 0.9911678115799804 Recall 0.9786821705426356 Cost 7\n\nPool: Number of false positives 3 Number of true positives 1 Number of false negatives 73\nTest: Number of false positives 12 Number of true positives 1010 Number of false negatives 22\nIteration 10 Precision 0.9882583170254403 Recall 0.9786821705426356 Cost 4\n\n\n\n# plot the confusion matrix\nimport itertools\ncm = confusion_matrix(y_c_test, preds)\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n# add the numbers inside the boxes\nthresh = cm.max() / 2.0\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] &gt; thresh else \"black\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\n\nText(0, 0.5, 'True Label')\n\n\n\n\n\n\n\n\n\n\npd.DataFrame({\"Cost\": labeling_cost, \"Train_Positives\": positives, \"Train_Negatives\": negatives, \"Test_Precision\": test_precision, \"Test_Recall\": test_recall})\n\n\n\n\n\n\n\n\nCost\nTrain_Positives\nTrain_Negatives\nTest_Precision\nTest_Recall\n\n\n\n\n0\n0\n21\n179\n0.972509\n0.274225\n\n\n1\n1957\n1905\n252\n0.816827\n0.903101\n\n\n2\n7732\n6291\n1641\n0.675083\n0.984496\n\n\n3\n12418\n6889\n5729\n0.988212\n0.974806\n\n\n4\n12451\n6904\n5747\n0.990196\n0.978682\n\n\n5\n12469\n6909\n5760\n0.989226\n0.978682\n\n\n6\n12476\n6911\n5765\n0.990206\n0.979651\n\n\n7\n12482\n6912\n5770\n0.991176\n0.979651\n\n\n8\n12489\n6914\n5775\n0.989226\n0.978682\n\n\n9\n12496\n6916\n5780\n0.991168\n0.978682\n\n\n10\n12500\n6917\n5783\n0.988258\n0.978682"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html",
    "href": "posts/2021-03-22-gp_kernels.html",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "",
    "text": "!pip install -qq GPy\nimport autograd.numpy as np\nimport pandas as pd\nimport GPy\nimport matplotlib.pyplot as plt\nfrom autograd import grad\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "href": "posts/2021-03-22-gp_kernels.html#rbf-radial-basis-function-kernel-stationarity-and-isotropy",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "RBF (Radial basis function) Kernel, Stationarity and Isotropy",
    "text": "RBF (Radial basis function) Kernel, Stationarity and Isotropy\nRBF is one of the most commonly used kernels in GPs due to it’s infinetely differentiability (extreme flexibility). This property helps us to model a vast variety of functions \\(X \\to Y\\).\nRBF kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2exp\\left(-\\frac{(x-x')^2}{2l^2}\\right)\n\\end{aligned}\n\\] Where, \\(\\sigma^2\\) is variance and \\(l\\) is known as lengthscale. #### Stationarity RBF is a stationary kernel and so it is invariant to translation in the input space. In other words, \\(\\mathcal{K}(x,x')\\) depends only on \\(x-x'\\).\n\nIsotropy\nRBF is also isotropic kernel, which means that \\(\\mathcal{K}(x,x')\\) depends only on \\(|x-x'|\\). Thus, we have \\(\\mathcal{K}(x,x') = \\mathcal{K}(x',x)\\).\nLet’s visualize few functions drawn from the RBF kernel\n\ndef K_rbf(X1, X2, sigma=1., l=1.):\n  return (sigma**2)*(np.exp(-0.5*np.square(X1-X2.T)/l**2))\n\n\n\nHelper functions\n\ndef plot_functions(kernel_func, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1)):\n  mean = np.zeros(X.shape[0])\n  cov = kernel_func(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  fig = plt.figure(figsize=(14,8), constrained_layout=True)\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1])\n  ax0.set_ylim(*ax0_ylim)\n  ax1 = fig.add_subplot(gs[1, 0:2])\n  ax1.set_ylim(*ax1_ylim)\n  ax2 = fig.add_subplot(gs[1, 2:4])\n  for func in functions:\n    ax0.plot(X, func,'o-');\n  ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel');\n  ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n  sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\ndef animate_functions(kernel_func, val_list, ax0_ylim=(-3,3), ax1_ylim=(-0.1,1.1), \n                      k_name='',p_name='',symbol=''):\n  fig = plt.figure(figsize=(14,8))\n  gs = fig.add_gridspec(2,4)\n  ax0 = fig.add_subplot(gs[0, 1:-1]);ax1 = fig.add_subplot(gs[1, 0:2]);ax2 = fig.add_subplot(gs[1, 2:4]);\n  def update(p):\n    ax0.cla();ax1.cla();ax2.cla();\n    ax0.set_ylim(*ax0_ylim);ax1.set_ylim(*ax1_ylim)\n    if p_name == 'Lengthscale':\n      cov = kernel_func(X, X, l=p)\n    elif p_name == 'Variance':\n      cov = kernel_func(X, X, sigma=np.sqrt(p))\n    elif p_name == 'Offset':\n      cov = kernel_func(X, X, c=p)\n    elif p_name == 'Period':\n      cov = kernel_func(X, X, p=p)\n    functions = np.random.multivariate_normal(mean, cov, size=5)\n    for func in functions:\n      ax0.plot(X, func,'o-');\n    ax0.set_xlabel('X');ax0.set_ylabel('Y');ax0.set_title('Functions drawn from '+k_name+' kernel\\n'+p_name+' ('+symbol+') = '+str(p));\n    ax1.plot(X, cov[:,4]);ax1.set_title('K(0,X)');ax1.set_title('K(0,X)');ax1.set_xlabel('X');ax1.set_ylabel('K(0,X)')\n    sns.heatmap(cov.round(2), ax=ax2, xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True, cbar=False);\n    ax2.set_xlabel('X');ax2.set_ylabel('X');ax2.set_title('Covariance matrix');\n\n  anim = FuncAnimation(fig, update, frames=val_list, blit=False)\n  plt.close()\n  rc('animation', html='jshtml')\n  return anim\n\nVerifying if our kernel is consistent with GPy kernels.\n\nX = np.linspace(101,1001,200).reshape(-1,1)\nsigma, l = 7, 11\nassert np.allclose(K_rbf(X,X,sigma,l), GPy.kern.RBF(1, variance=sigma**2, lengthscale=l).K(X,X)) \n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\nk_name = 'RBF'\nplot_functions(K_rbf, ax0_ylim=(-3.5,3))\n\n\n\n\n\n\n\n\nLet’s see the effect of varying parameters \\(\\sigma\\) and \\(l\\) of the RBF kernel function.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_rbf, val_list, k_name='RBF', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nl = 1.\nval_list = [1,4,9,16,25]\nanimate_functions(K_rbf, val_list, ax0_ylim=(-12,12), ax1_ylim=(-0.1, 26),\n                  k_name='RBF', p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWith increase in value of \\(l\\), functions drawn from the kernel become smoother. Covariance between a pair of points is increasing with increase in \\(l\\).\nIncreasing \\(\\sigma^2\\) increase the overall uncertainty (width of the space where 95% of the functions live) across all the points."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#matern-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Matern Kernel",
    "text": "Matern Kernel\nMatern kernels are given by a general formula as following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1, x_2) =  \\sigma^2\\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\n\\Bigg)^\\nu K_\\nu\\Bigg(\n\\frac{\\sqrt{2\\nu}}{l} |x_1-x_2|\\Bigg)\n\\end{aligned}\n\\] Where, \\(\\Gamma\\) is gamma function and \\(K_\\nu\\) is modified Bessel function of second order.\nThe general formula is not very intuitive about the functionality of this kernel. In practice, Matern with \\(\\nu=\\{0.5,1.5,2.5\\}\\) are used, where GP with each kernel is \\((\\lceil\\nu\\rceil-1)\\) times differentiable.\nMatern functions corresponding to each \\(\\nu\\) values are defined as the following, \\[\n\\begin{aligned}\nMatern12 \\to \\mathcal{K_{\\nu=0.5}}(x_1, x_2) &=  \\sigma^2exp\\left(-\\frac{|x_1-x_2|}{l}\\right)\\\\\nMatern32 \\to \\mathcal{K_{\\nu=1.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)exp\\left(-\\frac{\\sqrt{3}|x_1-x_2|}{l}\\right)\\\\\nMatern52 \\to \\mathcal{K_{\\nu=2.5}}(x_1, x_2) &=  \\sigma^2\\left(1+\\frac{\\sqrt{5}|x_1-x_2|}{l}+\\frac{5(x_1-x_2)^2)}{3l^2}\\right)exp\\left(-\\frac{\\sqrt{5}|x_1-x_2|}{l}\\right)\n\\end{aligned}\n\\] Matern kernels are stationary as well as isotropic. With \\(\\nu \\to \\infty\\) they converge to \\(RBF\\) kernel. \\(Matern12\\) is also known as \\(Exponential\\) kernel in toolkits such as GPy.\nNow, let’s draw few functions from each of these versions and try to get intuition behind each of them.\n\ndef K_m12(X1, X2, sigma=1., l=1.): # v = 0.5\n  return (sigma**2)*(np.exp(-np.abs(X1-X2.T)/l))\ndef K_m32(X1, X2, sigma=1., l=1.): # v = 1.5\n  return (sigma**2)*(1+((3**0.5)*np.abs(X1-X2.T))/l)*(np.exp(-(3**0.5)*np.abs(X1-X2.T)/l))\ndef K_m52(X1, X2, sigma=1., l=1.): # v = 2.5\n  return (sigma**2)*(1+(((5**0.5)*np.abs(X1-X2.T))/l)+((5*(X1-X2.T)**2)/(3*l**2)))*\\\n                    (np.exp(-(5**0.5)*np.abs(X1-X2.T)/l))\n\nVerifying if our kernels are consistent with GPy kernels.\n\nX = np.linspace(101,1001,50).reshape(-1,1)\nassert np.allclose(K_m32(X,X,sigma=7.,l=11.), GPy.kern.Matern32(1,lengthscale=11.,variance=7**2).K(X,X))\nassert np.allclose(K_m52(X,X,sigma=7.,l=11.), GPy.kern.Matern52(1,lengthscale=11.,variance=7**2).K(X,X))\n\n\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1.\nl = 3.\n\nfig, ax = plt.subplots(3,2,figsize=(14,10))\nnames = ['Matern12', 'Matern32', 'Matern52']\nfor k_i, kernel in enumerate([K_m12, K_m32, K_m52]):\n  mean = np.zeros(X.shape[0])\n  cov = kernel(X, X, sigma, l)\n  functions = np.random.multivariate_normal(mean, cov, size=5)\n  for func in functions:\n    ax[k_i,0].plot(X, func);\n  ax[k_i,0].set_xlabel('X');ax[k_i,0].set_ylabel('Y');ax[k_i,0].set_title('Functions drawn from '+names[k_i]+' kernel');\n  sns.heatmap(cov.round(2), ax=ax[k_i,1], xticklabels=X.ravel(), yticklabels=X.ravel(), annot=True);\n  ax[k_i,1].set_xlabel('X');ax[k_i,1].set_ylabel('X');ax[k_i,1].set_title('Covariance matrix');\nplt.tight_layout();\n\n\n\n\n\n\n\n\nFrom the above plot, we can say that smoothness is increasing in functions as we increase \\(\\nu\\). Thus, smoothness of functions in terms of kernels is in the following order: Matern12&lt;Matern32&lt;Matern52.\nLet us see effect of varying \\(\\sigma\\) and \\(l\\) on Matern32 which is more popular among the three.\n\nnp.random.seed(0)\nsigma = 1.\nval_list = [0.5,1,2,3,4,5]\nanimate_functions(K_m32, val_list, k_name='Matern32', p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that Matern32 kernel behaves similar to RBF with varying \\(l\\). Though, Matern32 is less smoother than RBF. A quick comparison would clarify this.\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_rbf(X,X, l=3.)[:,50], label='RBF')\nplt.plot(X, K_m32(X,X, l=3.)[:,50], label='Matern32')\nplt.legend();plt.xlabel('X');plt.ylabel('Covariance (K(0,X))');\nplt.title('K(0,X)');"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#periodic-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Periodic Kernel",
    "text": "Periodic Kernel\nPeriodic Kernel is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= \\sigma^2\\exp\\left(-\\frac{\\sin^2(\\pi|x_1 - x_2|/p)}{2l^2}\\right)\n\\end{aligned}\n\\] Where \\(p\\) is period. Let’s visualize few functions drawn from this kernel.\n\ndef K_periodic(X1, X2, sigma=1., l=1., p=3.):\n  return sigma**2 * np.exp(-0.5*np.square(np.sin(np.pi*(X1-X2.T)/p))/l**2)\n\nX = np.linspace(10,1001,50).reshape(-1,1)\nassert np.allclose(K_periodic(X,X,sigma=7.,l=11.,p=3.), \n                   GPy.kern.StdPeriodic(1,lengthscale=11.,variance=7**2,period=3.).K(X,X))\n\n\nnp.random.seed(0)\nX = np.arange(-4,5).reshape(-1,1)\nsigma = 1\nl = 1.\np = 3.\nk_name = 'Periodic'\nplot_functions(K_periodic)\n\n\n\n\n\n\n\n\nWe will investigate the effect of varying period \\(p\\) now.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.4,1.1),\n                  k_name='Periodic',p_name='Period')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nFrom the above animation we can see that, all points that are \\(p\\) distance apart from each other have exactly same values because they have correlation of exactly 1 (\\(\\sigma=1 \\to covariance=correlation\\)).\nNow, we will investigate effect of lenging lengthscale \\(l\\) while other parameters are constant.\n\nnp.random.seed(0)\nval_list = [1., 2., 3., 4., 5.]\n\nanimate_functions(K_periodic, val_list, ax1_ylim=(0.6,1.1),\n                  k_name='Periodic',p_name='Lengthscale', symbol='l')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that correlation between a pair of locations \\(\\{x_1,x_2|x_1-x_2&lt;p\\}\\) increases as the lengthscale is increased."
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "href": "posts/2021-03-22-gp_kernels.html#linear-kernel",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Linear Kernel",
    "text": "Linear Kernel\nLinear kernel (a.k.a. dot-product kernel) is given as the following, \\[\n\\begin{aligned}\n\\mathcal{K}(x_1,x_2)= (x_1-c)(x_2-c)+\\sigma^2\n\\end{aligned}\n\\] Let’s visualize few functions drawn from the linear kernel\n\ndef K_lin(X1, X2, sigma=1., c=1.):\n  return (X1-c)@(X2.T-c) + sigma**2\n\n\nnp.random.seed(0)\nsigma = 1.\nc = 1.\n\nplot_functions(K_lin, ax0_ylim=(-10,5), ax1_ylim=(-3,7))\n\n\n\n\n\n\n\n\nLet’s see the effect of varying parameters \\(\\sigma\\) and \\(c\\) of the linear kernel function.\n\nval_list = [-3,-2,-1,0,1,2,3]\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-15,12), ax1_ylim=(-3,23), \n                  p_name='Offset', symbol='c')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nnp.random.seed(1)\nval_list = np.square(np.array([1,2,3,4,5,8]))\n\nanimate_functions(K_lin, val_list, ax0_ylim=(-25,15), ax1_ylim=(-5,110), \n                  p_name='Variance', symbol='sigma')\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nVarying \\(c\\) parameter changes position of shallow region in covariance matrix. In other words, as \\(x \\to c\\), points close to \\(x\\) have variance \\(\\to \\sigma^2\\). Distant points have monotonically increasing variance.\nIncreasing \\(\\sigma^2\\) adds a constant in all variance and covariances. So, it allows more uncertainty across all points and weakens the monotonic trend of variance over distant points.\n\nNon-stationary behaviour of Linear kernel\nUnlike other stationary kernels, Linear kernel is not invariant of translations in the input space. The comparison below, visually supports this claim.\n\nfig, ax = plt.subplots(2,2,figsize=(14,8), sharex=True)\nkerns = [K_rbf, K_m32, K_periodic, K_lin]\nk_names = ['RBF', 'Matern32', 'Periodic', 'Linear']\nX = np.linspace(-10,10,21).reshape(-1,1)\ndef update(x):\n  count = 0\n  for i in range(2):\n    for j in range(2):\n      ax.ravel()[count].cla()\n      tmp_kern = kerns[count]\n      mean = np.zeros(X.shape[0])\n      cov = tmp_kern(X,X)\n      ax.ravel()[count].plot(X, cov[:,x]);\n      ax.ravel()[count].set_xlim(X[x-3],X[x+3])\n      ax.ravel()[count].set_xlabel('X');\n      ax.ravel()[count].set_ylabel('K('+str(X[x].round(2))+',X)');\n      ax.ravel()[count].set_title('Covariance K('+str(X[x].round(2))+',X) for '+k_names[count]+' kernel');\n      count += 1\n  ax.ravel()[3].set_ylim(-5,80)\n  plt.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=[5,7,9,11,13,15], blit=False)\nplt.close()\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "href": "posts/2021-03-22-gp_kernels.html#multiplications-of-kernels",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Multiplications of kernels",
    "text": "Multiplications of kernels\nIf a single kernel is having high bias in fitting a dataset, we can use mutiple of these kernels in multiplications and/or summations. First, let us see effect of multiplication of a few kernels.\n\nPeriodic * Linear\n\nX = np.linspace(-10,10,100).reshape(-1,1)\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50], label='Periodic')\nplt.plot(X, K_lin(X,X,sigma=0.01,c=0)[:,50], label='Linear')\nplt.plot(X, K_periodic(X,X,sigma=2.)[:,50]*K_lin(X,X,sigma=0.01,c=0)[:,50], label='Periodic*Linear')\nplt.legend(bbox_to_anchor=(1,1));plt.xlabel('X');plt.ylabel('Covariance')\nplt.title('K(0,*)');\n\n\n\n\n\n\n\n\n\n\nLinear * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nplt.plot(X, K_lin(X,X,c=-1)[:,50], label='Linear1')\nplt.plot(X, K_lin(X,X,c=1)[:,50], label='Linear2')\nplt.plot(X, K_lin(X,X,c=0.5)[:,50], label='Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50], label='Linear1*Linear3')\nplt.plot(X, K_lin(X,X,c=-1)[:,50]*K_lin(X,X,c=1)[:,50]*K_lin(X,X,c=0.5)[:,50], label='Linear1*Linear2*Linear3')\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\n\nMatern * Linear\n\nX = np.linspace(-1,1,100).reshape(-1,1)\nk1 = K_lin(X,X,c=1)[:,50]\nk2 = K_m32(X,X)[:,50]\nplt.plot(X, k1, label='Linear')\nplt.plot(X, k2, label='Matern32')\nplt.plot(X, k1*k2, label='Matern32*Linear')\nplt.legend(bbox_to_anchor=(1,1));"
  },
  {
    "objectID": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "href": "posts/2021-03-22-gp_kernels.html#appendix-extra-material",
    "title": "Understanding Kernels in Gaussian Processes",
    "section": "Appendix (Extra material)",
    "text": "Appendix (Extra material)\nAt this stage, we do not know how the fuctions are drawn from linear kernel based covariance matrix end up being lines with various intercepts and slopes.\n\n\nPredicting at a single point after observing value at a single point\nLet’s see how would be a GP prediction after observing value at a single point.\nOur kernel function is given by, * \\(K(x,x')=(x-c) \\cdot (x'-c)+\\sigma^2\\)\nNow, we observe value \\(y\\) at a location \\(x\\) and we want to predict value \\(y^*\\) at location \\(x^*\\). \\[\n\\begin{aligned}\n(y^*|x_1,y_1,x^*) &= K(x^*,x) \\cdot K^{-1}(x,x)\\cdot y \\\\\n&= \\left(\\frac{(x-c)(x^*-c)+\\sigma^2}{(x-c)(x-c)+\\sigma^2}\\right)\\cdot y\n\\end{aligned}\n\\] \\(c\\) and \\(\\sigma^2\\) do not vary in numerator and denominator so, the value of \\(y^* \\propto x^*\\).\n\n\n\nPredicting at a single point after observing values at two points\nNow, we’ll take a case where two values \\({y_1, y_2}\\) are observed at \\({x_1, x_2}\\). Let us try to predict value \\(y^*\\) at \\(x^*\\).\n$$ y^* =\n\\[\\begin{bmatrix}\nK(x_1, x^*) & K(x_2,x^*)\n\\end{bmatrix}\\begin{bmatrix}\nK(x_1, x_1) & K(x_1,x_2) \\\\\nK(x_2, x_1) & K(x_2,x_2)\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\n(x_1-c)^2+\\sigma^2 & (x_1-c) (x_2-c)+\\sigma^2 \\\\\n(x_2-c) (x_1-c)+\\sigma^2 & (x_2-c)^2 +\\sigma^2\n\\end{bmatrix}\\]\n^{-1}\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\\]\n\\\n& =\n\\[\\begin{bmatrix}\n(x_1-c)(x^*-c)+\\sigma^2 & (x_2-c)(x^*-c)+\\sigma^2\n\\end{bmatrix} \\frac{1}{\\sigma^2(x_1-x_2)^2}\n\\begin{bmatrix}\n(x_2-c)^2+\\sigma^2 & -[(x_1-c)(x_2-c)+\\sigma^2] \\\\\n-[(x_2-c) (x_1-c)+\\sigma^2] & (x_1-c)^2 +\\sigma^2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix} \\tag{1}\\]\nFrom Eq. (1) second term, we can say that if \\(\\sigma^2=0\\), matrix is not-invertible because determinant is zero. It means that, if \\(\\sigma^2=0\\), observing a single point is enough, we can infer values at infinite points after observing that single point.\nEvaluating Eq. (1) further, it converges to the following equation, \\[\n\\begin{aligned}\ny^* = \\frac{(x_1y_2-x_2y_1)+x^*(y_1-y_2)}{(x_1-x_2)}\n\\end{aligned}\n\\] Interestingly, we can see that output does not depend on \\(c\\) or \\(\\sigma^2\\) anymore. Let us verify experimentally if this is true for observing more than 2 data points.\n\n\nPrepering useful functions\n\nfrom scipy.optimize import minimize\n\n\ndef cov_func(x, x_prime, sigma, c):\n  return (x-c)@(x_prime-c) + sigma**2\n\ndef neg_log_likelihood(params):\n  n = X.shape[0]\n  sigma, c, noise_std = params\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) \n  return nll_ar[0,0]\n\ndef predict(params):\n  sigma, c, noise_std = params\n  k = cov_func(X, X.T, sigma, c)\n  np.fill_diagonal(k, k.diagonal()+noise_std**2)\n  k_inv = np.linalg.pinv(k)\n  k_star = cov_func(X_test, X.T, sigma, c)\n\n  mean = k_star@k_inv@Y\n  cov = cov_func(X_test, X_test.T, sigma, c) - k_star@k_inv@k_star.T\n  return mean, cov\n\n\n\nObserving more than two points and changing hyperparameters manually\n\nX = np.array([3,4,5,6,7,8]).reshape(-1,1)\nY = np.array([6,9,8,11,10,13]).reshape(-1,1)\nX_test = np.linspace(1,8,20).reshape(-1,1)\nparams_grid = [[1., 0.01, 10**-10], [100., 1., 10**-10], \n                [100., 0.01, 10**-10], [1., 2., 1.]] # sigma, c, noise_std\n\nX_extra = np.hstack([np.ones((X.shape[0], 1)), X])\nTheta = np.linalg.pinv(X_extra.T@X_extra)@X_extra.T@Y\nX_test_extra = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\nY_test_ideal = X_test_extra@Theta\n\nfig, ax = plt.subplots(1,4,figsize=(16,5), sharey=True)\nmeans = []\nfor p_i, params in enumerate(params_grid):\n  Y_test_mean, Y_test_cov = predict(params)\n  means.append(Y_test_mean)\n  ax[p_i].scatter(X, Y, label='train')\n  ax[p_i].scatter(X_test, Y_test_mean, label='test')\n  ax[p_i].legend();ax[p_i].set_xlabel('X');ax[p_i].set_ylabel('Y');\n  ax[p_i].set_title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\n\n\n\n\n\n\n\n\n\nnp.allclose(Y_test_ideal, means[0]),\\\nnp.allclose(Y_test_ideal, means[1]),\\\nnp.allclose(Y_test_ideal, means[2]),\\\nnp.allclose(Y_test_ideal, means[3])\n\n(True, True, True, False)\n\n\n\nmodel = GPy.models.GPRegression(X, Y, GPy.kern.Linear(input_dim=1))\n# model['Gaussian_noise'].fix(10**-10)\n# model.kern.variances.fix(10**-10)\nmodel.optimize()\nmodel.plot()\nplt.plot(X_test, Y_test_ideal, label='Normal Eq. fit')\nplt.plot(X_test,model.predict(X_test)[0], label='Prediction')\nplt.legend()\nmodel\n\n\n\n\nModel: GP regression\nObjective: 13.51314321804978\nNumber of Parameters: 2\nNumber of Optimization Parameters: 2\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nlinear.variances\n2.806515343539501\n+ve\n\n\n\nGaussian_noise.variance\n2.0834221617534134\n+ve\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that there is no change in fit with change in \\(c\\) and \\(\\sigma\\). 4th fit is not matching with the ideal fit obtained by normal equation because of high noise. Now, let us estimate parameters by minimizing negative log marginal likelihood.\n\nparams = [1., 1., 1.]\nresult = minimize(neg_log_likelihood, params, bounds=[(10**-5, 10**5), (10**-5, 10**5), (10**-5, 10**-5)])\nparams = result.x\nprint(params, result.fun)\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(Y_test_ideal, Y_test_mean)\n\n[9.99998123e-01 9.99998123e-01 1.00000000e-05] 10207223403405.541\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\ndef neg_log_likelihood(sigma, c, noise_std):\n  n = X.shape[0]\n  cov = cov_func(X, X.T, sigma, c)\n  cov = cov + (noise_std**2)*np.eye(n)\n  nll_ar =  0.5*(Y.T@np.linalg.pinv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov))\n  return nll_ar[0,0]\n\n\ngrad_func = grad(neg_log_likelihood, argnum=[0,1,2])\nalpha = 0.01\nloss = []\nsigma, c, noise_std = 1., 1., 1.\nfor _ in range(5000):\n  grads = grad_func(sigma, c, noise_std)\n  # print(grads)\n  sigma = sigma - alpha*grads[0]\n  c = c - alpha*grads[1]\n  noise_std = noise_std - alpha*grads[2]\n  loss.append(neg_log_likelihood(sigma, c, noise_std))\nprint(sigma, c, noise_std)\nplt.plot(loss);\nloss[-1]\n\n7.588989986845149 -2.830840439162303 32.2487569348891\n\n\n31.05187173290998\n\n\n\n\n\n\n\n\n\n\nparams = sigma, c, noise_std\nY_test_mean, Y_test_cov = predict(params)\nplt.scatter(X, Y, label='train')\nplt.scatter(X_test, Y_test_mean, label='test')\nplt.legend();plt.xlabel('X');plt.ylabel('Y');\nparams = np.round(params, 4)\nplt.title('sigma='+str(params[0])+', c='+str(params[1])+', noise='+str(params[2]));\nnp.allclose(means[0], Y_test_mean, rtol=10**-1, atol=10**-1)\n\nFalse"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes’ theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "href": "posts/2022-03-06-probabilistic-machine-learning.html#introduction",
    "title": "Probabilistic Machine Learning",
    "section": "",
    "text": "An inference problem requires statements about the value of an unobserved (latent) variable x based on observations y which are related to x, but may not be sufficient to fully determine x. This requires a notion of uncertainty.\n\nWe can define the following rules because \\(p(E) = 1\\) for any event \\(E\\).\n\nSum rule: \\(p(E) = p(E|A) + p(E|\\neg A)\\)\n\nProduct rule: \\(p(E, A) = p(E|A)p(A) = p(A|E)p(E)\\)\n\nBayes’ theorem: \\(p(E|A) = \\frac{p(A|E)p(E)}{p(A)}\\)"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html",
    "title": "Programatically download OpenAQ data",
    "section": "",
    "text": "# uncomment to install these libraries\n# !pip install boto3 botocore\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport boto3\nimport botocore\nimport os\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "href": "posts/2020-09-21-programatically_download_openaq_data.html#setup",
    "title": "Programatically download OpenAQ data",
    "section": "Setup",
    "text": "Setup\n\ns3 = boto3.client('s3', config=botocore.config.Config(signature_version=botocore.UNSIGNED))\nbucket_name = 'openaq-fetches'\nprefix = 'realtime-gzipped/'\n\npath = '/content/drive/MyDrive/IJCAI-21/data/OpenAQ-Delhi/'\n\nstart_date = '2020/01/01' # start date (inclusive)\nend_date = '2020/12/31' # end date (inclusive)\n\n\nDownload\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  clear_output(wait=True)\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  print('Downloading:', date)\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    f_name = file_obj['Key']\n    tmp_path = '/'.join((path+f_name).split('/')[:-1])\n    \n    if not os.path.exists(tmp_path):\n      os.makedirs(tmp_path)\n    \n    s3.download_file(bucket_name, f_name, path+f_name)\n\nDownloading: 2020-05-04\n\n\n\n\nValidate\n\nfor date in pd.date_range(start=start_date, end=end_date):\n  date = str(date).split(' ')[0] # keeping just YYYY-MM-DD from YYYY-MM-DD HH:MM:SS\n  data_dict = s3.list_objects(Bucket = bucket_name, Prefix = prefix+date)\n  \n  for file_obj in data_dict['Contents']:\n    assert os.path.exists(path+file_obj['Key']), file_obj['Key']\n\n\nprint('Validated')"
  },
  {
    "objectID": "posts/ssh-macos.html",
    "href": "posts/ssh-macos.html",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you’d like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThat’s it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/ssh-macos.html#terminology",
    "href": "posts/ssh-macos.html#terminology",
    "title": "Passwordless SSH setup for MacOS Hosts",
    "section": "",
    "text": "HOST: The computer physically present with you.\nREMOTE: The remote computer that you’d like to access via ssh.\nREMOTE-IP: Ip address of the REMOTE.\nPORT: The port on which the ssh server is running on REMOTE.\nssh-keygen # this will generate a public and private key pair. Rename it if you want.\nssh-copy-id -i ~/.ssh/id_rsa.pub -p PORT USERANAME@REMOTE-IP # this will copy the public key to REMOTE\nssh-add ~/.ssh/id_rsa # this command tells the HOST to use the private key for ssh connections\nThat’s it! You should now be able to ssh into the REMOTE without a password. After rebooting the HOST, if VSCode or CLI asks for a password, run ssh-add ~/.ssh/id_rsa again."
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html",
    "href": "posts/bayesian-gaussian-basis-regression.html",
    "title": "Bayesian Basis Regression",
    "section": "",
    "text": "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\nimport pandas as pd\nimport regdata as rd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\"\nrd.set_backend(\"torch\")"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "href": "posts/bayesian-gaussian-basis-regression.html#generate-data",
    "title": "Bayesian Basis Regression",
    "section": "Generate data",
    "text": "Generate data\n\n# x = torch.linspace(-1, 1, 100)\n# y = (torch.sin(x * 2 * torch.pi) + torch.randn(x.size()) * 0.1).unsqueeze(1)\nx, y, _ = rd.MotorcycleHelmet().get_data()\nx = x.ravel().to(torch.float32)\nidx = np.argsort(x)\nx = x[idx]\ny = y.to(torch.float32)\ny = y[idx]\n\nx = torch.vstack([torch.ones_like(x), x]).T\nprint(x.shape, y.shape)\nx = x.to(device)\ny = y.to(device)\nprint(x.dtype, y.dtype)\n\nplt.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n\ntorch.Size([94, 2]) torch.Size([94])\ntorch.float32 torch.float32\n\n\n\n\n\n\n\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, in_dim, out_dim, neurons, transform=None):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.transform = transform\n        if transform is None:\n            self.transform = lambda x: x\n            self.layers.append(nn.Linear(in_dim, neurons[0]))\n        else:\n            self.layers.append(nn.Linear(self.transform.n_grid + 1, neurons[0]))\n        for i in range(1, len(neurons)):\n            self.layers.append(nn.Linear(neurons[i - 1], neurons[i]))\n        self.layers.append(nn.Linear(neurons[-1], out_dim))\n\n    def forward(self, x):\n        x = self.transform(x)\n        # print(x.shape)\n        for layer in self.layers[:-1]:\n            x = F.gelu(layer(x))\n        return self.layers[-1](x)\n\n\nclass RBF(nn.Module):\n    def __init__(self, log_gauss_var, n_grid):\n        super().__init__()\n        self.log_gauss_var = nn.Parameter(torch.tensor(log_gauss_var))\n        self.n_grid = n_grid\n        self.grid = nn.Parameter(torch.linspace(-1, 1, n_grid))\n        self.register_buffer(\"bias\", torch.zeros(1))\n\n    def forward(self, x):\n        self.dist = dist.Normal(self.grid, torch.exp(self.log_gauss_var))\n        features = torch.exp(self.dist.log_prob(x[:, 1:2]))\n        # print(features.shape)\n        features = torch.cat(\n            [\n                torch.ones_like(self.bias.repeat(features.shape[0])).reshape(-1, 1),\n                features,\n            ],\n            dim=1,\n        )\n        return features\n\n\nRBF(0.0, 10).to(device)(x).shape\n\ntorch.Size([94, 11])\n\n\n\n# def transform_fn(x):\n#     all_x = []\n#     for i in range(2, 11):\n#         all_x.append(x[:, 1:2] ** i)\n#     return torch.hstack([x] + all_x)\n\n\ndef get_mn_sn(x, s0):\n    x = transform_fn(x)\n    sn_inv = (x.T @ x) / torch.exp(log_var_noise)\n    diag = sn_inv.diagonal()\n    diag += 1 / s0\n    sn = torch.inverse(sn_inv)\n    mn = sn @ ((x.T @ y) / torch.exp(log_var_noise))\n    return mn, sn\n\n\ndef neg_log_likelihood(x, y, m0, s0):\n    x = transform_fn(x)\n    cov = (x @ x.T) / s0\n    diag = cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    return (\n        -dist.MultivariateNormal(m0.repeat(y.shape[0]), cov).log_prob(y.ravel()).sum()\n    )\n\n\ndef get_pred_post(sn, mn, x):\n    x = transform_fn(x)\n    pred_cov = x @ sn @ x.T\n    diag = pred_cov.diagonal()\n    diag += torch.exp(log_var_noise)\n    pred_mean = x @ mn\n    return pred_mean, pred_cov\n\n\ndef plot_preds_and_95(ax, x, pred_mean, pred_cov):\n    with torch.no_grad():\n        x = x[:, 1].cpu().numpy()\n        pred_mean = pred_mean.ravel().cpu().numpy()\n        pred_var = pred_cov.diagonal().cpu().numpy()\n        ax.plot(x, pred_mean, color=\"red\", label=\"mean\")\n        ax.fill_between(\n            x,\n            (pred_mean - 2 * np.sqrt(pred_var)),\n            (pred_mean + 2 * np.sqrt(pred_var)),\n            color=\"red\",\n            alpha=0.2,\n            label=\"95% CI\",\n        )\n        return ax\n\n\nmlp = MLP(2, 1, [256, 256, 256]).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = True\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 30.6285: 100%|██████████| 500/500 [00:02&lt;00:00, 209.49it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    # ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\ntorch.exp(log_var_noise), s0, m0\n\n(tensor(0.1191, device='cuda:0', grad_fn=&lt;ExpBackward0&gt;),\n tensor(1.3897, device='cuda:0', requires_grad=True),\n tensor([-0.0693], device='cuda:0', requires_grad=True))"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "href": "posts/bayesian-gaussian-basis-regression.html#add-gaussian-transform",
    "title": "Bayesian Basis Regression",
    "section": "Add Gaussian transform",
    "text": "Add Gaussian transform\n\nmlp = MLP(2, 1, [256, 256, 256], transform=RBF(0.1, 10)).to(device)\n# mlp = RBF(0.1, 20).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: -29.9227: 100%|██████████| 500/500 [00:03&lt;00:00, 156.90it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.transform.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "href": "posts/bayesian-gaussian-basis-regression.html#just-gaussian-basis",
    "title": "Bayesian Basis Regression",
    "section": "Just Gaussian basis",
    "text": "Just Gaussian basis\n\n# mlp = MLP(2, 1, [32, 32, 32], transform=RBF(0.1, 10)).to(device)\nmlp = RBF(1.0, 5).to(device)\ntransform_fn = mlp.forward\n\nm0 = torch.zeros((1,)).to(device)\ns0 = torch.tensor(1.0).to(device)\nwith torch.no_grad():\n    log_var_noise = nn.Parameter(torch.tensor(0.1)).to(device)\n    log_var_noise.requires_grad = True\n    m0.requires_grad = False\n    s0.requires_grad = True\n\n\noptimizer = torch.optim.Adam([*list(mlp.parameters()), log_var_noise, m0, s0], lr=0.001)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = neg_log_likelihood(x, y, m0, s0)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 207.0843: 100%|██████████| 500/500 [00:02&lt;00:00, 195.61it/s]\n\n\n\n\n\n\n\n\n\n\nmn, sn = get_mn_sn(x, s0)\npred_mean, pred_var = get_pred_post(sn, mn, x)\n\nfig, ax = plt.subplots()\nax = plot_preds_and_95(ax, x, pred_mean, pred_var)\nwith torch.no_grad():\n    ax.scatter(x.cpu().numpy()[:, 1], y.cpu().numpy())\n    ax.vlines(mlp.grid.cpu().numpy(), -1, 1, color=\"black\", alpha=0.2)\nplt.show()"
  },
  {
    "objectID": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "href": "posts/bayesian-gaussian-basis-regression.html#appendix",
    "title": "Bayesian Basis Regression",
    "section": "Appendix",
    "text": "Appendix\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\ndata = pd.read_csv(\"~/datasets/uci/bike/hour.csv\", header=None).iloc[:, 1:]\ndata.shape\n\n(17379, 18)\n\n\n\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nX_train = x_scaler.fit_transform(X_train)\ny_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\nX_test = x_scaler.transform(X_test)\ny_test = y_scaler.transform(y_test.reshape(-1, 1))\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((10427, 17), (6952, 17), (10427, 1), (6952, 1))\n\n\n\n[X_train, X_test, y_train, y_test] = map(\n    lambda x: torch.tensor(x, dtype=torch.float32).to(device),\n    [X_train, X_test, y_train, y_test],\n)\n\n\nmlp = MLP(17, 1, [10, 10]).to(device)\n\noptimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\nlosses = []\npbar = tqdm(range(500))\nfor i in pbar:\n    optimizer.zero_grad()\n    loss = F.mse_loss(mlp(X_train), y_train)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0040: 100%|██████████| 500/500 [00:01&lt;00:00, 482.25it/s]\n\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_pred = mlp(X_test).cpu().numpy()\n    if isinstance(y_test, torch.Tensor):\n        y_test = y_test.cpu().numpy()\n    print(y_pred.shape, y_test.shape)\n    print(\"RMSE\", mean_squared_error(y_test, y_pred, squared=False))\n\n(6952, 1) (6952, 1)\nRMSE 0.08354535"
  },
  {
    "objectID": "posts/2023-03-28-nngp.html",
    "href": "posts/2023-03-28-nngp.html",
    "title": "Neural Network Gaussian Process",
    "section": "",
    "text": "# %%capture\n# %pip install -U --force-reinstall jaxutils\n# %pip install -U jax jaxlib optax\n\n\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nfrom jaxutils import Dataset\n\ntry:\n    from neural_tangents import stax\nexcept ModuleNotFoundError:\n    %pip install neural-tangents\n    from neural_tangents import stax\n\ntry:\n    import optax as ox\nexcept ModuleNotFoundError:\n    %pip install optax\n    import optax as ox\n\ntry:\n    import gpjax as gpx\nexcept ModuleNotFoundError:\n    %pip install gpjax\n    import gpjax as gpx\n\ntry:\n    import regdata as rd\nexcept ModuleNotFoundError:\n    %pip install regdata\n    import regdata as rd\n\nimport matplotlib.pyplot as plt\n\n\nclass NTK(gpx.kernels.AbstractKernel):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def __call__(self, params, x, y):\n        params = jax.tree_util.tree_map(jax.nn.softplus, params)\n        init_fn, apply_fn, kernel_fn = stax.serial(\n            stax.Dense(512, W_std=params[\"w1\"], b_std=params[\"b1\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w2\"], b_std=params[\"b2\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w3\"], b_std=params[\"b3\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w4\"], b_std=params[\"b4\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w5\"], b_std=params[\"b5\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w6\"], b_std=params[\"b6\"]), stax.Relu(),\n            stax.Dense(512, W_std=params[\"w7\"], b_std=params[\"b7\"]), stax.Relu(),\n            stax.Dense(1, W_std=params[\"w8\"], b_std=params[\"b8\"])\n        )\n        return kernel_fn(x.reshape(1, 1), y.reshape(1, 1)).nngp.squeeze()\n\n    def init_params(self, key):\n        # return init_fn(key, input_shape=(2,1))\n        return {\"w1\": 0.1, \"w2\": 0.2, \"w3\": 0.3, \"w4\": 0.4, \"w5\": 0.5,  \"w6\": 0.6, \"w7\": 0.7, \"w8\": 0.8,\n                \"b1\": 0.1, \"b2\": 0.2, \"b3\": 0.3, \"b4\": 0.4, \"b5\": 0.5,  \"b6\": 0.6, \"b7\": 0.7, \"b8\": 0.8\n                }\n\n    # This is depreciated. Can be removed once JaxKern is updated.\n    def _initialise_params(self, key):\n        return self.init_params(key)\n\n\nn = 100\nnoise = 0.3\nkey = jr.PRNGKey(123)\n# x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).sort().reshape(-1, 1)\n# f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n# signal = f(x)\n# y = signal + jr.normal(key, shape=signal.shape) * noise\nx, y, xtest = rd.MotorcycleHelmet().get_data()\ny = y.reshape(-1, 1)\n\nD = Dataset(X=x, y=y)\n\n# xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n# ytest = f(xtest)\n\nprint(x.shape, y.shape)\n\n(94, 1) (94, 1)\n\n\n\nkernel = NTK()\nprior = gpx.Prior(kernel=kernel)\nlikelihood = gpx.Gaussian(num_datapoints=D.n)\nposterior = prior * likelihood\n\n\nkey = jr.PRNGKey(1234)\nparameter_state = gpx.initialise(posterior, key)\nparams, trainable, bijectors = parameter_state.unpack()\nparams[\"likelihood\"][\"obs_noise\"] = jnp.array(0.1)\nparameter_state = gpx.parameters.ParameterState(params, trainable, bijectors)\nprint(params)\n\n{'kernel': {'w1': 0.1, 'w2': 0.2, 'w3': 0.3, 'w4': 0.4, 'w5': 0.5, 'w6': 0.6, 'w7': 0.7, 'w8': 0.8, 'b1': 0.1, 'b2': 0.2, 'b3': 0.3, 'b4': 0.4, 'b5': 0.5, 'b6': 0.6, 'b7': 0.7, 'b8': 0.8}, 'mean_function': {}, 'likelihood': {'obs_noise': Array(0.1, dtype=float32, weak_type=True)}}\n\n\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter w8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b1 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b2 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b3 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b4 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b5 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b6 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b7 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n/home/patel_zeel/0Notebooks/.conda/lib/python3.9/site-packages/gpjax/parameters.py:194: UserWarning: Parameter b8 has no transform. Defaulting to identity transfom.\n  warnings.warn(\n\n\n\nnegative_mll = jax.jit(posterior.marginal_log_likelihood(D, negative=True))\nnegative_mll(params)\n\nArray(415.1062, dtype=float32)\n\n\n\noptimiser = ox.adam(learning_rate=0.01)\n\ninference_state = gpx.fit(\n    objective=negative_mll,\n    parameter_state=parameter_state,\n    optax_optim=optimiser,\n    num_iters=500,\n)\n\nlearned_params, training_history = inference_state.unpack()\n\n100%|██████████| 500/500 [00:02&lt;00:00, 172.53it/s, Objective=76.34]\n\n\n\nplt.plot(training_history);\n\n\n\n\n\n\n\n\n\nlearned_params\n\n{'kernel': {'b1': Array(0.03292831, dtype=float32),\n  'b2': Array(-0.9647168, dtype=float32),\n  'b3': Array(-1.2660046, dtype=float32),\n  'b4': Array(-1.3792713, dtype=float32),\n  'b5': Array(-1.4311961, dtype=float32),\n  'b6': Array(-1.4504426, dtype=float32),\n  'b7': Array(-1.4371448, dtype=float32),\n  'b8': Array(-1.3471106, dtype=float32),\n  'w1': Array(1.0706716, dtype=float32),\n  'w2': Array(1.1768614, dtype=float32),\n  'w3': Array(1.2740505, dtype=float32),\n  'w4': Array(1.3689499, dtype=float32),\n  'w5': Array(1.462641, dtype=float32),\n  'w6': Array(1.5562503, dtype=float32),\n  'w7': Array(1.6506695, dtype=float32),\n  'w8': Array(1.7462935, dtype=float32)},\n 'likelihood': {'obs_noise': Array(0.184795, dtype=float32)},\n 'mean_function': {}}\n\n\n\nlatent_dist = posterior(learned_params, D)(xtest)\npredictive_dist = likelihood(learned_params, latent_dist)\n\npredictive_mean = predictive_dist.mean()\npredictive_std = predictive_dist.stddev()\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(x, y, \"o\", label=\"Observations\", color=\"tab:red\")\nax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=\"tab:blue\")\nax.fill_between(\n    xtest.squeeze(),\n    predictive_mean - 2 * predictive_std,\n    predictive_mean + 2 * predictive_std,\n    alpha=0.2,\n    color=\"tab:blue\",\n    label=\"Two sigma\",\n)\nax.plot(\n    xtest,\n    predictive_mean - predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\nax.plot(\n    xtest,\n    predictive_mean + predictive_std,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    linewidth=1,\n)\n\n# ax.plot(\n#     xtest, ytest, label=\"Latent function\", color=\"black\", linestyle=\"--\", linewidth=1\n# )\n\nax.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "",
    "text": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport GPy\nimport jax\nimport gpytorch\nimport botorch\nimport tinygp\nimport jax.numpy as jnp\nimport optax\nfrom IPython.display import clear_output\n\nfrom sklearn.preprocessing import StandardScaler\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#data",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Data",
    "text": "Data\n\nnp.random.seed(0) # We don't want surprices in a presentation :)\nN = 10\ntrain_x = torch.linspace(0, 1, N)\ntrain_y = torch.sin(train_x * (2 * math.pi)) + torch.normal(0, 0.1, size=(N,))\n \ntest_x = torch.linspace(0, 1, N*10)\ntest_y = torch.sin(test_x * (2 * math.pi))\n\n\nplt.plot(train_x, train_y, 'ko', label='train');\nplt.plot(test_x, test_y, label='test');\nplt.legend();"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#defining-kernel",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Defining kernel",
    "text": "Defining kernel\n\\[\\begin{equation}\n\\sigma_f^2 = \\text{variance}\\\\\n\\ell = \\text{lengthscale}\\\\\nk_{RBF}(x_1, x_2) = \\sigma_f^2 \\exp \\left[-\\frac{\\lVert x_1 - x_2 \\rVert^2}{2\\ell^2}\\right]\n\\end{equation}\\]\n\nGPy\n\ngpy_kernel = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\ngpy_kernel\n\n\n\n\n\n\nrbf.\nvalue\nconstraints\npriors\n\n\nvariance\n1.0\n+ve\n\n\n\nlengthscale\n1.0\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\ngpytorch_kernel.outputscale = 1. # variance\ngpytorch_kernel.base_kernel.lengthscale = 1. # lengthscale\n\ngpytorch_kernel\n\nScaleKernel(\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n  (raw_outputscale_constraint): Positive()\n)\n\n\n\n\nTinyGP\n\ndef RBFKernel(variance, lengthscale):\n    return jnp.exp(variance) * tinygp.kernels.ExpSquared(scale=jnp.exp(lengthscale))\n    \ntinygp_kernel = RBFKernel(variance=1., lengthscale=1.)\ntinygp_kernel\n\n&lt;tinygp.kernels.Product at 0x7f544039d710&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#define-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Define model",
    "text": "Define model\n\\[\n\\sigma_n^2 = \\text{noise variance}\n\\]\n\nGPy\n\ngpy_model = GPy.models.GPRegression(train_x.numpy()[:,None], train_y.numpy()[:,None], gpy_kernel)\ngpy_model.Gaussian_noise.variance = 0.1\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 16.757933772959404\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n1.0\n+ve\n\n\n\nrbf.lengthscale\n1.0\n+ve\n\n\n\nGaussian_noise.variance\n0.1\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, kernel):\n        super().__init__(train_x, train_y, likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ngpytorch_likelihood = gpytorch.likelihoods.GaussianLikelihood()\ngpytorch_model = ExactGPModel(train_x, train_y, gpytorch_likelihood, gpytorch_kernel)\n\ngpytorch_model.likelihood.noise = 0.1\ngpytorch_model\n\nExactGPModel(\n  (likelihood): GaussianLikelihood(\n    (noise_covar): HomoskedasticNoise(\n      (raw_noise_constraint): GreaterThan(1.000E-04)\n    )\n  )\n  (mean_module): ConstantMean()\n  (covar_module): ScaleKernel(\n    (base_kernel): RBFKernel(\n      (raw_lengthscale_constraint): Positive()\n    )\n    (raw_outputscale_constraint): Positive()\n  )\n)\n\n\n\n\nTinyGP\n\ndef build_gp(theta, X):\n    mean = theta[0] \n    variance, lengthscale, noise_variance = jnp.exp(theta[1:])\n    \n    kernel = variance * tinygp.kernels.ExpSquared(lengthscale)\n    \n    return tinygp.GaussianProcess(kernel, X, diag=noise_variance, mean=mean)\n\ntinygp_model = build_gp(theta=np.array([0., 1., 1., 0.1]), X=train_x.numpy())\n\ntinygp_model\n# __repr__\n\n&lt;tinygp.gp.GaussianProcess at 0x7f5440401850&gt;"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#train-the-model",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Train the model",
    "text": "Train the model\n\nGPy\n\ngpy_model.optimize(max_iters=50)\ngpy_model\n\n\n\n\nModel: GP regression\nObjective: 3.944394423452163\nNumber of Parameters: 3\nNumber of Optimization Parameters: 3\nUpdates: True\n\n\n\n\n\n\nGP_regression.\nvalue\nconstraints\npriors\n\n\nrbf.variance\n0.9376905183253631\n+ve\n\n\n\nrbf.lengthscale\n0.2559000163858406\n+ve\n\n\n\nGaussian_noise.variance\n0.012506184441481319\n+ve\n\n\n\n\n\n\n\n\nGPyTorch\n\nmll = gpytorch.mlls.ExactMarginalLogLikelihood(gpytorch_likelihood, gpytorch_model)\nbotorch.fit_gpytorch_model(mll)\n\ndisplay(gpytorch_model.mean_module.constant, # Mean\n        gpytorch_model.covar_module.outputscale, # Variance\n        gpytorch_model.covar_module.base_kernel.lengthscale, # Lengthscale \n        gpytorch_model.likelihood.noise) # Noise variance\n\n /opt/conda/lib/python3.7/site-packages/botorch/fit.py:143: UserWarning:CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/c10/cuda/CUDAFunctions.cpp:112.)\n\n\nParameter containing:\ntensor([0.0923], requires_grad=True)\n\n\ntensor(0.9394, grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([[0.2560]], grad_fn=&lt;SoftplusBackward0&gt;)\n\n\ntensor([0.0124], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nTinyGP\n\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(theta, X, y):\n    gp = build_gp(theta, X)\n    return -gp.condition(y)\n\n\nobj = jax.jit(jax.value_and_grad(neg_log_likelihood))\nresult = minimize(obj, [0., 1., 1., 0.1], jac=True, args=(train_x.numpy(), train_y.numpy()))\nresult.x[0], np.exp(result.x[1:])\n\n(0.09213499552879165, array([0.9395271 , 0.25604163, 0.01243025]))"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#inference",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Inference",
    "text": "Inference\n\ndef plot_gp(pred_y, var_y):\n    std_y = var_y ** 0.5\n    plt.figure()\n    plt.scatter(train_x, train_y, label='train')\n    plt.plot(test_x, pred_y, label='predictive mean')\n    plt.fill_between(test_x.ravel(), \n                     pred_y.ravel() - 2*std_y.ravel(), \n                     pred_y.ravel() + 2*std_y.ravel(), alpha=0.2, label='95% confidence')\n    plt.legend()\n\n\nGPy\n\npred_y, var_y = gpy_model.predict(test_x.numpy()[:, None])\nplot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nGPyTorch\n\ngpytorch_model.eval()\n\nwith torch.no_grad(), gpytorch.settings.fast_pred_var():\n    pred_dist = gpytorch_likelihood(gpytorch_model(test_x))\n    pred_y, var_y = pred_dist.mean, pred_dist.variance\n    plot_gp(pred_y, var_y)\n\n\n\n\n\n\n\n\n\n\nTinyGP\n\ntinygp_model = build_gp(result.x, train_x.numpy())\npred_y, var_y = tinygp_model.predict(train_y.numpy(), test_x.numpy(), return_var=True)\n\nplot_gp(pred_y, var_y)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#tiny-gp-on-co2-dataset",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "Tiny GP on CO2 dataset",
    "text": "Tiny GP on CO2 dataset\n\ndata = pd.read_csv(\"data/co2.csv\")\n\n# Train test split\nX = data[\"0\"].iloc[:290].values.reshape(-1, 1)\nX_test = data[\"0\"].iloc[290:].values.reshape(-1, 1)\ny = data[\"1\"].iloc[:290].values\ny_test = data[\"1\"].iloc[290:].values\n\n# Scaling the dataset\nXscaler = StandardScaler()\nX = Xscaler.fit_transform(X)\nX_test = Xscaler.transform(X_test)\n\nyscaler = StandardScaler()\ny = yscaler.fit_transform(y.reshape(-1, 1)).ravel()\ny_test = yscaler.transform(y_test.reshape(-1, 1)).ravel()\n\n\nplt.plot(X, y, label='train');\nplt.plot(X_test, y_test, label='test');\nplt.legend();\n\n\n\n\n\n\n\n\n\nclass SpectralMixture(tinygp.kernels.Kernel):\n    def __init__(self, weight, scale, freq):\n        self.weight = jnp.atleast_1d(weight)\n        self.scale = jnp.atleast_1d(scale)\n        self.freq = jnp.atleast_1d(freq)\n\n    def evaluate(self, X1, X2):\n        tau = jnp.atleast_1d(jnp.abs(X1 - X2))[..., None]\n        return jnp.sum(\n            self.weight\n            * jnp.prod(\n                jnp.exp(-2 * jnp.pi ** 2 * tau ** 2 / self.scale ** 2)\n                * jnp.cos(2 * jnp.pi * self.freq * tau),\n                axis=-1,\n            )\n        )\n    \ndef build_spectral_gp(theta):\n    kernel = SpectralMixture(\n        jnp.exp(theta[\"log_weight\"]),\n        jnp.exp(theta[\"log_scale\"]),\n        jnp.exp(theta[\"log_freq\"]),\n    )\n    return tinygp.GaussianProcess(\n        kernel, X, diag=jnp.exp(theta[\"log_diag\"]), mean=theta[\"mean\"]\n    )\n\n\nK = 4 # Number of mixtures\ndiv_factor = 0.4\nnp.random.seed(1)\nparams = {\n    \"log_weight\": np.abs(np.random.rand(K))/div_factor,\n    \"log_scale\": np.abs(np.random.rand(K))/div_factor,\n    \"log_freq\": np.abs(np.random.rand(K))/div_factor,\n    \"log_diag\": np.abs(np.random.rand(1))/div_factor,\n    \"mean\": 0.,\n}\n\n@jax.jit\n@jax.value_and_grad\ndef loss(theta):\n    return -build_spectral_gp(theta).condition(y)\n# opt = optax.sgd(learning_rate=0.001)\nopt = optax.adam(learning_rate=0.1)\nopt_state = opt.init(params)\nlosses = []\nfor i in range(100):\n    loss_val, grads = loss(params)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n    losses.append(loss_val)\n    clear_output(wait=True)\n    print(f\"iter {i}, loss {loss_val}\")\n\nopt_gp = build_spectral_gp(params)\n\nparams\n\niter 99, loss 27.987701416015625\n\n\n{'log_diag': DeviceArray([-2.7388687], dtype=float32),\n 'log_freq': DeviceArray([-3.6072493, -3.1795945, -3.4490397, -2.373117 ], dtype=float32),\n 'log_scale': DeviceArray([3.9890492, 3.8530042, 4.0878096, 4.4860597], dtype=float32),\n 'log_weight': DeviceArray([-1.3715047, -0.6132469, -2.413771 , -1.6582283], dtype=float32),\n 'mean': DeviceArray(0.38844627, dtype=float32)}\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nmu, var = opt_gp.predict(y, X_test, return_var=True)\n\nplt.plot(X, y, c='k')\nplt.fill_between(\n    X_test.ravel(), mu + np.sqrt(var), mu - np.sqrt(var), color=\"C0\", alpha=0.5\n)\nplt.plot(X_test, mu, color=\"C0\", lw=2)\n\n# plt.xlim(t.min(), 2025)\nplt.xlabel(\"year\")\n_ = plt.ylabel(\"CO$_2$ in ppm\")"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#k_mathbfy-textcov_functionx_train-x_train-sigma_f-ell-sigma_n",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)",
    "text": "\\(K_\\mathbf{y} = \\text{cov_function}(X_{train}, X_{train}, \\sigma_f, \\ell, \\sigma_n)\\)"
  },
  {
    "objectID": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "href": "posts/2022-01-25-gp_frameworks_comparison.html#gp-loss-log-pmathbfy-mid-mathbfx-theta-frac12-mathbfyt-k_y-1-mathbfy-frac12-log-leftk_yright-fracn2-log-2-pi",
    "title": "Comparing Gaussian Process Regression Frameworks",
    "section": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)",
    "text": "GP Loss: \\(\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\theta)=-\\frac{1}{2} \\mathbf{y}^{T} K_{y}^{-1} \\mathbf{y}-\\frac{1}{2} \\log \\left|K_{y}\\right|-\\frac{n}{2} \\log 2 \\pi\\)\n\nMinimize inverse term fully\nNow, Minimize both togather"
  },
  {
    "objectID": "posts/climate-modeling-with-siren.html",
    "href": "posts/climate-modeling-with-siren.html",
    "title": "Climate Modeling with SIRENs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\nimport numpy as np\nimport xarray as xr\nfrom tqdm.keras import TqdmCallback\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, initializers, activations\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport tensorflow_addons as tfa\n\nimport matplotlib.pyplot as plt\n\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2023-07-18 05:13:52.439735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-18 05:13:53.232689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/patel_zeel/miniconda3/envs/tensorflow_gpu/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    first_init = lambda input_dim: initializers.RandomUniform(-1 / input_dim, 1 / input_dim)\n    other_init = lambda input_dim: initializers.RandomUniform(-np.sqrt(6 / input_dim) / activation_scale, np.sqrt(6 / input_dim) / activation_scale)\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=first_init(input_dim), activation=lambda x: tf.sin(activation_scale*x)))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=other_init(features[i-1]), activation=lambda x: tf.sin(activation_scale*x)))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=other_init(features[-1]), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../../super_res/data/era5_low_res/2m_temperature/2m_temperature_2018_5.625deg.nc\")\ndata1 = xr.open_dataset(\"../../super_res/data/era5_high_res/2m_temperature/2m_temperature_2018_1.40625deg.nc\")\n\n\ndata5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (lon: 64, lat: 32, time: 8760)\nCoordinates:\n  * lon      (lon) float64 0.0 5.625 11.25 16.88 ... 337.5 343.1 348.8 354.4\n  * lat      (lat) float64 -87.19 -81.56 -75.94 -70.31 ... 75.94 81.56 87.19\n  * time     (time) datetime64[ns] 2018-01-01 ... 2018-12-31T23:00:00\nData variables:\n    t2m      (time, lat, lon) float32 ...\nAttributes:\n    Conventions:  CF-1.6\n    history:      2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...xarray.DatasetDimensions:lon: 64lat: 32time: 8760Coordinates: (3)lon(lon)float640.0 5.625 11.25 ... 348.8 354.4array([  0.   ,   5.625,  11.25 ,  16.875,  22.5  ,  28.125,  33.75 ,  39.375,\n        45.   ,  50.625,  56.25 ,  61.875,  67.5  ,  73.125,  78.75 ,  84.375,\n        90.   ,  95.625, 101.25 , 106.875, 112.5  , 118.125, 123.75 , 129.375,\n       135.   , 140.625, 146.25 , 151.875, 157.5  , 163.125, 168.75 , 174.375,\n       180.   , 185.625, 191.25 , 196.875, 202.5  , 208.125, 213.75 , 219.375,\n       225.   , 230.625, 236.25 , 241.875, 247.5  , 253.125, 258.75 , 264.375,\n       270.   , 275.625, 281.25 , 286.875, 292.5  , 298.125, 303.75 , 309.375,\n       315.   , 320.625, 326.25 , 331.875, 337.5  , 343.125, 348.75 , 354.375])lat(lat)float64-87.19 -81.56 ... 81.56 87.19array([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875])time(time)datetime64[ns]2018-01-01 ... 2018-12-31T23:00:00long_name :timearray(['2018-01-01T00:00:00.000000000', '2018-01-01T01:00:00.000000000',\n       '2018-01-01T02:00:00.000000000', ..., '2018-12-31T21:00:00.000000000',\n       '2018-12-31T22:00:00.000000000', '2018-12-31T23:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)t2m(time, lat, lon)float32...units :Klong_name :2 metre temperature[17940480 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([    0.0,   5.625,   11.25,  16.875,    22.5,  28.125,   33.75,  39.375,\n          45.0,  50.625,   56.25,  61.875,    67.5,  73.125,   78.75,  84.375,\n          90.0,  95.625,  101.25, 106.875,   112.5, 118.125,  123.75, 129.375,\n         135.0, 140.625,  146.25, 151.875,   157.5, 163.125,  168.75, 174.375,\n         180.0, 185.625,  191.25, 196.875,   202.5, 208.125,  213.75, 219.375,\n         225.0, 230.625,  236.25, 241.875,   247.5, 253.125,  258.75, 264.375,\n         270.0, 275.625,  281.25, 286.875,   292.5, 298.125,  303.75, 309.375,\n         315.0, 320.625,  326.25, 331.875,   337.5, 343.125,  348.75, 354.375],\n      dtype='float64', name='lon'))latPandasIndexPandasIndex(Index([-87.1875, -81.5625, -75.9375, -70.3125, -64.6875, -59.0625, -53.4375,\n       -47.8125, -42.1875, -36.5625, -30.9375, -25.3125, -19.6875, -14.0625,\n        -8.4375,  -2.8125,   2.8125,   8.4375,  14.0625,  19.6875,  25.3125,\n        30.9375,  36.5625,  42.1875,  47.8125,  53.4375,  59.0625,  64.6875,\n        70.3125,  75.9375,  81.5625,  87.1875],\n      dtype='float64', name='lat'))timePandasIndexPandasIndex(DatetimeIndex(['2018-01-01 00:00:00', '2018-01-01 01:00:00',\n               '2018-01-01 02:00:00', '2018-01-01 03:00:00',\n               '2018-01-01 04:00:00', '2018-01-01 05:00:00',\n               '2018-01-01 06:00:00', '2018-01-01 07:00:00',\n               '2018-01-01 08:00:00', '2018-01-01 09:00:00',\n               ...\n               '2018-12-31 14:00:00', '2018-12-31 15:00:00',\n               '2018-12-31 16:00:00', '2018-12-31 17:00:00',\n               '2018-12-31 18:00:00', '2018-12-31 19:00:00',\n               '2018-12-31 20:00:00', '2018-12-31 21:00:00',\n               '2018-12-31 22:00:00', '2018-12-31 23:00:00'],\n              dtype='datetime64[ns]', name='time', length=8760, freq=None))Attributes: (2)Conventions :CF-1.6history :2019-11-06 10:38:21 GMT by grib_to_netcdf-2.14.0: /opt/ecmwf/eccodes/bin/grib_to_netcdf -o /cache/data2/adaptor.mars.internal-1573035683.1772008-2550-1-601d5659-dae2-45e1-902b-45825d30e8d0.nc /cache/tmp/601d5659-dae2-45e1-902b-45825d30e8d0-adaptor.mars.internal-1573035683.1790879-2550-1-tmp.grib\n\n\n\ntime_stamp = slice(\"2018-01\", \"2018-03\")\ntrain_df = data5.sel(time=time_stamp).to_dataframe().reset_index()\ntest_df = data1.sel(time=time_stamp).to_dataframe().reset_index()\n\nX = np.stack([train_df.lat.values, train_df.lon.values, train_df.time.astype(np.int64) / 10**9], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values, test_df.time.astype(np.int64) / 10**9], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\n# rff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(4423680, 3), y.shape=(4423680, 1)\nX_test.shape=(70778880, 3), y_test.shape=(70778880, 1)\nX.shape=(4423680, 3), X_test.shape=(70778880, 3)\n\n\n\n32*64*24*(31+28+31)\n\n4423680\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\n# Scaling time\nif X.shape[1] == 3:\n    X_scaled[:, 2] = X_scaled[:, 2] * 10 - 5\n    X_test_scaled[:, 2] = X_test_scaled[:, 2] * 10 - 5\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = SIREN(3, 1, [256]*4, 30.0, 0.0)\n# model = MLP(3, 1, [256]*4, 0.0)\n# model = ResNet()S\n# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=1e-3,\n#     maximal_learning_rate=1e-2,\n#     scale_fn=lambda x: 1/(2.**(x-1)),\n#     step_size=2\n# )\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n2023-07-18 05:14:34.531498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n2023-07-18 05:14:34.531583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78884 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n\n\n\n0.00148\n\n0.00148\n\n\n\ncallbacks = [TqdmCallback(verbose=1)]\nhistory = model.fit(X_scaled, y_scaled, epochs=1000, batch_size=X_scaled.shape[0], verbose=0, callbacks=callbacks)\n\n  0%|          | 0/1000 [00:00&lt;?, ?epoch/s]2023-07-18 05:14:38.677299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n2023-07-18 05:14:39.357828: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fb81b946130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2023-07-18 05:14:39.357901: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n2023-07-18 05:14:39.363158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-07-18 05:14:40.399794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n2023-07-18 05:14:40.557542: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100%|██████████| 1000/1000 [04:46&lt;00:00,  3.49epoch/s, loss=0.000295]\n\n\n\nplt.plot(history.history['loss'][200:], label='loss');\n# plt.plot(history.history['val_loss'][200:], label='val_loss');\nplt.legend();\n\n\n\n\n\n\n\n\n\n128*256*24\n\n786432\n\n\n\nimg_index = 0\ny_pred = model.predict(X_test_scaled, batch_size=20480) * (y_max - y_min) + y_min\nprint(y_pred.shape)\nplt.imshow(y_pred[img_index*(256*128):(img_index+1)*(256*128)].reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n3456/3456 [==============================] - 5s 1ms/step\n(70778880, 1)\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\ndef get_lat_weights(lat):\n    lat_weights = np.cos(np.deg2rad(lat))\n    lat_weights = lat_weights / lat_weights.mean()\n    return lat_weights\n\nlat_weights = get_lat_weights(X_test[:, 0])\nprint(f\"{lat_weights.shape=}\")\n\nlat_squared_error = lat_weights * (y_pred.ravel() - y_test.ravel())**2\nlat_rmse = np.sqrt(lat_squared_error.mean())\nprint(f\"{lat_rmse=}\")\n# y_pred.shape, lat_weights.shape\n\nlat_weights.shape=(70778880,)\nlat_rmse=2.6118446730600438\n\n\n\n# lat_rmse=3.4826956884024356\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html",
    "href": "posts/2022-04-06-github_faqs.html",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "href": "posts/2022-04-06-github_faqs.html#q1-what-is-an-efficient-way-to-work-on-multiple-issues-at-once",
    "title": "GitHub Contrubuting FAQs",
    "section": "",
    "text": "Create separate branches for each issue. Do not work on the master branch.\n\n\nWe will see that in Q5."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "href": "posts/2022-04-06-github_faqs.html#q2-what-to-do-if-the-main-or-master-gets-updated-before-i-open-a-pr",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q2: What to do if the main (or master) gets updated before I open a PR?",
    "text": "Q2: What to do if the main (or master) gets updated before I open a PR?\nPull the changes directly to your branch with:\ngit pull https://github.com/probml/pyprobml"
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "href": "posts/2022-04-06-github_faqs.html#q3-what-to-do-with-the-forks-main-when-the-original-main-is-updated",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q3: What to do with the fork’s main when the original main is updated?",
    "text": "Q3: What to do with the fork’s main when the original main is updated?\nFetch upstream with GitHub GUI or use the same solution given in Q2."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "href": "posts/2022-04-06-github_faqs.html#q4-why-and-when-keeping-the-forks-main-up-to-date-with-the-original-main-is-important",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q4: Why and when keeping the fork’s main up to date with the original main is important?",
    "text": "Q4: Why and when keeping the fork’s main up to date with the original main is important?\nWhenever we need to create new branches (usually from the fork’s main)."
  },
  {
    "objectID": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "href": "posts/2022-04-06-github_faqs.html#q5-how-to-update-a-change-in-a-pr-that-is-open",
    "title": "GitHub Contrubuting FAQs",
    "section": "Q5: How to update a change in a PR that is open?",
    "text": "Q5: How to update a change in a PR that is open?\nPush the change to the corresponding branch and PR will get updated automatically."
  },
  {
    "objectID": "posts/torch-tips.html",
    "href": "posts/torch-tips.html",
    "title": "PyTorch Tips",
    "section": "",
    "text": "Several tips for building torch models from scratch from my experience. Some of the tips are like zen, they are not immediately intuitive but useful for efficient code.\n\nAll the initializations or new tensor creation should only happen in __init__ method. During the forward() call, ideally no new tensors should be created from scratch such as torch.zeros(), torch.ones() etc. Reason: Violating this can sometimes brake your forward pass and end-to-end backprop may become buggy.\n.cuda() and .cpu() are discouraged, use .to(device) instead. Reason: .to(device) is more dynamic and scalable.\nDo not save models with torch.save(model), that may become incompaitable with different torch versions and may take more memory. Save torch.save(model.state_dict()) instead.\nNeed to set parameter names dynamically? Use this example, zero=0;self.register_parameter(f\"name_{zero}\"). They can be accessed with model.name_0.\nHave something in model which is necessary for forward pass but does not require backprop? define those variables with self.register_buffer.\nLet .to(device) to be set outside the model defition. Reason: It is less confusing to the users this way and it is less messy with internal tools to set device such as:\n\nmodule.to(deivce) sends all parameters and buffers of model/submodules to the device.\n\nmodule.float() or module.double() will convert all model/submodule parameters and buffers into float32 and float64 respectively.\nLet .train() and .eval() to be set outside the model defition or set by user. Reason: It can be confusing to user if these things are used inside the model against torch conventions.\ntorch.no_grad() should not be used within the model. Reason: Sometimes user may want to backprop through that chunk of code.\nLink the multiple modules togather. Reason: Ideally, it is useful if model is built like a assembled product (say a car). You should be able to replace the parts as per your requirement. Several benefits on these lines are:\n\nsetting module.train() or module.eval() puts all submodules in train mode or eval mode respectively.\nAll submodules parameters can be accesses directly from the parent module with module.parameters().\n\nCreating a list of parameters in model __init__ definition? consider torch.nn.ModuleList(params) else individual parameters in the list will not be recognized as parameters."
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html",
    "href": "posts/2022-03-08-torch-essentials.html",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "href": "posts/2022-03-08-torch-essentials.html#lets-go-hands-on",
    "title": "Torch essentials",
    "section": "",
    "text": "import torch\nimport numpy as np\n\n\ntensor1 = torch.tensor([1,2,3.], dtype=torch.float32)\ntensor2 = torch.tensor([5,6,7.], dtype=torch.float64)\ndisplay(tensor1, tensor2)\n\ntensor([1., 2., 3.])\n\n\ntensor([5., 6., 7.], dtype=torch.float64)\n\n\n\ndisplay(type(tensor1), type(tensor2))\n\ntorch.Tensor\n\n\ntorch.Tensor\n\n\n\ndisplay(tensor1.dtype, tensor2.dtype)\n\ntorch.float32\n\n\ntorch.float64\n\n\n\nlong_tensor = tensor1.to(torch.int32) # device, dtype, tensor\ndisplay(long_tensor)\n\ntensor([1, 2, 3], dtype=torch.int32)\n\n\n\nlong_tensor.device\n\ndevice(type='cpu')\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlong_tensor_gpu = long_tensor.to(device)\nlong_tensor_gpu\n\ntensor([1, 2, 3], device='cuda:0', dtype=torch.int32)\n\n\n\nlong_tensor_born_on_gpu = torch.zeros(2,10, device=device).to(torch.float64)\nlong_tensor_born_on_gpu\n\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n       dtype=torch.float64)\n\n\n\ninspired_tensor = torch.tensor([1.,2.]).to(long_tensor_born_on_gpu)\ninspired_tensor\n\ntensor([1., 2.], device='cuda:0', dtype=torch.float64)\n\n\n\nnp_array = np.array([1,2,3.])\nnp_array.log()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/tmp/ipykernel_7181/236375699.py in &lt;module&gt;\n      1 np_array = np.array([1,2,3.])\n----&gt; 2 np_array.log()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'log'\n\n\n\n\npt_array = torch.tensor([1,2,3.])\npt_array.log() # sin(), cos(), tan(), exp()\n\ntensor([0.0000, 0.6931, 1.0986])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "href": "posts/2022-03-08-torch-essentials.html#gradient-is-all-you-need",
    "title": "Torch essentials",
    "section": "Gradient is all you need",
    "text": "Gradient is all you need\n\nimport matplotlib.pyplot as plt\n\n\nx = torch.rand(5,1)\ny = 3 * x + 2 + torch.randn_like(x)*0.1\n\nplt.scatter(x, y);\n\n\n\n\n\n\n\n\n\nx_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\nx_plus_ones.shape\n\ntorch.Size([5, 2])\n\n\n\ntheta = torch.zeros(2,1, requires_grad=True)\ntheta\n\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\ntheta.grad\n\n\ntheta.grad_fn\n\n\nlr = 0.1\n\ny_pred = x_plus_ones@theta\nloss = ((y_pred - y)**2).mean()\nloss.backward()\n# y_pred = torch.matmul(x_plus_ones, theta)\n# y_pred = torch.mm(x_plus_ones, theta)\n\n\ntheta.grad # dloss/dtheta\n\ntensor([[-6.3681],\n        [-2.8128]])\n\n\n\ntheta.grad_fn\n\n\ntheta.data -= lr * theta.grad.data\n\n\ntheta\n\ntensor([[0.6368],\n        [0.2813]], requires_grad=True)\n\n\n\ntheta.grad_fn\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)\n\n\n\n\n\n\n\n\n\nfor i in range(10):\n    theta.grad.data.zero_()\n    y_pred = x_plus_ones@theta\n    loss = ((y_pred - y)**2).mean()\n    loss.backward()\n    theta.data -= lr * theta.grad\n\n\nwith torch.no_grad():\n    plt.scatter(x, y)\n    plt.plot(x, x_plus_ones@theta)"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#advanced",
    "href": "posts/2022-03-08-torch-essentials.html#advanced",
    "title": "Torch essentials",
    "section": "Advanced",
    "text": "Advanced\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.theta = torch.nn.Parameter(torch.zeros(2,1))\n#         self.register_parameter(theta, torch.zeros(2,1))\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = x_plus_ones@self.theta\n        return y_pred\n\n\nmodel = LinearRegression()\nmodel\n\nLinearRegression()\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value)\n\ntheta Parameter containing:\ntensor([[0.],\n        [0.]], requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    \n    optimizer.step()\n\n\nmodel.state_dict()\n\nOrderedDict([('theta',\n              tensor([[0.9799],\n                      [0.9808]]))])"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "href": "posts/2022-03-08-torch-essentials.html#wanna-run-on-gpu",
    "title": "Torch essentials",
    "section": "Wanna run on GPU?",
    "text": "Wanna run on GPU?\n\nx_gpu = x.to(device)\ny_gpu = y.to(device)\n\n\nprint(model.theta)\nmodel.to(device)\nprint(model.theta)\n\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], requires_grad=True)\nParameter containing:\ntensor([[0.9799],\n        [0.9808]], device='cuda:0', requires_grad=True)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fn = torch.nn.MSELoss() # torch.nn.CrossEntropyLoss()\n\nfor i in range(10):\n    optimizer.zero_grad()\n    \n    y_pred = model(x_gpu)\n    loss = loss_fn(y_pred, y_gpu)\n    loss.backward()\n    \n    optimizer.step()"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "href": "posts/2022-03-08-torch-essentials.html#state-dictionary",
    "title": "Torch essentials",
    "section": "State dictionary",
    "text": "State dictionary\n\n# torch.save(model.state_dict(), path)\n# model.load_state_dict(torch.load(path))"
  },
  {
    "objectID": "posts/2022-03-08-torch-essentials.html#nn-way",
    "href": "posts/2022-03-08-torch-essentials.html#nn-way",
    "title": "Torch essentials",
    "section": "NN way",
    "text": "NN way\n\nclass LinearRegression(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(2, 1) # torch.nn.Linear(128, 64)\n        # What else? \n#         self.activation = torch.nn.ReLU()\n#         torch.nn.LSTM()\n#         torch.nn.Conv2d()\n    \n    def forward(self, x): # Don't call directly. it is called by __call__ method\n        x_plus_ones = torch.cat([torch.ones_like(x), x], dim=1)\n        y_pred = self.layer(x_plus_ones)\n        return y_pred"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html",
    "href": "posts/2024-12-10-cpcb-download.html",
    "title": "Download CPCB live data",
    "section": "",
    "text": "import os\nimport re\nfrom glob import glob\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import Select, WebDriverWait\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.chrome.options import Options\nfrom time import sleep\n\nHOME_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\"\nDOWNLOAD_OLD_DATA_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/caaqm-data-repository\"\nDOWNLOAD_PAGE_URL = \"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing/data\"\ndef click_it(driver, element):\n    driver.execute_script(\"arguments[0].click();\", element)\n    \ndef find_it(element, option):\n    return element.find_element(By.XPATH, f\"//li[contains(text(), '{option}')]\")\n\ndef select_dropdown_option(driver, element, option):\n    element.click()\n    option = find_it(element, option)\n    click_it(driver, option)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "href": "posts/2024-12-10-cpcb-download.html#dry-run-to-get-metadata",
    "title": "Download CPCB live data",
    "section": "Dry run to get metadata",
    "text": "Dry run to get metadata\n\n# headless chrome\noptions = Options()\noptions.add_argument(\"--headless\")\n\n# open the browser\ndriver = webdriver.Chrome(options=options)\n\n# open the website\ndriver.get(DOWNLOAD_OLD_DATA_URL)\n\n# wait for the page to load and the dropdowns to appear\ndropdowns = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".select-box\")))\nlen(dropdowns)\n\n5\n\n\n\ndrop_data_type, drop_frequency, drop_states, drop_cities, drop_stations = dropdowns\n\n\n# Select data type\nselect_dropdown_option(driver, drop_data_type, \"Raw data\")\n\n# Select frequency\nselect_dropdown_option(driver, drop_frequency, \"1 day\")\n\n# Get the states\ndrop_states.click() # Open the dropdown\nstates = drop_states.text.replace(\"▲\\n\", \"\").split(\"\\n\")\nprint(\"Number of states:\", len(states))\ndrop_states.click() # Close the dropdown\n\nNumber of states: 31\n\n\n\nmetadata_df = pd.DataFrame(columns=[\"State\", \"City\", \"Station\", \"site_id\"])\n\n# This loop took less than a minute to run\nprogress_bar = tqdm(total=600) # as of 2024, 560 stations. update this number if it changes\nfor state in states:\n    select_dropdown_option(driver, drop_states, state)\n    \n    # Get all cities\n    drop_cities.click() # Open the dropdown\n    cities = drop_cities.text.replace(\"▲\\n\", \"\").split(\"\\n\")\n    drop_cities.click() # Close the dropdown\n    \n    for city in cities:\n        select_dropdown_option(driver, drop_cities, city)\n        \n        # Get all stations\n        drop_stations.click() # Open the dropdown\n        stations = drop_stations.text.replace(\"▲\\n\", \"\").split(\"\\n\")\n        drop_stations.click() # Close the dropdown\n        \n        for station in stations:\n            # corner cases\n            if station == \"Municipal Corporation Office, Dharuhera - HSPCB\":\n                site_id = \"site_5044\"\n            elif station == \"Civil Lines, Ajmer - RSPCB\":\n                site_id = \"site_1392\"\n            else:\n                try:\n                    select_dropdown_option(driver, drop_stations, station)\n                except:\n                    print(\"Unable to select station\")\n                    print(station)\n                    print(drop_stations.text)\n                    continue\n                site_id = drop_stations.get_attribute(\"ng-reflect-model\")\n            metadata_df.loc[len(metadata_df)] = [state, city, station, site_id]\n            progress_bar.update(1)\n\n\n\n\n\nlen(metadata_df)\n\n560\n\n\n\nmetadata_df.head()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n2\nAndhra Pradesh\nChittoor\nGangineni Cheruvu, Chittoor - APPCB\nsite_5665\n\n\n3\nAndhra Pradesh\nKadapa\nYerramukkapalli, Kadapa - APPCB\nsite_5693\n\n\n4\nAndhra Pradesh\nRajamahendravaram\nAnand Kala Kshetram, Rajamahendravaram - APPCB\nsite_1399\n\n\n\n\n\n\n\n\nmetadata_df.tail()\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n555\nWest Bengal\nKolkata\nRabindra Bharati University, Kolkata - WBPCB\nsite_296\n\n\n556\nWest Bengal\nKolkata\nFort William, Kolkata - WBPCB\nsite_5110\n\n\n557\nWest Bengal\nKolkata\nVictoria, Kolkata - WBPCB\nsite_309\n\n\n558\nWest Bengal\nKolkata\nBidhannagar, Kolkata - WBPCB\nsite_5129\n\n\n559\nWest Bengal\nSiliguri\nWard-32 Bapupara, Siliguri - WBPCB\nsite_1419\n\n\n\n\n\n\n\n\nfor site_id, more_than_1 in (metadata_df.site_id.value_counts() &gt; 1).items():\n    if more_than_1:\n        print(metadata_df[metadata_df.site_id == site_id])\n\n           State        City                               Station    site_id\n25         Bihar  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n254  Maharashtra  Aurangabad  MIDC Chilkalthana, Aurangabad - MPCB  site_5788\n           State        City                              Station   site_id\n26         Bihar  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n255  Maharashtra  Aurangabad  More Chowk Waluj, Aurangabad - MPCB  site_198\n             State           City                                    Station  \\\n499  Uttar Pradesh  Greater Noida  Knowledge Park - V, Greater Noida - UPPCB   \n526  Uttar Pradesh          Noida  Knowledge Park - V, Greater Noida - UPPCB   \n\n       site_id  \n499  site_5121  \n526  site_5121  \n             State           City  \\\n498  Uttar Pradesh  Greater Noida   \n525  Uttar Pradesh          Noida   \n\n                                         Station    site_id  \n498  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n525  Knowledge Park - III, Greater Noida - UPPCB  site_1541  \n           State        City                              Station    site_id\n28         Bihar  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n257  Maharashtra  Aurangabad  Rachnakar Colony, Aurangabad - MPCB  site_5789\n           State        City                           Station    site_id\n27         Bihar  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n256  Maharashtra  Aurangabad  Gurdeo Nagar, Aurangabad - BSPCB  site_5544\n\n\n\n# clean up\ndrop_items = [metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - III, Greater Noida - UPPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"More Chowk Waluj, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"MIDC Chilkalthana, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Maharashtra\") & (metadata_df.Station == \"Gurdeo Nagar, Aurangabad - BSPCB\")].index.item(),\n              metadata_df[(metadata_df.State == \"Bihar\") & (metadata_df.Station == \"Rachnakar Colony, Aurangabad - MPCB\")].index.item(),\n              metadata_df[(metadata_df.City == \"Noida\") & (metadata_df.Station == \"Knowledge Park - V, Greater Noida - UPPCB\")].index.item()]\n\nmetadata_df.drop(drop_items, inplace=True)\nlen(metadata_df)\n\n554\n\n\n\nassert set(metadata_df.site_id.value_counts()) == {1}\n\n\nmetadata_df.to_csv(\"metadata.csv\", index=False)"
  },
  {
    "objectID": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "href": "posts/2024-12-10-cpcb-download.html#downloading-data",
    "title": "Download CPCB live data",
    "section": "Downloading data",
    "text": "Downloading data\n\n# URL is specific to PM2.5 and PM10 so update it as per your needs\ndef get_url(state, city, site_id):\n    return f\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-view-data-report/%2522%257B%255C%2522parameter_list%255C%2522%253A%255B%257B%255C%2522id%255C%2522%253A0%252C%255C%2522itemName%255C%2522%253A%255C%2522PM2.5%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_193%255C%2522%257D%252C%257B%255C%2522id%255C%2522%253A1%252C%255C%2522itemName%255C%2522%253A%255C%2522PM10%255C%2522%252C%255C%2522itemValue%255C%2522%253A%255C%2522parameter_215%255C%2522%257D%255D%252C%255C%2522criteria%255C%2522%253A%255C%252224%2520Hours%255C%2522%252C%255C%2522reportFormat%255C%2522%253A%255C%2522Tabular%255C%2522%252C%255C%2522fromDate%255C%2522%253A%255C%252201-01-2024%2520T00%253A00%253A00Z%255C%2522%252C%255C%2522toDate%255C%2522%253A%255C%252211-12-2024%2520T16%253A45%253A59Z%255C%2522%252C%255C%2522state%255C%2522%253A%255C%2522{state.replace(' ', '%2520')}%255C%2522%252C%255C%2522city%255C%2522%253A%255C%2522{city.replace(' ', '%2520')}%255C%2522%252C%255C%2522station%255C%2522%253A%255C%2522{site_id}%255C%2522%252C%255C%2522parameter%255C%2522%253A%255B%255C%2522parameter_193%255C%2522%252C%255C%2522parameter_215%255C%2522%255D%252C%255C%2522parameterNames%255C%2522%253A%255B%255C%2522PM2.5%255C%2522%252C%255C%2522PM10%255C%2522%255D%257D%2522\"\n\n\n# add download directory\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\"prefs\", {\n    \"download.default_directory\": \"/Users/project561/cpcb_downloads\"\n})\n\ndriver = webdriver.Chrome(options=options)\ndriver.get(HOME_URL)\n\nEnter Captcha manually before moving ahead\n\nmetadata_df = pd.read_csv(\"metadata.csv\")\nmetadata_df.head(2)\n\n\n\n\n\n\n\n\nState\nCity\nStation\nsite_id\n\n\n\n\n0\nAndhra Pradesh\nAmaravati\nSecretariat, Amaravati - APPCB\nsite_1406\n\n\n1\nAndhra Pradesh\nAnantapur\nGulzarpet, Anantapur - APPCB\nsite_5632\n\n\n\n\n\n\n\n\nfiles = glob(\"/Users/project561/cpcb_downloads/*.xlsx\")\nprint(\"Number of files in the download directory:\", len(files))\nsite_ids = [re.search(r\"site_\\d+?2024\", file).group()[:-4] for file in files]\n# assert len(set(site_ids)) == len(site_ids), pd.Series(site_ids).value_counts()\nsite_ids = set(site_ids)\n\nfor i in range(len(metadata_df)):\n    state, city, station, site_id = metadata_df.iloc[i]\n    if site_id in site_ids:\n        # print(\"Already downloaded\", i, state, city, station, site_id)\n        continue\n    print(\"Downloading\", i, state, city, station, site_id)\n    url = get_url(state, city, site_id)\n    \n    # open new tab\n    driver.execute_script(\"window.open('');\")\n    driver.switch_to.window(driver.window_handles[-1])\n    driver.get(url)\n    excel_button = WebDriverWait(driver, 20).until(\n    EC.element_to_be_clickable((By.CLASS_NAME, \"fa-file-excel-o\")))\n    click_it(driver, excel_button)\n    sleep(1)\n    \n    if len(driver.window_handles) &gt; 10:\n        # close first 9 windows\n        for _ in range(9):\n            driver.switch_to.window(driver.window_handles[0])\n            driver.close()\n            \n        driver.switch_to.window(driver.window_handles[-1])\n        sleep(1)\n\nNumber of files in the download directory: 302\nDownloading 301 Maharashtra Nagpur Ram Nagar, Nagpur - MPCB site_5793\nDownloading 302 Maharashtra Nagpur Mahal, Nagpur - MPCB site_5796\nDownloading 303 Maharashtra Nagpur Opp GPO Civil Lines, Nagpur - MPCB site_303\nDownloading 304 Maharashtra Nagpur Ambazari, Nagpur - MPCB site_5792\nDownloading 305 Maharashtra Nanded Sneh Nagar, Nanded - MPCB site_5795\nDownloading 306 Maharashtra Nashik Pandav Nagari, Nashik - MPCB site_5779\nDownloading 307 Maharashtra Nashik MIDC Ambad, Nashik - MPCB site_5781\nDownloading 308 Maharashtra Nashik Gangapur Road, Nashik - MPCB site_304\nDownloading 309 Maharashtra Nashik Hirawadi, Nashik - MPCB site_5782\nDownloading 310 Maharashtra Navi Mumbai Tondare-Taloja, Navi Mumbai - MPCB site_5803\nDownloading 311 Maharashtra Navi Mumbai Sanpada, Navi Mumbai - MPCB site_5815\nDownloading 312 Maharashtra Navi Mumbai Airoli, Navi Mumbai - MPCB site_261\nDownloading 313 Maharashtra Navi Mumbai Mahape, Navi Mumbai - MPCB site_5114\nDownloading 314 Maharashtra Navi Mumbai Kopripada-Vashi, Navi Mumbai - MPCB site_5805\nDownloading 315 Maharashtra Navi Mumbai Sector-19A Nerul, Navi Mumbai - IITM site_5401\nDownloading 316 Maharashtra Navi Mumbai Nerul, Navi Mumbai - MPCB site_5103\nDownloading 317 Maharashtra Navi Mumbai Sector-2E Kalamboli, Navi Mumbai - MPCB site_5799\nDownloading 318 Maharashtra Parbhani Masoom Colony, Parbhani - MPCB site_5794\nDownloading 319 Maharashtra Pimpri-Chinchwad Park Street Wakad, Pimpri Chinchwad - MPCB site_5764\nDownloading 320 Maharashtra Pimpri-Chinchwad Savta Mali Nagar, Pimpri-Chinchwad - IITM site_5998\nDownloading 321 Maharashtra Pimpri-Chinchwad Thergaon, Pimpri Chinchwad - MPCB site_5765\nDownloading 322 Maharashtra Pimpri-Chinchwad Gavalinagar, Pimpri Chinchwad - MPCB site_5763\nDownloading 323 Maharashtra Pune Revenue Colony-Shivajinagar, Pune - IITM site_5409\nDownloading 324 Maharashtra Pune Mhada Colony, Pune - IITM site_5404\nDownloading 325 Maharashtra Pune Savitribai Phule Pune University, Pune - MPCB site_5767\nDownloading 326 Maharashtra Pune Bhumkar Nagar, Pune - IITM site_5988\nDownloading 327 Maharashtra Pune Hadapsar, Pune - IITM site_5407\nDownloading 328 Maharashtra Pune Karve Road, Pune - MPCB site_292\nDownloading 329 Maharashtra Pune Alandi, Pune - IITM site_5405"
  },
  {
    "objectID": "posts/py_over_ipynb.html",
    "href": "posts/py_over_ipynb.html",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "",
    "text": "I have shifted from .ipynb files to .py files (and Jupyter to VS code) in the last couple of months. Here are some reasons why I feel .py files are better than .ipynb files:"
  },
  {
    "objectID": "posts/py_over_ipynb.html#fewer-errors",
    "href": "posts/py_over_ipynb.html#fewer-errors",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Fewer Errors",
    "text": "Fewer Errors\n\n.py files are easier to debug with a VS code like IDE, making it easier to find the errors.\nExecution of .py starts fresh, unlike some left out variables silently getting carried over from the last execution/deleted cells in .ipynb files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "href": "posts/py_over_ipynb.html#better-usage-of-a-shared-server",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Better Usage of a Shared Server",
    "text": "Better Usage of a Shared Server\n\n.py files release the resources (e.g., GPU memory) once executed. It could be inconvenient to repeatedly remind or be reminded by someone to release the resources manually from a Jupyter notebook."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-productivity",
    "href": "posts/py_over_ipynb.html#increased-productivity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Productivity",
    "text": "Increased Productivity\n\nYou can make use of fantastic auto-complete, syntax-highlighting extensions in VS code to save a lot of time while working with .py files."
  },
  {
    "objectID": "posts/py_over_ipynb.html#boost-collaboration",
    "href": "posts/py_over_ipynb.html#boost-collaboration",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Boost Collaboration",
    "text": "Boost Collaboration\n\n.py do not take time to render on GitHub because they are just plain text files, unlike .ipynb files.\nIt is a lot easier to see the changes made by others in a .py file than a .ipynb file."
  },
  {
    "objectID": "posts/py_over_ipynb.html#increased-modularity",
    "href": "posts/py_over_ipynb.html#increased-modularity",
    "title": "Why .py files are better than .ipynb files for ML codebase",
    "section": "Increased Modularity",
    "text": "Increased Modularity\n\nFunction and Class calls from other files are seamless with .py files.\n\nFeel free to comment your views/suggestions/additions in the comment box."
  },
  {
    "objectID": "posts/2022-10-27-mogp.html",
    "href": "posts/2022-10-27-mogp.html",
    "title": "Multi-Output Gaussian Processes",
    "section": "",
    "text": "Inspired from this GPSS video.\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\n\nimport optax\n\nimport matplotlib.pyplot as plt\nfrom tinygp import kernels"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#helper-functions",
    "href": "posts/2022-10-27-mogp.html#helper-functions",
    "title": "Multi-Output Gaussian Processes",
    "section": "Helper functions",
    "text": "Helper functions\n\ndef random_fill(key, params):\n    values, unravel_fn = ravel_pytree(params)\n    random_values = jax.random.normal(key, shape=values.shape)\n    return unravel_fn(random_values)\n\ndef get_real_params(params):\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = params[f'a{i}'].reshape(n_outputs, rank)\n    if method == 'icm':\n        params['var'] = jnp.exp(params['log_var'])\n        params['scale'] = jnp.exp(params['log_scale'])\n        params['noise'] = jnp.exp(params['log_noise'])\n    elif method == 'lmc':\n        for i in range(1, q_len+1):\n            params[f'var{i}'] = jnp.exp(params[f'log_var{i}'])\n            params[f'scale{i}'] = jnp.exp(params[f'log_scale{i}'])\n            params[f'noise{i}'] = jnp.exp(params[f'log_noise{i}'])\n    return params\n\ndef kron_cov_fn(params, x1, x2, add_noise=False):\n    params = get_real_params(params)\n    a_list = [params[f'a{i}'] for i in range(1, q_len+1)]\n\n    if method == 'icm':\n        kernel_fn = params['var'] * kernels.ExpSquared(scale=params['scale'])\n        cov = kernel_fn(x1, x2)\n        if add_noise:\n            cov = cov + jnp.eye(cov.shape[0])*params['noise']\n\n        B = jax.tree_util.tree_reduce(lambda x1, x2: x1@x1.T+x2@x2.T, a_list)\n#         print(B.shape, cov.shape)\n        return jnp.kron(B, cov)\n\n    elif method == 'lmc':\n        cov_list = []\n        for idx in range(1, q_len+1):\n            kernel_fn = params[f'var{idx}'] * kernels.ExpSquared(scale=params[f'scale{idx}'])\n            cov = kernel_fn(x1, x2)\n            if add_noise:\n                cov = cov + jnp.eye(cov.shape[0])*params[f'noise{idx}']\n\n            B = a_list[idx-1]@a_list[idx-1].T\n            cov_list.append(jnp.kron(B, cov))\n            \n        return jax.tree_util.tree_reduce(lambda x1, x2: x1+x2, cov_list)"
  },
  {
    "objectID": "posts/2022-10-27-mogp.html#configuration",
    "href": "posts/2022-10-27-mogp.html#configuration",
    "title": "Multi-Output Gaussian Processes",
    "section": "Configuration",
    "text": "Configuration\n\nq_len = 2\nrank = 2 # if 1, slfm\nn_outputs = 2\n\nmethod = 'lmc' # lmc, icm\n\nif rank = 1, lmc becomes slfm.\n\nGenerative process\n\nx_key = jax.random.PRNGKey(4)\n\nx = jax.random.uniform(x_key, shape=(40, 1)).sort(axis=0)\nx_test = jnp.linspace(0,1,100).reshape(-1, 1)\n\ne1_key, e2_key = jax.random.split(x_key)\n\ne1 = jax.random.normal(e1_key, shape=(x.shape[0],))\ne2 = jax.random.normal(e2_key, shape=(x.shape[0],))\n\nif method == 'icm':\n    noise = 0.01\n    gen_kernel = 1.2*kernels.ExpSquared(scale=0.2)\n    gen_covariance = gen_kernel(x, x) + jnp.eye(x.shape[0])*noise\n    gen_chol = jnp.linalg.cholesky(gen_covariance)\n    \n    y1 = gen_chol@e1\n    y2 = gen_chol@e2\n\n    y = jnp.concatenate([y1, y2])\n    \nelif method == 'lmc':\n    noise1 = 0.01\n    noise2 = 0.1\n    gen_kernel1 = 1.2*kernels.ExpSquared(scale=0.1)\n    gen_covariance1 = gen_kernel1(x, x) + jnp.eye(x.shape[0])*noise1\n    gen_chol1 = jnp.linalg.cholesky(gen_covariance1)\n\n    gen_kernel2 = 0.8*kernels.ExpSquared(scale=0.2)\n    gen_covariance2 = gen_kernel2(x, x) + jnp.eye(x.shape[0])*noise2\n    gen_chol2 = jnp.linalg.cholesky(gen_covariance2)\n    \n    y1 = gen_chol1@e1\n    y2 = gen_chol2@e2\n\n    y = jnp.concatenate([y1, y2])\n    \n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.legend();\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\n\n\n\n\n\n\ndef loss_fn(params):\n    mo_cov = kron_cov_fn(params, x, x, add_noise=True)\n#     print(y.shape, mo_cov.shape)\n    return -jax.scipy.stats.multivariate_normal.logpdf(y, jnp.zeros_like(y), mo_cov)\n\n\nkey = jax.random.PRNGKey(1)\nif method == 'icm':\n    params = {'log_var':0.0, 'log_scale':0.0, 'log_noise':0.0}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\nelif method == 'lmc':\n    params = {}\n    for i in range(1, q_len+1):\n        params[f'a{i}'] = jnp.zeros((n_outputs, rank))\n        params[f'log_var{i}'] = 0.0\n        params[f'log_scale{i}'] = 0.0\n        params[f'log_noise{i}'] = 0.0\n\nparams = random_fill(key, params)\nparams\n\n{'a1': DeviceArray([[-0.764527 ,  1.0286916],\n              [-1.0690447, -0.7921495]], dtype=float32),\n 'a2': DeviceArray([[ 0.8845895, -1.1941622],\n              [-1.7434924,  1.5159688]], dtype=float32),\n 'log_noise1': DeviceArray(-1.1254696, dtype=float32),\n 'log_noise2': DeviceArray(-0.22446911, dtype=float32),\n 'log_scale1': DeviceArray(0.39719132, dtype=float32),\n 'log_scale2': DeviceArray(-0.22453257, dtype=float32),\n 'log_var1': DeviceArray(-0.7590596, dtype=float32),\n 'log_var2': DeviceArray(-0.08601531, dtype=float32)}\n\n\n\nloss_fn(params)\n\nDeviceArray(116.04026, dtype=float32)\n\n\n\nkey = jax.random.PRNGKey(3)\nparams = random_fill(key, params)\n\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), (params, loss)\n\n(tuned_params, state), (params_history, loss_history) = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(loss_history);\n\n\n\n\n\n\n\n\n\ndef predict_fn(params, x_test):\n    cov = kron_cov_fn(params, x, x, add_noise=True)\n    test_cov = kron_cov_fn(params, x_test, x_test, add_noise=True)\n    cross_cov = kron_cov_fn(params, x_test, x, add_noise=False)\n    \n    chol = jnp.linalg.cholesky(cov)\n    k_inv_y = jax.scipy.linalg.cho_solve((chol, True), y)\n    k_inv_cross_cov = jax.scipy.linalg.cho_solve((chol, True), cross_cov.T)\n\n    pred_mean = cross_cov@k_inv_y\n    pred_cov = test_cov - cross_cov@k_inv_cross_cov\n    return pred_mean, pred_cov\n\n\npred_mean, pred_cov = predict_fn(tuned_params, x_test)\npred_conf = 2 * jnp.diag(pred_cov)**0.5\n\nplt.scatter(x, y1, label='y1')\nplt.scatter(x, y2, label='y2')\nplt.plot(x_test, pred_mean[:x_test.shape[0]], label='pred_y1')\nplt.plot(x_test, pred_mean[x_test.shape[0]:], label='pred_y2')\nplt.fill_between(x_test.ravel(), pred_mean[:x_test.shape[0]] - pred_conf[:x_test.shape[0]], pred_mean[:x_test.shape[0]] + pred_conf[:x_test.shape[0]], label='pred_conf_y1', alpha=0.3)\nplt.fill_between(x_test.ravel(), pred_mean[x_test.shape[0]:] - pred_conf[x_test.shape[0]:], pred_mean[x_test.shape[0]:] + pred_conf[x_test.shape[0]:], label='pred_conf_y2', alpha=0.3)\nplt.legend(bbox_to_anchor=(1,1));\n\n\n\n\n\n\n\n\n\nfor name, value in get_real_params(tuned_params).items():\n    if not name.startswith('log_'):\n        print(name, value)\n\na1 [[0.03664799 0.00039898]\n [0.3191718  0.00344488]]\na2 [[ 0.1351072   0.00248941]\n [-0.05392759 -0.04239884]]\nnoise1 0.6797133\nnoise2 0.4154678\nscale1 5.048228\nscale2 0.10743636\nvar1 0.016275918\nvar2 41.034225"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html",
    "title": "Uncertainty in Deep Learning",
    "section": "",
    "text": "import torch"
  },
  {
    "objectID": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "href": "posts/2022-03-05-uncertainty-in-deep-learning.html#introduction",
    "title": "Uncertainty in Deep Learning",
    "section": "1 - Introduction",
    "text": "1 - Introduction\n\nAn online deep learning book from Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\n\n\n1.1 - Deep Learning\nWe define a single layer network as the following:\n\nclass SingleLayerNetwork(torch.nn.Module):\n    def __init__(self, Q, D, K):\n        \"\"\"\n        Q: number of features\n        D: number of outputs\n        K: number of hidden features\n        \"\"\"\n        super().__init__()\n        self.input = torch.nn.Linear(Q, K) # Transforms Q features into K hidden features\n        self.output = torch.nn.Linear(K, D) # Transforms K hidden features to D output features\n        self.non_lin_transform = torch.nn.ReLU() # A non-linear transformation\n        \n    def forward(self, X):\n        \"\"\"\n        X: input (N x Q)\n        \"\"\"\n        self.linear_transformed_X = self.input(X)  # (N, Q) -&gt; (N, K)\n        self.non_lin_transformed_X = self.non_lin_transform(linear_transformed_X)  # (N, K) -&gt; (N, K)\n        output = self.output(self.non_lin_transformed_X)  # (N, K) -&gt; (N, D)\n        return output\n\n\nQ = 10 # Number of features\nN = 100 # Number of samples\nD = 15 # Number of outputs\nK = 32 # Number of hidden features\n\nX = torch.rand(N, Q) # Input\nY = torch.rand(N, D) # Output\n\n\nmodel = SingleLayerNetwork(Q=Q, D=D, K=K)\nmodel\n\nSingleLayerNetwork(\n  (input): Linear(in_features=10, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=15, bias=True)\n  (non_lin_transform): ReLU()\n)\n\n\n\nfor name, value in model.named_parameters():\n    print(name, value.shape)\n\ninput.weight torch.Size([32, 10])\ninput.bias torch.Size([32])\noutput.weight torch.Size([15, 32])\noutput.bias torch.Size([15])\n\n\nReLU is does not contain any parameters here so it is merely a function.\n\n\n1.2 Model Uncertainty\nIn which cases we want our model to be uncertain?\n\nWhen it encounters a out-of-the-distribution data\nWhen training data is noisy (irreducible/aleatoric uncertainty)\nWhen we have multiple predictors (model/epistemic uncertainty)"
  },
  {
    "objectID": "posts/seq_to_seq.html",
    "href": "posts/seq_to_seq.html",
    "title": "Seq to Seq",
    "section": "",
    "text": "Model\nMain Disadvantage\nSolved by\nHow?\n\n\n\n\nNN\nCan’t handle dynamic length input\nRNN\nRNN can handle dynamic length input\n\n\nRNN\nVanishing Gradient Problem\nLSTM\nLSTM can handle vanishing gradient problem\n\n\nLSTM\nNon parallelizable\nTransformer\nTransformer can parallelize the computation\n\n\nTrasformer\nlosses sequentiality\nTransformer\nPositional Encoding"
  },
  {
    "objectID": "posts/wrf-tutorial.html",
    "href": "posts/wrf-tutorial.html",
    "title": "WRF Tutorial",
    "section": "",
    "text": "Acronym\nFull Form\n\n\n\n\nWRF\nWeather Research and Forecasting model\n\n\nWPS\nWRF Preprocessing System\n\n\nWRF-ARW\nAdvanced Research WRF\n\n\nWRF-Hydro\nWRF Hydrological model\n\n\nWRF-Chem\nWRF Chemical model\n\n\nWRFDA\nWRF Data Assimilation"
  },
  {
    "objectID": "posts/wrf-tutorial.html#what-is-what",
    "href": "posts/wrf-tutorial.html#what-is-what",
    "title": "WRF Tutorial",
    "section": "",
    "text": "Acronym\nFull Form\n\n\n\n\nWRF\nWeather Research and Forecasting model\n\n\nWPS\nWRF Preprocessing System\n\n\nWRF-ARW\nAdvanced Research WRF\n\n\nWRF-Hydro\nWRF Hydrological model\n\n\nWRF-Chem\nWRF Chemical model\n\n\nWRFDA\nWRF Data Assimilation"
  },
  {
    "objectID": "posts/wrf-tutorial.html#explanation-in-simple-words",
    "href": "posts/wrf-tutorial.html#explanation-in-simple-words",
    "title": "WRF Tutorial",
    "section": "Explanation in simple words",
    "text": "Explanation in simple words\nWill add."
  },
  {
    "objectID": "posts/wrf-tutorial.html#system-information",
    "href": "posts/wrf-tutorial.html#system-information",
    "title": "WRF Tutorial",
    "section": "System information",
    "text": "System information\n\n!lsb_release -a\n\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 20.04.6 LTS\nRelease:    20.04\nCodename:   focal\n\n\n\n# processor info\n!cat /proc/cpuinfo\n\nprocessor   : 0\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 2001.451\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 0\ncpu cores   : 32\napicid      : 0\ninitial apicid  : 0\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 1\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 1\ncpu cores   : 32\napicid      : 1\ninitial apicid  : 1\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 2\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 2\ncpu cores   : 32\napicid      : 2\ninitial apicid  : 2\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 3\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 3\ncpu cores   : 32\napicid      : 3\ninitial apicid  : 3\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 4\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 4\ncpu cores   : 32\napicid      : 4\ninitial apicid  : 4\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 5\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 5\ncpu cores   : 32\napicid      : 5\ninitial apicid  : 5\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 6\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 6\ncpu cores   : 32\napicid      : 6\ninitial apicid  : 6\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 7\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 7\ncpu cores   : 32\napicid      : 7\ninitial apicid  : 7\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 8\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 8\ncpu cores   : 32\napicid      : 8\ninitial apicid  : 8\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 9\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 9\ncpu cores   : 32\napicid      : 9\ninitial apicid  : 9\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 10\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 10\ncpu cores   : 32\napicid      : 10\ninitial apicid  : 10\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 11\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 11\ncpu cores   : 32\napicid      : 11\ninitial apicid  : 11\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 12\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 2000.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 12\ncpu cores   : 32\napicid      : 12\ninitial apicid  : 12\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 13\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 13\ncpu cores   : 32\napicid      : 13\ninitial apicid  : 13\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 14\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 14\ncpu cores   : 32\napicid      : 14\ninitial apicid  : 14\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 15\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 15\ncpu cores   : 32\napicid      : 15\ninitial apicid  : 15\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 16\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 16\ncpu cores   : 32\napicid      : 16\ninitial apicid  : 16\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 17\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 17\ncpu cores   : 32\napicid      : 17\ninitial apicid  : 17\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 18\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 18\ncpu cores   : 32\napicid      : 18\ninitial apicid  : 18\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 19\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 19\ncpu cores   : 32\napicid      : 19\ninitial apicid  : 19\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 20\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 20\ncpu cores   : 32\napicid      : 20\ninitial apicid  : 20\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 21\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 21\ncpu cores   : 32\napicid      : 21\ninitial apicid  : 21\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 22\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 22\ncpu cores   : 32\napicid      : 22\ninitial apicid  : 22\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 23\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 23\ncpu cores   : 32\napicid      : 23\ninitial apicid  : 23\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 24\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 24\ncpu cores   : 32\napicid      : 24\ninitial apicid  : 24\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 25\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 25\ncpu cores   : 32\napicid      : 25\ninitial apicid  : 25\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 26\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 26\ncpu cores   : 32\napicid      : 26\ninitial apicid  : 26\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 27\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 27\ncpu cores   : 32\napicid      : 27\ninitial apicid  : 27\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 28\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 28\ncpu cores   : 32\napicid      : 28\ninitial apicid  : 28\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 29\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 29\ncpu cores   : 32\napicid      : 29\ninitial apicid  : 29\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 30\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 30\ncpu cores   : 32\napicid      : 30\ninitial apicid  : 30\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 31\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 0\nsiblings    : 32\ncore id     : 31\ncpu cores   : 32\napicid      : 31\ninitial apicid  : 31\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4699.82\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 32\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 0\ncpu cores   : 32\napicid      : 64\ninitial apicid  : 64\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 33\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 1\ncpu cores   : 32\napicid      : 65\ninitial apicid  : 65\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 34\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 2\ncpu cores   : 32\napicid      : 66\ninitial apicid  : 66\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 35\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 3\ncpu cores   : 32\napicid      : 67\ninitial apicid  : 67\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 36\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 4\ncpu cores   : 32\napicid      : 68\ninitial apicid  : 68\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 37\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 5\ncpu cores   : 32\napicid      : 69\ninitial apicid  : 69\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 38\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 6\ncpu cores   : 32\napicid      : 70\ninitial apicid  : 70\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 39\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 7\ncpu cores   : 32\napicid      : 71\ninitial apicid  : 71\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 40\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 8\ncpu cores   : 32\napicid      : 72\ninitial apicid  : 72\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 41\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 9\ncpu cores   : 32\napicid      : 73\ninitial apicid  : 73\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 42\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 10\ncpu cores   : 32\napicid      : 74\ninitial apicid  : 74\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 43\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 11\ncpu cores   : 32\napicid      : 75\ninitial apicid  : 75\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 44\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 12\ncpu cores   : 32\napicid      : 76\ninitial apicid  : 76\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 45\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 13\ncpu cores   : 32\napicid      : 77\ninitial apicid  : 77\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 46\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 14\ncpu cores   : 32\napicid      : 78\ninitial apicid  : 78\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 47\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 15\ncpu cores   : 32\napicid      : 79\ninitial apicid  : 79\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 48\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 2900.378\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 16\ncpu cores   : 32\napicid      : 80\ninitial apicid  : 80\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 49\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1549.380\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 17\ncpu cores   : 32\napicid      : 81\ninitial apicid  : 81\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 50\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 18\ncpu cores   : 32\napicid      : 82\ninitial apicid  : 82\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 51\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 19\ncpu cores   : 32\napicid      : 83\ninitial apicid  : 83\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 52\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 20\ncpu cores   : 32\napicid      : 84\ninitial apicid  : 84\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 53\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 21\ncpu cores   : 32\napicid      : 85\ninitial apicid  : 85\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 54\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 22\ncpu cores   : 32\napicid      : 86\ninitial apicid  : 86\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 55\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 23\ncpu cores   : 32\napicid      : 87\ninitial apicid  : 87\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 56\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 24\ncpu cores   : 32\napicid      : 88\ninitial apicid  : 88\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 57\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 25\ncpu cores   : 32\napicid      : 89\ninitial apicid  : 89\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 58\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 26\ncpu cores   : 32\napicid      : 90\ninitial apicid  : 90\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 59\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 2350.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 27\ncpu cores   : 32\napicid      : 91\ninitial apicid  : 91\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 60\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 28\ncpu cores   : 32\napicid      : 92\ninitial apicid  : 92\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 61\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 29\ncpu cores   : 32\napicid      : 93\ninitial apicid  : 93\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 62\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 30\ncpu cores   : 32\napicid      : 94\ninitial apicid  : 94\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]\n\nprocessor   : 63\nvendor_id   : AuthenticAMD\ncpu family  : 23\nmodel       : 49\nmodel name  : AMD EPYC 7452 32-Core Processor\nstepping    : 0\nmicrocode   : 0x830107a\ncpu MHz     : 1500.000\ncache size  : 512 KB\nphysical id : 1\nsiblings    : 32\ncore id     : 31\ncpu cores   : 32\napicid      : 95\ninitial apicid  : 95\nfpu     : yes\nfpu_exception   : yes\ncpuid level : 16\nwp      : yes\nflags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\nbugs        : sysret_ss_attrs spectre_v1 spectre_v2 spec_store_bypass retbleed smt_rsb srso\nbogomips    : 4673.53\nTLB size    : 3072 4K pages\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 43 bits physical, 48 bits virtual\npower management: ts ttp tm hwpstate cpb eff_freq_ro [13] [14]"
  },
  {
    "objectID": "posts/wrf-tutorial.html#wrf-wps-installation",
    "href": "posts/wrf-tutorial.html#wrf-wps-installation",
    "title": "WRF Tutorial",
    "section": "WRF-WPS Installation",
    "text": "WRF-WPS Installation\n\nCreate a new directory for all WRF codes and put everything into it.\nClone the repo recursively (includes WRF, WRFDA & WRF-Chem)\n\ngit clone --recurse-submodules https://github.com/wrf-model/WRF\n\nClone the repo (includes WPS)\n\ngit clone https://github.com/wrf-model/WPS\n\nFollow instructions given on https://www2.mmm.ucar.edu/wrf/OnLineTutorial/compilation_tutorial.php"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html",
    "title": "Get a list of contributors from a repo",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#config",
    "title": "Get a list of contributors from a repo",
    "section": "Config",
    "text": "Config\n\nowner = \"probml\"\nrepo = \"pyprobml\""
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-all-contributors-to-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Get all contributors to a repo",
    "text": "Get all contributors to a repo\n\ncontributors = pd.read_json(f\"https://api.github.com/repos/{owner}/{repo}/contributors?per_page=100\")\ncontributors = contributors.set_index(\"login\")\nprint(f\"Number of contributors: {len(contributors.index.unique())}\")\ncontributors.head(2)\n\nNumber of contributors: 47\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nnode_id\navatar_url\ngravatar_id\nurl\nhtml_url\nfollowers_url\nfollowing_url\ngists_url\nstarred_url\nsubscriptions_url\norganizations_url\nrepos_url\nevents_url\nreceived_events_url\ntype\nsite_admin\ncontributions\n\n\nlogin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmurphyk\n4632336\nMDQ6VXNlcjQ2MzIzMzY=\nhttps://avatars.githubusercontent.com/u/463233...\n\nhttps://api.github.com/users/murphyk\nhttps://github.com/murphyk\nhttps://api.github.com/users/murphyk/followers\nhttps://api.github.com/users/murphyk/following...\nhttps://api.github.com/users/murphyk/gists{/gi...\nhttps://api.github.com/users/murphyk/starred{/...\nhttps://api.github.com/users/murphyk/subscript...\nhttps://api.github.com/users/murphyk/orgs\nhttps://api.github.com/users/murphyk/repos\nhttps://api.github.com/users/murphyk/events{/p...\nhttps://api.github.com/users/murphyk/received_...\nUser\nFalse\n1777\n\n\nNeoanarika\n5188337\nMDQ6VXNlcjUxODgzMzc=\nhttps://avatars.githubusercontent.com/u/518833...\n\nhttps://api.github.com/users/Neoanarika\nhttps://github.com/Neoanarika\nhttps://api.github.com/users/Neoanarika/followers\nhttps://api.github.com/users/Neoanarika/follow...\nhttps://api.github.com/users/Neoanarika/gists{...\nhttps://api.github.com/users/Neoanarika/starre...\nhttps://api.github.com/users/Neoanarika/subscr...\nhttps://api.github.com/users/Neoanarika/orgs\nhttps://api.github.com/users/Neoanarika/repos\nhttps://api.github.com/users/Neoanarika/events...\nhttps://api.github.com/users/Neoanarika/receiv...\nUser\nFalse\n184"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#fetch-all-prs-from-a-repo",
    "title": "Get a list of contributors from a repo",
    "section": "Fetch all PRs from a repo",
    "text": "Fetch all PRs from a repo\n\npage_range = range(1, 6)\nget_pr_df = lambda page: pd.read_json(f\"https://api.github.com/repos/probml/pyprobml/pulls?state=all&per_page=100&page={page}\")\npull_requests = pd.concat(map(get_pr_df, page_range))\nprint(f\"Number of PRs: {len(pull_requests)}\")\npull_requests.head(2)\n\nNumber of PRs: 497\n\n\n\n  \n    \n      \n\n\n\n\n\n\nurl\nid\nnode_id\nhtml_url\ndiff_url\npatch_url\nissue_url\nnumber\nstate\nlocked\n...\nreview_comments_url\nreview_comment_url\ncomments_url\nstatuses_url\nhead\nbase\n_links\nauthor_association\nauto_merge\nactive_lock_reason\n\n\n\n\n0\nhttps://api.github.com/repos/probml/pyprobml/p...\n938329819\nPR_kwDOA-3vB8437cbb\nhttps://github.com/probml/pyprobml/pull/841\nhttps://github.com/probml/pyprobml/pull/841.diff\nhttps://github.com/probml/pyprobml/pull/841.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n841\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:posrprocessing', 'ref': ...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n1\nhttps://api.github.com/repos/probml/pyprobml/p...\n938317389\nPR_kwDOA-3vB8437ZZN\nhttps://github.com/probml/pyprobml/pull/840\nhttps://github.com/probml/pyprobml/pull/840.diff\nhttps://github.com/probml/pyprobml/pull/840.patch\nhttps://api.github.com/repos/probml/pyprobml/i...\n840\nclosed\nFalse\n...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/p...\nhttps://api.github.com/repos/probml/pyprobml/i...\nhttps://api.github.com/repos/probml/pyprobml/s...\n{'label': 'karm-patel:master', 'ref': 'master'...\n{'label': 'probml:master', 'ref': 'master', 's...\n{'self': {'href': 'https://api.github.com/repo...\nCONTRIBUTOR\nNaN\nNaN\n\n\n\n\n2 rows × 36 columns"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#get-a-list-of-contributors-sorted-by-count-of-prs",
    "title": "Get a list of contributors from a repo",
    "section": "Get a list of contributors sorted by count of PRs",
    "text": "Get a list of contributors sorted by count of PRs\n\npull_requests['login'] = pull_requests['user'].apply(lambda x: x[\"login\"])\nsorted_by_pr_count = pull_requests.groupby(\"login\").agg({'url': len}).sort_values(by='url', ascending=False)\nsorted_by_pr_count.rename(columns={'url': 'Number of PRs'}, inplace=True)\nsorted_by_pr_count.head(5)\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of PRs\n\n\nlogin\n\n\n\n\n\nDrishttii\n79\n\n\ngerdm\n55\n\n\nkaralleyna\n43\n\n\nalways-newbie161\n29\n\n\nkarm-patel\n29"
  },
  {
    "objectID": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "href": "posts/2022-05-17-contributors_sorted_by_prs.html#create-a-dashboard",
    "title": "Get a list of contributors from a repo",
    "section": "Create a dashboard",
    "text": "Create a dashboard\n\ndef get_href_user(user):\n  username, profile_link = user.split(\"|\")\n  return f\"[{username}]({profile_link})\"\n\ndashboard = pd.DataFrame(index=sorted_by_pr_count.index)\ndashboard[\"Avatar\"] = contributors.avatar_url.apply(lambda url: f'&lt;img width=\"25\" alt=\"image\" src=\"{url}\"&gt;')\ndashboard[\"Contributor\"] = (contributors.index +\"|\"+ contributors['html_url']).apply(get_href_user)\ndashboard[\"Number of PRs\"] = sorted_by_pr_count[\"Number of PRs\"]\nprint(dashboard.dropna().T.to_markdown())\n\n|               | Drishttii                                                                               | gerdm                                                                                  | karalleyna                                                                              | always-newbie161                                                                        | karm-patel                                                                              | Duane321                                                                                | Nirzu97                                                                                 | patel-zeel                                                                              | animesh-007                                                                             | ashishpapanai                                                                           | shivaditya-meduri                                                                       | Neoanarika                                                                             | andrewnc                                                                               | nappaillav                                                                              | Abdelrahman350                                                                          | mjsML                                                                                  | jdf22                                                                                  | kzymgch                                                                                 | nalzok                                                                                  | nitish1295                                                                              | Garvit9000c                                                                             | AnkitaKumariJain14                                                                      | rohit-khoiwal-30                                                                        | shobro                                                                                  | raymondyeh07                                                                           | khanshehjad                                                                             | alenm10                                                                                 | firatoncel                                                                             | AnandShegde                                                                             | Aadesh-1404                                                                             | nealmcb                                                                               | nipunbatra                                                                           | petercerno                                                                             | posgnu                                                                                  | mvervuurt                                                                              | hieuza                                                                                 | Prahitha                                                                                | TripleTop                                                                               | UmarJ                                                                                   | Vishal987595                                                                            | a-fakhri                                                                                | adamnemecek                                                                           | galv                                                                                   | jlh2018                                                                                 | krasserm                                                                              | yuanx749                                                                                |\n|:--------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------|\n| Avatar        | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/35187749?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4108759?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/36455180?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/66471669?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59387624?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/19956442?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/28842790?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/59758528?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/53366877?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/52123364?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/77324692?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5188337?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7716402?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/43855961?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47902062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/7131192?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1637094?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/10054419?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/13443062?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/21181046?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68856476?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/62535006?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/87682045?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/54628243?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/5696982?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/31896767?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/42214173?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/9141211?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/79975787?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/68186100?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/119472?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/60985?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1649209?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/30136201?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/6399881?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/1021144?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/44160152?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/48208522?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/34779641?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/97757583?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/65111198?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/182415?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/4767568?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/40842099?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/202907?v=4\"&gt; | &lt;img width=\"25\" alt=\"image\" src=\"https://avatars.githubusercontent.com/u/47032563?v=4\"&gt; |\n| Contributor   | [Drishttii](https://github.com/Drishttii)                                               | [gerdm](https://github.com/gerdm)                                                      | [karalleyna](https://github.com/karalleyna)                                             | [always-newbie161](https://github.com/always-newbie161)                                 | [karm-patel](https://github.com/karm-patel)                                             | [Duane321](https://github.com/Duane321)                                                 | [Nirzu97](https://github.com/Nirzu97)                                                   | [patel-zeel](https://github.com/patel-zeel)                                             | [animesh-007](https://github.com/animesh-007)                                           | [ashishpapanai](https://github.com/ashishpapanai)                                       | [shivaditya-meduri](https://github.com/shivaditya-meduri)                               | [Neoanarika](https://github.com/Neoanarika)                                            | [andrewnc](https://github.com/andrewnc)                                                | [nappaillav](https://github.com/nappaillav)                                             | [Abdelrahman350](https://github.com/Abdelrahman350)                                     | [mjsML](https://github.com/mjsML)                                                      | [jdf22](https://github.com/jdf22)                                                      | [kzymgch](https://github.com/kzymgch)                                                   | [nalzok](https://github.com/nalzok)                                                     | [nitish1295](https://github.com/nitish1295)                                             | [Garvit9000c](https://github.com/Garvit9000c)                                           | [AnkitaKumariJain14](https://github.com/AnkitaKumariJain14)                             | [rohit-khoiwal-30](https://github.com/rohit-khoiwal-30)                                 | [shobro](https://github.com/shobro)                                                     | [raymondyeh07](https://github.com/raymondyeh07)                                        | [khanshehjad](https://github.com/khanshehjad)                                           | [alenm10](https://github.com/alenm10)                                                   | [firatoncel](https://github.com/firatoncel)                                            | [AnandShegde](https://github.com/AnandShegde)                                           | [Aadesh-1404](https://github.com/Aadesh-1404)                                           | [nealmcb](https://github.com/nealmcb)                                                 | [nipunbatra](https://github.com/nipunbatra)                                          | [petercerno](https://github.com/petercerno)                                            | [posgnu](https://github.com/posgnu)                                                     | [mvervuurt](https://github.com/mvervuurt)                                              | [hieuza](https://github.com/hieuza)                                                    | [Prahitha](https://github.com/Prahitha)                                                 | [TripleTop](https://github.com/TripleTop)                                               | [UmarJ](https://github.com/UmarJ)                                                       | [Vishal987595](https://github.com/Vishal987595)                                         | [a-fakhri](https://github.com/a-fakhri)                                                 | [adamnemecek](https://github.com/adamnemecek)                                         | [galv](https://github.com/galv)                                                        | [jlh2018](https://github.com/jlh2018)                                                   | [krasserm](https://github.com/krasserm)                                               | [yuanx749](https://github.com/yuanx749)                                                 |\n| Number of PRs | 79                                                                                      | 55                                                                                     | 43                                                                                      | 29                                                                                      | 29                                                                                      | 29                                                                                      | 25                                                                                      | 23                                                                                      | 18                                                                                      | 17                                                                                      | 16                                                                                      | 10                                                                                     | 10                                                                                     | 10                                                                                      | 8                                                                                       | 7                                                                                      | 7                                                                                      | 6                                                                                       | 6                                                                                       | 5                                                                                       | 4                                                                                       | 4                                                                                       | 3                                                                                       | 3                                                                                       | 2                                                                                      | 2                                                                                       | 2                                                                                       | 2                                                                                      | 2                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                    | 1                                                                                      | 1                                                                                       | 1                                                                                      | 1                                                                                      | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                       | 1                                                                                     | 1                                                                                      | 1                                                                                       | 1                                                                                     | 1                                                                                       |"
  },
  {
    "objectID": "posts/2021-10-26-anonymization-tips.html",
    "href": "posts/2021-10-26-anonymization-tips.html",
    "title": "Anonymization tips for double-blind submission",
    "section": "",
    "text": "Use following command locally to search for author names, institute name and other terms you think may violate double-blind\n\ngit grep &lt;query&gt;\n\nAbove command matches the query everywhere and thus a safe way. Avoid GitHub search for this purpose, it fails to identify some terms many times and there is no regex there (yet)!\n\n\nDo not use full paths inside README file. If you move content in other repo, the links will either become unusable or may violate double-blind. So follow the example below.\n\nBad practice: [link](https://github.com/patel-zeel/reponame/blob/master/dataset)\nGood practice: [link](dataset)\n\nPoint no. 2 does not work for GitHub pages links (username.github.io/stuff). Thus, keep in mind to manually update those (if you have a better idea, let everyone know in comments below)\nDownload the repo zip locally and create an anonymized repository in your anonymized GitHub account. Open the GitHub web editor by pressing “.” (dot) at repo homepage.\nNow, you can select and drag all folders to the left pan of the web editor to upload them at once. Finally, commit with a meaningfull message and the changes will automatically be uploaded to the mail branch of your anonymized repo.\nUpdate the link in your manuscipt and submit !!\n\n\nEdit:\nAfter acceptance, transfer the ownership to personal account and delete the ownership of anonymized account from the personal account. This will remove all the traces of repository from the anonymized account. However, repository will still show that the commits were made by anonymized account which is anyway not violation of the doule-blind explicitely."
  },
  {
    "objectID": "posts/Rank1_GPs.html",
    "href": "posts/Rank1_GPs.html",
    "title": "Can Rank 1 GPs represent all GPs?",
    "section": "",
    "text": "from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gpytorch.kernels import RBFKernel, Kernel\n\n\nclass Rank1Kernel(nn.Module):\n    def __init__(self, input_dim, output_dim, n_neurons_per_layer, activation):\n        super().__init__()\n        self.init = nn.Linear(input_dim, n_neurons_per_layer[0])\n        self.n_neurons_per_layer = n_neurons_per_layer\n        self.activation = activation\n        \n        for i in range(1, len(n_neurons_per_layer)):\n            setattr(self, f'fc{i}', nn.Linear(n_neurons_per_layer[i-1], n_neurons_per_layer[i]))\n        \n        self.out = nn.Linear(n_neurons_per_layer[-1], output_dim)\n        \n    def forward(self, x1, x2):\n        def _forward(x):\n            x = self.init(x)\n            for i in range(1, len(self.n_neurons_per_layer)):\n                x = getattr(self, f'fc{i}')(x)\n                x = self.activation(x)\n            return self.out(x)\n        \n        x1 = _forward(x1)\n        x2 = _forward(x2)\n        \n        # print(x1.shape, x2.shape)\n        covar = x1 @ x2.T\n        # print(covar.shape, gt_covar.shape, x1.shape, x2.shape)\n        return covar\n\n\nfixed_kernel = RBFKernel()\nfixed_kernel.lengthscale = 0.3\n\nX1 = torch.linspace(-1, 1, 100).view(-1, 1)\n\n\nepochs = 1000\nn_neurons_per_layer = [64]*4\noutput_dim = 10\nkernel = Rank1Kernel(1, output_dim, n_neurons_per_layer, torch.sin)\noptimizer = torch.optim.Adam(kernel.parameters(), lr=0.001)\n\nlosses = []\nwith torch.no_grad():\n    gt_covar = fixed_kernel(X1, X1).evaluate_kernel().tensor\n    \nbar = tqdm(range(epochs))\nfor epoch in bar:\n    optimizer.zero_grad()\n    pred_covar = kernel(X1, X1)\n    loss = torch.mean((gt_covar - pred_covar)**2)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    bar.set_description(f\"Loss: {loss.item():.4f}\")\n\nLoss: 0.0001: 100%|██████████| 1000/1000 [00:06&lt;00:00, 150.34it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,2,figsize=(8, 3))\n\nsns.heatmap(gt_covar, ax=ax[0], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[0].set_title('Ground Truth Covariance')\n\nX_new = torch.linspace(-1.5, 1.5, 100).view(-1, 1)\nwith torch.no_grad():\n    est_covar = kernel(X_new, X_new)\nsns.heatmap(est_covar, ax=ax[1], cmap='RdYlGn_r', cbar=True, vmin=-2, vmax=2)\nax[1].set_title('Estimated Covariance');\n\n\n\n\n\n\n\n\n\n# plt.plot()\n\nX2 = torch.zeros(1, 1) + 1\nwith torch.no_grad():\n    variance = gt_covar[-1, :]\n    plt.plot(X1, variance.numpy(), label=\"fixed kernel\");\n    \n    variance = kernel(X1, X2)\n    plt.plot(X1, variance.numpy(), label=f\"rank-{output_dim} kernel\");\n    \n    plt.legend()\n\n\n\n\n\n\n\n\n\nprint(gt_covar.shape)\ntorch.random.manual_seed(2)\nnorm = dist.MultivariateNormal(torch.zeros(100), gt_covar + 1e-5 * torch.eye(100))\ny = norm.sample()\nplt.plot(X1, y);\n\ntorch.Size([100, 100])\n\n\n\n\n\n\n\n\n\n\nn = 6\n\nfig, ax = plt.subplots(1, n, figsize=(15, 2))\nd_x = X1\nd_y = y\nfor i in range(n):\n    print(f\"{i}: {torch.var(d_y)}\")\n    ax[i].plot(d_x, d_y)\n    d_x = d_x[1:] - d_x[:-1]\n    d_x = torch.cumsum(d_x, dim=0)\n    d_y = d_y[1:] - d_y[:-1]\n    \nf = lambda x: torch.zeros_like(x)\nax[-1].plot(d_x, f(d_x), c=\"r\", label=\"f(x)\")\n\n0: 0.5698477029800415\n1: 0.006691396702080965\n2: 0.0001796285796444863\n3: 0.00022799619182478637\n4: 0.0008216467685997486\n5: 0.00304242386482656"
  },
  {
    "objectID": "posts/PurpleAir.html",
    "href": "posts/PurpleAir.html",
    "title": "Download low-cost data from OpenAQ",
    "section": "",
    "text": "import requests\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom glob import glob\n\nfrom geopy.geocoders import Nominatim\n\nOpenAQ has an aws s3 bucket that contains all the data to download for free. This is a guide to download the data from the bucket. If you have enough space and bandwidth, aws s3 commands are the fastest way to download the data. If you don’t have enough space/bandwidth or you want to download specific data only then follow along.\nAcknowledgement: Much help is taken from ChatGPT for some complex Linux commands.\nWe will mostly use the following commands:\naws s3 ls\naws s3 cp\nScenario 1: We want to download PurpleAir sensors data for Delhi for entire 2022. I am taking Delhi’s example since there are far lesser sensors in Delhi than in the US. So, it will be easier for this demo.\nSome statistics that I have calculated for PurpleAir sensors in US are as follows:\n\n\n\nCountry\nNumber of sensors\nTotal size\nyears\n\n\n\n\nUSA\n22497\n90.945 GB\n2018, 2019, 2020, 2021, 2022, 2023\n\n\n\nLet’s see how to calculate these statistics for India.\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ &gt; /tmp/in.txt\n\nThe output of above command contains all sensor IDs within India. These sensor IDs are assigned by OpenAQ and they are different from PurpleAir “sensor_index”. The output looks like the following.\n\nwith open('/tmp/in.txt', 'r') as f:\n    lines = [line.strip() for line in f.readlines()]\n  \nprint(f\"Number of locations = {len(lines)}\\n\")\nprint(*lines[:3], sep='\\n')\n\nNumber of locations = 623\n\nPRE locationid=160485/\nPRE locationid=218334/\nPRE locationid=218336/\n\n\nCounting total size of files:\n\n!aws s3 --no-sign-request ls s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/ --recursive &gt; /tmp/in_files.txt\n\n\n!awk '{ sum += $3 } END { print (sum/1024/1024/1024) \" GB\"}' /tmp/in_files.txt\n\n1.20947 GB\n\n\nNumber of years:\n\n!cat /tmp/in_files.txt | grep -oP 'year=\\K\\w+' | sort | uniq\n\n2019\n2020\n2021\n2022\n2023\n\n\nLet’s find out which sensors among these belong to Delhi.\n\n!cat /tmp/in.txt | grep -oP 'locationid=\\K\\w+' | sort | uniq &gt; /tmp/in_locations.txt\n\nNow we use OpenAQ REST API. It has limit of 300 requests per 5 minutes. After the limit exceeds, it will return an error.\n\nurl = 'https://api.openaq.org/v2/locations'\nparams = {\n    'limit': 1000,\n    'country_id': 'IN',\n    \"modelName\": \"PurpleAir Sensor\"\n}\n\n# Request headers\nheaders = {\n    'accept': 'application/json'\n}\n\nresponse = requests.get(url, params=params, headers=headers)\n\nif response.status_code == 200:\n    data = response.json()\nelse:\n    print(response.status_code, response.reason)\n\n500 Internal Server Error\n\n\n\ndf = pd.DataFrame(data['results'])\ndf.shape\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df = pd.DataFrame(data['results'])\n      2 df.shape\n\nNameError: name 'data' is not defined\n\n\n\nThe following method works but takes too much time since it is online:\n\ndef get_city_name(coords):\n    latitude = coords[\"latitude\"]\n    longitude = coords[\"longitude\"]\n    geolocator = Nominatim(user_agent=\"myGeocoder\")  # Replace \"myGeocoder\" with your desired user agent\n    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n\n    if location:\n        address = location.raw.get('address', {})\n        city = address.get('city', '')\n        return city\n\n    return None\n\ndf[\"city\"] = df[\"coordinates\"].apply(get_city_name)\ndf\n\n\n\n\n\n\n\n\nid\ncity\nname\nentity\ncountry\nsources\nisMobile\nisAnalysis\nparameters\nsensorType\ncoordinates\nlastUpdated\nfirstUpdated\nmeasurements\nbounds\nmanufacturers\n\n\n\n\n0\n318146\nGangtok\nNASA_AQCS_201_cpa\nNone\nIN\nNone\nFalse\nNone\n[{'id': 135, 'unit': 'particles/cm³', 'count':...\nNone\n{'latitude': 27.31013, 'longitude': 88.59687}\n2023-07-26T02:34:05+00:00\n2022-04-23T07:42:24+00:00\n1168071\n[88.59687, 27.31013, 88.59687, 27.31013]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n1\n220706\nGangtok\nNASA_AQCS_139\nNone\nIN\nNone\nFalse\nNone\n[{'id': 100, 'unit': 'c', 'count': 28600, 'ave...\nNone\n{'latitude': 27.310116, 'longitude': 88.59682}\n2023-07-26T02:34:04+00:00\n2021-02-17T09:56:06+00:00\n2205110\n[88.59682, 27.310116, 88.59682, 27.310116]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n2\n66673\nHisar\nNASA_AQCS_160\nNone\nIN\nNone\nFalse\nNone\n[{'id': 132, 'unit': 'mb', 'count': 28651, 'av...\nNone\n{'latitude': 29.146254, 'longitude': 75.72236}\n2023-07-26T02:33:56+00:00\n2021-01-19T23:59:16+00:00\n2396934\n[75.72236, 29.146254, 75.72236, 29.146254]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n3\n72977\nBengaluru\nUT Sensor 101\nNone\nIN\nNone\nFalse\nNone\n[{'id': 126, 'unit': 'particles/cm³', 'count':...\nNone\n{'latitude': 13.045313, 'longitude': 77.573395}\n2023-07-26T02:33:55+00:00\n2021-01-14T01:18:23+00:00\n2498544\n[77.573395, 13.045313, 77.573395, 13.045313]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n4\n235916\nBengaluru\nUW Sensor 311\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm³', 'count':...\nNone\n{'latitude': 13.048528, 'longitude': 77.582275}\n2023-07-26T02:33:49+00:00\n2021-09-16T12:29:52+00:00\n1773924\n[77.582275, 13.048528, 77.582275, 13.048528]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n604\n73922\nNew Delhi District\nUS Embassy A\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'µg/m³', 'count': 347, 'ave...\nNone\n{'latitude': 28.5979, 'longitude': 77.1847}\n2021-02-05T18:25:34+00:00\n2021-01-08T12:12:29+00:00\n2082\n[77.1847, 28.5979, 77.1847, 28.5979]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n605\n73924\nNew Delhi District\nUS Embassy B\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'µg/m³', 'count': 347, 'ave...\nNone\n{'latitude': 28.5982, 'longitude': 77.1837}\n2021-02-05T18:23:59+00:00\n2021-01-08T12:11:29+00:00\n2082\n[77.1837, 28.5982, 77.1837, 28.5982]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n606\n219370\nBhubaneswar Municipal Corporation\nBhubaneswar India\nNone\nIN\nNone\nFalse\nNone\n[{'id': 2, 'unit': 'µg/m³', 'count': 180, 'ave...\nNone\n{'latitude': 20.2853, 'longitude': 85.7685}\n2021-02-03T10:18:41+00:00\n2021-02-03T03:02:39+00:00\n1080\n[85.7685, 20.2853, 85.7685, 20.2853]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n607\n71465\nGurugram District\nNASA_AQCS_152\nNone\nIN\nNone\nFalse\nNone\n[{'id': 1, 'unit': 'µg/m³', 'count': 1, 'avera...\nNone\n{'latitude': 28.4522, 'longitude': 77.0949}\n2021-01-14T01:18:54+00:00\n2021-01-14T01:18:54+00:00\n6\n[77.0949, 28.4522, 77.0949, 28.4522]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n608\n221974\nBengaluru\nUT sensor\nNone\nIN\nNone\nFalse\nNone\n[{'id': 130, 'unit': 'particles/cm³', 'count':...\nNone\n{'latitude': 13.0449, 'longitude': 77.5788}\n2019-12-17T10:32:35+00:00\n2019-12-17T10:32:35+00:00\n6\n[77.5788, 13.0449, 77.5788, 13.0449]\n[{'modelName': 'PurpleAir Sensor', 'manufactur...\n\n\n\n\n609 rows × 16 columns\n\n\n\nNow, we use another method of shapefile to do this:\n\n!wget --no-check-certificate \"https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\" -O /tmp/delhi.zip\n!unzip -o /tmp/delhi.zip -d /tmp/delhi\n\n--2023-07-26 08:37:34--  https://groups.google.com/group/datameet/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1\nResolving groups.google.com (groups.google.com)... 216.239.32.177, 216.239.36.177, 216.239.38.177, ...\nConnecting to groups.google.com (groups.google.com)|216.239.32.177|:443... connected.\nHTTP request sent, awaiting response... 302 Moved Temporarily\nLocation: https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0 [following]\n--2023-07-26 08:37:35--  https://06895207363394598426.googlegroups.com/attach/29b74b1aef5f2f13/Delhi.zip?part=0.1&vt=ANaJVrEpUPnptnb4Y-J5gJRBVJ29K0pIGKzeBG7492Ume1tyn1MY5eTDbztxP0Hdbc7u8XhmH_GbemY_HD60x5OvDhr7M2ib1h8YfDmlNxFefazGPgmAUj0\nResolving 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)... 142.251.10.137, 2404:6800:4003:c0f::89\nConnecting to 06895207363394598426.googlegroups.com (06895207363394598426.googlegroups.com)|142.251.10.137|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/zip]\nSaving to: ‘/tmp/delhi.zip’\n\n/tmp/delhi.zip          [ &lt;=&gt;                ]  16.42K  --.-KB/s    in 0.04s   \n\n2023-07-26 08:37:37 (429 KB/s) - ‘/tmp/delhi.zip’ saved [16812]\n\nArchive:  /tmp/delhi.zip\n  inflating: /tmp/delhi/Delhi.kml    \n  inflating: /tmp/delhi/Districts.dbf  \n  inflating: /tmp/delhi/Districts.prj  \n  inflating: /tmp/delhi/Districts.qpj  \n  inflating: /tmp/delhi/Districts.shp  \n  inflating: /tmp/delhi/Districts.shx  \n\n\n\ngdf = gpd.read_file('/tmp/delhi/Districts.shp')\ngdf.plot(color=\"none\", edgecolor=\"black\");\n\n\n\n\n\n\n\n\n\n# check if a point is within Delhi\ndef is_within_delhi(coords):\n    point = Point(coords[\"longitude\"], coords[\"latitude\"])\n    for i, row in gdf.iterrows():\n        if row.geometry.contains(point):\n            return True\n    return False\n\ndf[\"is_within_delhi\"] = df[\"coordinates\"].apply(is_within_delhi)\n\n\ndelhi_df = df[df[\"is_within_delhi\"]]\ndelhi_df.shape\n\n(311, 17)\n\n\n\ndelhi_df.city.value_counts()\n\n                      194\nNew Delhi District    112\nDwarka                  3\nGhaziabad               2\nName: city, dtype: int64\n\n\nSeems like many points were not detected by the online geopy encoder.\nNow, we know that out of 623, 311 sensors belong to Delhi. Let’s download the data for these sensors. For illustration, I will download data for 3 sensors for year 2022 and month of Jan.\n\n# dump delhi_df.id to a file\ndelhi_df.id.to_csv('/tmp/delhi_locations.txt', index=False, header=False)\n!head -n3 /tmp/delhi_locations.txt\n\n274208\n221227\n273205\n\n\n\n!head -n3 /tmp/delhi_locations.txt &gt; /tmp/delhi_locations_3.txt\n!while read -r sensor_id; do aws s3 --no-sign-request cp s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=$sensor_id/year=2022/month=01 /tmp/delhi_data --recursive; done &lt; /tmp/delhi_locations_3.txt\n\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220107.csv.gz to ../../../../tmp/delhi_data/location-274208-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220120.csv.gz to ../../../../tmp/delhi_data/location-274208-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220131.csv.gz to ../../../../tmp/delhi_data/location-274208-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220122.csv.gz to ../../../../tmp/delhi_data/location-274208-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220130.csv.gz to ../../../../tmp/delhi_data/location-274208-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220129.csv.gz to ../../../../tmp/delhi_data/location-274208-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220114.csv.gz to ../../../../tmp/delhi_data/location-274208-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220123.csv.gz to ../../../../tmp/delhi_data/location-274208-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220113.csv.gz to ../../../../tmp/delhi_data/location-274208-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=274208/year=2022/month=01/location-274208-20220121.csv.gz to ../../../../tmp/delhi_data/location-274208-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220107.csv.gz to ../../../../tmp/delhi_data/location-221227-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220104.csv.gz to ../../../../tmp/delhi_data/location-221227-20220104.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220115.csv.gz to ../../../../tmp/delhi_data/location-221227-20220115.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220105.csv.gz to ../../../../tmp/delhi_data/location-221227-20220105.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220108.csv.gz to ../../../../tmp/delhi_data/location-221227-20220108.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220106.csv.gz to ../../../../tmp/delhi_data/location-221227-20220106.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220102.csv.gz to ../../../../tmp/delhi_data/location-221227-20220102.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220117.csv.gz to ../../../../tmp/delhi_data/location-221227-20220117.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220118.csv.gz to ../../../../tmp/delhi_data/location-221227-20220118.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220110.csv.gz to ../../../../tmp/delhi_data/location-221227-20220110.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220131.csv.gz to ../../../../tmp/delhi_data/location-221227-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220113.csv.gz to ../../../../tmp/delhi_data/location-221227-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220119.csv.gz to ../../../../tmp/delhi_data/location-221227-20220119.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220121.csv.gz to ../../../../tmp/delhi_data/location-221227-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220123.csv.gz to ../../../../tmp/delhi_data/location-221227-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220116.csv.gz to ../../../../tmp/delhi_data/location-221227-20220116.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220128.csv.gz to ../../../../tmp/delhi_data/location-221227-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220122.csv.gz to ../../../../tmp/delhi_data/location-221227-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220101.csv.gz to ../../../../tmp/delhi_data/location-221227-20220101.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220111.csv.gz to ../../../../tmp/delhi_data/location-221227-20220111.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220120.csv.gz to ../../../../tmp/delhi_data/location-221227-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220109.csv.gz to ../../../../tmp/delhi_data/location-221227-20220109.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220129.csv.gz to ../../../../tmp/delhi_data/location-221227-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220103.csv.gz to ../../../../tmp/delhi_data/location-221227-20220103.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220112.csv.gz to ../../../../tmp/delhi_data/location-221227-20220112.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220114.csv.gz to ../../../../tmp/delhi_data/location-221227-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=221227/year=2022/month=01/location-221227-20220130.csv.gz to ../../../../tmp/delhi_data/location-221227-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220120.csv.gz to ../../../../tmp/delhi_data/location-273205-20220120.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220124.csv.gz to ../../../../tmp/delhi_data/location-273205-20220124.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220107.csv.gz to ../../../../tmp/delhi_data/location-273205-20220107.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220121.csv.gz to ../../../../tmp/delhi_data/location-273205-20220121.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220126.csv.gz to ../../../../tmp/delhi_data/location-273205-20220126.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220114.csv.gz to ../../../../tmp/delhi_data/location-273205-20220114.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220129.csv.gz to ../../../../tmp/delhi_data/location-273205-20220129.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220130.csv.gz to ../../../../tmp/delhi_data/location-273205-20220130.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220113.csv.gz to ../../../../tmp/delhi_data/location-273205-20220113.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220123.csv.gz to ../../../../tmp/delhi_data/location-273205-20220123.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220122.csv.gz to ../../../../tmp/delhi_data/location-273205-20220122.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220131.csv.gz to ../../../../tmp/delhi_data/location-273205-20220131.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220128.csv.gz to ../../../../tmp/delhi_data/location-273205-20220128.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220125.csv.gz to ../../../../tmp/delhi_data/location-273205-20220125.csv.gz\ndownload: s3://openaq-data-archive/records/csv.gz/provider=purpleair/country=in/locationid=273205/year=2022/month=01/location-273205-20220127.csv.gz to ../../../../tmp/delhi_data/location-273205-20220127.csv.gz\n\n\nVerify if we got all the sensors data as we needed.\n\n!ls /tmp/delhi_data/location-221227* | wc -l\n!ls /tmp/delhi_data/location-274208* | wc -l\n!ls /tmp/delhi_data/location-273205* | wc -l\n\n27\n10\n15\n\n\n\nsensor_df = pd.read_csv('/tmp/delhi_data/location-274208-20220107.csv.gz')\nsensor_df.parameter.value_counts()\n\npm10     64\npm25     64\npm1      64\num010    64\num025    64\num100    64\nName: parameter, dtype: int64"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html",
    "href": "posts/2022-08-01-conditional_neural_processes.html",
    "title": "Conditional Neural Processes in JAX",
    "section": "",
    "text": "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n# https://github.com/tensorflow/probability/issues/1523\nimport logging\n\nlogger = logging.getLogger()\n\n\nclass CheckTypesFilter(logging.Filter):\n    def filter(self, record):\n        return \"check_types\" not in record.getMessage()\n\n\nlogger.addFilter(CheckTypesFilter())\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntry:\n  import flax.linen as nn\nexcept ModuleNotFoundError:\n  %pip install flax\n  import flax.linen as nn\n\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install optax\n  import optax\n\ntry:\n  import tensorflow_probability.substrates.jax as tfp\nexcept ModuleNotFoundError:\n  %pip install tensorflow-probability\n  import tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#model",
    "href": "posts/2022-08-01-conditional_neural_processes.html#model",
    "title": "Conditional Neural Processes in JAX",
    "section": "Model",
    "text": "Model\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n\n    for n_features in self.features:\n      x = nn.Dense(n_features)(x)\n      x = nn.relu(x)\n\n    x = nn.Dense(2)(x)\n    loc, raw_scale = x[:, 0], x[:, 1]\n    scale = jax.nn.softplus(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#data",
    "href": "posts/2022-08-01-conditional_neural_processes.html#data",
    "title": "Conditional Neural Processes in JAX",
    "section": "Data",
    "text": "Data\n\nN = 100\nseed = jax.random.PRNGKey(0)\nx = jnp.linspace(-1, 1, N).reshape(-1, 1)\nf = lambda x: (jnp.sin(10*x) + x).flatten()\nnoise = jax.random.normal(seed, shape=(N,)) * 0.2\ny = f(x) + noise\n\nx_test = jnp.linspace(-2, 2, N*2+10).reshape(-1, 1)\ny_test = f(x_test) \n\nplt.scatter(x, y, label='train', zorder=5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.legend();"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#training",
    "href": "posts/2022-08-01-conditional_neural_processes.html#training",
    "title": "Conditional Neural Processes in JAX",
    "section": "Training",
    "text": "Training\n\ndef train_fn(model, optimizer, seed, n_iterations, n_context):\n  params = model.init(seed, x, y, x)\n  value_and_grad_fn = jax.value_and_grad(model.loss_fn)\n  state = optimizer.init(params)\n  indices = jnp.arange(N)\n  \n  def one_step(params_and_state, seed):\n    params, state = params_and_state\n    shuffled_indices = jax.random.permutation(seed, indices)\n    context_indices = shuffled_indices[:n_context]\n    target_indices = shuffled_indices[n_context:]\n    x_context, y_context = x[context_indices], y[context_indices]\n    x_target, y_target = x[target_indices], y[target_indices]\n    loss, grads = value_and_grad_fn(params, x_context, y_context, x_target, y_target)\n    updates, state = optimizer.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n\n  seeds = jax.random.split(seed, num=n_iterations)\n  (params, state), losses = jax.lax.scan(one_step, (params, state), seeds)\n  return params, losses\n\n\nencoder_features = [64, 16, 8]\nencoding_dims = 1\ndecoder_features = [16, 8]\nmodel = CNP(encoder_features, encoding_dims, decoder_features)\noptimizer = optax.adam(learning_rate=0.001)\n\nseed = jax.random.PRNGKey(2)\nn_context = int(0.7 * N)\nn_iterations = 20000\n\nparams, losses = train_fn(model, optimizer, seed, n_iterations=n_iterations, n_context=n_context)\n\n\nplt.plot(losses);"
  },
  {
    "objectID": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "href": "posts/2022-08-01-conditional_neural_processes.html#predict",
    "title": "Conditional Neural Processes in JAX",
    "section": "Predict",
    "text": "Predict\n\nloc, scale = model.apply(params, x, y, x_test)\nlower, upper = loc - 2*scale, loc + 2*scale\n\nplt.scatter(x, y, label='train', alpha=0.5)\nplt.scatter(x_test, y_test, label='test', alpha=0.5)\nplt.plot(x_test, loc);\nplt.fill_between(x_test.flatten(), lower, upper, alpha=0.4);\nplt.ylim(-5, 5);"
  },
  {
    "objectID": "posts/Basis_functions.html",
    "href": "posts/Basis_functions.html",
    "title": "Basis functions",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\n\n\ndata = pd.read_csv(\"../../beat_stgnp/dataset/bjair/NP/processed_raw.csv\")\ndata[\"time\"] = pd.to_datetime(data[\"time\"], format=\"%Y-%m-%d %H:%M:%S\")\ndata[\"time\"] = data[\"time\"].apply(lambda x: x.timestamp())\n\nx = [\"latitude\", \"longitude\", \"time\"]\ny = [\"PM25_Concentration\"]\n\nx_train, x_test, y_train, y_test = train_test_split(data[x], data[y], test_size=0.2, random_state=42)\nx_train, x_test, y_train, y_test = map(lambda x: x.values, [x_train, x_test, y_train, y_test])\n\nx_scaler = MinMaxScaler()\ny_scaler = StandardScaler()\nx_train = x_scaler.fit_transform(x_train)\ny_train = y_scaler.fit_transform(y_train)\nx_test = x_scaler.transform(x_test)\n\nmodel = RandomForestRegressor(n_estimators=1000, random_state=42)\nmodel.fit(x_train, y_train.ravel())\ny_pred = model.predict(x_test)\nprint(\"RMSE\", np.sqrt(np.mean((y_scaler.inverse_transform(y_pred).ravel() - y_test.ravel())**2)))\n\n /tmp/ipykernel_922642/3470971270.py:18: DataConversionWarning:A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel()."
  },
  {
    "objectID": "posts/2024-12-27-download_caaqm_locations.html",
    "href": "posts/2024-12-27-download_caaqm_locations.html",
    "title": "Download CPCB CAAQM locations",
    "section": "",
    "text": "try:\n    import selenium\nexcept ModuleNotFoundError:\n    %pip install selenium\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm, trange\nfrom time import sleep, time\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import Select\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n!rm log.txt\n\ndef print_it(*args, **kwargs):\n    print(*args, **kwargs)\n    with open('log.txt', 'a') as f:\n        print(*args, **kwargs, file=f)\n\nglobal_init = time()\n\nrm: log.txt: No such file or directory\n\n\n\n# Set up WebDriver\nop = webdriver.ChromeOptions()\n\ndriver = webdriver.Chrome(options=op)\n\n# Navigate to the website and manually solve the CAPTCHA\ndriver.get(\"https://airquality.cpcb.gov.in/ccr/#/caaqm-dashboard-all/caaqm-landing\")\n\n\nManually solve captcha before moving on to the next cell..\n\n\n# leaflet-marker-icon custom-div-icon map_markers station_status_live leaflet-zoom-animated leaflet-interactive\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\nall_stations_len = len(all_station_markers)\nprint(\"Total stations: \", all_stations_len)\n\nTotal stations:  558\n\n\n\ndef get_after(string, phrase):\n    return string[string.index(phrase) + len(phrase):]\n\ndata = {}\nall_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\nmarker_id = 0\nprogress_bar = tqdm(total=all_stations_len, desc=\"Progress\")\nwhile marker_id &lt; all_stations_len:\n    try:\n        marker = all_station_markers[marker_id]\n        driver.execute_script(\"arguments[0].click();\", marker)\n        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n        children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n        assert \"Station Name\" in children[3].text\n        \n        # parse it\n    \n        station, address, location = children[3].text.split('\\n')\n        station = get_after(station, \"Station Name: \")\n        address = get_after(address, \"Address: \")\n        latitude, longitude = location.split(\",\")\n        latitude = get_after(latitude, \"Latitude: \")\n        longitude = get_after(longitude, \"Longitude: \")\n        \n        data[station] = {\"address\": address, \"latitude\": float(latitude), \"longitude\": float(longitude)}\n        close = driver.find_element(By.CLASS_NAME, \"close\")\n        close.click()\n        sleep(0.5)\n        marker_id += 1\n        progress_bar.update(1)\n    except Exception as e:\n        driver.refresh()\n        input(\"Please manually solve the Captcha\")\n        all_station_markers = driver.find_elements(By.CLASS_NAME, 'leaflet-marker-icon')\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 12\n     10 marker = all_station_markers[marker_id]\n     11 driver.execute_script(\"arguments[0].click();\", marker)\n---&gt; 12 WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'close')))\n     13 children = driver.find_elements(By.CLASS_NAME, \"col-md-12\")\n     14 assert \"Station Name\" in children[3].text\n\nFile /opt/miniconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:102, in WebDriverWait.until(self, method, message)\n    100     screen = getattr(exc, \"screen\", None)\n    101     stacktrace = getattr(exc, \"stacktrace\", None)\n--&gt; 102 time.sleep(self._poll)\n    103 if time.monotonic() &gt; end_time:\n    104     break\n\nKeyboardInterrupt: \n\n\n\n\ndf = pd.DataFrame(data).T\ndf.index.name = \"station\"\ndf.head(2)\n\n\n\n\n\n\n\n\naddress\nlatitude\nlongitude\n\n\nstation\n\n\n\n\n\n\n\nSIDCO Kurichi, Coimbatore - TNPCB\nSIDCO Kurichi, Coimbatore, Tamil Nadu.\n10.942451\n76.978996\n\n\nMuradpur, Patna - BSPCB\nS K Memorial Hall Premises, Near Gandhi Maidan...\n25.619651\n85.147382\n\n\n\n\n\n\n\n\ndf.to_csv(\"station_data.csv\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Download CPCB CAAQM locations\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB CAAQM locations using Selenium\n\n\n\n\n\nDec 27, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload CPCB live data\n\n\n\n\n\n\nData\n\n\n\nDownload CPCB data with selenium\n\n\n\n\n\nDec 10, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nFoundation Models for Time Series Forecasting\n\n\n\n\n\n\nML\n\n\n\nExploring the foundation models for time series forecasting\n\n\n\n\n\nJul 6, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding GPT from scratch\n\n\n\n\n\n\nML\n\n\n\nBuilding GPT from scratch\n\n\n\n\n\nJul 1, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals across ML domains\n\n\n\n\n\n\nML\n\n\n\nKnowledge transfer between ML domains\n\n\n\n\n\nJun 25, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSeq to Seq\n\n\n\n\n\n\nML\n\n\n\nRational driven history of Seq to Seq models\n\n\n\n\n\nJun 24, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nWRF Tutorial\n\n\n\n\n\n\nML\n\n\n\nWRF end-to-end tutorial\n\n\n\n\n\nMar 4, 2024\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nLearnings from the Brick Kiln Project\n\n\n\n\n\n\nML\n\n\n\nLearnings from the Brick Kiln Project\n\n\n\n\n\nNov 28, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nData Handling for Large Scale ML\n\n\n\n\n\n\nML\n\n\n\nAn exploratory analysis of various dataset handling processes to optimize memory, diskspace and speed.\n\n\n\n\n\nSep 30, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nActive Learning with MNIST\n\n\n\n\n\n\nML\n\n\n\nActive Learning with MNIST\n\n\n\n\n\nSep 30, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\n\nML\n\n\n\nNon-Gaussian Likelihoods for MLPs\n\n\n\n\n\nSep 16, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\n\nML\n\n\n\nPruning vs Uncertainty\n\n\n\n\n\nSep 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Air Quality Data\n\n\n\n\n\n\nNumPy, Mathematics\n\n\n\nGoogle Air Quality API\n\n\n\n\n\nAug 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Basis Regression\n\n\n\n\n\n\nML\n\n\n\nBayesian Basis Regression\n\n\n\n\n\nAug 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow numpy handles day-to-day algebra?\n\n\n\n\n\n\nNumPy, Mathematics\n\n\n\nA deep dive into basic operations of numpy\n\n\n\n\n\nAug 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nCan Rank 1 GPs represent all GPs?\n\n\n\n\n\n\nML, GP\n\n\n\nA trial\n\n\n\n\n\nJul 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDownload low-cost data from OpenAQ\n\n\n\n\n\n\nML, GP\n\n\n\nA guide to download low-cost sensor data from OpenAQ\n\n\n\n\n\nJul 26, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-class classification with Gaussian Processes\n\n\n\n\n\n\nML, GP\n\n\n\nMulti-class GP classification with different strategies\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with GPs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of GPs for climate modeling\n\n\n\n\n\nJul 4, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Modeling with SIRENs\n\n\n\n\n\n\nML\n\n\n\nExploring the use of SIRENs for climate modeling\n\n\n\n\n\nJul 1, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGNNs and GPs\n\n\n\n\n\n\nML\n\n\n\nExploring similarities between GNNs and GPs\n\n\n\n\n\nJun 23, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nBasis functions\n\n\n\n\n\n\nML\n\n\n\nExploring basis functions\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Neural Networks for Regression\n\n\n\n\n\n\nML\n\n\n\nChallenges in using GNNs for regression using various strategies\n\n\n\n\n\nJun 12, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes for Image Interpolation\n\n\n\n\n\n\nML\n\n\n\nExtreme Image Interpolation with Conditional Neural processes\n\n\n\n\n\nMay 31, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPasswordless SSH setup for MacOS Hosts\n\n\n\n\n\n\nmacOS\n\n\n\nA tiny handbook to setup passwordless ssh in MacOS\n\n\n\n\n\nMay 14, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSine Combination Networks\n\n\n\n\n\n\nML\n\n\n\nChallenges in fitting to a combination of sine waves\n\n\n\n\n\nApr 29, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network Gaussian Process\n\n\n\n\n\n\nGP\n\n\nML\n\n\n\nExploring NTK kernels + GPJax with toy datasets\n\n\n\n\n\nMar 28, 2023\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Variational Gaussian processes in JAX\n\n\n\n\n\n\nGP\n\n\n\nA practical implementation of Hensman et al. 2015 from scratch in JAX\n\n\n\n\n\nOct 31, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-Output Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nExploring MOGPs from scratch\n\n\n\n\n\nOct 27, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Processes - A no-skip-math version\n\n\n\n\n\n\nML\n\n\n\nEnd-to-end math derivations for Gaussian process regression and classification\n\n\n\n\n\nOct 21, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTrain NN with KFAC-Laplace in JAX\n\n\n\n\n\n\nML\n\n\n\nExploring KFAC-Laplace approximation on simple problems in JAX\n\n\n\n\n\nOct 18, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Neural Processes in JAX\n\n\n\n\n\n\nML\n\n\n\nImplementing conditional neural processes from scratch in JAX\n\n\n\n\n\nAug 1, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nJAX Optimizers\n\n\n\n\n\n\nML\n\n\n\nPros and cons of several jax optimizers.\n\n\n\n\n\nJun 10, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGet a list of contributors from a repo\n\n\n\n\n\n\nGitHub\n\n\n\nGet contributors’ list using GitHub API and pandas\n\n\n\n\n\nMay 17, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nIteratively reweighted least squares (IRLS) logistic regression\n\n\n\n\n\n\nML\n\n\n\nImplementation of IRLS from Probabilistic ML book of Dr. Kevin Murphy and its comparison with naive second order implementation.\n\n\n\n\n\nMay 14, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGcloud cheatsheet\n\n\n\n\n\n\nGcloud\n\n\n\nMost used commands while working with gcloud\n\n\n\n\n\nApr 9, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Contrubuting FAQs\n\n\n\n\n\n\nGitHub\n\n\n\nThis is a collection of FAQs/road-blocks/queries/issues I had over the past 2 years of engagement with GitHub.\n\n\n\n\n\nApr 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nTorch essentials\n\n\n\n\n\n\nML\n\n\n\nPractical and direct introduction to PyTorch\n\n\n\n\n\nMar 8, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Machine Learning\n\n\n\n\n\n\nML\n\n\n\nA video lecture series from Prof. Philipp Hennig\n\n\n\n\n\nMar 6, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in Deep Learning\n\n\n\n\n\n\nML\n\n\n\nReview of PhD thesis of Dr. Yarin Gal\n\n\n\n\n\nMar 5, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch Tips\n\n\n\n\n\n\nML\n\n\n\nPyTorch zen tips\n\n\n\n\n\nFeb 25, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nConference Presentation Tips\n\n\n\n\n\n\nAcademic\n\n\n\nConference Presentation Tips\n\n\n\n\n\nJan 29, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Gaussian Process Regression Frameworks\n\n\n\n\n\n\nML\n\n\n\nA basic comparison among GPy, GPyTorch and TinyGP\n\n\n\n\n\nJan 25, 2022\n\n\nZeel B Patel, Harsh Patel, Shivam Sahni\n\n\n\n\n\n\n\n\n\n\n\n\nQuery by Committee\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to QBC with Random Forest Classifier.\n\n\n\n\n\nJan 24, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nKL divergence v/s cross-entropy\n\n\n\n\n\n\nML\n\n\n\nUnderstanding KL divergence\n\n\n\n\n\nJan 20, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nWhy .py files are better than .ipynb files for ML codebase\n\n\n\n\n\n\nPython\n\n\n\nWhere .py files are better than .ipynb files?\n\n\n\n\n\nJan 15, 2022\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nAnonymization tips for double-blind submission\n\n\n\n\n\n\nAcademic\n\n\n\nA last-minute help list\n\n\n\n\n\nOct 26, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nInput Warped GPs - A failed idea\n\n\n\n\n\n\nML\n\n\n\nAn idea of input warping GPs\n\n\n\n\n\nOct 23, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nSparseGPs in Stheno\n\n\n\n\n\nA simple demo of sparse regression in stheno with VFE and FITC methods.\n\n\n\n\n\nOct 12, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nDocker Cheatsheet\n\n\n\n\n\n\nDocker\n\n\n\nMost used command while working with Docker\n\n\n\n\n\nSep 28, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nHow to apply constraint on parameters in various GP libraries\n\n\n\n\n\nApply constraints in GPy, GPFlow, GPyTorch\n\n\n\n\n\nSep 27, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Kernels in Gaussian Processes\n\n\n\n\n\n\nML\n\n\n\nAn exploratory analysis of kernels in GPs\n\n\n\n\n\nMar 22, 2021\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nProgramatically download OpenAQ data\n\n\n\n\n\nSep 21, 2020\n\n\nZeel B Patel\n\n\n\n\n\n\n\n\n\n\n\n\nActive Learning with Bayesian Linear Regression\n\n\n\n\n\n\nML\n\n\n\nA programming introduction to Active Learning with Bayesian Linear Regression.\n\n\n\n\n\nMar 28, 2020\n\n\nZeel B Patel, Nipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Zeel. This is my blog, where I add coding + other resources related to my research. Head over to this page for my personal website."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html",
    "href": "posts/2022-01-24-query_by_committee.html",
    "title": "Query by Committee",
    "section": "",
    "text": "# Common imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\n\nplt.style.use('fivethirtyeight')\nrc('animation', html='jshtml')\n\n# Copy the models\nfrom copy import deepcopy\n\n# Sklearn imports\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Entropy function\nfrom scipy.stats import entropy\n\n# Progress helper\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-posterior-sampling",
    "title": "Query by Committee",
    "section": "QBC by posterior sampling",
    "text": "QBC by posterior sampling\n\nInteresting fact: For probabilistic models, QBC is similar to uncertainty sampling. How?\n\nDraw \\(k\\) parameter sets from the posterior distribution representing \\(k\\) different models.\nQuery a point which shows maximum disagreement among the points."
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "href": "posts/2022-01-24-query_by_committee.html#an-example-bayesian-linear-regression",
    "title": "Query by Committee",
    "section": "An example: Bayesian linear regression",
    "text": "An example: Bayesian linear regression\n\nnp.random.seed(0)\nN = 10\nX = np.linspace(-1,1,N).reshape(-1,1)\n\nt0 = 3\nt1 = 2\n\ny = X * t1 + t0 + np.random.rand(N,1)\n\nplt.scatter(X, y);\n\n\n\n\n\n\n\n\n\nAssume a posterior\n\nn_samples = 50\n\nt0_dist_samples = np.random.normal(t0, 0.1, size=n_samples)\nt1_dist_samples = np.random.normal(t1, 1, size=n_samples)\n\n\n\nPlot the models\n\nplt.scatter(X, y)\n\nfor i in range(len(t0_dist_samples)):\n    sample_t0 = t0_dist_samples[i]\n    sample_t1 = t1_dist_samples[i]\n    \n    plt.plot(X, X * sample_t1 + sample_t0,alpha=0.1)"
  },
  {
    "objectID": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "href": "posts/2022-01-24-query_by_committee.html#qbc-by-bootstrapping",
    "title": "Query by Committee",
    "section": "QBC by bootstrapping",
    "text": "QBC by bootstrapping\n\n2 class dataset\n\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=3, shuffle=True)\n\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=y);\n\n\n\n\n\n\n\n\n\n\nFull data fit with RF\n\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X, y);\n\nRandomForestClassifier(random_state=0)\n\n\n\n\nVisualize decision boundary\n\ngrid_X1, grid_X2 = np.meshgrid(np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100), \n                    np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100))\n\ngrid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())]\n\ngrid_pred = model.predict(grid_X)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[:,0], X[:,1], c=y);\nplt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2);\n\n\n\n\n\n\n\n\n\n\nTrain, pool, test split\n\nX_train_pool, X_test, y_train_pool, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nX_train, X_pool, y_train, y_pool = train_test_split(X_train_pool, y_train_pool, train_size=20, random_state=0)\n\nX_list = [X_train, X_pool, X_test]\ny_list = [y_train, y_pool, y_test]\nt_list = ['Train', 'Pool', 'Test']\n\nfig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True)\nfor i in range(3):\n    ax[i].scatter(X_list[i][:,0], X_list[i][:,1], c=y_list[i])\n    ax[i].set_title(t_list[i])\n    \n\n\n\n\n\n\n\n\n\n\nFitting a model on initial train data\n\nAL_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\nAL_model.fit(X_train, y_train);\n\nRandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\n\nGet the votes from trees on pool dataset\n\nvotes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n\nfor learner_idx, learner in enumerate(AL_model.estimators_):\n    votes[:, learner_idx] = learner.predict(X_pool)\n\n\nvotes.shape\n\n(780, 100)\n\n\n\nvotes\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 1., 1., ..., 0., 1., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nConvert to probabilities\n\np_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n\nfor vote_idx, vote in enumerate(votes):\n    vote_counter = {0 : (1-vote).sum(), 1 : vote.sum()}\n\n    for class_idx, class_label in enumerate(range(X.shape[1])):\n        p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n\n\np_vote\n\narray([[1.  , 0.  ],\n       [0.89, 0.11],\n       [0.06, 0.94],\n       ...,\n       [0.93, 0.07],\n       [1.  , 0.  ],\n       [1.  , 0.  ]])\n\n\n\n\nCalculate dissimilarity (entropy)\n\nexample_id = 2\n\n\nans = 0\nfor category in range(X_pool.shape[1]):\n    ans += (-p_vote[example_id][category] * np.log(p_vote[example_id][category]))\n\nans\n\n0.22696752250060448\n\n\n\nentr = entropy(p_vote, axis=1)\n\n\nentr[example_id]\n\n0.22696752250060448\n\n\n\n\nActive Learning Flow\n\ndef get_query_idx():\n    # Gather the votes\n    votes = np.zeros(shape=(X_pool.shape[0], len(AL_model.estimators_)))\n    for learner_idx, learner in enumerate(AL_model.estimators_):\n        votes[:, learner_idx] = learner.predict(X_pool)\n    \n    # Calcuate probability of votes\n    p_vote = np.zeros(shape=(X_pool.shape[0], X_pool.shape[1]))\n    for vote_idx, vote in enumerate(votes):\n        vote_counter = {0 : (1-vote).sum(), \n                    1 : vote.sum()}\n\n        for class_idx, class_label in enumerate(range(X.shape[1])):\n            p_vote[vote_idx, class_idx] = vote_counter[class_label]/len(AL_model.estimators_)\n    \n    # Calculate entropy for each example\n    entr = entropy(p_vote, axis=1)\n    \n    # Choose example with highest entropy (disagreement)\n    return entr.argmax()\n\n\n\nPrepare data for random sampling\n\nX_train_rand = X_train.copy()\ny_train_rand = y_train.copy()\nX_pool_rand = X_pool.copy()\ny_pool_rand = y_pool.copy()\n\nrandom_model = RandomForestClassifier(n_jobs=28, random_state=0)\n\n\n\nRun active learning\n\nAL_iters = 100\nnp.random.seed(0)\n\nAL_inds = []\nAL_models = []\nrandom_inds = []\nrandom_models = []\n\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    ######## Active Learning ############\n    # Fit the model\n    AL_model.fit(X_train, y_train)\n    AL_models.append(deepcopy(AL_model))\n    \n    # Query a point\n    query_idx = get_query_idx()\n    AL_inds.append(query_idx)\n    \n    # Add it to the train data\n    X_train = np.concatenate([X_train, X_pool[query_idx:query_idx+1, :]], axis=0)\n    y_train = np.concatenate([y_train, y_pool[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool = np.delete(X_pool, query_idx, axis=0)\n    y_pool = np.delete(y_pool, query_idx, axis=0)\n    \n    ######## Random Sampling ############\n     # Fit the model\n    random_model.fit(X_train_rand, y_train_rand)\n    random_models.append(deepcopy(random_model))\n    \n    # Query a point\n    query_idx = np.random.choice(len(X_pool))\n    random_inds.append(query_idx)\n    # Add it to the train data\n    X_train_rand = np.concatenate([X_train_rand, X_pool_rand[query_idx:query_idx+1, :]], axis=0)\n    y_train_rand = np.concatenate([y_train_rand, y_pool_rand[query_idx:query_idx+1]], axis=0)\n    \n    # Remove it from the pool data\n    X_pool_rand = np.delete(X_pool_rand, query_idx, axis=0)\n    y_pool_rand = np.delete(y_pool_rand, query_idx, axis=0)\n\niteration 99\n\n\n\n\nPlot accuracy\n\nrandom_scores = []\nAL_scores = []\nfor iteration in range(AL_iters):\n    clear_output(wait=True)\n    print(\"iteration\", iteration)\n    AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test)))\n    random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test)))\n    \nplt.plot(AL_scores, label='Active Learning');\nplt.plot(random_scores, label='Random Sampling');\nplt.legend();\nplt.xlabel('Iterations');\nplt.ylabel('Accuracy\\n(Higher is better)');\n\niteration 99\n\n\n\n\n\n\n\n\n\n\n\nPlot decision boundary\n\ndef update(i):\n    for each in ax:\n        each.cla()\n        \n    AL_grid_preds = AL_models[i].predict(grid_X)\n    random_grid_preds = random_models[i].predict(grid_X)\n    \n    # Active learning\n    ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label='initial_train', alpha=0.2)\n    ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], \n                  c=y_train[n_train:n_train+i], label='new_points')\n    ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[0].set_title('New points')\n    \n    ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[1].set_title('Test points');\n    ax[0].text(locs[0],locs[1],'Active Learning')\n    \n    # Random sampling\n    ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label='initial_train', alpha=0.2)\n    ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], \n                  c=y_train_rand[n_train:n_train+i], label='new_points')\n    ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[2].set_title('New points')\n    \n    ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label='test_set')\n    ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2);\n    ax[3].set_title('Test points');\n    ax[2].text(locs[0],locs[1],'Random Sampling');\n\n\nlocs = (2.7, 4)\nfig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True)\nax = ax.ravel()\nn_train = X_train.shape[0]-AL_iters\n\nanim = FuncAnimation(fig, func=update, frames=range(100))\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html",
    "href": "posts/2022-06-10-jaxoptimizers.html",
    "title": "JAX Optimizers",
    "section": "",
    "text": "%%capture\n%pip install -U jax\nimport jax\nimport jax.numpy as jnp\ntry:\n  import jaxopt\nexcept ModuleNotFoundError:\n  %pip install -qq jaxopt\n  import jaxopt\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  %pip install -qq optax\n  import optax\n\nimport tensorflow_probability.substrates.jax as tfp"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "href": "posts/2022-06-10-jaxoptimizers.html#loss-function",
    "title": "JAX Optimizers",
    "section": "Loss function",
    "text": "Loss function\n\ndef loss_fun(x, a):\n  return (((x['param1'] - a) + (x['param2'] - (a+1)))**2).sum()"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "href": "posts/2022-06-10-jaxoptimizers.html#initial-parameters",
    "title": "JAX Optimizers",
    "section": "Initial parameters",
    "text": "Initial parameters\n\nN = 3\ninit_params = lambda: {'param1': jnp.zeros(N), 'param2': jnp.ones(N)}\na = 2.0"
  },
  {
    "objectID": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "href": "posts/2022-06-10-jaxoptimizers.html#optimizers",
    "title": "JAX Optimizers",
    "section": "Optimizers",
    "text": "Optimizers\n\nJaxOpt ScipyMinimize\n\n%%time\nsolver = jaxopt.ScipyMinimize('L-BFGS-B', fun=loss_fun)\nans = solver.run(init_params(), a)\nprint(ans)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nOptStep(params={'param1': DeviceArray([1.9999999, 1.9999999, 1.9999999], dtype=float32), 'param2': DeviceArray([3., 3., 3.], dtype=float32)}, state=ScipyMinimizeInfo(fun_val=DeviceArray(4.2632564e-14, dtype=float32), success=True, status=0, iter_num=2))\nCPU times: user 78.3 ms, sys: 18.5 ms, total: 96.8 ms\nWall time: 95.8 ms\n\n\n\nPros\n\nTwo lines of code will do it all.\n\n\n\nCons\n\nIt only returns the final parameters and final loss. No option to retrive in-between loss values.\n\n\n\n\nOptax\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nvalue_and_grad_fun = jax.jit(jax.value_and_grad(loss_fun, argnums=0))\nparams = init_params()\nstate = optimizer.init(params)\n\nfor _ in range(100):\n  loss_value, gradients = value_and_grad_fun(params, a)\n  updates, state = optimizer.update(gradients, state)\n  params = optax.apply_updates(params, updates)\n\nprint(params)\n\n{'param1': DeviceArray([2.0084236, 2.0084236, 2.0084236], dtype=float32), 'param2': DeviceArray([3.0084238, 3.0084238, 3.0084238], dtype=float32)}\nCPU times: user 3.09 s, sys: 63.4 ms, total: 3.16 s\nWall time: 4.2 s\n\n\n\nPros:\n\nFull control in user’s hand. We can save intermediate loss values.\n\n\n\nCons:\n\nIts code is verbose, similar to PyTorch optimizers.\n\n\n\n\nJaxopt OptaxSolver\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nsolver = jaxopt.OptaxSolver(loss_fun, optimizer, maxiter=100)\nans = solver.run(init_params(), a)\nprint(ans)\n\nOptStep(params={'param1': DeviceArray([2.008423, 2.008423, 2.008423], dtype=float32), 'param2': DeviceArray([3.008423, 3.008423, 3.008423], dtype=float32)}, state=OptaxState(iter_num=DeviceArray(100, dtype=int32, weak_type=True), value=DeviceArray(0.00113989, dtype=float32), error=DeviceArray(0.09549397, dtype=float32), internal_state=(ScaleByAdamState(count=DeviceArray(100, dtype=int32), mu={'param1': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32), 'param2': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32)}, nu={'param1': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32), 'param2': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32)}), EmptyState()), aux=None))\nCPU times: user 719 ms, sys: 13.4 ms, total: 732 ms\nWall time: 1.09 s\n\n\n\nPros:\n\nLess lines of code.\nApplies lax.scan internally to make it fast [reference].\n\n\n\nCons:\n\nNot able to get in-between state/loss values\n\n\n\n\ntfp math minimize\n\n%%time\noptimizer = optax.adam(learning_rate=0.1)\nparams, losses = tfp.math.minimize_stateless(loss_fun, (init_params(), a), num_steps=1000, optimizer=optimizer)\nprint(params)\nprint(losses[:5])\n\n({'param1': DeviceArray([1.0000008, 1.0000008, 1.0000008], dtype=float32), 'param2': DeviceArray([1.9999989, 1.9999989, 1.9999989], dtype=float32)}, DeviceArray(0.9999999, dtype=float32))\n[48.       38.88006  30.751791 23.626852 17.507807]\nCPU times: user 880 ms, sys: 15.2 ms, total: 895 ms\nWall time: 1.53 s\n\n\n\nPros:\n\nOne line of code to optimize the function and return in-between losses.\n\n\n\nCons:\n\nBy default, it optimizes all arguments passed to the loss function. In above example, we can not control if a should be optimized or not. I have raised an issue here for this problem."
  },
  {
    "objectID": "posts/2022-10-18-kfac-laplace.html",
    "href": "posts/2022-10-18-kfac-laplace.html",
    "title": "Train NN with KFAC-Laplace in JAX",
    "section": "",
    "text": "from math import prod\nfrom functools import partial\nfrom time import time\n\nimport blackjax\nimport flax.linen as nn\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.tree_util as jtu\nimport jax.numpy as jnp\n# jnp.set_printoptions(linewidth=2000)\n\nimport optax\nfrom tqdm import trange\n\nimport arviz as az\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\njax.config.update(\"jax_enable_x64\", False)\n\n%reload_ext watermark\n\nSome helper functions:\n\njitter = 1e-6\n\ndef get_shapes(params):\n    return jtu.tree_map(lambda x:x.shape, params)\n\ndef svd_inverse(matrix):\n    U, S, V = jnp.linalg.svd(matrix+jnp.eye(matrix.shape[0])*jitter)\n    \n    return V.T/S@U.T\n\n\nDataset\nWe take XOR dataset to begin with:\n\nX = jnp.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = jnp.array([0, 1, 1, 0])\n\nX.shape, y.shape\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n((4, 2), (4,))\n\n\n\n\nNN model\n\nclass MLP(nn.Module):\n    features: []\n\n    @nn.compact\n    def __call__(self, x):\n        for n_features in self.features[:-1]:\n            x = nn.Dense(n_features, kernel_init=jax.nn.initializers.glorot_normal(), bias_init=jax.nn.initializers.normal())(x)\n            x = nn.relu(x)\n        \n        x = nn.Dense(features[-1])(x)\n        return x.ravel()\n\nLet us initialize the weights of NN and inspect shapes of the parameters:\n\nfeatures = [2, 1]\nkey = jax.random.PRNGKey(0)\n\nmodel = MLP(features)\nparams = model.init(key, X).unfreeze()\n\nget_shapes(params)\n\n{'params': {'Dense_0': {'bias': (2,), 'kernel': (2, 2)},\n  'Dense_1': {'bias': (1,), 'kernel': (2, 1)}}}\n\n\n\nmodel.apply(params, X)\n\nDeviceArray([ 0.00687164, -0.01380461,  0.        ,  0.        ], dtype=float32)\n\n\n\n\nNegative Log Joint\n\nnoise_var = 0.1\n\ndef neg_log_joint(params):\n    y_pred = model.apply(params, X)\n    flat_params = ravel_pytree(params)[0]\n    log_prior = jax.scipy.stats.norm.logpdf(flat_params).sum()\n    log_likelihood = jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=noise_var).sum()\n    \n    return -(log_prior + log_likelihood)\n\nTesting if it works:\n\nneg_log_joint(params)\n\nDeviceArray(105.03511, dtype=float32)\n\n\n\n\nFind MAP\n\nkey = jax.random.PRNGKey(0)\nparams = model.init(key, X).unfreeze()\nn_iters = 1000\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(neg_log_joint))\nopt = optax.adam(0.01)\nstate = opt.init(params)\n\ndef one_step(params_and_state, xs):\n    params, state = params_and_state\n    loss, grads = value_and_grad_fn(params)\n    updates, state = opt.update(grads, state)\n    params = optax.apply_updates(params, updates)\n    return (params, state), loss\n    \n(params, state), losses = jax.lax.scan(one_step, init=(params, state), xs=None, length=n_iters)\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\ny_map = model.apply(params, X)\ny_map\n\nDeviceArray([0.01383345, 0.98666817, 0.98563665, 0.01507111], dtype=float32)\n\n\n\nx = jnp.linspace(-0.1,1.1,100)\nX1, X2 = jnp.meshgrid(x, x)\n\ndef predict_fn(x1, x2):\n    return model.apply(params, jnp.array([x1,x2]).reshape(1,2))\n\npredict_fn_vec = jax.jit(jax.vmap(jax.vmap(predict_fn)))\n\nZ = predict_fn_vec(X1, X2).squeeze()\n\nplt.contourf(X1, X2, Z)\nplt.colorbar();\n\n\n\n\n\n\n\n\n\n\nFull Hessian Laplace\n\nflat_params, unravel_fn = ravel_pytree(params)\n\ndef neg_log_joint_flat(flat_params):\n    return neg_log_joint(unravel_fn(flat_params))\n\nH = jax.hessian(neg_log_joint_flat)(flat_params)\n\nsns.heatmap(H);\n\n\n\n\n\n\n\n\n\nposterior_cov = svd_inverse(H)\n\nsns.heatmap(posterior_cov);\n\n\n\n\n\n\n\n\nNote that we can sample parameters from the posterior and revert them to correct structure with the unravel_fn. Here is a class to do it all:\n\nclass FullHessianLaplace:\n    def __init__(self, map_params, model):\n        flat_params, self.unravel_fn = ravel_pytree(map_params)\n\n        def neg_log_joint_flat(flat_params):\n            params = unravel_fn(flat_params)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(flat_params)\n        \n        self.mean = flat_params\n        self.cov = svd_inverse(self.H)\n        self.model = model\n\n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample(self, seed):\n        sample = jax.random.multivariate_normal(seed, mean=self.mean, cov=self.cov)\n        return self.unravel_fn(sample)\n    \n    def sample(self, seed, shape):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nposterior = FullHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 100000\ny_pred_full = posterior.predict(X, seed=seed, shape=(n_samples,))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i]);\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_pred_mean={y_pred_full[:, i].mean():.3f}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nKFAC-Laplace\nWe need to invert partial Hessians to do KFAC-Laplace. We can use tree_flatten with ravel_pytree to ease the workflow. We need to: 1. pick up partial Hessians in pure matrix form to be able to invert them. 2. Create layer-wise distributions and sample them. These samples will be 1d arrays. 3. We need to convert those 1d arrays to params dictionary form so that we can plug it into the flax model and get posterior predictions.\nFirst we need to segregate the parameters layer-wise. We will use is_leaf condition to stop traversing the parameter PyTree at a perticular depth. See how it is different from vanilla tree_flatten:\n\nflat_params, tree_def = jtu.tree_flatten(params)\ndisplay(flat_params, tree_def)\n\n[DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n DeviceArray([[ 0.8275324 , -0.8314813 ],\n              [-0.8276633 ,  0.83254045]], dtype=float32),\n DeviceArray([0.01351773], dtype=float32),\n DeviceArray([[1.1750739],\n              [1.1685134]], dtype=float32)]\n\n\nPyTreeDef({'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}})\n\n\n\nis_leaf = lambda param: 'bias' in param\nlayers, tree_def = jtu.tree_flatten(params, is_leaf=is_leaf)\ndisplay(layers, tree_def)\n\n[{'bias': DeviceArray([-0.00024913,  0.00027019], dtype=float32),\n  'kernel': DeviceArray([[ 0.8275324 , -0.8314813 ],\n               [-0.8276633 ,  0.83254045]], dtype=float32)},\n {'bias': DeviceArray([0.01351773], dtype=float32),\n  'kernel': DeviceArray([[1.1750739],\n               [1.1685134]], dtype=float32)}]\n\n\nPyTreeDef({'params': {'Dense_0': *, 'Dense_1': *}})\n\n\nThe difference is clearly evident. Now, we need to flatten the inner dictionaries to get 1d arrays.\n\nflat_params = list(map(lambda x: ravel_pytree(x)[0], layers))\nunravel_fn_list = list(map(lambda x: ravel_pytree(x)[1], layers))\ndisplay(flat_params, unravel_fn_list)\n\n[DeviceArray([-2.4912864e-04,  2.7019347e-04,  8.2753241e-01,\n              -8.3148128e-01, -8.2766330e-01,  8.3254045e-01],            dtype=float32),\n DeviceArray([0.01351773, 1.1750739 , 1.1685134 ], dtype=float32)]\n\n\n[&lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;,\n &lt;function jax._src.flatten_util.ravel_pytree.&lt;locals&gt;.&lt;lambda&gt;(flat)&gt;]\n\n\n\ndef modified_neg_log_joint_fn(flat_params):\n    layers = jtu.tree_map(lambda unravel_fn, flat_param: unravel_fn(flat_param), unravel_fn_list, flat_params)\n    params = tree_def.unflatten(layers)\n    return neg_log_joint(params)\n\nfull_hessian = jax.hessian(modified_neg_log_joint_fn)(flat_params)\n\n# Pick diagonal entries from the Hessian\nuseful_hessians = [full_hessian[i][i] for i in range(len(full_hessian))]\nuseful_hessians\n\n[DeviceArray([[139.07985,   0.     , 138.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 410.62708,   0.     , 136.54236,   0.     ,\n               273.08472],\n              [138.07985,   0.     , 139.07985,   0.     ,   0.     ,\n                 0.     ],\n              [  0.     , 136.54236,   0.     , 137.54236,   0.     ,\n               136.54236],\n              [  0.     ,   0.     ,   0.     ,   0.     ,   1.     ,\n                 0.     ],\n              [  0.     , 273.08472,   0.     , 136.54236,   0.     ,\n               274.08472]], dtype=float32),\n DeviceArray([[400.99997,  82.72832,  83.44101],\n              [ 82.72832,  69.43975,   0.     ],\n              [ 83.44101,   0.     ,  70.35754]], dtype=float32)]\n\n\nEach entry in above list corresponds to layer-wise hessian matrices. Now, we need to create layer-wise distributions, sample from them and reconstruct params using the similar tricks we used above:\n\nclass KFACHessianLaplace:\n    def __init__(self, map_params, model):\n        self.model = model\n        layers, self.tree_def = jtu.tree_flatten(map_params, is_leaf=lambda x: 'bias' in x)\n        flat_layers = [ravel_pytree(layer) for layer in layers]\n        self.means = list(map(lambda x: x[0], flat_layers))\n        self.unravel_fn_list = list(map(lambda x: x[1], flat_layers))\n\n        def neg_log_joint_flat(flat_params):\n            flat_layers = [self.unravel_fn_list[i](flat_params[i]) for i in range(len(flat_params))]\n            params = self.tree_def.unflatten(flat_layers)\n            return neg_log_joint(params)\n\n        self.H = jax.hessian(neg_log_joint_flat)(self.means)\n        self.useful_H = [self.H[i][i] for i in range(len(self.H))]\n        \n        self.covs = [svd_inverse(matrix) for matrix in self.useful_H]\n        \n    def _vectorize(self, f, seed, shape, f_kwargs={}):\n        length = prod(shape)\n        seeds = jax.random.split(seed, num=length).reshape(shape+(2,))\n        \n        sample_fn = partial(f, **f_kwargs)\n        for _ in shape:\n            sample_fn = jax.vmap(sample_fn)\n        \n        return sample_fn(seed=seeds)\n    \n    def _sample_partial(self, seed, unravel_fn, mean, cov):\n        sample = jax.random.multivariate_normal(seed, mean=mean, cov=cov)\n        return unravel_fn(sample)\n    \n    def _sample(self, seed):\n        seeds = [seed for seed in jax.random.split(seed, num=len(self.means))]\n        flat_sample = jtu.tree_map(self._sample_partial, seeds, self.unravel_fn_list, self.means, self.covs)\n        sample = self.tree_def.unflatten(flat_sample)\n        return sample\n    \n    def sample(self, seed, n_samples=1):\n        return self._vectorize(self._sample, seed, shape)\n    \n    def _predict(self, X, seed):\n        sample = self._sample(seed)\n        return self.model.apply(sample, X)\n    \n    def predict(self, X, seed, shape):\n        return self._vectorize(self._predict, seed, shape, {'X': X})\n\n\nEstimating predictive posterior\n\nkfac_posterior = KFACHessianLaplace(params, model)\n\nseed = jax.random.PRNGKey(1)\nn_samples = 1000000\ny_pred_kfac = kfac_posterior.predict(X, seed=seed, shape=(n_samples, ))\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that KFAC is approximating the trend of Full Hessian Laplace. We can visualize the Covariance matrices as below.\n\nfig, ax = plt.subplots(1,2,figsize=(18,5))\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\n\n\n\n\n\n\n\n\n\n\nComparison with MCMC\nInspired from a blackjax docs example.\n\nkey = jax.random.PRNGKey(0)\nwarmup_key, inference_key = jax.random.split(key, 2)\nnum_warmup = 5000\nnum_samples = n_samples\n\ninitial_position = model.init(key, X)\ndef logprob(params): \n    return -neg_log_joint(params)\n\ndef inference_loop(rng_key, kernel, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, _ = kernel(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n\ninit = time()\nadapt = blackjax.window_adaptation(blackjax.nuts, logprob, num_warmup)\nfinal_state, kernel, _ = adapt.run(warmup_key, initial_position)\nstates = inference_loop(inference_key, kernel, final_state, num_samples)\nsamples = states.position.unfreeze()\nprint(f\"Sampled {n_samples} samples in {time()-init:.2f} seconds\")\n\nSampled 1000000 samples in 27.85 seconds\n\n\n\ny_pred_mcmc = jax.vmap(model.apply, in_axes=(0, None))(samples, X)\n\nulim = 5\nllim = -5\n\nfig, ax = plt.subplots(2,2,figsize=(12,4))\nax=ax.ravel()\nfor i in range(len(y)):\n    az.plot_dist(y_pred_full[:, i], ax=ax[i], label='full', color='r')\n    az.plot_dist(y_pred_kfac[:, i], ax=ax[i], label='kfac', color='b')\n    az.plot_dist(y_pred_mcmc[:, i], ax=ax[i], label='mcmc', color='k')\n    ax[i].grid(True)\n    ax[i].set_xticks(range(llim,ulim))\n    ax[i].set_xlim(llim, ulim)\n    ax[i].set_title(f\"X={X[i]}, y_map={y_map[i]:.3f}\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1,3,figsize=(18,5))\nfig.subplots_adjust(wspace=0.1)\nsns.heatmap(posterior.cov, ax=ax[0], annot=True, fmt = '.2f')\nax[0].set_title('Full')\n\nkfac_cov = posterior.cov * 0\noffset = 0\nfor cov in kfac_posterior.covs:\n    length = cov.shape[0]\n    kfac_cov = kfac_cov.at[offset:offset+length, offset:offset+length].set(cov)\n    offset += length\n\nsns.heatmap(kfac_cov, ax=ax[1], annot=True, fmt = '.2f')\nax[1].set_title('KFAC');\n\nmcmc_cov = jnp.cov(jax.vmap(lambda x: ravel_pytree(x)[0])(samples).T)\n\nsns.heatmap(mcmc_cov, ax=ax[2], annot=True, fmt = '.2f')\nax[2].set_title('MCMC');\n\n\n\n\n\n\n\n\n\n\nLibrary versions\n\n%watermark --iversions\n\nflax      : 0.6.1\nblackjax  : 0.8.2\noptax     : 0.1.3\nmatplotlib: 3.5.1\njax       : 0.3.23\narviz     : 0.12.1\nseaborn   : 0.11.2\njson      : 2.0.9"
  },
  {
    "objectID": "posts/Torch-DataLoaders.html",
    "href": "posts/Torch-DataLoaders.html",
    "title": "Data Handling for Large Scale ML",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport torch\nimport torch.nn as nn\nfrom numcodecs import GZip, Zstd, Blosc\n\nfrom time import time, sleep\nfrom tqdm import tqdm\nfrom glob import glob\nfrom os.path import join\nfrom torch.utils.data import DataLoader, Dataset\nfrom joblib import Parallel, delayed\nimport xarray as xr\nimport numpy as np\n\nfrom torchvision.models import vit_b_16\nfrom astra.torch.models import ViTClassifier\nfrom astra.torch.utils import train_fn"
  },
  {
    "objectID": "posts/Torch-DataLoaders.html#imports",
    "href": "posts/Torch-DataLoaders.html#imports",
    "title": "Data Handling for Large Scale ML",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport torch\nimport torch.nn as nn\nfrom numcodecs import GZip, Zstd, Blosc\n\nfrom time import time, sleep\nfrom tqdm import tqdm\nfrom glob import glob\nfrom os.path import join\nfrom torch.utils.data import DataLoader, Dataset\nfrom joblib import Parallel, delayed\nimport xarray as xr\nimport numpy as np\n\nfrom torchvision.models import vit_b_16\nfrom astra.torch.models import ViTClassifier\nfrom astra.torch.utils import train_fn"
  },
  {
    "objectID": "posts/Torch-DataLoaders.html#is-.nc-better-than-zarr",
    "href": "posts/Torch-DataLoaders.html#is-.nc-better-than-zarr",
    "title": "Data Handling for Large Scale ML",
    "section": "Is .nc better than zarr?",
    "text": "Is .nc better than zarr?\n\nos.system(f\"du -sh {base_path}\")\n\n1.8G    /home/patel_zeel/bkdb/bangladesh_pnas_pred/team1\n\n\n0\n\n\n\nsave_path = \"/tmp/nc_check_uncompressed\"\nos.makedirs(save_path, exist_ok=True)\nfiles = []\ndef zarr_to_nc(file):\n    with xr.open_zarr(file, consolidated=False) as ds:\n        ds.to_netcdf(join(save_path, file.split(\"/\")[-1].replace(\".zarr\", \".nc\")))\n\n_ = Parallel(n_jobs=32)(delayed(zarr_to_nc)(file) for file in tqdm(glob(join(base_path, \"*.zarr\"))))\n\nos.system(f\"du -sh {save_path}\")\n\n  0%|          | 0/1501 [00:00&lt;?, ?it/s]100%|██████████| 1501/1501 [00:24&lt;00:00, 62.47it/s] \n\n\n5.3G    /tmp/nc_check_uncompressed\n\n\n0\n\n\n\nsave_path = \"/tmp/nc_check_compressed\"\nos.system(f\"rm -rf {save_path}\")\nos.makedirs(save_path, exist_ok=True)\n\nencoding = {var: {\"zlib\": True, \"complevel\": 1} for var in [\"data\"]}\n\nfiles = []\ndef zarr_to_nc(file):\n    with xr.open_zarr(file, consolidated=False) as ds:\n        ds.to_netcdf(join(save_path, file.split(\"/\")[-1].replace(\".zarr\", \".nc\")), encoding=encoding)\n\n_ = Parallel(n_jobs=32)(delayed(zarr_to_nc)(file) for file in tqdm(glob(join(base_path, \"*.zarr\"))))\n\nos.system(f\"du -sh {save_path}\")\n\n100%|██████████| 1501/1501 [00:04&lt;00:00, 311.18it/s]\n\n\n1.8G    /tmp/nc_check_compressed\n\n\n0\n\n\n\nclass XarrayDatasetWithNC(Dataset):\n    def __init__(self, path, max_files):\n        self.base_path = path\n        self.all_files = glob(join(path, \"*.nc\"))[:max_files]\n        self.all_files.sort()\n        self.all_ds = [xr.open_dataset(file) for file in tqdm(self.all_files)]\n        self.lat_lags = [-2, -1, 0, 1, 2]\n        self.lon_lags = [-2, -1, 0, 1, 2]\n        \n    def __len__(self):\n        return len(self.all_files) * 25\n    \n    def __getitem__(self, idx):\n        file_idx = idx // 25\n        local_idx = idx % 25\n        lat_lag = self.lat_lags[local_idx // 5]\n        lon_lag = self.lon_lags[local_idx % 5]\n        \n        ds = self.all_ds[file_idx]\n        img =  ds.isel(lat_lag=lat_lag, lon_lag=lon_lag)['data'].values\n        return torch.tensor(np.einsum(\"hwc-&gt;chw\", img).astype(np.float32) / 255)\n\n\nnc_path = \"/tmp/nc_check_compressed\"\n\n\nbatch_size = 128\nnum_workers = 32\n\ndataset = XarrayDatasetWithNC(nc_path, max_files=max_files)\nprocess_it(dataset, batch_size, num_workers)\n\n100%|██████████| 500/500 [00:02&lt;00:00, 246.27it/s]\nTime: 0.7414: 100%|██████████| 98/98 [01:25&lt;00:00,  1.15it/s]\n\n\nAverage Iteration Processing Time: 0.8260 +- 0.0530\nTotal time for all iterations: 80.9527\nTotal Wall Time per iteration: 0.8725\nTotal Wall Time: 85.5034"
  },
  {
    "objectID": "posts/Torch-DataLoaders.html#additional-experiments",
    "href": "posts/Torch-DataLoaders.html#additional-experiments",
    "title": "Data Handling for Large Scale ML",
    "section": "Additional experiments",
    "text": "Additional experiments\n\nn_images = 60000\nt = 84.9131/500/25 * n_images\nprint(f\"Time to process {n_images} images: \", t/60, \"minutes\")\n\nTime to process 60000 images:  6.793048000000001 minutes\n\n\n\nfiles = glob(join(base_path, \"*.zarr\"))\ndata_tensors = []\nfor file in tqdm(files):\n    with xr.open_zarr(file, consolidated=False) as ds:\n        # print(ds['data'].values.reshape(-1, 224, 224, 3))\n        data_tensors.append(torch.tensor(np.einsum(\"nhwc-&gt;nchw\", ds['data'].values.reshape(-1, 224, 224, 3)).astype(np.float16) / 255))\n\n100%|██████████| 1501/1501 [02:44&lt;00:00,  9.13it/s]\n\n\n\nall_in_one = torch.concat(data_tensors, dim=0)\nall_in_one.shape\n\ntorch.Size([37525, 3, 224, 224])\n\n\n\nall_in_one = all_in_one.to('cuda')"
  },
  {
    "objectID": "posts/Torch-DataLoaders.html#insights",
    "href": "posts/Torch-DataLoaders.html#insights",
    "title": "Data Handling for Large Scale ML",
    "section": "Insights",
    "text": "Insights\n\nGPU Memory consumption is 17776MiB / 81920MiB for batch size 128 for ViT model\nUploading torch.Size([37525, 3, 224, 224]) of float32 data to GPU takes 22054MiB / 81920MiB of GPU Memory. Same data with float16 takes 11202MiB / 81920MiB of GPU Memory.\nIt seems .nc or .zarr are not making much difference in terms of time and/or memory."
  },
  {
    "objectID": "posts/GNNs_and_GPs.html",
    "href": "posts/GNNs_and_GPs.html",
    "title": "GNNs and GPs",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport regdata as rd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\nx_train, y_train, x_test = rd.Step().get_data()\ny_train = y_train.reshape(-1, 1)\nx_test = x_test * 1.5\nprint(x_train.shape, y_train.shape, x_test.shape)\n\nplt.scatter(x_train, y_train, label='train');\n\n(50, 1) (50, 1) (100, 1)\n\n\n\n\n\n\n\n\n\n\nkernel = GPy.kern.RBF(1, variance=1, lengthscale=1)\nmodel = GPy.models.GPRegression(x_train, y_train.reshape(-1, 1), kernel)\nmodel.Gaussian_noise.variance = 0.1\n\ny_pred_gp, y_var = model.predict(x_test)\n\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_test, y_pred_gp, label='pred');\n\n\n\n\n\n\n\n\n\nclass GCN_Forward(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = self.fc(x)\n        x = torch.matmul(A, x)\n        return x\n    \nclass GCN_Reverse(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.fc = nn.Linear(in_features, out_features)\n    \n    def forward(self, x, A):\n        x = torch.matmul(A, x)\n        x = self.fc(x)\n        return x\n\nclass NN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        self.features = features\n        \n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', nn.Linear(in_features, out_features))\n            \n        self.last_layer = nn.Linear(features[-1], 1)\n        \n    def forward(self, x, A):\n        for i in range(len(self.features) - 1):\n            if isinstance(getattr(self, f'layer_{i}'), GCN_Forward):\n                x = getattr(self, f'layer_{i}')(x, A)\n            else:\n                x = getattr(self, f'layer_{i}')(x)\n            x = nn.functional.gelu(x)\n            \n        x = self.last_layer(x)\n        return x\n\nclass GCN(NN):\n    def __init__(self, features):\n        super().__init__(features)\n        for i, (in_features, out_features) in enumerate(zip(features[:-1], features[1:])):\n            setattr(self, f'layer_{i}', GCN_Forward(in_features, out_features))\n\n\nA = torch.tensor(kernel.K(x_train, x_train)).float()\n# A.fill_diagonal_(0)\nA = A / A.sum(dim=0, keepdim=True)\n# A.fill_diagonal_(1)\n\nnum_epochs = 500\nfeatures = [1, 1024]\n\ngcn_model = GCN(features=features)\nnn_model = NN(features=features)\n\ngcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\nnn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.01)\n\ncriterion = nn.MSELoss()\n\nx_train_torch = torch.from_numpy(x_train).float()\ny_train_torch = torch.from_numpy(y_train).float()\n\ngcn_losses = []\nnn_losses = []\nfor epoch in range(num_epochs):\n    gcn_optimizer.zero_grad()\n    nn_optimizer.zero_grad()\n    \n    y_out_gcn = gcn_model(x_train_torch, A)\n    y_out_nn = nn_model(x_train_torch, A)\n    gcn_loss = criterion(y_out_gcn, y_train_torch)\n    nn_loss = criterion(y_out_nn, y_train_torch)\n    \n    gcn_loss.backward()\n    nn_loss.backward()\n    \n    gcn_losses.append(gcn_loss.item())\n    nn_losses.append(nn_loss.item())\n    \n    gcn_optimizer.step()\n    nn_optimizer.step()\n        \nplt.plot(gcn_losses, label='gcn');\nplt.plot(nn_losses, label='nn');\nplt.legend();\n\n\n\n\n\n\n\n\n\nA_test = torch.tensor(kernel.K(x_test, x_test)).float()\n# A_test.fill_diagonal_(0)\nA_test = A_test / A_test.sum(dim=0, keepdim=True)\n# A_test.fill_diagonal_(1)\n\ny_pred_nn = nn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\ny_pred_gcn = gcn_model(torch.from_numpy(x_test).float(), A_test).detach().numpy()\n\nplt.figure(figsize=(10, 6))\nplt.scatter(x_train, y_train, label='train');\nplt.plot(x_train, y_out_gcn.detach().numpy(), label='pred GCN train');\nplt.plot(x_train, y_out_nn.detach().numpy(), label='pred NN train');\nplt.plot(x_test, y_pred_gp, label='pred GP', linestyle='--');\nplt.plot(x_test, y_pred_nn, label='pred NN');\nplt.plot(x_test, y_pred_gcn, label='pred GCN');\nplt.ylim(-3, 3);\nplt.legend();"
  },
  {
    "objectID": "posts/air-quality-google-.html",
    "href": "posts/air-quality-google-.html",
    "title": "Google Air Quality Data",
    "section": "",
    "text": "import requests\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport dask\nfrom dask.distributed import Client, LocalCluster\n\nif \"key\" in locals():\n    pass\nelse:\n    key = input(\"Enter your key: \")\n\nurl = f\"https://airquality.googleapis.com/v1/history:lookup?key={key}\"\nurl\n\n'https://airquality.googleapis.com/v1/history:lookup?key=AIzaSyA9ytSec31of_INkpuB3TZ6vLR1nzme9iQ'\nif \"client\" in locals():\n    print(client)\nelse:\n    cluster = LocalCluster(n_workers=54, threads_per_worker=1)\n    client = Client(cluster)\n    print(client)\n\n&lt;Client: 'tcp://127.0.0.1:36239' processes=54 threads=54, memory=503.73 GiB&gt;\npayload = {\n    \"hours\": 24 * 30,\n    \"pageSize\": 168,\n    \"pageToken\": \"\",\n    \"location\": {\"latitude\": 28.636429, \"longitude\": 77.201067},\n    \"extraComputations\": [\"POLLUTANT_CONCENTRATION\", \"LOCAL_AQI\"],\n}\n\nheaders = {\"Content-Type\": \"application/json\"}\n\nresponse = requests.post(url, json=payload, headers=headers)\n\nres = response.json()\n24 * 30\n\n720\nlen(res[\"hoursInfo\"])\n\n168\nts = []\ncodes = []\npm25 = []\ndf = pd.DataFrame(columns=[\"timestamp\", \"value\", \"code\"])\n\nfor each in res[\"hoursInfo\"]:\n    ts.append(each[\"dateTime\"])\n    codes.append(each[\"pollutants\"][4][\"code\"])\n    pm25.append(each[\"pollutants\"][4][\"concentration\"][\"value\"])\n\ndf[\"timestamp\"] = ts\ndf[\"value\"] = pm25\ndf[\"code\"] = codes\ndf[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\ndf.head(20)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[5], line 9\n      7     ts.append(each[\"dateTime\"])\n      8     codes.append(each[\"pollutants\"][4][\"code\"])\n----&gt; 9     pm25.append(each[\"pollutants\"][4][\"concentration\"][\"value\"])\n     11 df[\"timestamp\"] = ts\n     12 df[\"value\"] = pm25\n\nKeyError: 'concentration'\nsensor_data = pd.read_excel(\n    \"/home/patel_zeel/blog/posts/site_12220230831125207.xlsx\", skiprows=16\n)\nsensor_data[\"From Date\"] = pd.to_datetime(\n    sensor_data[\"From Date\"], format=\"%d-%m-%Y %H:%M\"\n)\nsensor_data[\"To Date\"] = pd.to_datetime(sensor_data[\"To Date\"], format=\"%d-%m-%Y %H:%M\")\nsensor_data[\"mean_time\"] = sensor_data[[\"From Date\", \"To Date\"]].mean(axis=1)\nsensor_data[\"utc_time\"] = sensor_data[\"mean_time\"] - pd.Timedelta(hours=5, minutes=30)\n\nfig, ax = plt.subplots(figsize=(15, 4))\nsensor_data.plot(x=\"utc_time\", y=\"PM2.5\", ax=ax, label=\"sensor\")\ndf.plot(x=\"timestamp\", y=\"value\", ax=ax, label=\"google\")"
  },
  {
    "objectID": "posts/air-quality-google-.html#request-at-a-grid.",
    "href": "posts/air-quality-google-.html#request-at-a-grid.",
    "title": "Google Air Quality Data",
    "section": "Request at a grid.",
    "text": "Request at a grid.\n\nfrom aqmsp_data.data import load_camx\n\ncamx = load_camx(years=2022, months=1, days=1, variables=\"P25\")\ncamx\n\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\nchunk and silence this warning, set the option\n    &gt;&gt;&gt; with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n    ...     array[indexer]\n\nTo avoid creating the large chunks, set the option\n    &gt;&gt;&gt; with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n    ...     array[indexer]\n  return self.array[key]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (time: 24, latitude: 80, longitude: 80)\nCoordinates:\n  * longitude  (longitude) float64 76.85 76.86 76.87 76.88 ... 77.62 77.63 77.64\n  * latitude   (latitude) float64 28.2 28.21 28.22 28.23 ... 28.97 28.98 28.99\n  * time       (time) datetime64[ns] 2022-01-01T00:30:00 ... 2022-01-01T23:30:00\nData variables:\n    P25        (time, latitude, longitude) float32 dask.array&lt;chunksize=(24, 80, 80), meta=np.ndarray&gt;\nAttributes: (12/34)\n    CDATE:          2023126\n    CTIME:          95909\n    EXEC_ID:        ????????????????                                         ...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    FTYPE:          1\n    GDNAM:          ????????????????\n    ...             ...\n    XCELL:          0.009999999776482582\n    XCENT:          0.0\n    XORIG:          76.8499984741211\n    YCELL:          0.009999999776482582\n    YCENT:          0.0\n    YORIG:          28.200000762939453xarray.DatasetDimensions:time: 24latitude: 80longitude: 80Coordinates: (3)longitude(longitude)float6476.85 76.86 76.87 ... 77.63 77.64array([76.85, 76.86, 76.87, 76.88, 76.89, 76.9 , 76.91, 76.92, 76.93, 76.94,\n       76.95, 76.96, 76.97, 76.98, 76.99, 77.  , 77.01, 77.02, 77.03, 77.04,\n       77.05, 77.06, 77.07, 77.08, 77.09, 77.1 , 77.11, 77.12, 77.13, 77.14,\n       77.15, 77.16, 77.17, 77.18, 77.19, 77.2 , 77.21, 77.22, 77.23, 77.24,\n       77.25, 77.26, 77.27, 77.28, 77.29, 77.3 , 77.31, 77.32, 77.33, 77.34,\n       77.35, 77.36, 77.37, 77.38, 77.39, 77.4 , 77.41, 77.42, 77.43, 77.44,\n       77.45, 77.46, 77.47, 77.48, 77.49, 77.5 , 77.51, 77.52, 77.53, 77.54,\n       77.55, 77.56, 77.57, 77.58, 77.59, 77.6 , 77.61, 77.62, 77.63, 77.64])latitude(latitude)float6428.2 28.21 28.22 ... 28.98 28.99array([28.2 , 28.21, 28.22, 28.23, 28.24, 28.25, 28.26, 28.27, 28.28, 28.29,\n       28.3 , 28.31, 28.32, 28.33, 28.34, 28.35, 28.36, 28.37, 28.38, 28.39,\n       28.4 , 28.41, 28.42, 28.43, 28.44, 28.45, 28.46, 28.47, 28.48, 28.49,\n       28.5 , 28.51, 28.52, 28.53, 28.54, 28.55, 28.56, 28.57, 28.58, 28.59,\n       28.6 , 28.61, 28.62, 28.63, 28.64, 28.65, 28.66, 28.67, 28.68, 28.69,\n       28.7 , 28.71, 28.72, 28.73, 28.74, 28.75, 28.76, 28.77, 28.78, 28.79,\n       28.8 , 28.81, 28.82, 28.83, 28.84, 28.85, 28.86, 28.87, 28.88, 28.89,\n       28.9 , 28.91, 28.92, 28.93, 28.94, 28.95, 28.96, 28.97, 28.98, 28.99])time(time)datetime64[ns]2022-01-01T00:30:00 ... 2022-01-...array(['2022-01-01T00:30:00.000000000', '2022-01-01T01:30:00.000000000',\n       '2022-01-01T02:30:00.000000000', '2022-01-01T03:30:00.000000000',\n       '2022-01-01T04:30:00.000000000', '2022-01-01T05:30:00.000000000',\n       '2022-01-01T06:30:00.000000000', '2022-01-01T07:30:00.000000000',\n       '2022-01-01T08:30:00.000000000', '2022-01-01T09:30:00.000000000',\n       '2022-01-01T10:30:00.000000000', '2022-01-01T11:30:00.000000000',\n       '2022-01-01T12:30:00.000000000', '2022-01-01T13:30:00.000000000',\n       '2022-01-01T14:30:00.000000000', '2022-01-01T15:30:00.000000000',\n       '2022-01-01T16:30:00.000000000', '2022-01-01T17:30:00.000000000',\n       '2022-01-01T18:30:00.000000000', '2022-01-01T19:30:00.000000000',\n       '2022-01-01T20:30:00.000000000', '2022-01-01T21:30:00.000000000',\n       '2022-01-01T22:30:00.000000000', '2022-01-01T23:30:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)P25(time, latitude, longitude)float32dask.array&lt;chunksize=(24, 80, 80), meta=np.ndarray&gt;long_name :FPRM            units :micrograms/m**3 var_desc :VARIABLE FPRM                                                                   \n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n600.00 kiB\n600.00 kiB\n\n\nShape\n(24, 80, 80)\n(24, 80, 80)\n\n\nDask graph\n1 chunks in 11 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                         80 80 24\n\n\n\n\nIndexes: (3)longitudePandasIndexPandasIndex(Index([            76.85,             76.86, 76.86999999999999,\n                   76.88,             76.89, 76.89999999999999,\n                   76.91, 76.91999999999999, 76.92999999999999,\n                   76.94, 76.94999999999999,             76.96,\n                   76.97, 76.97999999999999,             76.99,\n                    77.0, 77.00999999999999,             77.02,\n                   77.03, 77.03999999999999,             77.05,\n       77.05999999999999,             77.07,             77.08,\n       77.08999999999999,              77.1,             77.11,\n       77.11999999999999,             77.13,             77.14,\n       77.14999999999999,             77.16, 77.16999999999999,\n       77.17999999999999,             77.19, 77.19999999999999,\n                   77.21,             77.22, 77.22999999999999,\n                   77.24,             77.25, 77.25999999999999,\n                   77.27,             77.28, 77.28999999999999,\n                    77.3, 77.30999999999999,             77.32,\n                   77.33, 77.33999999999999,             77.35,\n                   77.36, 77.36999999999999,             77.38,\n                   77.39, 77.39999999999999,             77.41,\n       77.41999999999999, 77.42999999999999,             77.44,\n       77.44999999999999,             77.46,             77.47,\n       77.47999999999999,             77.49,              77.5,\n       77.50999999999999,             77.52,             77.53,\n       77.53999999999999,             77.55, 77.55999999999999,\n                   77.57,             77.58, 77.58999999999999,\n                    77.6,             77.61, 77.61999999999999,\n                   77.63,             77.64],\n      dtype='float64', name='longitude'))latitudePandasIndexPandasIndex(Index([              28.2,              28.21,              28.22,\n                    28.23,              28.24,              28.25,\n       28.259999999999998,              28.27, 28.279999999999998,\n                    28.29,               28.3,              28.31,\n                    28.32,              28.33,              28.34,\n       28.349999999999998,              28.36,              28.37,\n                    28.38,              28.39,               28.4,\n                    28.41, 28.419999999999998,              28.43,\n       28.439999999999998,              28.45,              28.46,\n                    28.47,              28.48,              28.49,\n                     28.5, 28.509999999999998,              28.52,\n       28.529999999999998,              28.54,              28.55,\n                    28.56,              28.57,              28.58,\n                    28.59, 28.599999999999998,              28.61,\n                    28.62,              28.63,              28.64,\n                    28.65,              28.66, 28.669999999999998,\n                    28.68, 28.689999999999998,               28.7,\n                    28.71,              28.72,              28.73,\n                    28.74,              28.75, 28.759999999999998,\n                    28.77, 28.779999999999998,              28.79,\n                     28.8,              28.81,              28.82,\n                    28.83,              28.84, 28.849999999999998,\n                    28.86,              28.87,              28.88,\n                    28.89,               28.9,              28.91,\n       28.919999999999998,              28.93, 28.939999999999998,\n                    28.95,              28.96,              28.97,\n                    28.98,              28.99],\n      dtype='float64', name='latitude'))timePandasIndexPandasIndex(DatetimeIndex(['2022-01-01 00:30:00', '2022-01-01 01:30:00',\n               '2022-01-01 02:30:00', '2022-01-01 03:30:00',\n               '2022-01-01 04:30:00', '2022-01-01 05:30:00',\n               '2022-01-01 06:30:00', '2022-01-01 07:30:00',\n               '2022-01-01 08:30:00', '2022-01-01 09:30:00',\n               '2022-01-01 10:30:00', '2022-01-01 11:30:00',\n               '2022-01-01 12:30:00', '2022-01-01 13:30:00',\n               '2022-01-01 14:30:00', '2022-01-01 15:30:00',\n               '2022-01-01 16:30:00', '2022-01-01 17:30:00',\n               '2022-01-01 18:30:00', '2022-01-01 19:30:00',\n               '2022-01-01 20:30:00', '2022-01-01 21:30:00',\n               '2022-01-01 22:30:00', '2022-01-01 23:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (34)CDATE :2023126CTIME :95909EXEC_ID :????????????????                                                                FILEDESC :I/O API formatted CAMx AVRG output                                              FTYPE :1GDNAM :????????????????GDTYP :1HISTORY :Sun May  7 09:57:07 2023: ncrcat camxout.2023.05.06.nc camxout.2023.05.07.nc camxout.2023.05.08.nc camxout.2023.05.09.nc camxout.2023.05.10.nc camx120hr.nc\nIOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           NCO :netCDF Operators version 4.9.1 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)NCOLS :80NLAYS :1NROWS :80NTHIK :1NVARS :2P_ALP :0.0P_BET :0.0P_GAM :0.0SDATE :2023126STIME :0TSTEP :10000UPNAM :CAMXMETOU       VAR-LIST :P10             P25             VGLVLS :[0. 0.]VGTOP :-9.998999757492232e+36VGTYP :-9999WDATE :2023126WTIME :95909XCELL :0.009999999776482582XCENT :0.0XORIG :76.8499984741211YCELL :0.009999999776482582YCENT :0.0YORIG :28.200000762939453\n\n\n\nlat_grid, lon_grid = np.meshgrid(camx.latitude, camx.longitude)\nlat_lon_grid = np.vstack([lat_grid.ravel(), lon_grid.ravel()]).T\nprint(lat_lon_grid.shape)\n\nsession = requests.Session()\ndelayed_fn = dask.delayed(session.post)\nresponses = []\nfor lat, lon in tqdm(lat_lon_grid):\n    payload = {\n        \"hours\": 1,\n        \"pageSize\": 200,\n        \"pageToken\": \"\",\n        \"location\": {\"latitude\": lat, \"longitude\": lon},\n        \"extraComputations\": [\"POLLUTANT_CONCENTRATION\"],\n    }\n\n    headers = {\"Content-Type\": \"application/json\"}\n\n    response = delayed_fn(url, json=payload, headers=headers)\n    responses.append(response)\n\nall_res = dask.compute(*responses)\n\n(6400, 2)\n\n\n100%|██████████| 6400/6400 [00:01&lt;00:00, 5981.06it/s]\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[7], line 22\n     19     response = delayed_fn(url, json=payload, headers=headers)\n     20     responses.append(response)\n---&gt; 22 all_res = dask.compute(*responses)\n\nFile ~/miniconda3/lib/python3.9/site-packages/dask/base.py:666, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    663     keys.append(x.__dask_keys__())\n    664     postcomputes.append(x.__dask_postcompute__())\n--&gt; 666 results = schedule(dsk, keys, **kwargs)\n    667 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile ~/miniconda3/lib/python3.9/site-packages/dask/threaded.py:89, in get(dsk, keys, cache, num_workers, pool, **kwargs)\n     86     elif isinstance(pool, multiprocessing.pool.Pool):\n     87         pool = MultiprocessingPoolExecutor(pool)\n---&gt; 89 results = get_async(\n     90     pool.submit,\n     91     pool._max_workers,\n     92     dsk,\n     93     keys,\n     94     cache=cache,\n     95     get_id=_thread_get_id,\n     96     pack_exception=pack_exception,\n     97     **kwargs,\n     98 )\n    100 # Cleanup pools associated to dead threads\n    101 with pools_lock:\n\nFile ~/miniconda3/lib/python3.9/site-packages/dask/local.py:500, in get_async(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\n    498 while state[\"waiting\"] or state[\"ready\"] or state[\"running\"]:\n    499     fire_tasks(chunksize)\n--&gt; 500     for key, res_info, failed in queue_get(queue).result():\n    501         if failed:\n    502             exc, tb = loads(res_info)\n\nFile ~/miniconda3/lib/python3.9/site-packages/dask/local.py:137, in queue_get(q)\n    136 def queue_get(q):\n--&gt; 137     return q.get()\n\nFile ~/miniconda3/lib/python3.9/queue.py:171, in Queue.get(self, block, timeout)\n    169 elif timeout is None:\n    170     while not self._qsize():\n--&gt; 171         self.not_empty.wait()\n    172 elif timeout &lt; 0:\n    173     raise ValueError(\"'timeout' must be a non-negative number\")\n\nFile ~/miniconda3/lib/python3.9/threading.py:312, in Condition.wait(self, timeout)\n    310 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    311     if timeout is None:\n--&gt; 312         waiter.acquire()\n    313         gotit = True\n    314     else:\n\nKeyboardInterrupt: \n\n\n\n\ndfs = []\nfor res, (lat, lon) in tqdm(zip(all_res, lat_lon_grid)):\n    res = res.json()\n    df = pd.DataFrame(columns=[\"timestamp\", \"value\", \"code\"])\n    ts = []\n    codes = []\n    pm25 = []\n    for each in res[\"hoursInfo\"]:\n        ts.append(each[\"dateTime\"])\n        codes.append(each[\"pollutants\"][4][\"code\"])\n        try:\n            pm25.append(each[\"pollutants\"][4][\"concentration\"][\"value\"])\n        except KeyError:\n            pm25.append(np.nan)\n\n    df[\"timestamp\"] = ts\n    df[\"value\"] = pm25\n    df[\"code\"] = codes\n    df[\"lat\"] = lat\n    df[\"lon\"] = lon\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    dfs.append(df)\nmaster_df = pd.concat(dfs)\nassert master_df[\"code\"].nunique() == 1 and master_df[\"code\"].unique()[0] == \"pm25\"\n\nds = master_df.set_index([\"lat\", \"lon\", \"timestamp\"]).to_xarray()\nds.isel(timestamp=0)[\"value\"].plot(x=\"lon\", y=\"lat\", cmap=\"RdYlGn_r\")\n\n0it [00:00, ?it/s]6400it [00:12, 497.91it/s]"
  },
  {
    "objectID": "posts/air-quality-google-.html#appendix",
    "href": "posts/air-quality-google-.html#appendix",
    "title": "Google Air Quality Data",
    "section": "Appendix",
    "text": "Appendix\n\nimport xarray as xr\n\n\nglobal_date = \"2023-07-26\"\nds_met = xr.open_dataset(\n    f'../../sarath_auto_download/data/camxmet2d.delhi.{global_date.replace(\"-\",\"\")}.96hours.nc'\n)\nds_aq = xr.open_dataset(\n    f'../../sarath_auto_download/data/camx120hr_merged_{global_date.replace(\"-\",\"\")}.nc'\n)\nds_met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (TSTEP: 96, VAR: 14, DATE-TIME: 2, LAY: 1, ROW: 80, COL: 80)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables: (12/15)\n    TFLAG       (TSTEP, VAR, DATE-TIME) int32 ...\n    TSURF_K     (TSTEP, LAY, ROW, COL) float32 ...\n    SNOWEW_M    (TSTEP, LAY, ROW, COL) float32 ...\n    SNOWAGE_HR  (TSTEP, LAY, ROW, COL) float32 ...\n    PRATE_MMpH  (TSTEP, LAY, ROW, COL) float32 ...\n    CLOUD_OD    (TSTEP, LAY, ROW, COL) float32 ...\n    ...          ...\n    SWSFC_WpM2  (TSTEP, LAY, ROW, COL) float32 ...\n    SOLM_M3pM3  (TSTEP, LAY, ROW, COL) float32 ...\n    CLDTOP_KM   (TSTEP, LAY, ROW, COL) float32 ...\n    CAPE        (TSTEP, LAY, ROW, COL) float32 ...\n    PBL_WRF_M   (TSTEP, LAY, ROW, COL) float32 ...\n    PBL_YSU_M   (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023207\n    CTIME:          72116\n    WDATE:          2023207\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 96VAR: 14DATE-TIME: 2LAY: 1ROW: 80COL: 80Coordinates: (0)Data variables: (15)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [2688 values with dtype=int32]TSURF_K(TSTEP, LAY, ROW, COL)float32...long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                [614400 values with dtype=float32]SNOWEW_M(TSTEP, LAY, ROW, COL)float32...long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               [614400 values with dtype=float32]SNOWAGE_HR(TSTEP, LAY, ROW, COL)float32...long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             [614400 values with dtype=float32]PRATE_MMpH(TSTEP, LAY, ROW, COL)float32...long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             [614400 values with dtype=float32]CLOUD_OD(TSTEP, LAY, ROW, COL)float32...long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               [614400 values with dtype=float32]U10_MpS(TSTEP, LAY, ROW, COL)float32...long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                [614400 values with dtype=float32]V10_MpS(TSTEP, LAY, ROW, COL)float32...long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                [614400 values with dtype=float32]T2_K(TSTEP, LAY, ROW, COL)float32...long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   [614400 values with dtype=float32]SWSFC_WpM2(TSTEP, LAY, ROW, COL)float32...long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             [614400 values with dtype=float32]SOLM_M3pM3(TSTEP, LAY, ROW, COL)float32...long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             [614400 values with dtype=float32]CLDTOP_KM(TSTEP, LAY, ROW, COL)float32...long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              [614400 values with dtype=float32]CAPE(TSTEP, LAY, ROW, COL)float32...long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   [614400 values with dtype=float32]PBL_WRF_M(TSTEP, LAY, ROW, COL)float32...long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              [614400 values with dtype=float32]PBL_YSU_M(TSTEP, LAY, ROW, COL)float32...long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              [614400 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023207CTIME :72116WDATE :2023207WTIME :72116SDATE :2023207STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\nplt.figure(figsize=(15, 3))\nds_met[\"PBL_WRF_M\"].isel(LAY=0).mean(dim=[\"ROW\", \"COL\"]).plot()\ntwin_x = plt.gca().twinx()\nds_aq[\"P25\"].isel(LAY=0, TSTEP=range(24, 120)).mean(dim=[\"ROW\", \"COL\"]).plot(\n    ax=twin_x, color=\"r\"\n)\n# set ylabel color as red\ntwin_x.set_ylabel(\"PM2.5\", color=\"r\")\n# set yticks color as red\ntwin_x.tick_params(axis=\"y\", colors=\"r\")\ntwin_x.set_xlabel(\"Time\")\n\nText(0.5, 0, 'Time')\n\n\n\n\n\n\n\n\n\n\nds_met\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (TSTEP: 96, VAR: 14, DATE-TIME: 2, LAY: 1, ROW: 80, COL: 80)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables: (12/15)\n    TFLAG       (TSTEP, VAR, DATE-TIME) int32 ...\n    TSURF_K     (TSTEP, LAY, ROW, COL) float32 ...\n    SNOWEW_M    (TSTEP, LAY, ROW, COL) float32 ...\n    SNOWAGE_HR  (TSTEP, LAY, ROW, COL) float32 ...\n    PRATE_MMpH  (TSTEP, LAY, ROW, COL) float32 ...\n    CLOUD_OD    (TSTEP, LAY, ROW, COL) float32 ...\n    ...          ...\n    SWSFC_WpM2  (TSTEP, LAY, ROW, COL) float32 ...\n    SOLM_M3pM3  (TSTEP, LAY, ROW, COL) float32 ...\n    CLDTOP_KM   (TSTEP, LAY, ROW, COL) float32 ...\n    CAPE        (TSTEP, LAY, ROW, COL) float32 ...\n    PBL_WRF_M   (TSTEP, LAY, ROW, COL) float32 ...\n    PBL_YSU_M   (TSTEP, LAY, ROW, COL) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023207\n    CTIME:          72116\n    WDATE:          2023207\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 96VAR: 14DATE-TIME: 2LAY: 1ROW: 80COL: 80Coordinates: (0)Data variables: (15)TFLAG(TSTEP, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [2688 values with dtype=int32]TSURF_K(TSTEP, LAY, ROW, COL)float32...long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                [614400 values with dtype=float32]SNOWEW_M(TSTEP, LAY, ROW, COL)float32...long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               [614400 values with dtype=float32]SNOWAGE_HR(TSTEP, LAY, ROW, COL)float32...long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             [614400 values with dtype=float32]PRATE_MMpH(TSTEP, LAY, ROW, COL)float32...long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             [614400 values with dtype=float32]CLOUD_OD(TSTEP, LAY, ROW, COL)float32...long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               [614400 values with dtype=float32]U10_MpS(TSTEP, LAY, ROW, COL)float32...long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                [614400 values with dtype=float32]V10_MpS(TSTEP, LAY, ROW, COL)float32...long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                [614400 values with dtype=float32]T2_K(TSTEP, LAY, ROW, COL)float32...long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   [614400 values with dtype=float32]SWSFC_WpM2(TSTEP, LAY, ROW, COL)float32...long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             [614400 values with dtype=float32]SOLM_M3pM3(TSTEP, LAY, ROW, COL)float32...long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             [614400 values with dtype=float32]CLDTOP_KM(TSTEP, LAY, ROW, COL)float32...long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              [614400 values with dtype=float32]CAPE(TSTEP, LAY, ROW, COL)float32...long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   [614400 values with dtype=float32]PBL_WRF_M(TSTEP, LAY, ROW, COL)float32...long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              [614400 values with dtype=float32]PBL_YSU_M(TSTEP, LAY, ROW, COL)float32...long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              [614400 values with dtype=float32]Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023207CTIME :72116WDATE :2023207WTIME :72116SDATE :2023207STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\nnp.array([1, 2, 3])[:]\n\narray([1, 2, 3])\n\n\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(columns=[\"lag\"], index=list(ds_met.data_vars)[1:])\n\nfor lag in range(-24, 24):\n    for var in list(ds_met.data_vars)[1:]:\n        if lag == 0:\n            met_series = ds_met[var].isel(LAY=0).mean(dim=[\"ROW\", \"COL\"]).values\n            aq_series = (\n                ds_aq[\"P25\"]\n                .isel(LAY=0, TSTEP=range(24, 120))\n                .mean(dim=[\"ROW\", \"COL\"])\n                .values\n            )\n        elif lag &gt; 0:\n            met_series = ds_met[var].isel(LAY=0).mean(dim=[\"ROW\", \"COL\"]).values[lag:]\n            aq_series = (\n                ds_aq[\"P25\"]\n                .isel(LAY=0, TSTEP=range(24, 120))\n                .mean(dim=[\"ROW\", \"COL\"])\n                .values[:-lag]\n            )\n        else:\n            met_series = ds_met[var].isel(LAY=0).mean(dim=[\"ROW\", \"COL\"]).values[:lag]\n            aq_series = (\n                ds_aq[\"P25\"]\n                .isel(LAY=0, TSTEP=range(24, 120))\n                .mean(dim=[\"ROW\", \"COL\"])\n                .values[-lag:]\n            )\n        # print(f\"{var}: {np.corrcoef(met_series, aq_series)[0, 1]}\")\n        df.loc[var, lag] = np.corrcoef(met_series, aq_series)[0, 1]\n\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/patel_zeel/miniconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n\n\n\ndf.index.values\n\narray(['TSURF_K', 'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD',\n       'U10_MpS', 'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3',\n       'CLDTOP_KM', 'CAPE', 'PBL_WRF_M', 'PBL_YSU_M'], dtype=object)\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nplt.figure(figsize=(6, 3))\nfor var in [\"TSURF_K\", \"T2_K\", \"SWSFC_WpM2\", \"PBL_WRF_M\", \"PBL_YSU_M\"]:\n    df.loc[var].plot(label=var)\nfor var in [\"CLOUD_OD\", \"V10_MpS\"]:\n    df.loc[var].plot(label=var, linestyle=\"--\")\n\nplt.legend(bbox_to_anchor=(1, 1))\nplt.ylabel(\"Correlation with P25 from CAMx\")\nplt.tight_layout()\nplt.savefig(\"lag.pdf\")\n\n\n\n\n\n\n\n\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nfig, ax = plt.subplots(figsize=(15, 4))\nmappable = (\n    ds_met[\"PBL_WRF_M\"]\n    .isel(TSTEP=1, LAY=0)\n    .plot(\n        x=\"COL\", y=\"ROW\", cmap=\"RdYlGn_r\", ax=ax, vmin=0, vmax=2000, add_colorbar=False\n    )\n)\nfig.colorbar(mappable)\n\n\ndef plot_it(t):\n    ax.cla()\n    tmp = ds_met[\"PBL_WRF_M\"].isel(TSTEP=t, LAY=0)\n    tmp.plot(\n        x=\"COL\", y=\"ROW\", cmap=\"RdYlGn_r\", ax=ax, vmin=0, vmax=2000, add_colorbar=False\n    )\n    ax.set_xlabel(\"Longitude\")\n    ax.set_ylabel(\"Latitude\")\n    ax.set_title(f\"Mean PBLH: {tmp.mean().values:.2f} m\")\n\n\nanim = FuncAnimation(fig, plot_it, frames=range(1, 13), interval=500)\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nfrom aqmsp_data.data import load_caaqm\n\ncaaqm = load_caaqm(years=2022, months=1, days=1, variables=\"PM2.5\")\n\n\nimport pandas as pd\nfrom glob import glob\n\nfiles = glob(\"/home/patel_zeel/aqmsp/aqmsp_data/data/caaqm/raw_2023/*.xlsx\")\n\ndf = pd.read_excel(\n    files[-3],\n    header=None,\n)\n# print(df.head(7))\nstation = df.iloc[6, 1]\nlat = caaqm.sel(station=station).latitude.values.item()\nlon = caaqm.sel(station=station).longitude.values.item()\ndf = df.iloc[16:, :]\n# assign first row as column names\ndf.columns = df.iloc[0].values\n# drop first row\ndf = df.iloc[1:, :]\nprint(station)\ndf[\"time\"] = pd.to_datetime(df[\"From Date\"], format=\"%d-%m-%Y %H:%M\") + pd.Timedelta(\n    minutes=30\n)\ndf\n\nVivek Vihar, Delhi - DPCC\n\n\n\n\n\n\n\n\n\nFrom Date\nTo Date\nPM2.5\nPM10\nAT\nBP\nRH\ntime\n\n\n\n\n17\n01-01-2023 00:00\n01-01-2023 01:00\n141\n211\n11.95\n997.83\n78.9\n2023-01-01 00:30:00\n\n\n18\n01-01-2023 01:00\n01-01-2023 02:00\n149\n210\n11.85\n997.47\n79.1\n2023-01-01 01:30:00\n\n\n19\n01-01-2023 02:00\n01-01-2023 03:00\n141\n186\n11.3\n997.03\n80.25\n2023-01-01 02:30:00\n\n\n20\n01-01-2023 03:00\n01-01-2023 04:00\n138\n174\n10.3\n996.5\n83.33\n2023-01-01 03:30:00\n\n\n21\n01-01-2023 04:00\n01-01-2023 05:00\n122\n161\n10.05\n996.38\n84.08\n2023-01-01 04:30:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5077\n30-07-2023 20:00\n30-07-2023 21:00\n14\n95.75\n30.87\n972.4\n66.17\n2023-07-30 20:30:00\n\n\n5078\n30-07-2023 21:00\n30-07-2023 22:00\n18.25\n114.75\n30.6\n972.42\n69.22\n2023-07-30 21:30:00\n\n\n5079\n30-07-2023 22:00\n30-07-2023 23:00\n20.5\n100.5\n30.48\n972.43\n72\n2023-07-30 22:30:00\n\n\n5080\n30-07-2023 23:00\n31-07-2023 00:00\n17.75\n103\n30.42\n972.35\n71.92\n2023-07-30 23:30:00\n\n\n5081\n31-07-2023 00:00\n31-07-2023 00:00\n23\n124\n30.4\n972.3\n71.8\n2023-07-31 00:30:00\n\n\n\n\n5065 rows × 8 columns\n\n\n\n\ndef process_it(ds, date):\n    if ds.TSTEP.size == 120:\n        print(\"120\")\n        ds = ds.isel(LAY=0, TSTEP=range(24, 48))\n    else:\n        print(\"96\")\n        ds = ds.isel(LAY=0, TSTEP=range(24))\n    ds1 = ds.assign(time=(\"TSTEP\", pd.date_range(date, periods=24, freq=\"H\")))\n    lats = np.arange(80) * ds1.YCELL + ds1.YORIG\n    lons = np.arange(80) * ds1.XCELL + ds1.XORIG\n    ds2 = ds1.assign_coords(lat=(\"ROW\", lats), lon=(\"COL\", lons))\n    ds3 = ds2.swap_dims({\"TSTEP\": \"time\", \"ROW\": \"lat\", \"COL\": \"lon\"})\n    ds3[\"time\"] = ds3[\"time\"] + pd.Timedelta(hours=5, minutes=30)\n    return ds3\n\n\nds_met_processed = process_it(ds_met, global_date)\nds_met_processed\n\n96\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (time: 24, VAR: 14, DATE-TIME: 2, lat: 80, lon: 80)\nCoordinates:\n  * time        (time) datetime64[ns] 2023-07-26T05:30:00 ... 2023-07-27T04:3...\n  * lat         (lat) float64 28.2 28.21 28.22 28.23 ... 28.96 28.97 28.98 28.99\n  * lon         (lon) float64 76.85 76.86 76.87 76.88 ... 77.62 77.63 77.64\nDimensions without coordinates: VAR, DATE-TIME\nData variables: (12/15)\n    TFLAG       (time, VAR, DATE-TIME) int32 ...\n    TSURF_K     (time, lat, lon) float32 ...\n    SNOWEW_M    (time, lat, lon) float32 ...\n    SNOWAGE_HR  (time, lat, lon) float32 ...\n    PRATE_MMpH  (time, lat, lon) float32 ...\n    CLOUD_OD    (time, lat, lon) float32 ...\n    ...          ...\n    SWSFC_WpM2  (time, lat, lon) float32 ...\n    SOLM_M3pM3  (time, lat, lon) float32 ...\n    CLDTOP_KM   (time, lat, lon) float32 ...\n    CAPE        (time, lat, lon) float32 ...\n    PBL_WRF_M   (time, lat, lon) float32 ...\n    PBL_YSU_M   (time, lat, lon) float32 ...\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023207\n    CTIME:          72116\n    WDATE:          2023207\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:time: 24VAR: 14DATE-TIME: 2lat: 80lon: 80Coordinates: (3)time(time)datetime64[ns]2023-07-26T05:30:00 ... 2023-07-...array(['2023-07-26T05:30:00.000000000', '2023-07-26T06:30:00.000000000',\n       '2023-07-26T07:30:00.000000000', '2023-07-26T08:30:00.000000000',\n       '2023-07-26T09:30:00.000000000', '2023-07-26T10:30:00.000000000',\n       '2023-07-26T11:30:00.000000000', '2023-07-26T12:30:00.000000000',\n       '2023-07-26T13:30:00.000000000', '2023-07-26T14:30:00.000000000',\n       '2023-07-26T15:30:00.000000000', '2023-07-26T16:30:00.000000000',\n       '2023-07-26T17:30:00.000000000', '2023-07-26T18:30:00.000000000',\n       '2023-07-26T19:30:00.000000000', '2023-07-26T20:30:00.000000000',\n       '2023-07-26T21:30:00.000000000', '2023-07-26T22:30:00.000000000',\n       '2023-07-26T23:30:00.000000000', '2023-07-27T00:30:00.000000000',\n       '2023-07-27T01:30:00.000000000', '2023-07-27T02:30:00.000000000',\n       '2023-07-27T03:30:00.000000000', '2023-07-27T04:30:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float6428.2 28.21 28.22 ... 28.98 28.99array([28.200001, 28.210001, 28.220001, 28.230001, 28.240001, 28.250001,\n       28.260001, 28.270001, 28.280001, 28.290001, 28.300001, 28.310001,\n       28.320001, 28.330001, 28.340001, 28.350001, 28.360001, 28.370001,\n       28.380001, 28.390001, 28.400001, 28.410001, 28.420001, 28.430001,\n       28.440001, 28.450001, 28.460001, 28.470001, 28.480001, 28.490001,\n       28.500001, 28.510001, 28.520001, 28.530001, 28.540001, 28.550001,\n       28.560001, 28.570001, 28.580001, 28.590001, 28.600001, 28.610001,\n       28.620001, 28.630001, 28.640001, 28.650001, 28.660001, 28.670001,\n       28.680001, 28.690001, 28.700001, 28.710001, 28.720001, 28.730001,\n       28.740001, 28.750001, 28.760001, 28.770001, 28.780001, 28.790001,\n       28.800001, 28.810001, 28.820001, 28.830001, 28.840001, 28.850001,\n       28.860001, 28.870001, 28.880001, 28.890001, 28.900001, 28.910001,\n       28.920001, 28.930001, 28.940001, 28.950001, 28.960001, 28.970001,\n       28.980001, 28.990001])lon(lon)float6476.85 76.86 76.87 ... 77.63 77.64array([76.849998, 76.859998, 76.869998, 76.879998, 76.889998, 76.899998,\n       76.909998, 76.919998, 76.929998, 76.939998, 76.949998, 76.959998,\n       76.969998, 76.979998, 76.989998, 76.999998, 77.009998, 77.019998,\n       77.029998, 77.039998, 77.049998, 77.059998, 77.069998, 77.079998,\n       77.089998, 77.099998, 77.109998, 77.119998, 77.129998, 77.139998,\n       77.149998, 77.159998, 77.169998, 77.179998, 77.189998, 77.199998,\n       77.209998, 77.219998, 77.229998, 77.239998, 77.249998, 77.259998,\n       77.269998, 77.279998, 77.289998, 77.299998, 77.309998, 77.319998,\n       77.329998, 77.339998, 77.349998, 77.359998, 77.369998, 77.379998,\n       77.389998, 77.399998, 77.409998, 77.419998, 77.429998, 77.439998,\n       77.449998, 77.459998, 77.469998, 77.479998, 77.489998, 77.499998,\n       77.509998, 77.519998, 77.529998, 77.539998, 77.549998, 77.559998,\n       77.569998, 77.579998, 77.589998, 77.599998, 77.609998, 77.619998,\n       77.629998, 77.639998])Data variables: (15)TFLAG(time, VAR, DATE-TIME)int32...units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                [672 values with dtype=int32]TSURF_K(time, lat, lon)float32...long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                [153600 values with dtype=float32]SNOWEW_M(time, lat, lon)float32...long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               [153600 values with dtype=float32]SNOWAGE_HR(time, lat, lon)float32...long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             [153600 values with dtype=float32]PRATE_MMpH(time, lat, lon)float32...long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             [153600 values with dtype=float32]CLOUD_OD(time, lat, lon)float32...long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               [153600 values with dtype=float32]U10_MpS(time, lat, lon)float32...long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                [153600 values with dtype=float32]V10_MpS(time, lat, lon)float32...long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                [153600 values with dtype=float32]T2_K(time, lat, lon)float32...long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   [153600 values with dtype=float32]SWSFC_WpM2(time, lat, lon)float32...long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             [153600 values with dtype=float32]SOLM_M3pM3(time, lat, lon)float32...long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             [153600 values with dtype=float32]CLDTOP_KM(time, lat, lon)float32...long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              [153600 values with dtype=float32]CAPE(time, lat, lon)float32...long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   [153600 values with dtype=float32]PBL_WRF_M(time, lat, lon)float32...long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              [153600 values with dtype=float32]PBL_YSU_M(time, lat, lon)float32...long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              [153600 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2023-07-26 05:30:00', '2023-07-26 06:30:00',\n               '2023-07-26 07:30:00', '2023-07-26 08:30:00',\n               '2023-07-26 09:30:00', '2023-07-26 10:30:00',\n               '2023-07-26 11:30:00', '2023-07-26 12:30:00',\n               '2023-07-26 13:30:00', '2023-07-26 14:30:00',\n               '2023-07-26 15:30:00', '2023-07-26 16:30:00',\n               '2023-07-26 17:30:00', '2023-07-26 18:30:00',\n               '2023-07-26 19:30:00', '2023-07-26 20:30:00',\n               '2023-07-26 21:30:00', '2023-07-26 22:30:00',\n               '2023-07-26 23:30:00', '2023-07-27 00:30:00',\n               '2023-07-27 01:30:00', '2023-07-27 02:30:00',\n               '2023-07-27 03:30:00', '2023-07-27 04:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))latPandasIndexPandasIndex(Index([28.200000762939453, 28.210000762715936,  28.22000076249242,\n         28.2300007622689, 28.240000762045383, 28.250000761821866,\n        28.26000076159835,  28.27000076137483, 28.280000761151314,\n       28.290000760927796,  28.30000076070428,  28.31000076048076,\n       28.320000760257244, 28.330000760033727,  28.34000075981021,\n       28.350000759586692, 28.360000759363174, 28.370000759139657,\n        28.38000075891614, 28.390000758692622, 28.400000758469105,\n       28.410000758245587,  28.42000075802207, 28.430000757798553,\n       28.440000757575035, 28.450000757351518,    28.460000757128,\n       28.470000756904483, 28.480000756680965, 28.490000756457448,\n        28.50000075623393, 28.510000756010413, 28.520000755786896,\n        28.53000075556338,  28.54000075533986, 28.550000755116343,\n       28.560000754892826,  28.57000075466931,  28.58000075444579,\n       28.590000754222274, 28.600000753998756,  28.61000075377524,\n        28.62000075355172, 28.630000753328204, 28.640000753104687,\n        28.65000075288117, 28.660000752657652, 28.670000752434134,\n       28.680000752210617,   28.6900007519871, 28.700000751763582,\n       28.710000751540065, 28.720000751316547,  28.73000075109303,\n       28.740000750869513, 28.750000750645995, 28.760000750422478,\n        28.77000075019896, 28.780000749975443, 28.790000749751925,\n       28.800000749528408,  28.81000074930489, 28.820000749081373,\n       28.830000748857856,  28.84000074863434,  28.85000074841082,\n       28.860000748187304, 28.870000747963786,  28.88000074774027,\n        28.89000074751675, 28.900000747293234, 28.910000747069716,\n         28.9200007468462,  28.93000074662268, 28.940000746399164,\n       28.950000746175647,  28.96000074595213, 28.970000745728612,\n       28.980000745505095, 28.990000745281577],\n      dtype='float64', name='lat'))lonPandasIndexPandasIndex(Index([ 76.8499984741211, 76.85999847389758, 76.86999847367406,\n       76.87999847345054, 76.88999847322702,  76.8999984730035,\n       76.90999847277999, 76.91999847255647, 76.92999847233295,\n       76.93999847210944, 76.94999847188592,  76.9599984716624,\n       76.96999847143888, 76.97999847121537, 76.98999847099185,\n       76.99999847076833, 77.00999847054482,  77.0199984703213,\n       77.02999847009778, 77.03999846987426, 77.04999846965075,\n       77.05999846942723, 77.06999846920371,  77.0799984689802,\n       77.08999846875668, 77.09999846853316, 77.10999846830964,\n       77.11999846808612,  77.1299984678626, 77.13999846763909,\n       77.14999846741557, 77.15999846719205, 77.16999846696854,\n       77.17999846674502,  77.1899984665215, 77.19999846629798,\n       77.20999846607447, 77.21999846585095, 77.22999846562743,\n       77.23999846540391,  77.2499984651804, 77.25999846495688,\n       77.26999846473336, 77.27999846450984, 77.28999846428633,\n       77.29999846406281, 77.30999846383929, 77.31999846361578,\n       77.32999846339226, 77.33999846316874, 77.34999846294522,\n        77.3599984627217, 77.36999846249819, 77.37999846227467,\n       77.38999846205115, 77.39999846182764, 77.40999846160412,\n        77.4199984613806, 77.42999846115708, 77.43999846093357,\n       77.44999846071005, 77.45999846048653, 77.46999846026301,\n        77.4799984600395, 77.48999845981598, 77.49999845959246,\n       77.50999845936894, 77.51999845914543, 77.52999845892191,\n       77.53999845869839, 77.54999845847487, 77.55999845825136,\n       77.56999845802784, 77.57999845780432,  77.5899984575808,\n       77.59999845735729, 77.60999845713377, 77.61999845691025,\n       77.62999845668674, 77.63999845646322],\n      dtype='float64', name='lon'))Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023207CTIME :72116WDATE :2023207WTIME :72116SDATE :2023207STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\nclosest_ds = ds_met_processed.sel(lat=lat, lon=lon, method=\"nearest\")\nfig, ax = plt.subplots(figsize=(15, 4))\n\nprint(lat, lon)\nclosest_ds[\"TSURF_K\"].plot(ax=ax, label=\"TSURF_K\")\nclosest_ds[\"T2_K\"].plot(ax=ax, label=\"T2_K\")\ntmp_df = df.set_index(\"time\")\n# select data from global_data\ntmp_df = tmp_df.loc[closest_ds[\"time\"].values[0] : closest_ds[\"time\"].values[-1]]\ntmp_df[\"AT_K\"] = tmp_df[\"AT\"] + 273.15\ntmp_df.plot(ax=ax, y=\"AT_K\")\nplt.legend()\n# tmp_df\n\n28.672342 77.31526"
  },
  {
    "objectID": "posts/gcloud.html",
    "href": "posts/gcloud.html",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don’t know if this is required or not. This command should trigger installation of “gcloud Beta Commands” automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#initial-setup",
    "href": "posts/gcloud.html#initial-setup",
    "title": "Gcloud cheatsheet",
    "section": "",
    "text": "Following this guide.\n\nTo set default email, project-id & zone:\n\ngcloud config set account your-email-account\ngcloud config set project your-project-id\ngcloud config set compute/zone us-central1-f  # us-central1-f for free v2 TPUs and europe-west4-a for free v3 TPUs (only if you have free TRC access)\n\nTo get currently active project and zone related info:\n\ngcloud info\n\nTo create an identity (I don’t know if this is required or not. This command should trigger installation of “gcloud Beta Commands” automatically in another shell and then you need to rerun the following command):\n\ngcloud beta services identity create --service tpu.googleapis.com"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms",
    "href": "posts/gcloud.html#working-with-tpu-vms",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs",
    "text": "Working with TPU VMs\nThere are two different terms here: “TPU VMs” and “TPU nodes”. TPU nodes can be connected externally via another VM. TPU VMs are stand-alone systems with TPUs, RAM and CPU (96 core Intel 2 GHz processor and 335 GB RAM). We may be charged via GCP for the VM (CPUs and RAM). (I will update this info once I know for sure):\n\n\nTo create a TPU VM in preferred zone via CLI (be careful about the --zone to avoid charges, check the first email received from TRC team to see what kind of TPUs are free in different zones. if --zone is not passed, VM will be created in the default zone that we set initially. This command triggered installation of “gcloud Alpha Commands”):\n\ngcloud alpha compute tpus tpu-vm create vm-1 --accelerator-type v2-8 --version tpu-vm-tf-2.8.0 --zone us-central1-f\n\nTo get the list of TPU nodes/VMs:\n\ngcloud compute tpus list\n\nTo delete a TPU node/VM:\n\ngcloud compute tpus delete vm-1\n\nTo connect with a vm via ssh (this automatically creates ssh key pair and places in default ssh config location):\n\ngcloud alpha compute tpus tpu-vm ssh vm-1\n\nFollow this guide to create and attach a persistent disk with the TPU VM"
  },
  {
    "objectID": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "href": "posts/gcloud.html#working-with-tpu-vms-via-vs-code",
    "title": "Gcloud cheatsheet",
    "section": "Working with TPU VMs via VS-code",
    "text": "Working with TPU VMs via VS-code\n\nInstall the following extension on VS-code: \nUse the following button to connect to a remote machine (use “Connect to Host…” button): \nManually update the default ssh config file (in my case, “C:\\.ssh”) to add a VM in VS-code (you can use VS-code command palette to figure out the config file for you and edit it. Please see the screeshot below).\n\n\n\nNote that ssh public-private key pair with name google_compute_engine is automatically generated when you connect with the VM for the first time with gcloud alpha compute tpus tpu-vm ssh command. The VM config for me looks like this:\n\nHost Cloud-TPU-Node-2\n  HostName &lt;External-IP-of-your-TPU-VM&gt;\n  User zeelp\n  Port 22\n  IdentityFile C:\\Users\\zeelp\\.ssh\\google_compute_engine"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html",
    "href": "posts/2023-04-29-sine-combination-netowrks.html",
    "title": "Sine Combination Networks",
    "section": "",
    "text": "We know that any continuous signal can be represented as a sum of sinusoids. The question is, how many sinusoids do we need to represent a signal? In this notebook, we will explore this question.\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#random-combination-of-sinusoids",
    "title": "Sine Combination Networks",
    "section": "Random Combination of Sinusoids",
    "text": "Random Combination of Sinusoids\n\nN = 1000\nx = jnp.linspace(-10, 10, N).reshape(-1, 1)\ny = jnp.sin(x) + jnp.sin(2*x) #+ jax.random.normal(jax.random.PRNGKey(0), (N, 1)) * 0.1\nplt.plot(x, y, \"kx\");\nprint(x.shape, y.shape)\n\n(1000, 1) (1000, 1)"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#recover-the-signal",
    "title": "Sine Combination Networks",
    "section": "Recover the Signal",
    "text": "Recover the Signal\n\ndef get_weights(key):\n    w1 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    key = jax.random.split(key)[0]\n    w2 = jax.random.uniform(key, (), minval=0.0, maxval=5.0)\n    return w1, w2\n    \ndef get_sine(weights, x):\n    w1, w2 = weights\n    return jnp.sin(w1*x) + jnp.sin(w2*x)\n\ndef loss_fn(weights, x, y):\n    output = get_sine(weights, x)\n    w1, w2 = weights\n    return jnp.mean((output.ravel() - y.ravel())**2)\n\n\ndef one_step(weights_and_state, xs):\n    weights, state = weights_and_state\n    loss, grads = value_and_grad_fn(weights, x, y)\n    updates, state = optimizer.update(grads, state)\n    weights = optax.apply_updates(weights, updates)\n    return (weights, state), (loss, weights)\n\nepochs = 1000\noptimizer = optax.adam(1e-2)\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nfig, ax = plt.subplots(4, 3, figsize=(15, 12))\nfig2, ax2 = plt.subplots(4, 3, figsize=(15, 12))\nax = ax.ravel()\nax2 = ax2.ravel()\nfor seed in tqdm(range(12)):\n    key = jax.random.PRNGKey(seed)\n    init_weights = get_weights(key)\n    state = optimizer.init(init_weights)\n    (weights, _), (loss_history, _) = jax.lax.scan(one_step, (init_weights, state), None, length=epochs)\n    y_pred = get_sine(weights, x)\n    ax[seed].plot(x, y, \"kx\")\n    ax[seed].plot(x, y_pred, \"r-\")\n    ax[seed].set_title(f\"w_init=({init_weights[0]:.2f}, {init_weights[1]:.2f}), w_pred=({weights[0]:.2f}, {weights[1]:.2f}), loss={loss_fn(weights, x, y):.2f}\")\n    ax2[seed].plot(loss_history)\nfig.tight_layout()\n\n100%|██████████| 12/12 [00:00&lt;00:00, 15.91it/s]"
  },
  {
    "objectID": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "href": "posts/2023-04-29-sine-combination-netowrks.html#plot-loss-surface",
    "title": "Sine Combination Networks",
    "section": "Plot loss surface",
    "text": "Plot loss surface\n\nw1 = jnp.linspace(0, 3, 100)\nw2 = jnp.linspace(0, 3, 100)\nW1, W2 = jnp.meshgrid(w1, w2)\nloss = jax.vmap(jax.vmap(lambda w1, w2: loss_fn((w1, w2), x, y)))(W1, W2)\n\n# plot the loss surface in 3D\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(W1, W2, loss, cmap=\"viridis\", alpha=0.9);\nax.set_xlabel(\"w1\");\nax.set_ylabel(\"w2\");\n# top view\nax.view_init(30, 45)"
  },
  {
    "objectID": "posts/docker_cheatsheet.html",
    "href": "posts/docker_cheatsheet.html",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#images",
    "href": "posts/docker_cheatsheet.html#images",
    "title": "Docker Cheatsheet",
    "section": "",
    "text": "One can download (pull) a docker image from Docker Hub or respective websites:\n# docker pull &lt;image-name&gt;:&lt;tag&gt;\ndocker pull tensorflow/tensorflow:2.4.0-gpu-jupyter\nCheck the downloaded images with:\ndocker images\nSometimes we may need to do something on an image without saving or alloting extra space. To create and run a temporary container to be deleted on stopping:\ndocker run --rm -it tensorflow/tensorflow:2.4.0-gpu-jupyter"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#containers",
    "href": "posts/docker_cheatsheet.html#containers",
    "title": "Docker Cheatsheet",
    "section": "Containers",
    "text": "Containers\nCreate a new container from an image with following flags 1. -v: for telling docker to use a shared directory between host and container 2. -p \\&lt;host-port\\&gt;\\&lt;container-port\\&gt;: is to use port forwarding, an application running on &lt;container-port&gt; can be accessed only with &lt;host-port&gt; in host. 3. --name generates a name for the container for easy reference in other commands 4. --gpus all tells docker to use all GPUs available on host 5. --memory-swap Restricts RAM+Swap usage 6. -it makes sure container awaits after starting instead of instantly shutting down if no startup scripts are configured.\nTo create new container with default params:\ndocker create tensorflow/tensorflow:2.4.0-gpu-jupyter \nTo create new container with manual params:\ndocker create -it \\\n-v \\path_in_host:\\path_in_container \\\n-p9000:8888 \\\n--name aaai \\\n--cpus 2 \\\n--gpus all \\ # To use specific gpus: --gpu '\"device=0,2\"'\n--memory 90g \\ # Uses 90g memory\n--memory-swap 100g \\ # --memory-swap is a modifier flag that only has meaning if --memory is also set. In this case 10g of swap will be used.\ntensorflow/tensorflow:2.4.0-gpu-jupyter\nUpdate some of the above configurations after container creation:\n# change RAM limit of a container named \"aaai\"\ndocker update --memory-swap 50g aaai\n\nNote: In general, changes made to container persist when container is stopped.\n\nCheck containers:\ndocker ps # shows running containers\ndocker ps -a # shows all containers\nStart a container (default script will be executed with this if any):\n# docker start &lt;container-name&gt;\ndocker start aaai\nStop a container:\n# docker stop &lt;container-name&gt; \ndocker stop aaai\nDelete a container:\n# docker rm &lt;container-name&gt;\ndocker rm aaai\nGo to a running container’s shell:\n#docker exec -it &lt;container-name&gt; bash\ndocker exec -it aaai bash # -it stands for interactive\nExecute any command on a running container without opening a shell in container:\n# docker exec -it &lt;container-name&gt; &lt;command&gt;\ndocker exec -it aaai jupyter notebook list\nCheck container logs (including shell commands output):\n# docker logs &lt;container-name&gt;\ndocker logs aaai"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#system",
    "href": "posts/docker_cheatsheet.html#system",
    "title": "Docker Cheatsheet",
    "section": "System",
    "text": "System\nCheck all images, all containers and space occupied by them:\ndocker system df -v"
  },
  {
    "objectID": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "href": "posts/docker_cheatsheet.html#set-up-rootless-docker",
    "title": "Docker Cheatsheet",
    "section": "Set up rootless docker",
    "text": "Set up rootless docker\nRefer to this guide: https://docs.docker.com/engine/security/rootless/\nMain steps:\n\nRun dockerd-rootless-setuptool.sh install.\nSetup PATH and DOCKER_HOME as suggested by command output.\nsystemctl --user restart docker.\nTry docker images to check if things worked.\nTry docker run --rm hello-world to check if things really worked."
  },
  {
    "objectID": "posts/GPT-from-scratch.html",
    "href": "posts/GPT-from-scratch.html",
    "title": "Building GPT from scratch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html",
    "href": "posts/Multiclass_GP_classification.html",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "",
    "text": "import GPy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.gaussian_process import GaussianProcessClassifier, kernels\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#common-functions",
    "href": "posts/Multiclass_GP_classification.html#common-functions",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Common functions",
    "text": "Common functions\n\ndef get_kernel(ard):\n    return GPy.kern.RBF(2, ARD=ard)\n\ndef create_and_fit_model(model_class, X, y, ard, **kwargs):\n    model = model_class(X, y, get_kernel(ard), **kwargs)\n    model.optimize()\n    return model"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "href": "posts/Multiclass_GP_classification.html#generate-synthetic-data",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Generate Synthetic Data",
    "text": "Generate Synthetic Data\n\nX, y = make_blobs(n_samples=200, centers=9, random_state=0)\n# X, y = make_moons(n_samples=200, noise=0.1, random_state=0)\ny = y.reshape(-1, 1)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');\n\n\n\n\n\n\n\n\n\ngrid1 = np.linspace(X.min(axis=0)[0]-1, X.max(axis=0)[0]+1, 50)\ngrid2 = np.linspace(X.min(axis=0)[1]-1, X.max(axis=0)[1]+1, 50)\nGrid1, Grid2 = np.meshgrid(grid1, grid2)\nX_grid = np.vstack([Grid1.ravel(), Grid2.ravel()]).T"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#train-test-split",
    "href": "posts/Multiclass_GP_classification.html#train-test-split",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Train-test split",
    "text": "Train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\ny_train_one_hot = encoder.transform(y_train)\ny_test_one_hot = encoder.transform(y_test)"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a regression problem",
    "text": "Treat it as a regression problem\nHere we can regress over the class labels as if they are discrete realizations of a continuous variable. We will round the predictions to the nearest integer to get the class labels.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].round().astype(int)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.67      0.46      0.55        13\n           1       0.33      0.57      0.42         7\n           2       0.57      0.62      0.59        13\n           3       0.71      0.45      0.56        11\n           4       0.71      0.91      0.80        11\n           5       0.67      0.80      0.73        10\n           6       0.33      0.50      0.40         6\n           7       0.85      0.65      0.73        17\n           8       1.00      0.83      0.91        12\n\n    accuracy                           0.65       100\n   macro avg       0.65      0.64      0.63       100\nweighted avg       0.69      0.65      0.66       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.60      0.46      0.52        13\n           1       0.27      0.43      0.33         7\n           2       0.57      0.62      0.59        13\n           3       0.83      0.45      0.59        11\n           4       0.73      1.00      0.85        11\n           5       0.62      0.80      0.70        10\n           6       0.38      0.50      0.43         6\n           7       0.92      0.65      0.76        17\n           8       1.00      0.92      0.96        12\n\n    accuracy                           0.66       100\n   macro avg       0.66      0.65      0.64       100\nweighted avg       0.70      0.66      0.67       100\n\n\n\nWe get the raw predictions as below:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nmappables = []\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0]\n    mappable = ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    mappables.append(mappable)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\n# put a common colorbar for both mappables\nfig.colorbar(mappables[0], ax=ax, cax=fig.add_axes([0.92, 0.1, 0.02, 0.8]));\n\n\n\n\n\n\n\n\nNow, let us see the classification boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].round().astype(int)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-multi-output-regression-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a multi-output regression problem",
    "text": "Treat it as a multi-output regression problem\nIn this method, we learn a shared GP model among each class in one v/s rest setting. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=True)\nnon_ard_model = create_and_fit_model(GPy.models.GPRegression, X_train, y_train_one_hot, ard=False)\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nprint(\"Non-ARD\")\npreds = non_ard_model.predict(X_test)[0].argmax(axis=1)\nprint(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\nNon-ARD\n              precision    recall  f1-score   support\n\n           0       0.80      0.62      0.70        13\n           1       0.67      0.86      0.75         7\n           2       0.85      0.85      0.85        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.91      0.91       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\nWhat will happen if we ignore the points where the predictions are below 0.5?\n\nprint(\"ARD\")\npred_probas = ard_model.predict(X_test)[0]\npred_proba = pred_probas.max(axis=1)\n\nmask = pred_proba &gt; 0.5\npreds = ard_model.predict(X_test)[0].argmax(axis=1)\npreds_masked = preds[mask]\nground_truth_masked = y_test[mask]\n\nprint(ground_truth_masked.shape, preds_masked.shape)\nprint(classification_report(ground_truth_masked, preds_masked))\n\nARD\n(98, 1) (98,)\n              precision    recall  f1-score   support\n\n           0       0.78      0.64      0.70        11\n           1       0.67      0.86      0.75         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.90      0.90      0.90        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.92        98\n   macro avg       0.91      0.92      0.91        98\nweighted avg       0.92      0.92      0.92        98\n\n\n\nLet’s visualize the uncertain points:\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='rainbow', edgecolor='k')\nplt.scatter(X_test[~mask, 0], X_test[~mask, 1], s=150, c=y_test[~mask], cmap='rainbow', edgecolor='k', label='uncertain points');\nplt.legend();\n\n\n\n\n\n\n\n\nWe see that some points close to the decision boundary are uncertain. We can now plot the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model, non_ard_model]:\n    y_grid = model.predict(X_grid)[0].argmax(axis=1) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={model.kern.ARD}\")\n    i += 1\n    \n# known = set()\n# for i in range(len(y_grid)):\n#     pred = y_grid[i]\n#     if pred in known:\n#         x = np.random.uniform()\n#         if x &lt; 0.2:\n#             known.remove(pred)\n#     else:\n#         known.add(pred)\n#         ax[0].text(X_grid[i, 0], X_grid[i, 1], str(pred), fontsize=10, color='k', ha='center', va='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\npd.Series(y_grid).value_counts()\n\n7    478\n5    396\n8    378\n4    324\n6    297\n1    265\n2    201\n3     98\n0     63\ndtype: int64"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "href": "posts/Multiclass_GP_classification.html#treat-it-as-a-one-vs-rest-classification-problem",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Treat it as a One v/s Rest classification problem",
    "text": "Treat it as a One v/s Rest classification problem\nHere we learn a separate model for each class. We can get the predictions by taking the argmax of the predictions from each model.\n\nard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=[0.01, 0.01])\n# non_ard_kernel = kernels.ConstantKernel() * kernels.RBF(length_scale=0.1)\nard_model = GaussianProcessClassifier(kernel=ard_kernel, random_state=0)\n# non_ard_model = GaussianProcessClassifier(kernel=non_ard_kernel, random_state=0)\nard_model.fit(X_train, y_train.ravel())\n# non_ard_model.fit(X_train, y_train.ravel())\n\nprint(\"ARD\")\npreds = ard_model.predict(X_test)\nprint(classification_report(y_test, preds))\n# print(\"Non-ARD\")\n# preds = non_ard_model.predict(X_test)\n# print(classification_report(y_test, preds))\n\nARD\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        13\n           1       0.00      0.00      0.00         7\n           2       1.00      0.08      0.14        13\n           3       0.00      0.00      0.00        11\n           4       0.00      0.00      0.00        11\n           5       0.00      0.00      0.00        10\n           6       0.00      0.00      0.00         6\n           7       0.00      0.00      0.00        17\n           8       0.12      1.00      0.22        12\n\n    accuracy                           0.13       100\n   macro avg       0.12      0.12      0.04       100\nweighted avg       0.14      0.13      0.04       100\n\n\n\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n /home/patel_zeel/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n\nVisualizing the decision boundary:\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.ravel()\ni = 0\nfor model in [ard_model]:\n    y_grid = model.predict_proba(X_grid).argmax(axis=1)\n    ax[i].contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\n    ax[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k')\n    ax[i].set_title(f\"ARD={bool(i)}\")\n    i += 1\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "href": "posts/Multiclass_GP_classification.html#try-random-forest-model",
    "title": "Multi-class classification with Gaussian Processes",
    "section": "Try random forest model",
    "text": "Try random forest model\n\nmodel = RandomForestClassifier(n_estimators=1000, random_state=0)\nmodel.fit(X_train, y_train.ravel())\n\nprint(\"Random Forest\")\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))\n\nRandom Forest\n              precision    recall  f1-score   support\n\n           0       0.78      0.54      0.64        13\n           1       0.60      0.86      0.71         7\n           2       0.92      0.85      0.88        13\n           3       0.92      1.00      0.96        11\n           4       1.00      1.00      1.00        11\n           5       0.91      1.00      0.95        10\n           6       1.00      1.00      1.00         6\n           7       1.00      1.00      1.00        17\n           8       1.00      1.00      1.00        12\n\n    accuracy                           0.91       100\n   macro avg       0.90      0.92      0.90       100\nweighted avg       0.91      0.91      0.91       100\n\n\n\n\nfig, ax = plt.subplots(figsize=(6, 4))\ny_grid = model.predict(X_grid) + 0.5  # due to some reason, unless we add 0.5, the contourf is not showing all the colors\nax.contourf(Grid1, Grid2, y_grid.reshape(Grid1.shape), cmap='rainbow', alpha=0.5)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow', edgecolor='k');"
  },
  {
    "objectID": "posts/foundation-models-for-time-series.html",
    "href": "posts/foundation-models-for-time-series.html",
    "title": "Foundation Models for Time Series Forecasting",
    "section": "",
    "text": "# Config\nimport os\n\n# Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Monitoring\nfrom tqdm.notebook import tqdm\n\n# IO\nfrom os.path import join, exists, basename, dirname\nfrom glob import glob\n\n# Parallel processing\nfrom joblib import Parallel, delayed\n\nimport xarray as xr"
  },
  {
    "objectID": "posts/foundation-models-for-time-series.html#data",
    "href": "posts/foundation-models-for-time-series.html#data",
    "title": "Foundation Models for Time Series Forecasting",
    "section": "Data",
    "text": "Data\n\nds = xr.open_zarr(\"zip:///::https://huggingface.co/datasets/Zeel/P1/resolve/main/all_in_one.zarr.zip\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25GB\nDimensions:      (Timestamp: 245376, station: 537)\nCoordinates:\n  * Timestamp    (Timestamp) datetime64[ns] 2MB 2017-01-01 ... 2023-12-31T23:...\n    address      (station) &lt;U187 402kB ...\n    city         (station) &lt;U18 39kB ...\n    latitude     (station) float64 4kB ...\n    longitude    (station) float64 4kB ...\n    state        (station) &lt;U17 37kB ...\n  * station      (station) &lt;U64 137kB '32Bungalows, Bhilai - CECB' ... 'Ward-...\nData variables: (12/24)\n    AT           (Timestamp, station) float64 1GB ...\n    BP           (Timestamp, station) float64 1GB ...\n    Benzene      (Timestamp, station) float64 1GB ...\n    CO           (Timestamp, station) float64 1GB ...\n    Eth-Benzene  (Timestamp, station) float64 1GB ...\n    MP-Xylene    (Timestamp, station) float64 1GB ...\n    ...           ...\n    TOT-RF       (Timestamp, station) float64 1GB ...\n    Toluene      (Timestamp, station) float64 1GB ...\n    VWS          (Timestamp, station) float64 1GB ...\n    WD           (Timestamp, station) float64 1GB ...\n    WS           (Timestamp, station) float64 1GB ...\n    Xylene       (Timestamp, station) float64 1GB ...xarray.DatasetDimensions:Timestamp: 245376station: 537Coordinates: (7)Timestamp(Timestamp)datetime64[ns]2017-01-01 ... 2023-12-31T23:45:00array(['2017-01-01T00:00:00.000000000', '2017-01-01T00:15:00.000000000',\n       '2017-01-01T00:30:00.000000000', ..., '2023-12-31T23:15:00.000000000',\n       '2023-12-31T23:30:00.000000000', '2023-12-31T23:45:00.000000000'],\n      dtype='datetime64[ns]')address(station)&lt;U187...[537 values with dtype=&lt;U187]city(station)&lt;U18...[537 values with dtype=&lt;U18]latitude(station)float64...[537 values with dtype=float64]longitude(station)float64...[537 values with dtype=float64]state(station)&lt;U17...[537 values with dtype=&lt;U17]station(station)&lt;U64'32Bungalows, Bhilai - CECB' ......array(['32Bungalows, Bhilai - CECB', 'AIIMS, Raipur - CECB',\n       'Bhatagaon New ISBT, Raipur - CECB', ...,\n       'Sidhu Kanhu Indoor Stadium, Durgapur - WBPCB',\n       'Victoria, Kolkata - WBPCB', 'Ward-32 Bapupara, Siliguri - WBPCB'],\n      dtype='&lt;U64')Data variables: (24)AT(Timestamp, station)float64...unit :°C[131766912 values with dtype=float64]BP(Timestamp, station)float64...unit :mmHg[131766912 values with dtype=float64]Benzene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]CO(Timestamp, station)float64...unit :mg/m³[131766912 values with dtype=float64]Eth-Benzene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]MP-Xylene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]NH3(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]NO(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]NO2(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]NOx(Timestamp, station)float64...unit :ppb[131766912 values with dtype=float64]O Xylene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]Ozone(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]PM10(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]PM2.5(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]RF(Timestamp, station)float64...unit :mm[131766912 values with dtype=float64]RH(Timestamp, station)float64...unit :%[131766912 values with dtype=float64]SO2(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]SR(Timestamp, station)float64...unit :W/mt2[131766912 values with dtype=float64]TOT-RF(Timestamp, station)float64...unit :mm[131766912 values with dtype=float64]Toluene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]VWS(Timestamp, station)float64...unit :m/s[131766912 values with dtype=float64]WD(Timestamp, station)float64...unit :deg[131766912 values with dtype=float64]WS(Timestamp, station)float64...unit :m/s[131766912 values with dtype=float64]Xylene(Timestamp, station)float64...unit :µg/m³[131766912 values with dtype=float64]Indexes: (2)TimestampPandasIndexPandasIndex(DatetimeIndex(['2017-01-01 00:00:00', '2017-01-01 00:15:00',\n               '2017-01-01 00:30:00', '2017-01-01 00:45:00',\n               '2017-01-01 01:00:00', '2017-01-01 01:15:00',\n               '2017-01-01 01:30:00', '2017-01-01 01:45:00',\n               '2017-01-01 02:00:00', '2017-01-01 02:15:00',\n               ...\n               '2023-12-31 21:30:00', '2023-12-31 21:45:00',\n               '2023-12-31 22:00:00', '2023-12-31 22:15:00',\n               '2023-12-31 22:30:00', '2023-12-31 22:45:00',\n               '2023-12-31 23:00:00', '2023-12-31 23:15:00',\n               '2023-12-31 23:30:00', '2023-12-31 23:45:00'],\n              dtype='datetime64[ns]', name='Timestamp', length=245376, freq=None))stationPandasIndexPandasIndex(Index(['32Bungalows, Bhilai - CECB', 'AIIMS, Raipur - CECB',\n       'Bhatagaon New ISBT, Raipur - CECB',\n       'Civic Center, Bhilai - Bhilai Steel Plant',\n       'Govt. Higher Secondary School, Milupara - CECB',\n       'Hathkhoj, Bhilai - CECB', 'Krishak Nagar, Raipur - CECB',\n       'Mangala, Bilaspur - NTPC', 'Nawapara SECL Colony, Chhal - CECB',\n       'OP Jindal Industrial Park, Tumidih - CECB',\n       ...\n       'Ghusuri, Howrah - WBPCB', 'Jadavpur, Kolkata - WBPCB',\n       'Padmapukur, Howrah - WBPCB',\n       'Priyambada Housing Estate, Haldia - WBPCB',\n       'Rabindra Bharati University, Kolkata - WBPCB',\n       'Rabindra Sarobar, Kolkata - WBPCB',\n       'SVSPA Campus, Barrackpore - WBPCB',\n       'Sidhu Kanhu Indoor Stadium, Durgapur - WBPCB',\n       'Victoria, Kolkata - WBPCB', 'Ward-32 Bapupara, Siliguri - WBPCB'],\n      dtype='object', name='station', length=537))Attributes: (0)\n\n\n\none_station_ds = ds.sel(station=\"IGI Airport (T3), Delhi - IMD\", Timestamp=slice(\"2022\", \"2023\"))[[\"PM2.5\"]]\none_station_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1MB\nDimensions:    (Timestamp: 70080)\nCoordinates:\n  * Timestamp  (Timestamp) datetime64[ns] 561kB 2022-01-01 ... 2023-12-31T23:...\n    address    &lt;U187 748B ...\n    city       &lt;U18 72B ...\n    latitude   float64 8B ...\n    longitude  float64 8B ...\n    state      &lt;U17 68B ...\n    station    &lt;U64 256B 'IGI Airport (T3), Delhi - IMD'\nData variables:\n    PM2.5      (Timestamp) float64 561kB ...xarray.DatasetDimensions:Timestamp: 70080Coordinates: (7)Timestamp(Timestamp)datetime64[ns]2022-01-01 ... 2023-12-31T23:45:00array(['2022-01-01T00:00:00.000000000', '2022-01-01T00:15:00.000000000',\n       '2022-01-01T00:30:00.000000000', ..., '2023-12-31T23:15:00.000000000',\n       '2023-12-31T23:30:00.000000000', '2023-12-31T23:45:00.000000000'],\n      dtype='datetime64[ns]')address()&lt;U187...[1 values with dtype=&lt;U187]city()&lt;U18...[1 values with dtype=&lt;U18]latitude()float64...[1 values with dtype=float64]longitude()float64...[1 values with dtype=float64]state()&lt;U17...[1 values with dtype=&lt;U17]station()&lt;U64'IGI Airport (T3), Delhi - IMD'array('IGI Airport (T3), Delhi - IMD', dtype='&lt;U64')Data variables: (1)PM2.5(Timestamp)float64...unit :µg/m³[70080 values with dtype=float64]Indexes: (1)TimestampPandasIndexPandasIndex(DatetimeIndex(['2022-01-01 00:00:00', '2022-01-01 00:15:00',\n               '2022-01-01 00:30:00', '2022-01-01 00:45:00',\n               '2022-01-01 01:00:00', '2022-01-01 01:15:00',\n               '2022-01-01 01:30:00', '2022-01-01 01:45:00',\n               '2022-01-01 02:00:00', '2022-01-01 02:15:00',\n               ...\n               '2023-12-31 21:30:00', '2023-12-31 21:45:00',\n               '2023-12-31 22:00:00', '2023-12-31 22:15:00',\n               '2023-12-31 22:30:00', '2023-12-31 22:45:00',\n               '2023-12-31 23:00:00', '2023-12-31 23:15:00',\n               '2023-12-31 23:30:00', '2023-12-31 23:45:00'],\n              dtype='datetime64[ns]', name='Timestamp', length=70080, freq=None))Attributes: (0)\n\n\n\ndata = one_station_ds['PM2.5'].to_dataframe()[['PM2.5']]\n\n# convert to hourly data\ndata = data.resample('h').mean()\n\n# how much missing data\nprint(f\"Missing data: {data.isna().sum().values[0]}\")\n\n# fill missing data\ndata = data.interpolate(method='linear')\n\nprint(f\"Missing data after interpolation: {data.isna().sum().values[0]}\")\n\ndata.head()\n\nMissing data: 298\nMissing data after interpolation: 0\n\n\n\n\n\n\n\n\n\nPM2.5\n\n\nTimestamp\n\n\n\n\n\n2022-01-01 00:00:00\n273.5475\n\n\n2022-01-01 01:00:00\n268.8675\n\n\n2022-01-01 02:00:00\n258.0225\n\n\n2022-01-01 03:00:00\n194.9100\n\n\n2022-01-01 04:00:00\n197.9975\n\n\n\n\n\n\n\n\nimport timesfm\n\ntfm = timesfm.TimesFm(\n    context_len=32,\n    horizon_len=24,\n    input_patch_len=32,\n    output_patch_len=128,\n    num_layers=20,\n    model_dims=1280,\n    backend=\"gpu\",\n)\ntfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\n\nMultiprocessing context has already been set.\nConstructing model weights.\n\n\nWARNING:absl:No registered CheckpointArgs found for handler type: &lt;class 'paxml.checkpoints.FlaxCheckpointHandler'&gt;\nWARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\nWARNING:absl:train_state_unpadded_shape_dtype_struct is not provided. We assume `train_state` is unpadded.\n\n\nConstructed model weights in 3.76 seconds.\nRestoring checkpoint from /home/patel_zeel/.cache/huggingface/hub/models--google--timesfm-1.0-200m/snapshots/8775f7531211ac864b739fe776b0b255c277e2be/checkpoints.\n\n\n\n---------------------------------------------------------------------------\nMemoryError                               Traceback (most recent call last)\nCell In[6], line 12\n      1 import timesfm\n      3 tfm = timesfm.TimesFm(\n      4     context_len=32,\n      5     horizon_len=24,\n   (...)\n     10     backend=\"gpu\",\n     11 )\n---&gt; 12 tfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m\")\n\nFile ~/timesfm/src/timesfm.py:270, in TimesFm.load_from_checkpoint(self, checkpoint_path, repo_id, checkpoint_type, step)\n    268 self._logging(f\"Restoring checkpoint from {checkpoint_path}.\")\n    269 start_time = time.time()\n--&gt; 270 self._train_state = checkpoints.restore_checkpoint(\n    271     train_state_local_shapes,\n    272     checkpoint_dir=checkpoint_path,\n    273     checkpoint_type=checkpoint_type,\n    274     state_specs=train_state_partition_specs,\n    275     step=step,\n    276 )\n    277 self._logging(\n    278     f\"Restored checkpoint in {time.time() - start_time:.2f} seconds.\"\n    279 )\n    281 # Initialize and jit the decode fn.\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoints.py:246, in restore_checkpoint(state_global_shapes, checkpoint_dir, global_mesh, checkpoint_type, state_specs, step, enforce_restore_shape_check, state_unpadded_shape_dtype_struct, tensorstore_use_ocdbt, restore_transformations)\n    240 if checkpoint_type == CheckpointType.GDA:\n    241   restore_args = {\n    242       'specs': state_specs,\n    243       'mesh': global_mesh,\n    244       'transforms': restore_transformations,\n    245   }\n--&gt; 246 output = checkpoint_manager.restore(\n    247     step,\n    248     state_global_shapes,\n    249     state_unpadded_shape_dtype_struct,\n    250     restore_kwargs=restore_args,\n    251 )\n    252 # Note: `aux_items` argument wasn't passed to checkpoint_manager.restore()\n    253 # so this returns a TrainState instance.\n    254 return cast(train_states.TrainState, output)\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoint_managers.py:568, in OrbaxCheckpointManager.restore(self, step, train_state, train_state_unpadded_shape_dtype_struct, train_input_pipeline, restore_kwargs, aux_items, aux_restore_kwargs)\n    565 if train_input_pipeline and self._train_checkpoint_exists(step):\n    566   items[INPUT_ITEM_NAME] = train_input_pipeline\n--&gt; 568 restored = self._manager.restore(\n    569     step, items=items, restore_kwargs=restore_kwargs\n    570 )\n    572 # Skip metadata checks if using transformations, since the TrainState may be\n    573 # completely altered.\n    574 if self.version &gt; 1.0 and not uses_transformations:\n    575   # If unpadded shapes were not provided, skip the shape check for now, as\n    576   # there are many callers that need to be changed.\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/checkpoint_manager.py:1055, in CheckpointManager.restore(self, step, items, restore_kwargs, directory, args)\n   1052     args = typing.cast(args_lib.Composite, args)\n   1054 restore_directory = self._get_read_step_directory(step, directory)\n-&gt; 1055 restored = self._checkpointer.restore(restore_directory, args=args)\n   1056 if self._single_item:\n   1057   return restored[DEFAULT_ITEM_NAME]\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/checkpointer.py:170, in Checkpointer.restore(self, directory, *args, **kwargs)\n    168 logging.info('Restoring item from %s.', directory)\n    169 ckpt_args = construct_checkpoint_args(self._handler, False, *args, **kwargs)\n--&gt; 170 restored = self._handler.restore(directory, args=ckpt_args)\n    171 logging.info('Finished restoring checkpoint from %s.', directory)\n    172 utils.sync_global_processes('Checkpointer:restore', self._active_processes)\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/composite_checkpoint_handler.py:470, in CompositeCheckpointHandler.restore(self, directory, args)\n    468     continue\n    469   handler = self._get_or_set_handler(item_name, arg)\n--&gt; 470   restored[item_name] = handler.restore(\n    471       self._get_item_directory(directory, item_name), args=arg\n    472   )\n    473 return CompositeResults(**restored)\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/composite_checkpoint_handler.py:138, in _AsyncLegacyCheckpointHandlerWrapper.restore(self, directory, args)\n    137 def restore(self, directory: epath.Path, args: '_AsyncWrapperArgs'):\n--&gt; 138   return self._handler.restore(directory, *args.args, **args.kwargs)\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/paxml/checkpoints.py:685, in FlaxCheckpointHandler.restore(self, directory, item, restore_args, transforms, transforms_default_to_original, version)\n    680 str_pytree_state = str(pytree_state)\n    681 input_target = {\n    682     'flattened_state': flattened_state,\n    683     'str_pytree_state': str_pytree_state,\n    684 }\n--&gt; 685 restored_target = super().restore(directory, input_target)\n    686 # Flax restore_checkpoint returned input_target unchanged if\n    687 # no step specified and no checkpoint files present.\n    688 if restored_target is input_target:\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:1089, in PyTreeCheckpointHandler.restore(self, directory, item, restore_args, transforms, transforms_default_to_original, legacy_transform_fn, args)\n   1085   raise FileNotFoundError(\n   1086       f'Requested directory for restore does not exist at {directory}'\n   1087   )\n   1088 byte_limiter = get_byte_limiter(self._concurrent_gb)\n-&gt; 1089 structure, use_zarr3_metadata = self._get_internal_metadata(directory)\n   1090 # `checkpoint_restore_args` has a structure relative to the checkpoint,\n   1091 # while `restore_args` remains structured relative to the output.\n   1092 param_infos, checkpoint_restore_args = _get_restore_parameters(\n   1093     directory,\n   1094     item,\n   (...)\n   1102     else self._use_zarr3,\n   1103 )\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:1312, in PyTreeCheckpointHandler._get_internal_metadata(self, directory)\n   1296 def _get_internal_metadata(\n   1297     self, directory: epath.Path\n   1298 ) -&gt; Tuple[PyTree, Optional[bool]]:\n   1299   \"\"\"Gets limited information needed to fully restore the checkpoint.\n   1300 \n   1301   This information just consists of the restore type for each leaf, as well\n   (...)\n   1310     checkpoint.\n   1311   \"\"\"\n-&gt; 1312   aggregate_tree = self._read_aggregate_file(directory)\n   1313   flat_aggregate = utils.to_flat_dict(aggregate_tree, keep_empty_nodes=True)\n   1314   try:\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:1172, in PyTreeCheckpointHandler._read_aggregate_file(self, directory)\n   1170 checkpoint_path = directory / self._aggregate_filename\n   1171 if checkpoint_path.exists():\n-&gt; 1172   return self._aggregate_handler.deserialize(checkpoint_path)\n   1173 elif self._use_ocdbt:\n   1174   raise FileNotFoundError(\n   1175       f'Checkpoint structure file does not exist at {directory}.'\n   1176   )\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/orbax/checkpoint/aggregate_handlers.py:86, in MsgpackHandler.deserialize(self, path)\n     84 \"\"\"See superclass documentation.\"\"\"\n     85 if path.exists():\n---&gt; 86   msgpack = path.read_bytes()\n     87   return msgpack_utils.msgpack_restore(msgpack)\n     88 else:\n\nFile /opt/anaconda3/envs/tfm_env/lib/python3.10/site-packages/etils/epath/abstract_path.py:152, in Path.read_bytes(self)\n    150 \"\"\"Reads contents of self as bytes.\"\"\"\n    151 with self.open('rb') as f:\n--&gt; 152   return f.read()\n\nMemoryError:"
  },
  {
    "objectID": "posts/learnings_from_brick_kiln_project.html",
    "href": "posts/learnings_from_brick_kiln_project.html",
    "title": "Learnings from the Brick Kiln Project",
    "section": "",
    "text": "Labeling is the most important and effort-taking part of the project. It is also most confusing part if not done properly for the images. For example, we needed to make this decision for the images of the brick kilns: “If brick kiln firing chamber is visible fully or partially at a level where a human would be able to identify it as a brick kiln, we mark it as a brick kiln”.\nTo ensure good quality of labels, one should allow a small number of images to be labeled by multiple people and then compare the labels. This will help in identifying the mistakes in the labeling process and also help in improving the labeling instructions."
  },
  {
    "objectID": "posts/learnings_from_brick_kiln_project.html#points",
    "href": "posts/learnings_from_brick_kiln_project.html#points",
    "title": "Learnings from the Brick Kiln Project",
    "section": "",
    "text": "Labeling is the most important and effort-taking part of the project. It is also most confusing part if not done properly for the images. For example, we needed to make this decision for the images of the brick kilns: “If brick kiln firing chamber is visible fully or partially at a level where a human would be able to identify it as a brick kiln, we mark it as a brick kiln”.\nTo ensure good quality of labels, one should allow a small number of images to be labeled by multiple people and then compare the labels. This will help in identifying the mistakes in the labeling process and also help in improving the labeling instructions."
  },
  {
    "objectID": "posts/presentation_tips.html",
    "href": "posts/presentation_tips.html",
    "title": "Conference Presentation Tips",
    "section": "",
    "text": "General\n\nFirst page goes like this:\n\nTitle\nAuthors (Underline presenting author, no need to put * in case of equal contribution)\nAffiliations\nConference name\n\nIf importing figures from paper, avoid including the captions.\nInclude lot of images and less maths\nTalk should end with summary and not the future work or thank you slide or something.\nCite the references on the same slide in bottom.\n\nRefer to “Giving talks” section of this blog.\n\n\nDos and Don’ts\n\nNever put too detailed information difficult to grasp: a table with many numbers, a complex derivation all in one go, very complicated diagram."
  },
  {
    "objectID": "posts/2021-10-23-warped-gp.html",
    "href": "posts/2021-10-23-warped-gp.html",
    "title": "Input Warped GPs - A failed idea",
    "section": "",
    "text": "Comments\n\nWe are warping inputs \\(\\mathbf{x}\\) into \\(\\mathbf{w}\\cdot\\mathbf{x}\\)\nLearning second level GP over \\(\\mathbf{w}\\).\nAppling penalty over \\(\\mathbf{w}\\) if varies too much unnecessary.\nSee problems at the end of the notebook.\nWe need to check mathematical concerns related to this transformation.\n\n\nimport math\nimport numpy as np\nimport torch\nimport gpytorch\nfrom matplotlib import pyplot as plt\nimport regdata as rd\nfrom sklearn.cluster import KMeans\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nclass ExactNSGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood, num_latent):\n        super(ExactNSGPModel, self).__init__(train_x, train_y, likelihood)\n#         inds = np.random.choice(train_x.shape[0], size=num_latent, replace=False)\n#         self.x_bar = train_x[inds]\n        self.x_bar = torch.tensor(KMeans(n_clusters=num_latent).fit(train_x).cluster_centers_).to(train_x)\n        self.w_bar = torch.nn.Parameter(torch.ones(num_latent,).to(self.x_bar))\n        self.bias = torch.nn.Parameter(torch.zeros(1,).to(self.x_bar))\n        self.latent_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n#       We can fix noise to be minimum but it is not ideal. Ideally, noise should automatically reduce to reasonable value.\n#         self.latent_likelihood.raw_noise.requires_grad = False\n#         self.latent_likelihood.raw_noise = torch.tensor(-10.)\n        self.latent_model = ExactGPModel(self.x_bar, self.w_bar, self.latent_likelihood)\n        \n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        self.latent_model.eval()\n        with gpytorch.settings.detach_test_caches(False):  # needed to back propagate thru predictive posterior\n            self.latent_model.set_train_data(self.x_bar, self.w_bar, strict=False)\n            self.w = self.latent_likelihood(self.latent_model(x))  # predictive posterior\n        x_warped = x*self.w.mean[:, None] + self.bias\n        mean_x = self.mean_module(x_warped)\n        covar_x = self.covar_module(x_warped)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\ndef training(model, likelihood):\n    training_iter = 100\n\n    # Find optimal model hyperparameters\n    model.train()\n    likelihood.train()\n\n    # Use the adam optimizer\n    optimizer = torch.optim.Adam([\n        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n    ], lr=0.1)\n\n    # \"Loss\" for GPs - the marginal log likelihood\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n    for i in range(training_iter):\n        # Zero gradients from previous iteration\n        optimizer.zero_grad()\n        # Output from model\n        output = model(train_x)\n        # Calc loss and backprop gradients\n        try:\n            loss = -mll(output, train_y) + torch.square(model.w.mean-1).mean()\n#             print(model.latent_likelihood.noise)\n        except AttributeError:\n            loss = -mll(output, train_y)\n        loss.backward()\n#         print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n#             i + 1, training_iter, loss.item(),\n#             model.covar_module.base_kernel.lengthscale.item(),\n#             model.likelihood.noise.item()\n#         ))\n        optimizer.step()\n    \ndef predict_plot(model, likelihood, title):\n    # Get into evaluation (predictive posterior) mode\n    model.eval()\n    likelihood.eval()\n\n    # Test points are regularly spaced along [0,1]\n    # Make predictions by feeding model through likelihood\n    with torch.no_grad():\n        observed_pred = likelihood(model(test_x))\n\n    with torch.no_grad():\n        # Initialize plot\n        f, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n        # Get upper and lower confidence bounds\n        lower, upper = observed_pred.confidence_region()\n        # Plot training data as black stars\n        ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n        # Plot predictive means as blue line\n        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n        # Shade between the lower and upper confidence bounds\n        ax.fill_between(test_x.numpy().ravel(), lower.numpy(), upper.numpy(), alpha=0.5)\n        ax.legend(['Observed Data', 'Mean', 'Confidence'])\n        ax.set_title(title)\n    return observed_pred\n\n\ndef GP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactGPModel(train_x, train_y, likelihood)\n    \n    training(model, likelihood)\n    predict_plot(model, likelihood, 'GP')\n\ndef NSGP(num_latent):\n\n    # initialize likelihood and model\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    model = ExactNSGPModel(train_x, train_y, likelihood, num_latent)\n    \n    training(model, likelihood)\n    observed_pred = predict_plot(model, likelihood, 'NSGP')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x*model.w.mean[:, None], observed_pred.mean.numpy())\n        plt.title('Warped test inputs v/s test outputs')\n    \n    with torch.no_grad():\n        model.train()\n        model.forward(test_x)\n        plt.figure(figsize=(10,6))\n        plt.plot(test_x, model.w.mean, label='interpolated')\n        plt.scatter(model.x_bar, model.w_bar, label='learned')\n        plt.ylim(0,2)\n        plt.title('Test input v/s weights')\n        plt.legend()\n\n\n\nTesting over various datasets\n\ntrain_x, train_y, test_x = rd.DellaGattaGene(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Heinonen4(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Jump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.MotorcycleHelmet(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.Olympic(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineJump1D(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_x, train_y, test_x = rd.SineNoisy(backend='torch').get_data()\nGP(0)\nNSGP(num_latent=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblems\n\nTransformation from x to x_warped is not monotonic."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html",
    "href": "posts/numpy-algebra- copy.html",
    "title": "How numpy handles day-to-day algebra?",
    "section": "",
    "text": "import numpy as np\n\nnp.__version__\n\n118 False\n140 False\numath 8 False\numath 10 False\numath 12 True\ncore 73 False\nYes!\nnumeric 29 False\nnumeric 41 False\nnumeric 1128 False\nnumeric 2515 False\nnumeric 2517 True\ncore 75 False\ncore 77 True\ncore 83 True\ncore 94 True\n142 False\n144 True\n\n\n'1.25.2'"
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#motivation",
    "href": "posts/numpy-algebra- copy.html#motivation",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Motivation",
    "text": "Motivation\nIn this blog post, we will try to figure out how numpy handles seemingly simple math operations. The motivation behind this exploration is to figure out if there are a few foundational operations behind most of the frequently used functions in numpy. For the sake of right level of abstraction, we will not look into addition, subtraction, multiplication, and division."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#pi",
    "href": "posts/numpy-algebra- copy.html#pi",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Pi",
    "text": "Pi\n\nBackground\npi is an irrational number and computing its value to a certain precision is a challenging task. This video talks in detail how people used to compute pi in the past. At the time of writing this blog post, Google holds the record for computing pi to the highest precision to 100 trilian digits. They used y-cruncher program (it’s free. try it!) with Chudnovsky algorithm to compute pi. Here are the first 100 digits of pi:\n\\(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679\\)\n\n\nSource code\nThis is how pi is defined in numpy source code upto 36 digits.\n#define NPY_PI        3.141592653589793238462643383279502884  /* pi */\nLet’s verify it.\n\nprint(f\"{np.pi:.64f}\")\n\n3.1415926535897931159979634685441851615905761718750000000000000000\n\n\nHmm, that looks off. From 16th digit onwards, the values are different. Let’s try to figure out why.\n\npi = 3.141592653589793238462643383279502884\npi = np.array(pi, dtype=np.float64)\npi = f\"{pi:.64f}\"\nnp_pi = f\"{np.pi:.64f}\"\nassert np_pi == pi\n\nOkay, so it seems like converting 36 digits of pi to 64 bit precision went wrong from 16th digit onwards. What a waste of last 20 digits of pi due to floating point errors! Anyways, let’s move on."
  },
  {
    "objectID": "posts/numpy-algebra- copy.html#power",
    "href": "posts/numpy-algebra- copy.html#power",
    "title": "How numpy handles day-to-day algebra?",
    "section": "Power",
    "text": "Power\nLet’s find out what happens when you execute the following code in numpy.\n\nnumber = np.float64(1.1)\nnumber**1.2\n\n1.1211693641406024"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html",
    "href": "posts/2021-10-12-sparsegps.html",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#imports",
    "href": "posts/2021-10-12-sparsegps.html#imports",
    "title": "SparseGPs in Stheno",
    "section": "",
    "text": "# !pip install -U regdata\n\n\nimport regdata as rd\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport wbml.out as out\nfrom wbml.plot import tweak\n\nfrom stheno import B, GP, EQ, PseudoObsVFE, PseudoObsFITC\nfrom varz.torch import Vars, minimise_l_bfgs_b, parametrised, Positive\nimport lab.torch"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#data-preperation",
    "href": "posts/2021-10-12-sparsegps.html#data-preperation",
    "title": "SparseGPs in Stheno",
    "section": "Data preperation",
    "text": "Data preperation\n\n# Define points to predict at.\nx = B.linspace(0, 10, 100)\nx_obs = B.linspace(0, 7, 50_000)\nx_ind = B.linspace(0, 10, 20)\n\n# Construct a prior.\nf = GP(EQ().periodic(2 * B.pi))\n\n# Sample a true, underlying function and observations.\nf_true = B.sin(x)\ny_obs = B.sin(x_obs) + B.sqrt(0.5) * B.randn(*x_obs.shape)"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#plotting-function",
    "href": "posts/2021-10-12-sparsegps.html#plotting-function",
    "title": "SparseGPs in Stheno",
    "section": "Plotting function",
    "text": "Plotting function\n\ndef plot(method):\n    if method == 'VFE':\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            obs.mu(f.measure)[:, 0],\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()\n    else:\n        # Plot result.\n        plt.plot(x, f_true, label=\"True\", style=\"test\")\n        plt.scatter(\n            x_obs,\n            y_obs,\n            label=\"Observations\",\n            style=\"train\",\n            c=\"tab:green\",\n            alpha=0.35,\n        )\n        plt.scatter(\n            x_ind,\n            B.dense(f_post(x_ind).mean),\n            label=\"Inducing Points\",\n            style=\"train\",\n            s=20,\n        )\n        plt.plot(x, mean, label=\"Prediction\", style=\"pred\")\n        plt.fill_between(x, lower, upper, style=\"pred\")\n        tweak()\n\n        plt.show()"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-variational-free-energy-vfe-method",
    "title": "SparseGPs in Stheno",
    "section": "Sparse regression with Variational Free Energy (VFE) method",
    "text": "Sparse regression with Variational Free Energy (VFE) method\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsVFE(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('VFE')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "href": "posts/2021-10-12-sparsegps.html#sparse-regression-with-fully-independent-training-conditional-fitc-mehod",
    "title": "SparseGPs in Stheno",
    "section": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod",
    "text": "Sparse Regression with Fully Independent Training Conditional (FITC) mehod\n\n# Compute a pseudo-point approximation of the posterior.\nobs = PseudoObsFITC(f(x_ind), (f(x_obs, 0.5), y_obs))\n\n# Compute the ELBO.\nout.kv(\"ELBO\", obs.elbo(f.measure))\n\n# Compute the approximate posterior.\nf_post = f | obs\n\n# Make predictions with the approximate posterior.\nmean, lower, upper = f_post(x, 0.5).marginal_credible_bounds()\nplot('FITC')\n\nELBO:       -5.345e+04"
  },
  {
    "objectID": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "href": "posts/2021-10-12-sparsegps.html#hyperparameter-tuning-noisy-sine-data",
    "title": "SparseGPs in Stheno",
    "section": "Hyperparameter tuning (Noisy Sine data)",
    "text": "Hyperparameter tuning (Noisy Sine data)\n\ndef model(vs):\n    \"\"\"Constuct a model with learnable parameters.\"\"\"\n    return vs['variance']*GP(EQ().stretch(vs['length_scale']))\n\n\ntorch.manual_seed(123)\n\ndataObj = rd.SineNoisy(scale_X=False, scale_y=False, return_test=True, backend='torch')\nx_obs, y_obs, x = dataObj.get_data()\n\n\nplt.scatter(x_obs, y_obs, s=2);\n\n\n\n\n\n\n\n\n\nVFE\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsVFE(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nFITC\n\nvs = Vars(torch.float64)\nvs.positive(name=\"noise\")\nvs.positive(name=\"length_scale\");\nvs.positive(name=\"variance\");\nvs.positive(init=torch.linspace(0.4,0.6,10), shape=(10,), name='x_ind')\nvs.requires_grad(True)\n\noptimizer = torch.optim.Adam(vs.get_latent_vars(), lr=0.1)\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nlosses = []\n\ndef update(i):\n    optimizer.zero_grad()\n    gp = model(vs)\n    obs = PseudoObsFITC(gp(vs['x_ind']), (gp(x_obs, vs['noise']), y_obs))\n    loss = -obs.elbo(gp.measure)\n    losses.append(loss.item())\n    loss.backward()\n    optimizer.step()\n    \n    gp_post = gp | obs\n    mean, lower, upper = gp_post(x, vs['noise']).marginal_credible_bounds()\n    ind_mean = B.dense(gp_post(vs['x_ind']).mean)\n    \n    ax[0].cla();ax[1].cla();\n    ax[0].scatter(x_obs, y_obs, s=2)\n    with torch.no_grad():\n        ax[0].plot()\n        ax[0].plot(x, B.dense(mean), label='Prediction')\n        ax[0].fill_between(x.ravel(), lower, upper, alpha=0.2, label='Uncertainty')\n        ax[0].plot(x, dataObj.f(x), label='True')\n        ax[0].scatter(vs['x_ind'], ind_mean, label='Inducing points')\n    ax[0].set_xlabel('X')\n    ax[0].legend()\n    \n    ax[1].plot(losses, label='loss')\n    ax[1].set_xlabel('Iterations')\n    ax[1].legend()\n    \nanim = FuncAnimation(fig, update, range(50))\nrc('animation', html='jshtml')\nplt.close()\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/GNN_for_regression.html",
    "href": "posts/GNN_for_regression.html",
    "title": "Graph Neural Networks for Regression",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport GPy\n\nimport torch\nimport torch.nn as nn\n\nfrom tqdm import trange\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ndevice = \"cuda\""
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "href": "posts/GNN_for_regression.html#create-a-synthetic-dataset",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a synthetic dataset",
    "text": "Create a synthetic dataset\n\nnp.random.seed(0)\ntorch.random.manual_seed(4)\n\nN = 50\nx = np.linspace(-1, 1, N).reshape(-1, 1)\nkernel = GPy.kern.RBF(input_dim=1, variance=1, lengthscale=0.1)\ny = np.random.multivariate_normal(np.zeros(N), kernel.K(x)).reshape(-1, 1)\ny_noisy = y + np.random.normal(0, 0.1, N).reshape(-1, 1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y_noisy, test_size=0.4, random_state=0)\n\nplt.plot(x, y, label=\"True\");\nplt.plot(train_x, train_y, 'o', label='train')\nplt.plot(test_x, test_y, 'o', label='test')\nplt.legend();\n\nx, y, y_noisy = map(lambda x: torch.tensor(x).float().to(device), (x, y, y_noisy))\ntrain_x, test_x, train_y, test_y = map(lambda x: torch.tensor(x).float().to(device), (train_x, test_x, train_y, test_y))\nprint(x.shape, y.shape, y_noisy.shape)\n\ntorch.Size([50, 1]) torch.Size([50, 1]) torch.Size([50, 1])"
  },
  {
    "objectID": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "href": "posts/GNN_for_regression.html#fit-with-a-simple-mlp",
    "title": "Graph Neural Networks for Regression",
    "section": "Fit with a simple MLP",
    "text": "Fit with a simple MLP\n\ndef fit(model, x, y, A=None, lr=0.01, epochs=100):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    loss_fn = nn.MSELoss()\n    \n    if A is None:\n        inputs = (x,)\n    else:\n        inputs = (x, A)\n    \n    losses = []\n    pbar = trange(epochs)\n    for epoch in pbar:\n        optimizer.zero_grad()\n        y_hat = model(*inputs)\n        loss = loss_fn(y_hat, y)\n        losses.append(loss.item())\n        pbar.set_description(f\"Epoch {epoch} Loss: {loss.item()}\")\n        loss.backward()\n        optimizer.step()\n            \n    return losses\n\nclass SimpleMLP(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [nn.Linear(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\ntorch.manual_seed(0)\nmodel = SimpleMLP([10, 10, 10]).to(device)\nfit(model, train_x, train_y, lr=0.01, epochs=1000);\n\npred_y = model(x)\n\n(x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\nplt.plot(x_, y_, label=\"True\");\nplt.plot(train_x_, train_y_, 'o', label='train')\nplt.plot(test_x_, test_y_, 'o', label='test')\nplt.plot(x_, pred_y_, label='pred')\nplt.legend();\n\nEpoch 999 Loss: 0.07143261283636093: 100%|██████████| 1000/1000 [00:02&lt;00:00, 410.79it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "href": "posts/GNN_for_regression.html#create-a-gcn-layer",
    "title": "Graph Neural Networks for Regression",
    "section": "Create a GCN layer",
    "text": "Create a GCN layer\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x, A):    \n        return self.linear(A @ x)\n    \n    \nclass GCN(nn.Module):\n    def __init__(self, features):\n        super().__init__()\n        layers = [GCNLayer(1, features[0]), nn.ReLU()]\n        for in_features, out_features in zip(features, features[1:]):\n            layers.append(GCNLayer(in_features, out_features))\n            layers.append(nn.ReLU())\n            \n        layers.append(nn.Linear(features[-1], 1))\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x, A):\n        for layer in self.layers:\n            if isinstance(layer, GCNLayer):\n                x = layer(x, A)\n            else:\n                x = layer(x)\n        return x\n    \ndef get_eucledean_A(x, exponent):\n    d = ((x - x.T)**2)**0.5\n    d = torch.where(d==0, torch.min(d[d!=0])/2, d)  # self distance is 0, so replace it with half of the min distance\n    A = 1/(d**exponent)\n    return A/A.sum(dim=1, keepdim=True)\n\ndef get_KNN_A(x, k):\n    d = torch.abs(x - x.T)\n    A = torch.zeros_like(d)\n    _, indices = torch.topk(d, k, dim=1, largest=False)\n    for i, index in enumerate(indices):\n        A[i, index] = 1\n    return A/A.sum(dim=1, keepdim=True)\n\ndef fit_and_plot(title):\n    model = GCN([10, 10, 10]).to(device)\n    losses = fit(model, train_x, train_y, A=A_train, lr=0.001, epochs=3000);\n\n    pred_y = model(x, A_all)\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n    axes = ax[0]\n    axes.plot(losses)\n    axes.set_title(\"Losses\")\n\n    (x_, y_, train_x_, train_y_, test_x_, test_y_, pred_y_) = map(lambda x: x.cpu().detach().numpy(), (x, y, train_x, train_y, test_x, test_y, pred_y))\n    axes = ax[1]\n    axes.plot(x_, y_, label=\"True\");\n    axes.plot(train_x_, train_y_, 'o', label='train')\n    axes.plot(test_x_, test_y_, 'o', label='test')\n    axes.plot(x_, pred_y_, label='pred')\n    axes.set_title(title)\n    axes.legend();"
  },
  {
    "objectID": "posts/GNN_for_regression.html#idw-setting",
    "href": "posts/GNN_for_regression.html#idw-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "IDW setting",
    "text": "IDW setting\n\nexponent = 1\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.05447980388998985: 100%|██████████| 3000/3000 [00:07&lt;00:00, 390.93it/s] \n\n\n\n\n\n\n\n\n\n\nexponent = 2\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.06475391983985901: 100%|██████████| 3000/3000 [00:07&lt;00:00, 413.49it/s]\n\n\n\n\n\n\n\n\n\n\nexponent = 3\nA_train = get_eucledean_A(train_x, exponent).to(device)\nA_all = get_eucledean_A(x, exponent).to(device)\ntitle = f\"Distance based adjacency matrix with exponent {exponent}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.043554823845624924: 100%|██████████| 3000/3000 [00:08&lt;00:00, 367.28it/s]"
  },
  {
    "objectID": "posts/GNN_for_regression.html#knn-setting",
    "href": "posts/GNN_for_regression.html#knn-setting",
    "title": "Graph Neural Networks for Regression",
    "section": "KNN Setting",
    "text": "KNN Setting\n\nK = 1\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.04107221961021423: 100%|██████████| 3000/3000 [00:07&lt;00:00, 383.88it/s] \n\n\n\n\n\n\n\n\n\n\nK = 3\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.14372628927230835: 100%|██████████| 3000/3000 [00:07&lt;00:00, 404.74it/s]\n\n\n\n\n\n\n\n\n\n\nK = 7\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.13950258493423462: 100%|██████████| 3000/3000 [00:07&lt;00:00, 381.66it/s]\n\n\n\n\n\n\n\n\n\n\nK = 15\nA_train = get_KNN_A(train_x, K).to(device)\nA_all = get_KNN_A(x, K).to(device)\ntitle = f\"KNN based adjacency matrix with K={K}\"\n\nfit_and_plot(title)\n\nEpoch 2999 Loss: 0.33879855275154114: 100%|██████████| 3000/3000 [00:07&lt;00:00, 376.56it/s]"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html",
    "href": "posts/2022-10-31-stochastic-variational-gp.html",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "",
    "text": "I recently read a compact and clean explanation of SVGP in the following blog post by Dr. Martin Ingram:\nNow, I am attempting to implement a practical code from scratch for the same (What is practical about it? Sometimes math does not simply translate to code without careful modifications). I am assuming that you have read the blog post cited above before moving further. Let’s go for coding!"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#imports",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Imports",
    "text": "Imports\n\n# JAX\nimport jax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as jnp\nimport jax.scipy as jsp\n\n# Partially initialize functions\nfrom functools import partial\n\n# TFP\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\n# GP Kernels\nfrom tinygp import kernels\n\n# sklearn\nfrom sklearn.datasets import make_moons, make_blobs, make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Optimization\nimport optax\n\n# Plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['scatter.edgecolors'] = \"k\"\n\n# Progress bar\nfrom tqdm import tqdm\n\n# Jitter\nJITTER = 1e-6\n\n# Enable JAX 64bit\njax.config.update(\"jax_enable_x64\", True)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#dataset",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Dataset",
    "text": "Dataset\nFor this blog post, we will stick to the classification problem and pick a reasonable classification dataset.\n\nn_samples = 100\nnoise = 0.1\nrandom_state = 0\nshuffle = True\n\nX, y = make_moons(\n    n_samples=n_samples, random_state=random_state, noise=noise, shuffle=shuffle\n)\nX = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n\nX, y = map(jnp.array, (X, y))\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#methodology",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Methodology",
    "text": "Methodology\nTo define a GP, we need a kernel function. Let us use the RBF or Exponentiated Quadratic or Squared Exponential kernel.\n\nlengthscale = 1.0\nvariance = 1.0\n\nkernel_fn = variance * kernels.ExpSquared(scale=lengthscale)\n\nkernel_fn(X, X).shape\n\n(100, 100)\n\n\nAs explained in the blog post, we want to minimize the following loss function:\n\\[\nKL[q(u|\\eta) || p(u|y, \\theta)] = KL[q(u|\\eta) || p(u | \\theta)] - \\mathbb{E}_{u \\sim q(u|\\eta)} \\log p(y | u, \\theta) + const\n\\]\nLet us break down the loss and discuss each componant.\n\nKL divergence\nIn the first term, we want to compute the KL divergence between prior and variational distribution of GP at inducing points. First, we need to define the inducing points.\n\nkey = jax.random.PRNGKey(0)\nn_inducing = 10\nn_dim = X.shape[1]\n\nX_inducing = jax.random.normal(key, shape=(n_inducing, n_dim))\nX_inducing.shape\n\n(10, 2)\n\n\nNow, defining the prior and variational distributions.\n\ngp_mean = 0.43  # a scalar parameter to train\n\nprior_mean = gp_mean * jnp.zeros(n_inducing)\nprior_cov = kernel_fn(X_inducing, X_inducing)\n\nprior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n\nvariational_mean = jax.random.uniform(key, shape=(n_inducing,)) # a vector parameter to train\n\nA covariance matrix can not be learned directly due to positive definite constraint. We can decompose a covariance matrix in a following way:\n\\[\n\\begin{aligned}\nK &= diag(\\boldsymbol{\\sigma})\\Sigma diag(\\boldsymbol{\\sigma})\\\\\n  &= diag(\\boldsymbol{\\sigma})LL^T diag(\\boldsymbol{\\sigma})\n\\end{aligned}\n\\]\nWhere, \\(\\Sigma\\) is a correlation matrix, \\(L\\) is a lower triangular cholesky decomposition of \\(\\Sigma\\) and \\(\\boldsymbol{\\sigma}\\) is the variance vector. We can use tfb.CorrelationCholesky to generate \\(L\\) from an unconstrained vector:\n\nrandom_vector = jax.random.normal(key, shape=(3,))\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\ncorrelation = corr_chol@corr_chol.T\ncorrelation\n\nDeviceArray([[ 1.        ,  0.54464529, -0.7835968 ],\n             [ 0.54464529,  1.        , -0.33059078],\n             [-0.7835968 , -0.33059078,  1.        ]], dtype=float64)\n\n\nTo constrain \\(\\boldsymbol{\\sigma}\\), any positivity constraint would suffice. So, combining these tricks, we can model the covariance as following:\n\nrandom_vector = jax.random.normal(\n    key, shape=(n_inducing * (n_inducing - 1) // 2,)\n)  # a trainable parameter\nlog_sigma = jax.random.normal(key, shape=(n_inducing, 1))  # a trainable parameter\n\n\nsigma = jnp.exp(log_sigma)\ncorr_chol = tfb.CorrelationCholesky()(random_vector)\nvariational_cov = sigma * sigma.T * (corr_chol @ corr_chol.T)\nprint(variational_cov.shape)\n\nvariational_distribution = tfd.MultivariateNormalFullCovariance(variational_mean, variational_cov\n)\n\n(10, 10)\n\n\nNow, we can compute the KL divergence:\n\nvariational_distribution.kl_divergence(prior_distribution)\n\nDeviceArray(416.89357355, dtype=float64)\n\n\n\n\nExpectation over the likelihood\nWe want to compute the following expectation:\n\\[\n-\\sum_{i=1}^N \\mathbb{E}_{f_i \\sim q(f_i | \\eta, \\theta)} \\log p(y_i| f_i, \\theta)\n\\]\nNote that, \\(p(y_i| f_i, \\theta)\\) can be any likelihood depending upon the problem, but for classification, we may use a Bernoulli likelihood.\n\nf = jax.random.normal(key, shape=y.shape)\nlikelihood_distribution = tfd.Bernoulli(logits=f)\n\nlog_likelihood = likelihood_distribution.log_prob(y).sum()\nlog_likelihood\n\nDeviceArray(-72.04665624, dtype=float64)\n\n\nWe need to sample \\(f_i\\) from \\(q(f_i | \\eta, \\theta)\\) which has the following form:\n\\[\n\\begin{aligned}\nq(u) &\\sim \\mathcal{N}(\\boldsymbol{m}, S)\\\\\nq(f_i | \\eta, \\theta) &\\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\\\\n\\mu_i &= A\\boldsymbol{m}\\\\\n\\sigma_i^2 &= K_{ii} + A(S - K_{mm})A^T\\\\\nA &= K_{im}K_{mm}^{-1}\n\\end{aligned}\n\\]\nNote that matrix inversion is often unstable with jnp.linalg.inv and thus we will use cholesky tricks to compute \\(A\\).\n\ndef q_f(x_i):\n    x_i = x_i.reshape(1, -1) # ensure correct shape\n    K_im = kernel_fn(x_i, X_inducing)\n    K_mm = kernel_fn(X_inducing, X_inducing)\n    chol_mm = jnp.linalg.cholesky(K_mm + jnp.eye(K_mm.shape[0])*JITTER)\n    A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n    \n    mu_i = A@variational_mean\n    sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_cov - prior_cov)@A.T\n    \n    return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n\nHere is a function to compute log likelihood for a single data-point:\n\ndef log_likelihood(x_i, y_i, seed):\n    sample = q_f(x_i).sample(seed=seed)\n    log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n    return log_likelihood.squeeze()\n\n\nlog_likelihood(X[0], y[0], seed=key)\n\nDeviceArray(-0.17831203, dtype=float64)\n\n\nWe can use jax.vmap to compute log_likelihood over a batch. With that, we can leverage the stochastic variational inference following section 10.3.1 (Eq. 10.108) from pml book2. Basically, in each iteration, we need to multiply the batch log likelihood with \\(\\frac{N}{B}\\) to get an unbiased minibatch approximation where \\(N\\) is size of the full dataset and \\(B\\) is the batch size.\n\nbatch_size = 10\n\nseeds = jax.random.split(key, num=batch_size)\n\nll = len(y)/batch_size * jax.vmap(log_likelihood)(X[:batch_size], y[:batch_size], seeds).sum()\nll\n\nDeviceArray(-215.46520331, dtype=float64)\n\n\nNote that, once the parameters are optimized, we can use the derivations of \\(q(f_i | \\eta, \\theta)\\) to compute the posterior distribution. We have figured out all the pieces by now so it is the time to put it togather in a single class. Some pointers to note are the following:\n\nWe define a single function get_constrained_params to transform all unconstrained parameters.\njax.lax.scan gives a huge boost to a training loop.\nThere is some repeatation of code due to lack of super code optimization. You can do it at your end if needed."
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#all-in-one",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "All in one",
    "text": "All in one\n\nclass SVGP:\n    def __init__(self, X_inducing, data_size):\n        self.X_inducing = X_inducing\n        self.n_inducing = len(X_inducing)\n        self.data_size = data_size\n        \n    def init_params(self, seed):\n        variational_corr_chol_param = tfb.CorrelationCholesky().inverse(jnp.eye(self.n_inducing))\n        \n        dummy_params = {\"log_variance\": jnp.zeros(()),\n               \"log_scale\": jnp.zeros(()), \n               \"mean\": jnp.zeros(()),\n               \"X_inducing\": self.X_inducing,\n               \"variational_mean\": jnp.zeros(self.n_inducing),\n               \"variational_corr_chol_param\": variational_corr_chol_param,\n               \"log_variational_sigma\": jnp.zeros((self.n_inducing, 1)),\n               }\n        \n        flat_params, unravel_fn = ravel_pytree(dummy_params)\n        random_params = jax.random.normal(key, shape=(len(flat_params), ))\n        params = unravel_fn(random_params)\n        return params\n    \n    @staticmethod\n    def get_constrained_params(params):\n        return {\"mean\": params[\"mean\"],\n                \"variance\": jnp.exp(params['log_variance']), \n                \"scale\": jnp.exp(params['log_scale']), \n                \"X_inducing\": params[\"X_inducing\"],\n                \"variational_mean\": params[\"variational_mean\"],\n                \"variational_corr_chol_param\": params[\"variational_corr_chol_param\"],\n                \"variational_sigma\": jnp.exp(params[\"log_variational_sigma\"])}\n    \n    @staticmethod\n    def get_q_f(params, x_i, prior_distribution, variational_distribution):\n        x_i = x_i.reshape(1, -1) # ensure correct shape\n        \n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        K_im = kernel_fn(x_i, params[\"X_inducing\"])\n        K_mm = prior_distribution.covariance()\n        chol_mm = jnp.linalg.cholesky(K_mm)\n        A = jsp.linalg.cho_solve((chol_mm, True), K_im.T).T\n\n        mu_i = A@params[\"variational_mean\"]\n        sigma_sqr_i = kernel_fn(x_i, x_i) + A@(variational_distribution.covariance() - K_mm)@A.T\n\n        return tfd.Normal(loc=mu_i, scale=sigma_sqr_i**0.5)\n    \n    def get_distributions(self, params):\n        kernel_fn = params['variance'] * kernels.ExpSquared(scale=params[\"scale\"])\n        prior_mean = params[\"mean\"]\n        prior_cov = kernel_fn(params[\"X_inducing\"], params[\"X_inducing\"]) + jnp.eye(self.n_inducing)*JITTER\n        prior_distribution = tfd.MultivariateNormalFullCovariance(prior_mean, prior_cov)\n\n        corr_chol = tfb.CorrelationCholesky()(params[\"variational_corr_chol_param\"])\n        sigma = jnp.diag(params[\"variational_sigma\"])\n        variational_cov = sigma*sigma.T*(corr_chol@corr_chol.T) + jnp.eye(self.n_inducing)*JITTER\n        variational_distribution = tfd.MultivariateNormalFullCovariance(params[\"variational_mean\"], variational_cov)\n        \n        return prior_distribution, variational_distribution\n    \n    def loss_fn(self, params, X_batch, y_batch, seed):\n        params = self.get_constrained_params(params)\n        \n        # Get distributions\n        prior_distribution, variational_distribution = self.get_distributions(params)\n        \n        # Compute kl\n        kl = variational_distribution.kl_divergence(prior_distribution)\n\n        # Compute log likelihood\n        def log_likelihood_fn(x_i, y_i, seed):\n            q_f = self.get_q_f(params, x_i, prior_distribution, variational_distribution)\n            sample = q_f.sample(seed=seed)\n            log_likelihood = tfd.Bernoulli(logits=sample).log_prob(y_i)\n            return log_likelihood.squeeze()\n        \n        seeds = jax.random.split(seed, num=len(y_batch))\n        log_likelihood = jax.vmap(log_likelihood_fn)(X_batch, y_batch, seeds).sum() * self.data_size/len(y_batch)\n\n        return kl - log_likelihood\n    \n    def fit_fn(self, X, y, init_params, optimizer, n_iters, batch_size, seed):\n        state = optimizer.init(init_params)\n        value_and_grad_fn = jax.value_and_grad(self.loss_fn)\n        \n        def one_step(params_and_state, seed):\n            params, state = params_and_state\n            idx = jax.random.choice(seed, self.data_size, (batch_size,), replace=False)\n            X_batch, y_batch = X[idx], y[idx]\n            \n            seed2 = jax.random.split(seed, 1)[0]\n            loss, grads = value_and_grad_fn(params, X_batch, y_batch, seed2)\n            updates, state = optimizer.update(grads, state)\n            params = optax.apply_updates(params, updates)\n            return (params, state), (loss, params)\n        \n        seeds = jax.random.split(seed, num=n_iters)\n        (best_params, _), (loss_history, params_history) = jax.lax.scan(one_step, (init_params, state), xs=seeds)\n        return best_params, loss_history, params_history\n\n    def predict_fn(self, params, X_new):\n        constrained_params = self.get_constrained_params(params)\n        prior_distribution, variational_distribution = self.get_distributions(constrained_params)\n        \n        def _predict_fn(x_i):    \n            # Get posterior\n            q_f = self.get_q_f(constrained_params, x_i, prior_distribution, variational_distribution)\n            return q_f.mean().squeeze(), q_f.variance().squeeze()\n        \n        mean, var = jax.vmap(_predict_fn)(X_new)\n        return mean.squeeze(), var.squeeze()"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#train-and-predict",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Train and predict",
    "text": "Train and predict\n\nn_inducing = 20\nn_epochs = 100\nbatch_size = 10\ndata_size = len(y)\nn_iters = n_epochs*(data_size/batch_size)\nn_iters\n\n1000.0\n\n\n\nkey = jax.random.PRNGKey(0)\nkey2, subkey = jax.random.split(key)\noptimizer = optax.adam(learning_rate=0.01)\n\nX_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\nmodel = SVGP(X_inducing, data_size)\n\ninit_params = model.init_params(key2)\n\nmodel.loss_fn(init_params, X, y, key)\nbest_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\nplt.figure()\nplt.plot(loss_history);\nplt.title(\"Loss\");\n\n\n\n\n\n\n\n\n\nx = jnp.linspace(-3.5, 3.5, 100)\nseed = jax.random.PRNGKey(123)\n\nX1, X2 = jnp.meshgrid(x, x)\nf = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\npred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\nlogits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\nproba = jax.nn.sigmoid(logits)\n\nproba_mean = proba.mean(axis=0)\nproba_std2 = proba.std(axis=0)*2\n\n\nfig, ax = plt.subplots(1, 2, figsize=(12,4))\ncplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot1, ax=ax[0])\n\ncplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\nplt.colorbar(cplot2, ax=ax[1])\n\nax[0].scatter(X[:, 0], X[:, 1], c=y);\nax[1].scatter(X[:, 0], X[:, 1], c=y);\n\nax[0].set_title(\"Posterior $\\mu$\");\nax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");"
  },
  {
    "objectID": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "href": "posts/2022-10-31-stochastic-variational-gp.html#some-more-datasets",
    "title": "Stochastic Variational Gaussian processes in JAX",
    "section": "Some more datasets",
    "text": "Some more datasets\n\ndef fit_and_plot(X, y):\n    X = StandardScaler().fit_transform(X)  # Yes, this is useful for GPs\n    X, y = map(jnp.array, (X, y))\n\n    X_inducing = jax.random.choice(key, X, (n_inducing,), replace=False)\n    model = SVGP(X_inducing, data_size)\n\n    init_params = model.init_params(key2)\n\n    model.loss_fn(init_params, X, y, key)\n    best_params, loss_history, params_history = model.fit_fn(X, y, init_params, optimizer, n_iters, batch_size, subkey)\n\n    plt.figure()\n    plt.plot(loss_history);\n    plt.title(\"Loss\");\n    \n    f = lambda x1, x2: model.predict_fn(best_params, jnp.array([x1, x2]).reshape(1, -1))\n    pred_mean, pred_var = jax.vmap(jax.vmap(f))(X1, X2)\n    logits = tfd.Normal(pred_mean, pred_var**0.5).sample(seed=seed, sample_shape=(10000,))\n    proba = jax.nn.sigmoid(logits)\n\n    proba_mean = proba.mean(axis=0)\n    proba_std2 = proba.std(axis=0)*2\n    \n    fig, ax = plt.subplots(1, 2, figsize=(12,4))\n    cplot1 = ax[0].contourf(X1, X2, proba_mean.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot1, ax=ax[0])\n\n    cplot2 = ax[1].contourf(X1, X2, proba_std2.squeeze(), alpha=0.5, levels=20)\n    plt.colorbar(cplot2, ax=ax[1])\n\n    ax[0].scatter(X[:, 0], X[:, 1], c=y);\n    ax[1].scatter(X[:, 0], X[:, 1], c=y);\n\n    ax[0].set_title(\"Posterior $\\mu$\");\n    ax[1].set_title(\"Posterior $\\mu \\pm 2*\\sigma$\");\n\n\nmake_blobs\n\nX, y = make_blobs(n_samples=n_samples, random_state=random_state, centers=2)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmake_circles\n\nX, y = make_circles(n_samples=n_samples, random_state=random_state, noise=noise, factor=0.1)\n\nplt.scatter(X[:, 0], X[:, 1], c=y);\nfit_and_plot(X, y)"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html",
    "href": "posts/2021-09-27-constraints.html",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "",
    "text": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('font', **{'size':18})"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpy",
    "href": "posts/2021-09-27-constraints.html#gpy",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPy",
    "text": "GPy\n\nfrom paramz.transformations import Logexp\n\n\ngpy_trans = Logexp()\n\n\nx = torch.arange(-1000,10000).to(torch.float)\nplt.plot(x, gpy_trans.f(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpytorch",
    "href": "posts/2021-09-27-constraints.html#gpytorch",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPyTorch",
    "text": "GPyTorch\n\nfrom gpytorch.constraints import Positive\n\n\ngpytorch_trans = Positive()\n\n\nplt.plot(x, gpytorch_trans.transform(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');"
  },
  {
    "objectID": "posts/2021-09-27-constraints.html#gpflow",
    "href": "posts/2021-09-27-constraints.html#gpflow",
    "title": "How to apply constraint on parameters in various GP libraries",
    "section": "GPFlow",
    "text": "GPFlow\n\nfrom gpflow.utilities.bijectors import positive\n\n\ngpflow_trans = positive()\n\n\nplt.plot(x, gpflow_trans(x));\nplt.xlabel('X')\nplt.ylabel('f(X)');\n\n\n\n\n\n\n\n\n\nnp.allclose(gpy_trans.f(x), gpytorch_trans.transform(x))\n\nTrue\n\n\n\nnp.allclose(gpy_trans.f(x), gpflow_trans(x))\n\nTrue"
  },
  {
    "objectID": "posts/kl-divergence.html",
    "href": "posts/kl-divergence.html",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#ground",
    "href": "posts/kl-divergence.html#ground",
    "title": "KL divergence v/s cross-entropy",
    "section": "",
    "text": "In a classification problem, for a data-point \\(\\mathbf{x}_i\\), we have the true label \\(y_i\\) associated with it.\nLet us assume that we have three possible outcomes \\(\\{L1, L2, L3\\}\\) and for current \\(\\mathbf{x}_i\\), corresponding \\(y_i\\) is \\(L2\\). Then Ground truth probability distribution is the following:\n\\[\np_G(y = L1) = 0\\\\\np_G(y = L2) = 1\\\\\np_G(y=L3) = 0\n\\]\nLet us assume that our classifier model Predicted the following distribution:\n\\[\np_P(y = L1) = 0.1\\\\\np_P(y = L2) = 0.8\\\\\np_P(y=L3) = 0.1\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence",
    "href": "posts/kl-divergence.html#kl-divergence",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence",
    "text": "KL divergence\nWe can use KL divergence to check how good is our model. The formula is:\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} p_G(y_i)\\log\\frac{p_G(y_i)}{p_P(y_i)}\n\\]\nFor our example,\n\\[\nD_{KL}(p_G\\;\\rVert\\;p_P) = \\log\\frac{1}{0.8}\n\\]\nIt is evident that if \\(p_P(y = L2)\\) decreses from \\(0.8\\), \\(D_{KL}(p_G\\;\\rVert\\;p_P)\\) will increase and vice versa. Note that KL divergence is not symmetric which means \\(D_{KL}(p_G\\;\\rVert\\;p_P) \\ne D_{KL}(p_P\\;\\rVert\\;p_G)\\)."
  },
  {
    "objectID": "posts/kl-divergence.html#cross-entory",
    "href": "posts/kl-divergence.html#cross-entory",
    "title": "KL divergence v/s cross-entropy",
    "section": "Cross-entory",
    "text": "Cross-entory\nCross-entropy is another measure for distribution similarity. The formula is:\n\\[\nH(p_G, p_P) = \\sum_{y_i \\in \\{L1, L2, L3\\}} - p_G(y_i)\\log p_P(y_i)\n\\]\nFor our example:\n\\[\nH(p_G, p_P) = -\\log 0.8 = \\log \\frac{1}{0.8}\n\\]"
  },
  {
    "objectID": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "href": "posts/kl-divergence.html#kl-divergence-vs-cross-entropy",
    "title": "KL divergence v/s cross-entropy",
    "section": "KL divergence v/s cross-entropy",
    "text": "KL divergence v/s cross-entropy\nThis shows that KL divergence and cross-entropy will return the same values for a simple classification problem. Then why do we use cross-entropy as a loss function and not KL divergence?\nThat’s because KL divergence will compute additional constant terms (zero here) that are not adding any value in minimization."
  },
  {
    "objectID": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "href": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "title": "Active Learning with Bayesian Linear Regression",
    "section": "",
    "text": "A quick wrap-up for Bayesian Linear Regression (BLR)\nWe have a feature matrix \\(X\\) and a target vector \\(Y\\). We want to obtain \\(\\theta\\) vector in such a way that the error \\(\\epsilon\\) for the following equation is minimum.\n\\[\nY = X^T\\theta + \\epsilon\n\\] Prior PDF for \\(\\theta\\) is,\n\\[\np(\\theta) \\sim \\mathcal{N}(M_0, S_0)\n\\]\nWhere \\(S_0\\) is prior covariance matrix, and \\(M_0\\) is prior mean.\nPosterier PDF can be given as,\n\\[\n\\begin{aligned}\np(\\theta|X,Y) &\\sim \\mathcal{N}(\\theta | M_n, S_n) \\\\\nS_n &= (S_0^{-1} + \\sigma_{mle}^{-2}X^TX) \\\\\nM_n &= S_n(S_0^{-1}M_0+\\sigma_{mle}^{-2}X^TY)\n\\end{aligned}\n\\]\nMaximum likelihood estimation of \\(\\sigma\\) can be calculated as,\n\\[\n\\begin{aligned}\n\\theta_{mle} &= (X^TX)^{-1}X^TY \\\\\n\\sigma_{mle} &= ||Y - X^T\\theta_{mle}||\n\\end{aligned}\n\\]\nFinally, predicted mean \\(\\hat{Y}_{mean}\\) and predicted covariance matrix \\(\\hat{Y}_{cov}\\) can be given as,\n\\[\n\\begin{aligned}\n\\hat{Y} &\\sim \\mathcal{N}(\\hat{Y}_{mean}, \\hat{Y}_{cov}) \\\\\n\\hat{Y}_{mean} &= XM_n \\\\\n\\hat{Y}_{cov} &= X^TS_nX\n\\end{aligned}\n\\]\nNow, let’s put everything together and write a class for Bayesian Linear Regression.\n\n\nCreating scikit-learn like class with fit predict methods for BLR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 0 # random seed for train_test_split\n\n\nclass BLR():\n  def __init__(self,S0, M0): # M0 -&gt; prior mean, S0 -&gt; prior covariance matrix\n    self.S0 = S0\n    self.M0 = M0\n\n  def fit(self,x,y, return_self = False):\n    self.x = x\n    self.y = y\n\n    # Maximum likelihood estimation for sigma parameter\n    theta_mle = np.linalg.pinv(x.T@x)@(x.T@y)\n    sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2\n    sigma_mle = np.sqrt(sigma_2_mle)\n\n    # Calculating predicted mean and covariance matrix for theta\n    self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x)\n    self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze())\n\n    # Calculating predicted mean and covariance matrix for data\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    if return_self:\n      return (self.y_hat_map, self.pred_var)\n    \n  def predict(self, x):\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    return (self.y_hat_map, self.pred_var)\n\n  def plot(self, s=1): # s -&gt; size of dots for scatter plot\n    individual_var = self.pred_var.diagonal()\n    plt.figure()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.plot(self.x[:,1], self.y_hat_map, color='black', label='model')\n    plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color='black', label='uncertainty')\n    plt.scatter(self.x[:,1], self.y, label='actual data',s=s)\n    plt.title('MAE is '+str(np.mean(np.abs(self.y - self.y_hat_map))))\n    plt.legend()\n\n\n\nCreating & visualizing dataset\nTo start with, let’s create a random dataset with degree 3 polynomial function with some added noise.\n\\[\nY = (5X^3 - 4X^2 + 3X - 2) + \\mathcal{N}(0,1)\n\\]\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, )\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\nWe’ll try to fit a degree 5 polynomial function to our data.\n\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nN_features = X.shape[1]\n\n\nplt.scatter(X[:,1], Y, s=0.5, label = 'data points')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLearning a BLR model on the entire data\nWe’ll take \\(M_0\\) (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about \\(M_0\\). We’re taking \\(S_0\\) (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other.\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\n\n\n\nVisualising the fit\n\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nThis doesn’t look like a good fit, right? Let’s set the prior closer to the real values and visualize the fit again.\n\n\nVisualising the fit after changing the prior\n\nnp.random.seed(seed)\nS0 = np.eye(N_features)\nM0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, )\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nHmm, better. Now let’s see how it fits after reducing the noise and setting the prior mean to zero vector again.\n\n\nVisualising the fit after reducing the noise\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\n\n\n\n\nWhen the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit.\n\n\nIntuition to Active Learning (Uncertainty Sampling) with an example\nLet’s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How?\nWe train our model with existing data and test it on all the suspected patients’ data. Let’s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing.\nThis method is called Uncertainty Sampling in Active Learning. Now let’s formally define Active Learning. From Wikipedia,\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.\nNow, we’ll go through the active learning procedure step by step.\n\n\nTrain set, test set, and pool. What is what?\nThe train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty.\nSo, the algorithm can be represented as the following,\n\nTrain the model with the train set.\nTest the performance on the test set (This is expected to improve).\nTest the model with the pool.\nQuery for the most uncertain datapoint from the pool.\nAdd that datapoint into the train set.\nRepeat step 1 to step 5 for \\(K\\) iterations (\\(K\\) ranges from \\(0\\) to the pool size).\n\n\n\nCreating initial train set, test set, and pool\nLet’s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let’s start with 2 data points as the train set.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed)\n\nVisualizing train, test and pool.\n\nplt.scatter(test_X[:,1], test_Y, label='test set',color='r', s=2)\nplt.scatter(train_X[:,1], train_Y, label='train set',marker='s',color='k', s=50)\nplt.scatter(pool_X[:,1], pool_Y, label='pool',color='b', s=2)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nLet’s initialize a few dictionaries to keep track of each iteration.\n\ntrain_X_iter = {} # to store train points at each iteration\ntrain_Y_iter = {} # to store corresponding labels to the train set at each iteration\nmodels = {} # to store the models at each iteration\nestimations = {} # to store the estimations on the test set at each iteration\ntest_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n\n\n\nTraining & testing initial learner on train set (Iteration 0)\nNow we will train the model for the initial train set, which is iteration 0.\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\nCreating a plot method to visualize train, test and pool with estimations and uncertainty.\n\ndef plot(ax, model, init_title=''):\n  # Plotting the pool\n  ax.scatter(pool_X[:,1], pool_Y, label='pool',s=1,color='r',alpha=0.4)\n  \n  # Plotting the test data\n  ax.scatter(test_X[:,1], test_Y, label='test data',s=1, color='b', alpha=0.4)\n  \n  # Combining the test & the pool\n  test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y)\n  \n  # Sorting test_pool for plotting\n  sorted_inds = np.argsort(test_pool_X[:,1])\n  test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n  \n  # Plotting test_pool with uncertainty\n  model.predict(test_pool_X)\n  individual_var = model.pred_var.diagonal()\n  ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n  ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                  , alpha=0.2, color='black', label='uncertainty')\n  \n  # Plotting the train data\n  ax.scatter(model.x[:,1], model.y,s=40, color='k', marker='s', label='train data')\n  ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n  \n  # Plotting MAE on the test set\n  model.predict(test_X)\n  ax.set_title(init_title+' MAE is '+str(np.mean(np.abs(test_Y - model.y_hat_map))))\n  ax.set_xlabel('x')\n  ax.set_ylabel('y')\n  ax.legend()\n\nPlotting the estimations and uncertainty.\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nLet’s check the maximum uncertainty about any point for the model.\n\nmodels[0].pred_var.diagonal().max()\n\n4.8261426545316604e-29\n\n\nOops!! There is almost no uncertainty in the model. Why? let’s try again with more train points.\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed)\n\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\n\n\n\n\nNow uncertainty is visible, and currently, it’s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model.\nLet’s evaluate the performance on the test set.\n\nestimations[0], _ = models[0].predict(test_X)\ntest_mae_error[0] = np.mean(np.abs(test_Y - estimations[0]))\n\nMean Absolute Error (MAE) on the test set is\n\ntest_mae_error[0]\n\n0.5783654195019617\n\n\n\n\nMoving the most uncertain point from the pool to the train set\nIn the previous plot, we saw that the model was least certain about the left-most point. We’ll move that point from the pool to the train set and see the effect.\n\nesimations_pool, _ = models[0].predict(pool_X)\n\nFinding out a point having the most uncertainty.\n\nin_var = models[0].pred_var.diagonal().argmax()\nto_add_x = pool_X[in_var,:]\nto_add_y = pool_Y[in_var]\n\nAdding the point from the pool to the train set.\n\ntrain_X_iter[1] = np.vstack([train_X_iter[0], to_add_x])\ntrain_Y_iter[1] = np.append(train_Y_iter[0], to_add_y)\n\nDeleting the point from the pool.\n\npool_X = np.delete(pool_X, in_var, axis=0)\npool_Y = np.delete(pool_Y, in_var)\n\n\n\nTraining again and visualising the results (Iteration 1)\nThis time, we will pass previously learnt prior to the next iteration.\n\nS0 = np.eye(N_features)\nmodels[1] = BLR(S0, models[0].MN)\n\n\nmodels[1].fit(train_X_iter[1], train_Y_iter[1])\n\n\nestimations[1], _ = models[1].predict(test_X)\ntest_mae_error[1] = np.mean(np.abs(test_Y - estimations[1]))\n\nMAE on the test set is\n\ntest_mae_error[1]\n\n0.5779411133071186\n\n\nVisualizing the results.\n\nfig, ax = plt.subplots()\nplot(ax, models[1])\n\n\n\n\n\n\n\n\nBefore & after adding most uncertain point\n\nfig, ax = plt.subplots(1,2, figsize=(13.5,4.5))\nplot(ax[0], models[0],'Before')\nplot(ax[1], models[1],'After')\n\n\n\n\n\n\n\n\nWe can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data.\nNow let’s do this for few more iterations in a loop and visualise the results.\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x= np.zeros((num_iterations+1, N_features))\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(2, num_iterations+1):\n    # Making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool, var = models[iteration-1].predict(pool_X)\n    \n    # Finding the point from the pool with highest uncertainty\n    in_var = var.diagonal().argmax()\n    to_add_x = pool_X[in_var,:]\n    to_add_y = pool_Y[in_var]\n    points_added_x[iteration-1,:] = to_add_x\n    points_added_y.append(to_add_y)\n    \n    # Adding the point to the train set from the pool\n    train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x])\n    train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var, axis=0)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set\n    models[iteration] = BLR(S0, models[iteration-1].MN)\n    models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration])\n    \n    estimations[iteration], _ = models[iteration].predict(test_X)\n    test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n2 0.49023173501654815\n3 0.4923391714942153\n4 0.49040074812746753\n5 0.49610198614600165\n6 0.5015282102751122\n7 0.5051264429971314\n8 0.5099913097301352\n9 0.504455016053513\n10 0.5029219102020734\n11 0.5009762782262487\n12 0.5004883097883343\n13 0.5005169638980388\n14 0.5002731089932334\n15 0.49927485683909884\n16 0.49698416490822594\n17 0.49355398855432897\n18 0.49191185613804617\n19 0.491164833699368\n20 0.4908067530719673\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let’s visualise fits for all the iterations. We’ll discuss this behaviour after that.\n\n\nVisualizing active learning procedure\n\nprint('Initial model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[0].MN[::-1]))\nprint('\\nFinal model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[num_iterations].MN[::-1]))\n\nInitial model\nY = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63\n\nFinal model\nY = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58\n\n\n\ndef update(iteration):\n    ax.cla()\n    plot(ax, models[iteration])\n    fig.tight_layout()\n\n\nfig, ax = plt.subplots()\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250)\nplt.close()\nrc('animation', html='jshtml')\n\n\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually.\nNow, let’s put everything together and create a class for active learning procedure\n\n\nCreating a class for active learning procedure\n\nclass ActiveL():\n  def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1):\n    self.X_init = X\n    self.y = y\n    self.S0 = S0\n    self.M0 = M0\n    self.train_X_iter = {} # to store train points at each iteration\n    self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration\n    self.models = {} # to store the models at each iteration\n    self.estimations = {} # to store the estimations on the test set at each iteration\n    self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n    self.test_size = test_size\n    self.degree = degree\n    self.iterations = iterations\n    self.seed = seed\n    self.train_size = degree + 2\n\n  def data_preperation(self):\n    # Adding polynomial features\n    self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init)\n    N_features = self.X.shape[1]\n    \n    # Splitting into train, test and pool\n    train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, \n                                                                            test_size=self.test_size,\n                                                                            random_state=self.seed)\n    self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, \n                                                                            train_size=self.train_size, \n                                                                            random_state=self.seed)\n    \n    # Setting BLR prior incase of not given\n    if self.M0 == None:\n      self.M0 = np.zeros((N_features, ))\n    if self.S0 == None:\n      self.S0 = np.eye(N_features)\n    \n  def main(self):\n    # Training for iteration 0\n    self.train_X_iter[0] = self.train_X\n    self.train_Y_iter[0] = self.train_Y\n    self.models[0] = BLR(self.S0, self.M0)\n    self.models[0].fit(self.train_X, self.train_Y)\n\n    # Running loop for all iterations\n    for iteration in range(1, self.iterations+1):\n      # Making predictions on the pool set based on model learnt in the respective train set \n      estimations_pool, var = self.models[iteration-1].predict(self.pool_X)\n      \n      # Finding the point from the pool with highest uncertainty\n      in_var = var.diagonal().argmax()\n      to_add_x = self.pool_X[in_var,:]\n      to_add_y = self.pool_Y[in_var]\n      \n      # Adding the point to the train set from the pool\n      self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x])\n      self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y)\n      \n      # Deleting the point from the pool\n      self.pool_X = np.delete(self.pool_X, in_var, axis=0)\n      self.pool_Y = np.delete(self.pool_Y, in_var)\n      \n      # Training on the new set\n      self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN)\n      self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration])\n      \n      self.estimations[iteration], _ = self.models[iteration].predict(self.test_X)\n      self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean()\n\n  def _plot_iter_MAE(self, ax, iteration):\n    ax.plot(list(self.test_mae_error.values())[:iteration+1], 'ko-')\n    ax.set_title('MAE on test set over iterations')\n    ax.set_xlim((-0.5, self.iterations+0.5))\n    ax.set_ylabel(\"MAE on test set\")\n    ax.set_xlabel(\"# Points Queried\")\n  \n  def _plot(self, ax, model):\n    # Plotting the pool\n    ax.scatter(self.pool_X[:,1], self.pool_Y, label='pool',s=1,color='r',alpha=0.4)\n    \n    # Plotting the test data\n    ax.scatter(self.test_X[:,1], self.test_Y, label='test data',s=1, color='b', alpha=0.4)\n    \n    # Combining test_pool\n    test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y)\n    \n    # Sorting test_pool\n    sorted_inds = np.argsort(test_pool_X[:,1])\n    test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n    \n    # Plotting test_pool with uncertainty\n    preds, var = model.predict(test_pool_X)\n    individual_var = var.diagonal()\n    ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n    ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                    , alpha=0.2, color='black', label='uncertainty')\n    \n    # plotting the train data\n    ax.scatter(model.x[:,1], model.y,s=10, color='k', marker='s', label='train data')\n    ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n    \n    # plotting MAE\n    preds, var = model.predict(self.test_X)\n    ax.set_title('MAE is '+str(np.mean(np.abs(self.test_Y - preds))))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    \n  def visualise_AL(self):\n    fig, ax = plt.subplots(1,2,figsize=(13,5))\n    def update(iteration):\n      ax[0].cla()\n      ax[1].cla()\n      self._plot(ax[0], self.models[iteration])\n      self._plot_iter_MAE(ax[1], iteration)\n      fig.tight_layout()\n\n    print('Initial model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[0].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n    print('\\nFinal model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[self.iterations].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250)\n    plt.close()\n\n    rc('animation', html='jshtml')\n    return anim\n\n\n\nVisualizing a different polynomial fit on the same dataset\nLet’s try to fit a degree 7 polynomial to the same data now.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nmodel = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7\n\nFinal model\nY = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations.\n\n\nActive learning for diabetes dataset from the Scikit-learn module\nLet’s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We’ll choose only ‘weight’ feature, which seems to have more correlation with the target.\nWe’ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let’s check the performance of Scikit-learn linear regression model.\n\nX, Y = datasets.load_diabetes(return_X_y=True)\nX = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression\n\n# Normalizing\nX = (X - X.min())/(X.max() - X.min())\nY = (Y - Y.min())/(Y.max() - Y.min())\n\nVisualizing the dataset.\n\nplt.scatter(X, Y)\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s fit the Scikit-learn linear regression model with 50% train-test split.\n\nfrom sklearn.linear_model import LinearRegression\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed)\n\n\nclf = LinearRegression()\n\n\nclf.fit(train_X, train_Y)\npred_Y = clf.predict(test_X)\n\nVisualizing the fit & MAE.\n\nplt.scatter(X, Y, label='data', s=5)\nplt.plot(test_X, pred_Y, label='model', color='r')\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.title('MAE is '+str(np.mean(np.abs(pred_Y - test_Y))))\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNow we’ll fit the same data with our BLR model\n\nmodel = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = 0.41 + 0.16 X^1\n\nFinal model\nY = 0.13 + 0.86 X^1\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nInitially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It’s interesting to see that our initial train points tend to make a vertical fit, but the model doesn’t get carried away by that and stabilizes the self with prior.\n\nprint('MAE for Scikit-learn Linear Regression is',np.mean(np.abs(pred_Y - test_Y)))\nprint('MAE for Bayesian Linear Regression is', model.test_mae_error[20])\n\nMAE for Scikit-learn Linear Regression is 0.15424985705353944\nMAE for Bayesian Linear Regression is 0.15738001811804758\n\n\nAt the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we’ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit."
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html",
    "href": "posts/non-gaussian-likelihood-mlps.html",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "",
    "text": "# %pip install mapie\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.distributions as dist\n\nfrom tqdm import tqdm\n\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mapie.metrics import regression_coverage_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)\n\nN = 100\nx = dist.Uniform(-1, 1).sample((N, 1)).sort(dim=0).values\nx_test = torch.linspace(-1, 1, 2 * N).view(-1, 1).sort(dim=0).values\ny = 3 * x**3 - 2 * x + 1\ny_noisy = y + dist.Gamma(0.1, 0.3).sample((N, 1))\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\n\nplt.legend()\nprint(\"x.shape:\", x.shape, \"y.shape:\", y.shape)\n\nx.shape: torch.Size([100, 1]) y.shape: torch.Size([100, 1])"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#define-a-gaussiangamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Define a Gaussian/Gamma MLP",
    "text": "Define a Gaussian/Gamma MLP\n\nclass ProbabilisticMLP(nn.Module):\n    def __init__(self, input_dim, feature_dims, type):\n        super().__init__()\n        self.input_dim = input_dim\n        self.feature_dims = feature_dims\n        self.type = type  # \"gaussian\" or \"gamma\"\n\n        self.layers = nn.ModuleList()\n        self.layers.append(nn.Linear(input_dim, feature_dims[0]))\n        for i in range(len(feature_dims) - 1):\n            self.layers.append(nn.Linear(feature_dims[i], feature_dims[i + 1]))\n        self.layers.append(nn.Linear(feature_dims[-1], 2))\n\n        # likelihood parameters\n        # if self.type == \"gaussian\":\n        #     self.register_buffer(\"likelihood_mean\", torch.zeros(1))\n        #     self.likelihood_log_std = nn.Parameter(torch.zeros(1))\n        # elif self.type == \"gamma\":\n        #     self.likelihood_log_concentration = nn.Parameter(torch.zeros(1))\n        #     self.likelihood_log_rate = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = torch.relu(layer(x))\n\n        if self.type == \"gaussian\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_mean = self.likelihood_mean.expand(y_pred.shape[0])\n            # likelihood_log_std = self.likelihood_log_std.expand(y_pred.shape[0])\n            # likelihood_std = torch.exp(likelihood_log_std)\n            # return y_pred, likelihood_mean, likelihood_std\n\n            y_out = self.layers[-1](x)\n            mean = y_out[:, 0]\n            log_std = y_out[:, 1]\n            std = torch.exp(log_std)\n            return mean.ravel(), std.ravel()\n\n        elif self.type == \"gamma\":\n            # y_pred = self.layers[-1](x)\n            # likelihood_log_concentration = self.likelihood_log_concentration.expand(\n            #     y_pred.shape[0]\n            # )\n            # likelihood_log_rate = self.likelihood_log_rate.expand(y_pred.shape[0])\n            # likelihood_concentration = torch.exp(likelihood_log_concentration)\n            # likelihood_rate = torch.exp(likelihood_log_rate)\n            # return y_pred, likelihood_concentration, likelihood_rate\n\n            y_out = self.layers[-1](x)\n            log_concentration = y_out[:, 0]\n            log_rate = y_out[:, 1]\n            concentration = torch.exp(log_concentration)\n            rate = torch.exp(log_rate)\n            return concentration, rate\n\n    def loss_fn(self, y, param1, param2):\n        if self.type == \"gaussian\":\n            # epsilon = y - y_pred\n            # mean = param1\n            # std = param2\n            # dist = torch.distributions.Normal(mean, std + 1e-6)\n            # return -dist.log_prob(epsilon).mean()\n            mean = param1\n            std = param2\n            dist = torch.distributions.Normal(mean, std + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()\n\n        elif self.type == \"gamma\":\n            # epsilon = torch.clip(y - y_pred, min=1e-6, max=1e6)\n            # concentration = param1\n            # rate = param2\n            # dist = torch.distributions.Gamma(concentration, rate)\n            # return -dist.log_prob(epsilon).mean()\n            concentration = param1\n            rate = param2\n            dist = torch.distributions.Gamma(concentration + 1e-3, rate + 1e-3)\n            return -dist.log_prob(y.ravel()).mean()"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gaussian-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gaussian MLP",
    "text": "Fit Gaussian MLP\n\ntorch.manual_seed(0)\n\nmodel = ProbabilisticMLP(1, [32, 32], \"gaussian\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 500\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.4503: 100%|██████████| 500/500 [00:01&lt;00:00, 291.18it/s]\n\n\n\n\n\n\n\n\n\n\n# sns.kdeplot(param2.cpu().detach().numpy(), label=\"std\")\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x_test.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n    # y_mean = y_pred.cpu().numpy().ravel() + mean.cpu().numpy().ravel()\n    # y_std = std.cpu().numpy().ravel()\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mean, label=\"y_mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    y_mean - 2 * y_std,\n    y_mean + 2 * y_std,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    y_mean, y_std = model(x.to(device))\n    y_mean = y_mean.cpu().numpy().ravel()\n    y_std = y_std.cpu().numpy().ravel()\n\nupper = y_mean + 2 * y_std\nlower = y_mean - 2 * y_std\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.91"
  },
  {
    "objectID": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "href": "posts/non-gaussian-likelihood-mlps.html#fit-gamma-mlp",
    "title": "Non-Gaussian Likelihoods for MLPs",
    "section": "Fit Gamma MLP",
    "text": "Fit Gamma MLP\n\nmodel = ProbabilisticMLP(1, [32, 32, 32], \"gamma\").to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 1000\n\npbar = tqdm(range(n_epochs))\nlosses = []\nfor epoch in pbar:\n    optimizer.zero_grad()\n    param1, param2 = model(x.to(device))\n    loss = model.loss_fn(y_noisy.to(device), param1, param2)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())\n\n    pbar.set_description(f\"loss: {loss.item():.4f}\")\n\nplt.plot(losses)\n\nloss: 0.0775: 100%|██████████| 1000/1000 [00:03&lt;00:00, 266.98it/s]\n\n\n\n\n\n\n\n\n\n\nfrom scipy.special import gammaincinv, gamma\n\nwith torch.no_grad():\n    concetration, rate = model(x_test.to(device))\n    concetration = concetration.cpu().ravel().numpy()\n    rate = rate.cpu().ravel().numpy()\n\n    y_mode = (concetration - 1) / rate\n\n    quantile_fn = lambda p: gammaincinv(concetration, gamma(concetration) * p) / rate\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nplt.plot(x, y, label=\"true\", color=\"C0\")\nplt.scatter(x, y_noisy, label=\"noisy data\", color=\"C1\")\nplt.plot(x_test, y_mode, label=\"mean\", color=\"C2\")\nplt.fill_between(\n    x_test.squeeze(),\n    lower,\n    upper,\n    alpha=0.3,\n    color=\"C2\",\n    label=\"95% CI\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    param1, param2 = model(x.to(device))\n    concetration = param1.cpu().numpy().ravel()\n    rate = param2.cpu().numpy().ravel()\n\n    upper = quantile_fn(0.975)\n    lower = quantile_fn(0.025)\n\nregression_coverage_score(y_noisy.numpy(), lower, upper)\n\n0.07"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom time import time\n\n# Enable high precision\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n# To enable animation inside notebook\nplt.rc(\"animation\", html=\"jshtml\")"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#create-dataset",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Create dataset",
    "text": "Create dataset\n\nfeatures, labels = make_blobs(100, n_features=2, centers=2, random_state=0)\nplt.scatter(features[:, 0], features[:, 1], c=labels);\n\n\n\n\n\n\n\n\n\nprint(features.shape, features.dtype, labels.shape, labels.dtype)\n\n(100, 2) float64 (100,) int64"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-newtons-method-naive-way",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing Newton’s method (naive way)",
    "text": "Implementing Newton’s method (naive way)\nWe will first try to implement Eq. 10.31 directly from PML book1:\n\\[\n\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_{t}-\\eta_{t} \\mathbf{H}_{t}^{-1} \\boldsymbol{g}_{t}\n\\]\n\ndef get_logits(params, feature):  # for a single data-point\n  logits = jnp.sum(feature * params[\"w\"]) + params[\"b\"]\n  return logits\n\ndef naive_loss(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n\n  # Check if label is 1 or 0\n  is_one = (label == 1)\n  loss_if_one = lambda: -jnp.log(prob)  # loss if label is 1\n  loss_if_zero = lambda: -jnp.log(1 - prob)  # loss if labels is 0\n\n  # Use lax.cond to convert if..else.. in jittable format\n  loss = jax.lax.cond(is_one, loss_if_one, loss_if_zero)\n\n  return loss\n\ndef naive_loss_batch(params, features, labels):  # for a batch of data-points\n   losses = jax.vmap(naive_loss, in_axes=(None, 0, 0))(params, features, labels)\n   return jnp.mean(losses)\n\nWriting the train function\n\ndef naive_train_step(params, features, labels, learning_rate):\n  # Find gradient\n  loss_value, grads = jax.value_and_grad(naive_loss_batch)(params, features, labels)\n\n  # Find Hessian\n  hess = jax.hessian(naive_loss_batch)(params, features, labels)\n\n  # Adjust Hessian matrix nicely\n  hess_matrix = jnp.block([[hess[\"b\"][\"b\"], hess[\"b\"][\"w\"]],\n                           [hess[\"w\"][\"b\"], hess[\"w\"][\"w\"]]])\n  \n  # Adjust gradient vector nicely\n  grad_vector = jnp.r_[grads[\"b\"], grads[\"w\"]]\n\n  # Find H^-1g\n  h_inv_g = jnp.dot(jnp.linalg.inv(hess_matrix), grad_vector)\n\n  # Get back the structure\n  h_inv_g = {\"b\": h_inv_g[0], \"w\": h_inv_g[1:]}\n\n  # Apply the update\n  params = jax.tree_map(lambda p, g: p - learning_rate*g, params, h_inv_g)\n\n  return params, loss_value\n\n# First order method\n# vg = jax.value_and_grad(naive_loss_batch)\n# def train_step(params, features, labels, learning_rate):\n#   # Find gradient\n#   loss_value, grads = vg(params, features, labels)\n\n#   # Apply the update\n#   params = jax.tree_map(lambda p, g: p - learning_rate*g, params, grads)\n\n#   return params, loss_value\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3, ))\n# \"b\" should have shape (1,) for hessian trick with jnp.block to work\nparams = {\"w\": random_params[:2], \"b\": random_params[2].reshape(1,)}\nlearning_rate = 1.0\nepochs = 20\n\ntrain_step_jitted = jax.jit(naive_train_step)\n\nhistory = {\"loss\": [], \"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels, learning_rate)\n\ninit = time()\nfor _ in range(epochs):\n  history[\"params\"].append(params)\n  params, loss_value = train_step_jitted(params, features, labels, learning_rate)\n  history[\"loss\"].append(loss_value)\nprint(time() - init, \"seconds\")\nprint(params)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n0.0015490055084228516 seconds\n{'b': DeviceArray([13.22076694], dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}\n\n\nA helper function to animate the learning.\n\ndef animate(history):\n  fig, ax = plt.subplots(1, 2, figsize=(10,4))\n  def update(idx):\n    # Clear previous frame\n    ax[0].cla()\n    ax[1].cla()\n\n    # Plot data\n    params = history[\"params\"][idx]\n    losses = history[\"loss\"][:idx]\n    ax[0].scatter(features[:, 0], features[:, 1], c=labels)\n    \n    # Calculate and plot decision boundary\n    x0_min, x0_max = features[:, 0].min(), features[:, 0].max()\n    x1_min = -(params[\"b\"] + params[\"w\"][0] * x0_min)/params[\"w\"][1]\n    x1_max = -(params[\"b\"] + params[\"w\"][0] * x0_max)/params[\"w\"][1]\n\n    ax[0].plot([x0_min, x0_max], [x1_min, x1_max], label='decision boundary')\n\n    # Plot losses\n    ax[1].plot(losses, label=\"loss\")\n    ax[1].set_xlabel(\"Iterations\")\n\n    ax[0].legend()\n    ax[1].legend()\n\n  anim = FuncAnimation(fig, update, range(epochs))\n  plt.close()\n  return anim\n\n\nanimate(history)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#implementing-irls-algorithm",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Implementing IRLS algorithm",
    "text": "Implementing IRLS algorithm\n\ndef get_s_and_z(params, feature, label):  # for a single data-point\n  logits = get_logits(params, feature)\n  prob = jax.nn.sigmoid(logits)\n  s = prob * (1 - prob)\n  z = logits + (label - prob)/s\n  return s, z\n\ndef irls_train_step(params, features, labels):\n  s, z = jax.vmap(get_s_and_z, in_axes=(None, 0, 0))(params, features, labels)\n  S = jnp.diag(s.flatten())  # convert into a diagonal matrix\n\n  # Add column with ones\n  X = jnp.c_[jnp.ones(len(z)), features]\n\n  # Get weights\n  weights = jnp.linalg.inv(X.T@S@X)@X.T@S@z.flatten()\n\n  # get correct format\n  params = {\"b\": weights[0], \"w\": weights[1:]}\n\n  return params\n\n\nkey = jax.random.PRNGKey(0)\nrandom_params = jax.random.normal(key, shape=(3,))\nparams = {\"w\": random_params[:2], \"b\": random_params[2]}\nepochs = 20\n\ntrain_step_jitted = jax.jit(irls_train_step)\n\nirls_history = {\"params\": []}\n\n# warm up\ntrain_step_jitted(params, features, labels)\n\ninit = time()\nfor _ in range(epochs):\n  irls_history[\"params\"].append(params)\n  params = train_step_jitted(params, features, labels)\nprint(time() - init, \"seconds\")\nprint(params)\n\n0.0016303062438964844 seconds\n{'b': DeviceArray(13.22076694, dtype=float64), 'w': DeviceArray([ 0.59021174, -5.18797851], dtype=float64)}"
  },
  {
    "objectID": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "href": "posts/2022-05-14-iteratively_reweighted_least_squares.html#comparison",
    "title": "Iteratively reweighted least squares (IRLS) logistic regression",
    "section": "Comparison",
    "text": "Comparison\n\nnaive_params_b = list(map(lambda x: x[\"b\"], history[\"params\"]))\nirls_params_b = list(map(lambda x: x[\"b\"], irls_history[\"params\"]))\n\nnaive_params_w = list(map(lambda x: x[\"w\"], history[\"params\"]))\nirls_params_w = list(map(lambda x: x[\"w\"], irls_history[\"params\"]))\n\n\nplt.plot(naive_params_b, \"o-\", label=\"Naive\")\nplt.plot(irls_params_b, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Bias\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(naive_params_w, \"o-\", label=\"Naive\")\nplt.plot(irls_params_w, label=\"IRLS\")\nplt.xlabel(\"Iterations\")\nplt.title(\"Weights\")\nplt.legend();"
  },
  {
    "objectID": "posts/CNPs_for_Images.html",
    "href": "posts/CNPs_for_Images.html",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# turn off preallocation by JAX\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nimport distrax as dx\n\nimport optax\n\n# load mnist dataset from tensorflow datasets\nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n# define initializers\ndef first_layer_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-1.0/num_input, maxval=1.0/num_input)\n\ndef other_layers_init(key, shape, dtype=jnp.float32):\n    num_input = shape[0]  # reverse order compared to torch\n    return jax.random.uniform(key, shape, dtype, minval=-np.sqrt(6 / num_input)/30, maxval=np.sqrt(6 / num_input)/30)\n\nclass Encoder(nn.Module):\n  features: list\n  encoding_dims: int\n\n  @nn.compact\n  def __call__(self, x_context, y_context):\n    x = jnp.hstack([x_context, y_context.reshape(x_context.shape[0], -1)])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n    \n    \n    for n_features in self.features[1:]:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.encoding_dims)(x)\n\n    representation = x.mean(axis=0, keepdims=True)   # option 1\n    return representation  # (1, encoding_dims)\n\nclass Decoder(nn.Module):\n  features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, representation, x):\n    representation = jnp.repeat(representation, x.shape[0], axis=0)\n    x = jnp.hstack([representation, x])\n    \n    x = nn.Dense(self.features[0], kernel_init=first_layer_init, bias_init=first_layer_init)(x)\n    x = jnp.sin(30*x)\n    # x = nn.Dense(self.features[0])(x)\n    # x = nn.relu(x)\n\n    for n_features in self.features:\n      x = nn.Dense(n_features, kernel_init=other_layers_init, bias_init=other_layers_init)(x)\n      x = jnp.sin(30*x)\n      # x = nn.Dense(n_features)(x)\n      # x = nn.relu(x)\n\n    x = nn.Dense(self.output_dim*2)(x)\n    loc, raw_scale = x[:, :self.output_dim], x[:, self.output_dim:]\n    scale = jnp.exp(raw_scale)\n    \n    return loc, scale\n\nclass CNP(nn.Module):\n  encoder_features: list\n  encoding_dims: int\n  decoder_features: list\n  output_dim: int\n\n  @nn.compact\n  def __call__(self, x_content, y_context, x_target):\n    representation = Encoder(self.encoder_features, self.encoding_dims)(x_content, y_context)\n    loc, scale = Decoder(self.decoder_features, self.output_dim)(representation, x_target)\n    return loc, scale\n\n  def loss_fn(self, params, x_context, y_context, x_target, y_target):\n    loc, scale = self.apply(params, x_context, y_context, x_target)\n    predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=0.005+scale)\n    return -predictive_distribution.log_prob(y_target)"
  },
  {
    "objectID": "posts/CNPs_for_Images.html#load-mnist",
    "href": "posts/CNPs_for_Images.html#load-mnist",
    "title": "Conditional Neural Processes for Image Interpolation",
    "section": "Load MNIST",
    "text": "Load MNIST\n\nds = tfds.load('mnist')\n\n\ndef dataset_to_arrays(dataset):\n    data = []\n    labels = []\n    stopper = 0\n    end = 100\n    for sample in dataset:\n        data.append(sample[\"image\"].numpy())\n        labels.append(sample[\"label\"].numpy())\n        stopper += 1\n        if stopper == end:\n            break\n    return np.array(data), np.array(labels)[..., None]\n\ntrain_data, train_labels = dataset_to_arrays(ds[\"train\"])\ntest_data, test_labels = dataset_to_arrays(ds[\"test\"])\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n2023-06-02 09:58:48.609001: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n2023-06-02 09:58:48.681190: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n((100, 28, 28, 1), (100, 1), (100, 28, 28, 1), (100, 1))\n\n\n\ncoords = np.linspace(-1, 1, 28)\nx, y = np.meshgrid(coords, coords)\ntrain_X = jnp.stack([x, y], axis=-1).reshape(-1, 2)\n\ntrain_y = jax.vmap(lambda x: x.reshape(-1, 1))(train_data) / 255.0\ntrain_X.shape, train_y.shape, type(train_X), type(train_y)\n\n((784, 2),\n (100, 784, 1),\n jaxlib.xla_extension.ArrayImpl,\n jaxlib.xla_extension.ArrayImpl)\n\n\n\niterations = 10000\n\ndef loss_fn(params, context_X, context_y, target_X, target_y):\n  def loss_fn_per_sample(context_X, context_y, target_X, target_y):\n    loc, scale = model.apply(params, context_X, context_y, target_X)\n    # predictive_distribution = dx.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n    # return -predictive_distribution.log_prob(target_y)\n    return jnp.square(loc.ravel() - target_y.ravel()).mean()\n  \n  return jax.vmap(loss_fn_per_sample, in_axes=(None, 0, None, 0))(context_X, context_y, target_X, target_y).mean()\n\nvalue_and_grad_fn = jax.jit(jax.value_and_grad(loss_fn))\nmodel = CNP([256]*2, 128, [256]*4, 1)\nparams = model.init(jax.random.PRNGKey(0), train_X, train_y[0], train_X)\noptimizer = optax.adam(1e-5)\nstate = optimizer.init(params)\n\n# losses = []\n# for iter in tqdm(range(iterations)):\n#   tmp_index = jax.random.permutation(jax.random.PRNGKey(iter), index)\n#   context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n#   context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n#   target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n#   target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  \n#   # print(context_X.shape, context_y.shape, target_X.shape, target_y.shape)\n#   # print(loss_fn(params, context_X, context_y, target_X, target_y).shape)\n  \n#   loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n#   updates, state = optimizer.update(grads, state)\n#   params = optax.apply_updates(params, updates)\n#   losses.append(loss.item())\n\ndef one_step(params_and_state, key):\n  params, state = params_and_state\n  tmp_index = jax.random.permutation(key, train_X.shape[0])\n  context_X = train_X[tmp_index][:int(train_X.shape[0]*0.05)]\n  context_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.05), :]\n  target_X = train_X[tmp_index][int(train_X.shape[0]*0.05):]\n  target_y = train_y[:, tmp_index, :][:, int(train_X.shape[0]*0.05):, :]\n  loss, grads = value_and_grad_fn(params, context_X, context_y, target_X, target_y)\n  updates, state = optimizer.update(grads, state)\n  params = optax.apply_updates(params, updates)\n  return (params, state), loss\n\n(params, state), loss_history = jax.lax.scan(one_step, (params, state), jax.random.split(jax.random.PRNGKey(0), iterations))\n\n\nplt.plot(loss_history[10:]);\n\n\n\n\n\n\n\n\n\ntest_key = jax.random.PRNGKey(0)\ntmp_index = jax.random.permutation(test_key, train_X.shape[0])\ncontext_X = train_X[tmp_index][:int(train_X.shape[0]*0.5)]\ncontext_y = train_y[:, tmp_index, :][:, :int(train_X.shape[0]*0.5), :]\ntarget_X = train_X#[tmp_index][int(train_X.shape[0]*0.5):]\ntarget_y = train_y#[:, tmp_index, :][:, int(train_X.shape[0]*0.5):, :]\n\nid = 91\nplt.imshow(train_y[id].reshape(28, 28), cmap=\"gray\", interpolation=None);\n\nlocs, scales = jax.vmap(model.apply, in_axes=(None, None, 0, None))(params, context_X, context_y, target_X)\n# full_preds = jnp.concatenate([context_y, locs], axis=1)\n# full_preds = full_preds.at[:, tmp_index, :].set(full_preds).__array__()\n\nplt.figure()\nplt.imshow(locs[id].reshape(28, 28), cmap=\"gray\", interpolation=None);"
  },
  {
    "objectID": "posts/climate-modeling-with-SpecialGP.html",
    "href": "posts/climate-modeling-with-SpecialGP.html",
    "title": "Climate Modeling with GPs",
    "section": "",
    "text": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pyproj\nimport numpy as np\nimport xarray as xr\n\nfrom skgpytorch.models import GPRegression\n\nimport matplotlib.pyplot as plt\n\n\n# def haversine(lon1, lat1, lon2, lat2):\n#     \"\"\"\n#     Calculate the great circle distance in kilometers between two points \n#     on the earth (specified in decimal degrees)\n#     \"\"\"\n#     # convert decimal degrees to radians \n#     lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n#     # haversine formula \n#     dlon = lon2 - lon1 \n#     dlat = lat2 - lat1 \n#     a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n#     c = 2 * np.arcsin(np.sqrt(a)) \n#     r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n#     return c * r\n\n# def new_coords(lat1, long1):\n#     new_lat1 = haversine(0, 0, 0, lat1)\n#     new_long1 = haversine(0, 0, long1, 0)\n#     return new_lat1, new_long1\n\ndef lat_long_to_cartesian(latitude, longitude):\n    # Convert latitude and longitude to radians\n    phi = np.radians(latitude)\n    lam = np.radians(longitude)\n\n    # Constants for WGS 84 ellipsoid\n    a = 6378137.0  # equatorial radius in meters\n    e = 0.0818191908426  # eccentricity\n\n    # Calculate Earth's radius at the given latitude\n    R = a / np.sqrt(1 - (e ** 2) * (np.sin(phi) ** 2))\n\n    # Convert to Cartesian coordinates\n    X = R * np.sin(lam)\n    Y = R * np.tan(phi)\n\n    return X, Y\n\ndef wgs84_coords(lat, lon):    \n    # Define coordinate systems\n    wgs84 = pyproj.CRS.from_epsg(4326)  # WGS 84 lat-long system\n    utm_zone_32n = pyproj.CRS.from_string(\"+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n\n    # Create a transformer object\n    transformer = pyproj.Transformer.from_crs(wgs84, utm_zone_32n)\n\n    # Convert lat-long coordinates to UTM coordinates\n    utm_easting, utm_northing = transformer.transform(lon, lat)\n\n    return utm_northing, utm_easting\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------\n# Position embedding utils\n# --------------------------------------------------------\n\n\n# --------------------------------------------------------\n# 2D sine-cosine position embedding\n# References:\n# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n# MoCo v3: https://github.com/facebookresearch/moco-v3\n# --------------------------------------------------------\ndef get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size_h, dtype=np.float32)\n    grid_w = np.arange(grid_size_w, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n\n\ndef get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n    return emb\n\n\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n\n\n# --------------------------------------------------------\n# Interpolate position embeddings for high-resolution\n# References:\n# DeiT: https://github.com/facebookresearch/deit\n# --------------------------------------------------------\ndef interpolate_pos_embed(model, checkpoint_model, new_size=(64, 128)):\n    if \"net.pos_embed\" in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model[\"net.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        orig_num_patches = pos_embed_checkpoint.shape[-2]\n        patch_size = model.patch_size\n        w_h_ratio = 2\n        orig_h = int((orig_num_patches // w_h_ratio) ** 0.5)\n        orig_w = w_h_ratio * orig_h\n        orig_size = (orig_h, orig_w)\n        new_size = (new_size[0] // patch_size, new_size[1] // patch_size)\n        # print (orig_size)\n        # print (new_size)\n        if orig_size[0] != new_size[0]:\n            print(\"Interpolate PEs from %dx%d to %dx%d\" % (orig_size[0], orig_size[1], new_size[0], new_size[1]))\n            pos_tokens = pos_embed_checkpoint.reshape(-1, orig_size[0], orig_size[1], embedding_size).permute(\n                0, 3, 1, 2\n            )\n            new_pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size[0], new_size[1]), mode=\"bicubic\", align_corners=False\n            )\n            new_pos_tokens = new_pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            checkpoint_model[\"net.pos_embed\"] = new_pos_tokens\n\n\ndef interpolate_channel_embed(checkpoint_model, new_len):\n    if \"net.channel_embed\" in checkpoint_model:\n        channel_embed_checkpoint = checkpoint_model[\"net.channel_embed\"]\n        old_len = channel_embed_checkpoint.shape[1]\n        if new_len &lt;= old_len:\n            checkpoint_model[\"net.channel_embed\"] = channel_embed_checkpoint[:, :new_len]\n\n\ndef SIREN(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), kernel_initializer=initializers.RandomUniform(-1 / input_dim, 1 / input_dim), activation=tf.sin))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[i-1]) / activation_scale, np.sqrt(6 / features[i-1]) / activation_scale), activation=tf.sin))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, kernel_initializer=initializers.RandomUniform(-np.sqrt(6 / features[-1]) / activation_scale, np.sqrt(6 / features[-1]) / activation_scale), activation='linear'))\n    return model\n\ndef MLP(input_dim, output_dim, features, activation_scale, dropout):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(features[0], input_shape=(input_dim,), activation=activations.relu))\n    for i in range(1, len(features)):\n        model.add(layers.Dense(features[i], activation=activations.relu))\n        model.add(layers.Dropout(dropout))\n    model.add(layers.Dense(output_dim, activation='linear'))\n    return model\n    \ndef ResNet():\n    resnet = ResNet50(include_top=False, weights=None, input_shape=(64, 32, 1), pooling='avg')\n    model = tf.keras.Sequential()\n    model.add(resnet)\n    model.add(layers.Dense(2048, activation='relu'))\n    model.add(layers.Dense(32768, activation='linear'))\n    return model\n\n\ndata5 = xr.open_dataset(\"../data/2m_temperature_2018_5.625deg_Jan.nc\").to_dataframe().reset_index()\ndata1 = xr.open_dataset(\"../data/2m_temperature_2018_1.40625deg_Jan.nc\").to_dataframe().reset_index()\n\n\ndata5.head()\n\n\n\n\n\n\n\n\nlon\nlat\ntime\nt2m\n\n\n\n\n0\n0.0\n-87.1875\n2018-01-01 00:00:00\n250.728180\n\n\n1\n0.0\n-87.1875\n2018-01-01 01:00:00\n250.468552\n\n\n2\n0.0\n-87.1875\n2018-01-01 02:00:00\n250.250931\n\n\n3\n0.0\n-87.1875\n2018-01-01 03:00:00\n250.040314\n\n\n4\n0.0\n-87.1875\n2018-01-01 04:00:00\n249.993790\n\n\n\n\n\n\n\n\ntime_stamp = \"2018-01-01 01:00:00\"\ntrain_df = data5[data5.time == time_stamp]\ntest_df = data1[data1.time == time_stamp]\n\nX = np.stack([train_df.lat.values, train_df.lon.values], axis=1)\ny = train_df[[\"t2m\"]].values\nprint(f\"{X.shape=}, {y.shape=}\")\n\nX_test = np.stack([test_df.lat.values, test_df.lon.values], axis=1)\ny_test = test_df[[\"t2m\"]].values\nprint(f\"{X_test.shape=}, {y_test.shape=}\")\n\nrff = np.random.normal(size=(2, 16)) * 0.01\n# X = np.concatenate([np.sin(X @ rff), np.cos(X @ rff)], axis=1)\n# print(f\"{sin_cos.shape=}\")\n# X = X @ sin_cos\n# X_test = np.concatenate([np.sin(X_test @ rff), np.cos(X_test @ rff)], axis=1)\n\nprint(f\"{X.shape=}, {X_test.shape=}\")\n\nX.shape=(2048, 2), y.shape=(2048, 1)\nX_test.shape=(32768, 2), y_test.shape=(32768, 1)\nX.shape=(2048, 2), X_test.shape=(32768, 2)\n\n\n\nX_max = np.max(X, axis=0, keepdims=True)\nX_min = np.min(X, axis=0, keepdims=True)\n\nX_scaled = (X - X_min) / (X_max - X_min)\nX_test_scaled = (X_test - X_min) / (X_max - X_min)\n\ny_min = np.min(y, axis=0, keepdims=True)\ny_max = np.max(y, axis=0, keepdims=True)\n\ny_scaled = (y - y_min) / (y_max - y_min)\n\n# y_mean = np.mean(y, axis=0, keepdims=True)\n# y_std = np.std(y, axis=0, keepdims=True)\n\n# y_scaled = (y - y_mean) / y_std\n\n\nmodel = MLP(2, 1, [256]*4, 30.0, 0.0)\n# model = ResNet()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n\n\nhistory = model.fit(X_scaled, y_scaled, epochs=5000, batch_size=X_scaled.shape[0], verbose=0)\n\n\nplt.plot(history.history['loss']);\n\n\n\n\n\n\n\n\n\ny_pred = model.predict(X_test_scaled) * (y_max - y_min) + y_min\nplt.imshow(y_pred.reshape(256, 128), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n1024/1024 [==============================] - 1s 1ms/step\n\n\n\n\n\n\n\n\n\n\nplt.imshow(y.reshape(64, 32), origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\n\n\n\n\n\n\n\n\n\ndiff = y_pred.reshape(256, 128) - y_test.reshape(256, 128)\nplt.imshow(diff, origin='lower', extent=[-180, 180, -90, 90], cmap='coolwarm', interpolation=\"none\");\nplt.colorbar();\nplt.title(\"Diff\")\n\nText(0.5, 1.0, 'Diff')\n\n\n\n\n\n\n\n\n\n\n# rmse = np.sqrt(np.mean(np.abs(X_test[:, 0:1])*(y_pred.ravel() - y_test.ravel())**2))/np.mean(y_test.ravel() * np.abs(X_test[:, 0:1]))\nrmse = np.sqrt(np.mean((y_pred.ravel() - y_test.ravel())**2))\nprint(f\"{rmse=}\")\n\nrmse=2.7606046\n\n\n\nmean_bias = np.mean(y_pred.ravel() - y_test.ravel())\nprint(f\"{mean_bias=}\")\n\nmean_bias=0.10866926"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html",
    "href": "posts/2022-10-21-gaussian-processes.html",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\n\nfrom tinygp.kernels import ExpSquared\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#regression",
    "href": "posts/2022-10-21-gaussian-processes.html#regression",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Regression",
    "text": "Regression\nIn this post, we will consider the regression problem of finding a reasonable map \\(X \\to \\boldsymbol{y}\\) along with uncertainty. We can do this in a simplest setting with Bayesian linear regression assuming a MultiVariate Normal (MVN) prior \\(\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\theta, \\Sigma_\\theta)\\) (why MVN? because \\(\\theta \\in (-\\infty, \\infty)\\)) and Normal likelihood \\(y \\sim \\mathcal{N}(\\boldsymbol{x}^T\\theta, \\sigma_n^2)\\) with i.i.d. assumption.\nTo start with Gaussian process regression, let us first focus on \\(\\boldsymbol{y}\\) (and ignore \\(X\\)). We assume \\(\\boldsymbol{f}\\) as a random variable and \\(\\boldsymbol{y}\\) as a realization of \\(\\boldsymbol{f}\\) with some noise. It would be a natural probabilistic assumption to assume \\(\\boldsymbol{f}\\) to be MVN distributed since its range is \\((-\\infty, \\infty)\\).\n\\[\np(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\n\\tag{prior}\n\\]\nNow, we need to bring in \\(X\\) in a reasonable way to this formulation. A core assumption connecting \\(X\\) with \\(\\boldsymbol{y}\\) is the following: &gt; if two inputs \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{x}'\\) are close to each other (how to define the closeness? kernels!), corresponding \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{y}'\\) are likely to be similar.\nWe use something known as covariance function or kernel (later is more prevalent) to define this closeness. For example, RBF or squared exponential is a well-known kernel:\n\\[\nk_{RBF}(\\boldsymbol{x}, \\boldsymbol{x}') = \\sigma^2 \\exp \\left(-{\\frac {\\|\\boldsymbol{x} -\\boldsymbol{x}' \\|^{2}}{2\\ell ^{2}}}\\right)\n\\tag{kernel}\n\\]\n\nx = jnp.array(0.0).reshape(1, 1)\nx_prime = jnp.linspace(-5,5,100).reshape(-1, 1)\n\nplt.plot(x_prime, ExpSquared()(x_prime, x));\nplt.xlabel(\"$x'$\")\nplt.title(f\"$k(x,x')$ where $x={x[0][0]}$ and $x' \\in ${plt.xlim()}\");\n\n\n\n\n\n\n\n\nThe plot above shows that value of \\(k(\\boldsymbol{x}, \\boldsymbol{x}')\\) increases as \\(\\boldsymbol{x}'\\) approaches \\(\\boldsymbol{x}\\) and reduces as it moves far from \\(\\boldsymbol{x}\\). Now, we will connect \\(X\\) with \\(\\boldsymbol{f}\\) (and thus with \\(\\boldsymbol{y}\\)) through kernel \\(k\\) with two following assumptions:\n\nDiagonal entries of \\(K_{ff}\\) represent variance of \\(f_i\\), which can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_i)\\).\nNon-diagonal entries of \\(K_{ff}\\) represent covariance between \\(f_i\\) and \\(f_j\\) and can be represented by \\(k(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\).\n\nAt this point, we have made everything clear about prior \\(p(\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{m}_f, K_{ff})\\). Now, we will look at the likelihood. As mentioned earlier, \\(\\boldsymbol{y}\\) is noisy realization of \\(f\\) so the following likelihood would be a simple and natural choice.\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) \\sim \\mathcal{N}(\\boldsymbol{f}, \\sigma_n^2I)\n\\tag{likelihood}\n\\]\nTill now, we followed bottom-up approach and defined prior and likelihood for this problem. Now we will explore the top-down approach.\nOur ultimate goal is derive \\(p(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X)\\) at new inputs \\(X^*\\). This can be written as:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) = \\int p(\\boldsymbol{y}^*|\\boldsymbol{f}^*)p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)d\\boldsymbol{f}^*\n\\tag{pred post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) is the posterior distribution at inputs \\(X^*\\). Once we derive posterior \\(p(\\boldsymbol{f}|\\boldsymbol{y},X)\\), We can find \\(p(\\boldsymbol{f}^*|X^*,\\boldsymbol{y}, X)\\) like following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) = \\int p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)p(\\boldsymbol{f}|\\boldsymbol{y}, X)d\\boldsymbol{f}\n\\tag{post new}\n\\]\nHere, \\(p(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X)\\) is a conditional Gaussian distribution with the following closed form:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{f}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{f}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*})\n\\tag{cond}\n\\]\nPosterior \\(p(\\boldsymbol{f}|\\boldsymbol{y}, X)\\) can be derived following “Bayes’ rule for Gaussians” (section 2.2.6.2 in pml book2):\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff})\n\\tag{post}\n\\]\nWe can now substitute Eq. (post) and Eq. (cond) in Eq. (post new). The integral can be solved with using Eq. 2.90 in section 2.2.6.2 in pml book2 and also mentioned in Eq. (int gaussians) in Appendix.\n\\[\n\\begin{aligned}\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{\\mu}^*, \\Sigma^*)\\\\\n\\boldsymbol{\\mu}^* &= \\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\left[\\boldsymbol{m}_f + K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\right]-\\boldsymbol{m}_{f})\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(K_{ff}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f))\\\\\n&=\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f)\\\\\n\\\\\n\\Sigma^* &= K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}\\left[K_{ff} - K_{ff}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[I - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff}\\right]K_{ff}^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}\\left[K_{ff}^{-1} - \\left(K_{ff} + \\sigma_n^2I\\right)^{-1}\\right]K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}K_{ff^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\n&=K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*}\\\\\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) &\\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*})\n\\end{aligned}\n\\]\nNow, we are almost there. Plugging in the above formula in Eq. (pred post) and using known result in Eq. (int gaussians), we get the predictive posterior as following:\n\\[\np(\\boldsymbol{y}^*|X^*,\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}\\left(K_{ff}+\\sigma_n^2I\\right)^{-1}(\\boldsymbol{y} - \\boldsymbol{m}_f), K_{f^*f^*} - K_{f^*f}\\left(K_{ff} + \\sigma_n^2I\\right)^{-1}K_{ff^*} + \\sigma_n^2I)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe did not exploit the special structure of likelihood variance \\(\\sigma_n^2I\\) anywhere, so, these derivations hold true for full rank likelihood covariance matrices also.\n\n\n\nOptimization\nWe perform type-II likelihood estimation (in other words, minimize log marginal likelihood or evidence term). Our goal is to find optimal model \\(\\mathcal{M}\\) represented by prior (or kernel) hyperparameters and likelihood hyperparameters. We can get the log marginal likelihood using Eq. (int gaussians):\n\\[\n\\begin{aligned}\np(\\boldsymbol{y}|X, \\mathcal{M}) &= \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\\\\n&\\sim \\int \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{f}, \\sigma_n^2I) \\mathcal{N}(\\boldsymbol{f}|\\boldsymbol{m}_f, K_{ff})\\\\\n&\\sim \\mathcal{N}(\\boldsymbol{y}|\\boldsymbol{m}_f, K_{ff}+\\sigma_n^2I)\n\\end{aligned}\n\\]\nFor case of RBF kernel, \\(\\mathcal{M}\\) parameters will be \\(\\{\\sigma, \\ell, \\sigma_n\\}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "href": "posts/2022-10-21-gaussian-processes.html#classification-with-laplace-approximation",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Classification (with Laplace approximation)",
    "text": "Classification (with Laplace approximation)\nWe will derive a GP predictive posterior for binary case only because for multi-class, it gets a bit complex. Our assumption for prior over the \\(\\boldsymbol{f}\\) can still be the same but likelihood needs to be changed because \\(\\boldsymbol{y}\\) is no more a real number but rather a binary value e.g. 0 or 1. From Bayesian point-of-view, Bernoulli likelihood would be the most appropriate as a likelihood here:\n\\[\np(\\boldsymbol{y}|\\boldsymbol{f}) = \\prod_{i=1}^{N} \\sigma(f_i)^{y_i=1}(1-\\sigma(f_i))^{y_i=0}\n\\tag{class likelihood}\n\\]\nSince, MVN prior and Bernoulli likelihood are not conjugate, we need to use an approximate method of inference here. We use Laplace approximation to get the MAP estimate \\(\\boldsymbol{\\hat{f}}\\) and by computing the Hessian \\(H\\) of negative log joint (log prior + log likelihood) with respect to \\(\\boldsymbol{\\hat{f}}\\), we can get the posterior distribution as the following:\n\\[\np(\\boldsymbol{f}|\\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{\\hat{f}}, H^{-1})\n\\tag{class post}\n\\]\nEq. (cond) will be the same in this case, and thus, we can solve Eq. (post new) as we did for regression case, like the following:\n\\[\np(\\boldsymbol{f}^*|X^*, \\boldsymbol{y}, X) \\sim \\mathcal{N}(\\boldsymbol{m}_{f^*}+K_{f^*f}K_{ff}^{-1}(\\boldsymbol{\\hat{f}}-\\boldsymbol{m}_{f}), K_{f^*f^*} - K_{f^*f}K_{ff}^{-1}K_{ff^*} + K_{f^*f}K_{ff}^{-1}H^{-1}K_{ff}^{-1}K_{ff^*})\n\\]\n\nOptimization\nTo perform Type-II likelihood estimation for binary classification, we first need to derive the log marginal likelihood which can be approximated with Laplace approximation. First, we define the following quantity:\n\\[\n\\boldsymbol{\\psi}(\\boldsymbol{f}) \\triangleq \\log p(\\boldsymbol{y}|\\boldsymbol{f}) + \\log p(\\boldsymbol{f})\n\\]\nNow, computing the log marginal likelihood as suggested in section 3.4.4 of GPML book:\n\\[\n\\begin{aligned}\n\\log p(\\boldsymbol{y}|X, \\mathcal{M}) &\\sim \\log \\left[ \\int p(\\boldsymbol{y}|\\boldsymbol{f}) p(\\boldsymbol{f})d\\boldsymbol{f}\\right]\\\\\n&= \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{f})\\right)d\\boldsymbol{f} \\right]\\\\\n&\\thickapprox \\log \\left[ \\int \\exp\\left(\\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) -\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f} \\right]\\\\\n&= \\log \\left[ \\exp \\boldsymbol{\\psi}(\\boldsymbol{\\hat{f}}) \\int exp\\left(-\\frac{1}{2}(\\mathbf{f}-\\hat{\\mathbf{f}})^{\\top} H(\\mathbf{f}-\\hat{\\mathbf{f}})\\right)d\\boldsymbol{f}\\right]\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) + \\log p(\\boldsymbol{\\hat{f}}) - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}|\\\\\n&= \\log p(\\boldsymbol{y}|\\boldsymbol{\\hat{f}}) -\\frac{1}{2}\\boldsymbol{\\hat{f}}^TK_{ff}^{-1}\\boldsymbol{\\hat{f}} - \\frac{1}{2}\\log|K_{ff}| - \\frac{N}{2}\\log(2\\pi) - \\frac{1}{2}\\log|H^{-1}| - \\frac{N}{2}\\log(2\\pi)\n\\end{aligned}\n\\]\nOur final optimization algorithm would be as following: 1. For N iterations do 2. to 4. 2. Optimize for \\(\\boldsymbol{\\hat{f}}\\) with M iterations using standard MAP estimation (maybe use non-centered parametrization). 3. Compute gradient of parameters of \\(\\mathcal{M}\\) w.r.t. log marginal likelihood 4. Update parameters of \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "posts/2022-10-21-gaussian-processes.html#appendix",
    "href": "posts/2022-10-21-gaussian-processes.html#appendix",
    "title": "Gaussian Processes - A no-skip-math version",
    "section": "Appendix",
    "text": "Appendix\n\\[\n\\int \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{x}+\\boldsymbol{b}, \\Sigma) \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}, K) = \\mathcal{N}(\\boldsymbol{y}|W\\boldsymbol{\\mu}+b, WKW^T+\\Sigma)\n\\tag{int gaussians}\n\\]"
  }
]