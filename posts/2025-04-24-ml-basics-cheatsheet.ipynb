{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "author: Zeel B Patel\n",
    "badges: true\n",
    "categories:\n",
    "  - ML\n",
    "date: \"2025-04-24\"\n",
    "description: Preparing for ML Breadth and Depth for Interviews\n",
    "title: Machine Learning Cheat Sheet\n",
    "toc: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "### Expectation\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X] &= \\sum_{x} x P(X=x) \\\\\n",
    "\\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} x p(x) dx \\\\\n",
    "\\bar{x} &= \\frac{1}{n} \\sum_{i=1}^{n} x_i \\\\\n",
    "\\mathbb{E}[aX + b] &= a\\mathbb{E}[X] + b\n",
    "\\end{align*}\n",
    "\n",
    "### Variance\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\\n",
    "\\text{Var}(X) &= \\sum_{x} (x - \\mathbb{E}[X])^2 P(X=x) \\\\\n",
    "\\text{Var}(X) &= \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])^2 p(x) dx\\\\\n",
    "\\text{Var}(X) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])^2 \\\\\n",
    "\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n",
    "\\end{align*}\n",
    "\n",
    "### Standard Deviation\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{SD}(X) &= \\sqrt{\\text{Var}(X)}\n",
    "\\end{align*}\n",
    "\n",
    "### Covariance\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cov}(X, Y) &= \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] \\\\\n",
    "\\text{Cov}(X, Y) &= \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y] \\\\\n",
    "\\text{Cov}(X, Y) &= \\sum_{x,y} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) P(X=x, Y=y) \\\\\n",
    "\\text{Cov}(X, Y) &= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])(y - \\mathbb{E}[Y]) p(x,y) dx dy \\\\\n",
    "\\text{Cov}(X, Y) &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mathbb{E}[X])(y_i - \\mathbb{E}[Y]) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Correlation\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Corr}(X, Y) &= \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}} \\\\\n",
    "&= \\frac{\\text{Cov}(X, Y)}{\\text{SD}(X) \\text{SD}(Y)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "### Linearity of Expectation\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[X + Y] &= \\mathbb{E}[X] + \\mathbb{E}[Y] \\\\\n",
    "\\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] &= \\sum_{i=1}^{n} \\mathbb{E}[X_i]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Entity   | Math             | Shape  | Type       |\n",
    "| :------- | :--------------- | :----- | :--------- |\n",
    "| Features | $X$              | (n, d) | Continuous |\n",
    "| Weights  | $W$              | (d, 1) | Continuous |\n",
    "| Bias     | $b$              | (1, 1) | Continuous |\n",
    "| Output   | $\\boldsymbol{y}$ | (n, 1) | Binary     |\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{y}} &= \\sigma(XW + b) \\\\\n",
    "&= \\sigma(\\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times 1} + \\underbrace{b}_{1}) \\\\\n",
    "&= \\sigma(\\underbrace{\\boldsymbol{z}}_{n \\times 1}) \\\\\n",
    "\\text{where } \\sigma(a) &= \\frac{1}{1 + e^{-a}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n}\n",
    "\\begin{cases}\n",
    "\\log(\\hat{\\boldsymbol{y}}_i) & \\text{if } \\boldsymbol{y}_i = 1 \\\\\n",
    "\\log(1 - \\hat{\\boldsymbol{y}}_i) & \\text{if } \\boldsymbol{y}_i = 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to write the loss function is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\boldsymbol{y}_i \\log(\\hat{\\boldsymbol{y}}_i) + (1 - \\boldsymbol{y}_i) \\log(1 - \\hat{\\boldsymbol{y}}_i)\\right)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Logistic Regression\n",
    "| Entity   | Math             | Shape  | Type       |\n",
    "| :------- | :--------------- | :----- | :--------- |\n",
    "| Features | $X$              | (n, d) | Continuous |\n",
    "| Weights  | $W$              | (d, k) | Continuous |\n",
    "| Bias     | $\\boldsymbol{b}$ | (1, k) | Continuous |\n",
    "| Output   | $Y$ | (n, k) | One-Hot   |\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{Y} &= \\text{softmax}(XW + \\boldsymbol{b}) \\\\\n",
    "&= \\text{softmax}(\\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times k} + \\underbrace{\\boldsymbol{b}}_{1 \\times k}) \\\\\n",
    "&= \\text{softmax}(\\underbrace{Z}_{n \\times k}) \\\\\n",
    "\\text{where } \\hat{Y}_{ij} = \\text{softmax}(Z_{ij}) &= \\frac{e^{Z_{ij}}}{\\sum_{j=1}^{k} e^{Z_{ij}}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} &= \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} \\\\\n",
    "&= \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "#### True Positive Rate (TPR)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{True Positive Rate} &= \\frac{\\text{True Positives}}{\\text{Actual Positives}} \\\\\n",
    "&= \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "#### False Positive Rate (FPR)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{False Positive Rate} &= \\frac{\\text{False Positives}}{\\text{Actual Negatives}} \\\\\n",
    "&= \\frac{FP}{FP + TN}\n",
    "\\end{align*}\n",
    "\n",
    "#### ROC Curve (Receiver Operating Characteristic Curve)\n",
    "\n",
    "- X-axis: FPR, Y-axis: TPR\n",
    "\n",
    "#### Area Under the ROC Curve (AUC)\n",
    "\n",
    "- AUC = 1: Perfect model\n",
    "- AUC = 0.5: Random model\n",
    "- AUC < 0.5: Model is worse than random\n",
    "\n",
    "#### Precision\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} &= \\frac{\\text{True Positives}}{\\text{Predicted Positives}} \\\\\n",
    "&= \\frac{TP}{TP + FP}\n",
    "\\end{align*}\n",
    "\n",
    "#### Recall\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} &= \\frac{\\text{True Positives}}{\\text{Actual Positives}} \\\\\n",
    "&= \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "#### F1 Score\n",
    "\n",
    "Harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1 Score} &= 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\\\\n",
    "&= \\frac{2TP}{2TP + FP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "#### Matthews Correlation Coefficient\n",
    "Similar to Pearson correlation coefficient, but for binary classification.\n",
    "\n",
    "> The Matthews correlation coefficient (MCC) is pearson correlation coefficient between the observed and predicted binary classifications.\n",
    "\n",
    "Derivation: (Homework)\n",
    "Range: -1 to 1\n",
    "\n",
    "- 1: Total disagreement\n",
    "- 0: Random agreement\n",
    "- 1: Perfect agreement\n",
    "\n",
    "Formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MCC} &= \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "\\end{align*}\n",
    "\n",
    "#### Cohen's Kappa\n",
    "Definition: Cohen's Kappa is a statistic that measures agreement between two rates while correcting for chance.\n",
    "Range: -1 to 1\n",
    "\n",
    "- 1: Total disagreement\n",
    "- 0: Random agreement\n",
    "- 1: Perfect agreement\n",
    "\n",
    "Formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cohen's Kappa} &= \\frac{P_o - P_e}{1 - P_e} \\\\\n",
    "P_o &= \\text{Accuracy} \\\\\n",
    "P_e &= \\sum_{i=1}^{k} \\frac{\\text{predicted}_i}{\\text{total samples}} \\cdot \\frac{\\text{actual}_i}{\\text{total samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Linear Regression\n",
    "| Entity   | Math             | Shape  | Type       |\n",
    "| :------- | :--------------- | :----- | :--------- |\n",
    "| Features | $X$              | (n, d) | Continuous |\n",
    "| Weights  | $W$              | (d, 1) | Continuous |\n",
    "| Bias     | $b$              | (1, 1) | Continuous |\n",
    "| Output   | $\\boldsymbol{y}$ | (n, 1) | Continuous |\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{y}} &= XW + b \\\\\n",
    "&= \\underbrace{X}_{n \\times d} \\underbrace{W}_{d \\times 1} + \\underbrace{b}_{1} \\\\\n",
    "&= \\underbrace{\\boldsymbol{z}}_{n \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Mean Squared Loss} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "Normal Equation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{X} &= \\begin{bmatrix}\n",
    "1 & X\n",
    "\\end{bmatrix} \\\\\n",
    "\\tilde{W} &= \\begin{bmatrix}\n",
    "b \\\\\n",
    "W\n",
    "\\end{bmatrix} \\\\\n",
    "\\hat{\\boldsymbol{y}} &= \\tilde{X} \\tilde{W} \\\\\n",
    "\\tilde{W} &= (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T \\boldsymbol{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MAE} &= \\frac{1}{n} \\sum_{i=1}^{n} |\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i|\n",
    "\\end{align*}\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MSE} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{RMSE} &= \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2}\n",
    "\\end{align*}\n",
    "\n",
    "#### R-squared (Coefficient of Determination)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{R-squared} &= 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} \\\\\n",
    "\\text{where } \\text{SS}_{\\text{res}} &= \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i)^2 \\\\\n",
    "\\text{and } \\text{SS}_{\\text{tot}} &= \\sum_{i=1}^{n} (\\boldsymbol{y}_i - \\bar{\\boldsymbol{y}})^2\n",
    "\\end{align*}\n",
    "\n",
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MAPE} &= \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{\\boldsymbol{y}_i - \\hat{\\boldsymbol{y}}_i}{\\boldsymbol{y}_i} \\right| \\times 100\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### K-Means Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly initialize $k$ centroids\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_j &= \\text{randomly initialize } j \\text{th centroid}\n",
    "\\end{align*}\n",
    "\n",
    "* Assign each data point to the nearest centroid\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}_i &= \\arg\\min_{j} ||x_i - \\mu_j||^2\n",
    "\\end{align*}\n",
    "\n",
    "* Update the centroids\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_j &= \\frac{1}{n_j} \\sum_{i: \\hat{y}_i = j} x_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-Means is guaranteed to converge, but not necessarily to the global optimum.\n",
    "* K-Means is sensitive to the initial placement of centroids.\n",
    "* K-Means is sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "| High Bias | High Variance |\n",
    "| :-------- | :------------ |\n",
    "| Simple model | Complex model |\n",
    "| Underfitting | Overfitting |\n",
    "| High training error | Low training error |\n",
    "| High test error | High test error |\n",
    "| Not sensitive to noise | Sensitive to noise |\n",
    "\n",
    "Decompose the error into three components:\n",
    "\n",
    "* This decomposition is applied to a single point $(x, y)$.\n",
    "* Train K models on different training datasets of the same distribution.\n",
    "* All expectations are taken over predictions $\\hat{y}_i$ of the K models on the same point $(x, y)$ where $i = 1, \\ldots, K$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Error} &= \\text{Variance} + \\text{Bias}^2 + \\sigma^2 \\\\\n",
    "E[(\\hat{y} - y)^2] &= E[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2] + (\\mathbb{E}[\\hat{y}] - y)^2 + \\sigma^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t) \\\\\n",
    "\\text{where } \\alpha &= \\text{learning rate} \\\\\n",
    "\\text{and } J(\\theta) &= \\text{cost function}\n",
    "\\end{align*}\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t, x_i, y_i) \\\\\n",
    "\\text{where } (x_i, y_i) &= \\text{randomly selected training example}\n",
    "\\end{align*}\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\nabla J(\\theta_t, B) \\\\\n",
    "\\text{where } B &= \\text{mini-batch of training examples}\n",
    "\\end{align*}\n",
    "\n",
    "### SGD with Momentum\n",
    "\n",
    "\\begin{align*}\n",
    "v_t &= \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta_t) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha v_t\n",
    "\\end{align*}\n",
    "where $\\beta$ is the momentum term (usually set to 0.9).\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "\\begin{align*}\n",
    "s_t &= \\beta s_{t-1} + (1 - \\beta) (\\nabla J(\\theta_t))^2 \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{s_t} + \\epsilon} \\nabla J(\\theta_t)\n",
    "\\end{align*}\n",
    "where $\\epsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "### Adam\n",
    "\n",
    "\\begin{align*}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(\\theta_t) \\\\\n",
    "s_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(\\theta_t))^2 \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat{s}_t &= \\frac{s_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{s}_t} + \\epsilon} \\hat{m}_t\n",
    "\\end{align*}\n",
    "where $\\beta_1$ and $\\beta_2$ are the exponential decay rates for the first and second moment estimates, respectively (usually set to 0.9 and 0.999)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeel_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
