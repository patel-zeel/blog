{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /ML/2022/06/10/JaxOptimizers\n",
    "author: Zeel B Patel\n",
    "badges: true\n",
    "categories:\n",
    "- ML\n",
    "date: '2022-06-10'\n",
    "description: Pros and cons of several jax optimizers.\n",
    "output-file: 2022-06-10-jaxoptimizers.html\n",
    "title: JAX Optimizers\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sOLzqiZZyVqF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U jax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "try:\n",
    "  import jaxopt\n",
    "except ModuleNotFoundError:\n",
    "  %pip install -qq jaxopt\n",
    "  import jaxopt\n",
    "try:\n",
    "  import optax\n",
    "except ModuleNotFoundError:\n",
    "  %pip install -qq optax\n",
    "  import optax\n",
    "\n",
    "import tensorflow_probability.substrates.jax as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu6QaUkZ1BaL"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OL0jrlLMyhHx"
   },
   "outputs": [],
   "source": [
    "def loss_fun(x, a):\n",
    "  return (((x['param1'] - a) + (x['param2'] - (a+1)))**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qovNfTZU1DUx"
   },
   "source": [
    "## Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "R40W08-jyy24"
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "init_params = lambda: {'param1': jnp.zeros(N), 'param2': jnp.ones(N)}\n",
    "a = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp1TNzE61Fa5"
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqfcbRFdzG2Q"
   },
   "source": [
    "### JaxOpt ScipyMinimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0E10mnHzFlE",
    "outputId": "00736b13-a770-452d-db7d-c708c8045da5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptStep(params={'param1': DeviceArray([1.9999999, 1.9999999, 1.9999999], dtype=float32), 'param2': DeviceArray([3., 3., 3.], dtype=float32)}, state=ScipyMinimizeInfo(fun_val=DeviceArray(4.2632564e-14, dtype=float32), success=True, status=0, iter_num=2))\n",
      "CPU times: user 78.3 ms, sys: 18.5 ms, total: 96.8 ms\n",
      "Wall time: 95.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver = jaxopt.ScipyMinimize('L-BFGS-B', fun=loss_fun)\n",
    "ans = solver.run(init_params(), a)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhSn6XW50LWC"
   },
   "source": [
    "#### Pros\n",
    "* Two lines of code will do it all.\n",
    "\n",
    "#### Cons\n",
    "* It only returns the final parameters and final loss. No option to retrive in-between loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4b0gjLF0TMX"
   },
   "source": [
    "### Optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZxHoAoWzpAl",
    "outputId": "0427970b-4d02-4c3a-9976-0dbcfd069508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'param1': DeviceArray([2.0084236, 2.0084236, 2.0084236], dtype=float32), 'param2': DeviceArray([3.0084238, 3.0084238, 3.0084238], dtype=float32)}\n",
      "CPU times: user 3.09 s, sys: 63.4 ms, total: 3.16 s\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optax.adam(learning_rate=0.1)\n",
    "value_and_grad_fun = jax.jit(jax.value_and_grad(loss_fun, argnums=0))\n",
    "params = init_params()\n",
    "state = optimizer.init(params)\n",
    "\n",
    "for _ in range(100):\n",
    "  loss_value, gradients = value_and_grad_fun(params, a)\n",
    "  updates, state = optimizer.update(gradients, state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwjqkeQd2SLO"
   },
   "source": [
    "#### Pros:\n",
    "* Full control in user's hand. We can save intermediate loss values.\n",
    "\n",
    "#### Cons:\n",
    "* Its code is verbose, similar to PyTorch optimizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9l-2V3F3gPw"
   },
   "source": [
    "### Jaxopt OptaxSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZB_KWP5b1nUb",
    "outputId": "e393257f-00c5-421c-bd65-fba0799c78c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptStep(params={'param1': DeviceArray([2.008423, 2.008423, 2.008423], dtype=float32), 'param2': DeviceArray([3.008423, 3.008423, 3.008423], dtype=float32)}, state=OptaxState(iter_num=DeviceArray(100, dtype=int32, weak_type=True), value=DeviceArray(0.00113989, dtype=float32), error=DeviceArray(0.09549397, dtype=float32), internal_state=(ScaleByAdamState(count=DeviceArray(100, dtype=int32), mu={'param1': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32), 'param2': DeviceArray([0.02871927, 0.02871927, 0.02871927], dtype=float32)}, nu={'param1': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32), 'param2': DeviceArray([0.44847375, 0.44847375, 0.44847375], dtype=float32)}), EmptyState()), aux=None))\n",
      "CPU times: user 719 ms, sys: 13.4 ms, total: 732 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optax.adam(learning_rate=0.1)\n",
    "solver = jaxopt.OptaxSolver(loss_fun, optimizer, maxiter=100)\n",
    "ans = solver.run(init_params(), a)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6iCcmvY6V8H"
   },
   "source": [
    "#### Pros:\n",
    "* Less lines of code.\n",
    "* Applies `lax.scan` internally to make it fast [[reference](https://github.com/google/jaxopt/blob/60f3425f70bc6a9555cc13dcc993933dc2772c7d/jaxopt/_src/loop.py#L68)].\n",
    "\n",
    "#### Cons:\n",
    "* Not able to get in-between state/loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d1i5CtD7o_k"
   },
   "source": [
    "### tfp math minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2-eDVMg6la1",
    "outputId": "f10e36bd-93f7-4e10-829c-0afb94e2b815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'param1': DeviceArray([1.0000008, 1.0000008, 1.0000008], dtype=float32), 'param2': DeviceArray([1.9999989, 1.9999989, 1.9999989], dtype=float32)}, DeviceArray(0.9999999, dtype=float32))\n",
      "[48.       38.88006  30.751791 23.626852 17.507807]\n",
      "CPU times: user 880 ms, sys: 15.2 ms, total: 895 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optax.adam(learning_rate=0.1)\n",
    "params, losses = tfp.math.minimize_stateless(loss_fun, (init_params(), a), num_steps=1000, optimizer=optimizer)\n",
    "print(params)\n",
    "print(losses[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIHAVvUuB9HP"
   },
   "source": [
    "#### Pros:\n",
    "* One line of code to optimize the function and return in-between losses.\n",
    "\n",
    "#### Cons:\n",
    "* By default, it optimizes all arguments passed to the loss function. In above example, we can not control if `a` should be optimized or not. I have raised an issue [here](https://github.com/tensorflow/probability/issues/1575) for this problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4v19WFfzv9Y1OH8FjCz8d",
   "collapsed_sections": [],
   "name": "2022-06-10-JaxOptimizers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
